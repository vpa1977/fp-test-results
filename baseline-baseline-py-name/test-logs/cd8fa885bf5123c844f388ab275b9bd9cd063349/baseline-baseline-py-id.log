#####
baseline-baseline-py-id - Run 1
2024-02-23 09:58:07
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.62it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.61it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  60.19 M, 100.000% Params, 11.58 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.016% Params, 118.01 MMac, 1.019% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.000% Params, 1.61 MMac, 0.014% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.007% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.007% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.359% Params, 680.39 MMac, 5.875% MACs, 
    (0): Bottleneck(
      75.01 k, 0.125% Params, 236.43 MMac, 2.042% MACs, 
      (conv1): Conv2d(4.1 k, 0.007% Params, 12.85 MMac, 0.111% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.028% Params, 52.99 MMac, 0.458% MACs, 
        (0): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    2.34 M, 3.887% Params, 1.92 GMac, 16.555% MACs, 
    (0): Bottleneck(
      379.39 k, 0.630% Params, 376.02 MMac, 3.247% MACs, 
      (conv1): Conv2d(32.77 k, 0.054% Params, 102.76 MMac, 0.887% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 802.82 KMac, 0.007% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.008% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.219% Params, 103.56 MMac, 0.894% MACs, 
        (0): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (4): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (5): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (6): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (7): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    40.61 M, 67.473% Params, 8.05 GMac, 69.501% MACs, 
    (0): Bottleneck(
      1.51 M, 2.513% Params, 374.26 MMac, 3.232% MACs, 
      (conv1): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 401.41 KMac, 0.003% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.004% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 0.874% Params, 103.16 MMac, 0.891% MACs, 
        (0): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (6): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (7): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (8): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (9): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (10): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (11): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (12): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (13): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (14): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (15): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (16): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (17): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (18): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (19): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (20): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (21): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (22): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (23): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (24): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (25): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (26): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (27): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (28): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (29): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (30): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (31): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (32): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (33): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (34): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (35): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 24.861% Params, 811.02 MMac, 7.003% MACs, 
    (0): Bottleneck(
      6.04 M, 10.034% Params, 373.38 MMac, 3.224% MACs, 
      (conv1): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 200.7 KMac, 0.002% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.002% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 3.491% Params, 102.96 MMac, 0.889% MACs, 
        (0): Conv2d(2.1 M, 3.484% Params, 102.76 MMac, 0.887% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.001% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 3.404% Params, 2.05 MMac, 0.018% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  10%|█         | 10/100 [00:00<00:00, 96.90it/s]Warming up with batch_size=1:  20%|██        | 20/100 [00:00<00:00, 97.24it/s]Warming up with batch_size=1:  30%|███       | 30/100 [00:00<00:00, 97.39it/s]Warming up with batch_size=1:  40%|████      | 40/100 [00:00<00:00, 97.53it/s]Warming up with batch_size=1:  50%|█████     | 50/100 [00:00<00:00, 97.60it/s]Warming up with batch_size=1:  60%|██████    | 60/100 [00:00<00:00, 97.65it/s]Warming up with batch_size=1:  70%|███████   | 70/100 [00:00<00:00, 97.67it/s]Warming up with batch_size=1:  80%|████████  | 80/100 [00:00<00:00, 97.65it/s]Warming up with batch_size=1:  90%|█████████ | 90/100 [00:00<00:00, 97.67it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:01<00:00, 97.65it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:01<00:00, 97.57it/s]
STAGE:2024-02-23 09:57:35 177168:177168 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:57:35 177168:177168 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:57:35 177168:177168 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   1%|          | 8/1000 [00:00<00:13, 72.65it/s]Measuring inference for batch_size=1:   2%|▏         | 16/1000 [00:00<00:13, 73.03it/s]Measuring inference for batch_size=1:   2%|▏         | 24/1000 [00:00<00:13, 73.11it/s]Measuring inference for batch_size=1:   3%|▎         | 32/1000 [00:00<00:13, 73.15it/s]Measuring inference for batch_size=1:   4%|▍         | 40/1000 [00:00<00:13, 73.16it/s]Measuring inference for batch_size=1:   5%|▍         | 48/1000 [00:00<00:13, 73.18it/s]Measuring inference for batch_size=1:   6%|▌         | 56/1000 [00:00<00:12, 73.16it/s]Measuring inference for batch_size=1:   6%|▋         | 64/1000 [00:00<00:12, 73.16it/s]Measuring inference for batch_size=1:   7%|▋         | 72/1000 [00:00<00:12, 73.13it/s]Measuring inference for batch_size=1:   8%|▊         | 80/1000 [00:01<00:12, 73.15it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:01<00:12, 73.11it/s]Measuring inference for batch_size=1:  10%|▉         | 96/1000 [00:01<00:12, 73.10it/s]Measuring inference for batch_size=1:  10%|█         | 104/1000 [00:01<00:12, 73.10it/s]Measuring inference for batch_size=1:  11%|█         | 112/1000 [00:01<00:12, 73.07it/s]Measuring inference for batch_size=1:  12%|█▏        | 120/1000 [00:01<00:12, 73.06it/s]Measuring inference for batch_size=1:  13%|█▎        | 128/1000 [00:01<00:11, 73.04it/s]Measuring inference for batch_size=1:  14%|█▎        | 136/1000 [00:01<00:11, 73.07it/s]Measuring inference for batch_size=1:  14%|█▍        | 144/1000 [00:01<00:11, 73.09it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:02<00:11, 73.14it/s]Measuring inference for batch_size=1:  16%|█▌        | 160/1000 [00:02<00:11, 73.17it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:02<00:11, 73.20it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:02<00:11, 73.20it/s]Measuring inference for batch_size=1:  18%|█▊        | 184/1000 [00:02<00:11, 73.22it/s]Measuring inference for batch_size=1:  19%|█▉        | 192/1000 [00:02<00:11, 73.24it/s]Measuring inference for batch_size=1:  20%|██        | 200/1000 [00:02<00:10, 73.22it/s]Measuring inference for batch_size=1:  21%|██        | 208/1000 [00:02<00:10, 73.21it/s]Measuring inference for batch_size=1:  22%|██▏       | 216/1000 [00:02<00:10, 73.22it/s]Measuring inference for batch_size=1:  22%|██▏       | 224/1000 [00:03<00:10, 73.23it/s]Measuring inference for batch_size=1:  23%|██▎       | 232/1000 [00:03<00:10, 73.22it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:03<00:10, 73.23it/s]Measuring inference for batch_size=1:  25%|██▍       | 248/1000 [00:03<00:10, 73.21it/s]Measuring inference for batch_size=1:  26%|██▌       | 256/1000 [00:03<00:10, 73.22it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:03<00:10, 73.19it/s]Measuring inference for batch_size=1:  27%|██▋       | 272/1000 [00:03<00:09, 73.18it/s]Measuring inference for batch_size=1:  28%|██▊       | 280/1000 [00:03<00:09, 73.20it/s]Measuring inference for batch_size=1:  29%|██▉       | 288/1000 [00:03<00:09, 73.23it/s]Measuring inference for batch_size=1:  30%|██▉       | 296/1000 [00:04<00:09, 73.23it/s]Measuring inference for batch_size=1:  30%|███       | 304/1000 [00:04<00:09, 73.21it/s]Measuring inference for batch_size=1:  31%|███       | 312/1000 [00:04<00:09, 73.21it/s]Measuring inference for batch_size=1:  32%|███▏      | 320/1000 [00:04<00:09, 73.20it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:04<00:09, 73.20it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:04<00:09, 73.23it/s]Measuring inference for batch_size=1:  34%|███▍      | 344/1000 [00:04<00:08, 73.20it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:04<00:08, 73.17it/s]Measuring inference for batch_size=1:  36%|███▌      | 360/1000 [00:04<00:08, 73.13it/s]Measuring inference for batch_size=1:  37%|███▋      | 368/1000 [00:05<00:08, 73.10it/s]Measuring inference for batch_size=1:  38%|███▊      | 376/1000 [00:05<00:08, 73.09it/s]Measuring inference for batch_size=1:  38%|███▊      | 384/1000 [00:05<00:08, 73.10it/s]Measuring inference for batch_size=1:  39%|███▉      | 392/1000 [00:05<00:08, 73.11it/s]Measuring inference for batch_size=1:  40%|████      | 400/1000 [00:05<00:08, 73.09it/s]Measuring inference for batch_size=1:  41%|████      | 408/1000 [00:05<00:08, 73.10it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:05<00:07, 73.10it/s]Measuring inference for batch_size=1:  42%|████▏     | 424/1000 [00:05<00:07, 73.11it/s]Measuring inference for batch_size=1:  43%|████▎     | 432/1000 [00:05<00:07, 73.09it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:06<00:07, 73.07it/s]Measuring inference for batch_size=1:  45%|████▍     | 448/1000 [00:06<00:07, 73.06it/s]Measuring inference for batch_size=1:  46%|████▌     | 456/1000 [00:06<00:07, 73.04it/s]Measuring inference for batch_size=1:  46%|████▋     | 464/1000 [00:06<00:07, 73.04it/s]Measuring inference for batch_size=1:  47%|████▋     | 472/1000 [00:06<00:07, 73.04it/s]Measuring inference for batch_size=1:  48%|████▊     | 480/1000 [00:06<00:07, 73.07it/s]Measuring inference for batch_size=1:  49%|████▉     | 488/1000 [00:06<00:07, 73.06it/s]Measuring inference for batch_size=1:  50%|████▉     | 496/1000 [00:06<00:06, 73.06it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:06<00:06, 73.11it/s]Measuring inference for batch_size=1:  51%|█████     | 512/1000 [00:07<00:06, 73.11it/s]Measuring inference for batch_size=1:  52%|█████▏    | 520/1000 [00:07<00:06, 73.14it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:07<00:06, 73.15it/s]Measuring inference for batch_size=1:  54%|█████▎    | 536/1000 [00:07<00:06, 73.16it/s]Measuring inference for batch_size=1:  54%|█████▍    | 544/1000 [00:07<00:06, 73.13it/s]Measuring inference for batch_size=1:  55%|█████▌    | 552/1000 [00:07<00:06, 73.12it/s]Measuring inference for batch_size=1:  56%|█████▌    | 560/1000 [00:07<00:06, 73.10it/s]Measuring inference for batch_size=1:  57%|█████▋    | 568/1000 [00:07<00:05, 73.12it/s]Measuring inference for batch_size=1:  58%|█████▊    | 576/1000 [00:07<00:05, 73.14it/s]Measuring inference for batch_size=1:  58%|█████▊    | 584/1000 [00:07<00:05, 73.16it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [00:08<00:05, 73.18it/s]Measuring inference for batch_size=1:  60%|██████    | 600/1000 [00:08<00:05, 73.20it/s]Measuring inference for batch_size=1:  61%|██████    | 608/1000 [00:08<00:05, 73.20it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:08<00:05, 73.21it/s]Measuring inference for batch_size=1:  62%|██████▏   | 624/1000 [00:08<00:05, 73.21it/s]Measuring inference for batch_size=1:  63%|██████▎   | 632/1000 [00:08<00:05, 73.19it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [00:08<00:04, 73.16it/s]Measuring inference for batch_size=1:  65%|██████▍   | 648/1000 [00:08<00:04, 73.21it/s]Measuring inference for batch_size=1:  66%|██████▌   | 656/1000 [00:08<00:04, 73.27it/s]Measuring inference for batch_size=1:  66%|██████▋   | 664/1000 [00:09<00:04, 73.31it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:09<00:04, 73.31it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [00:09<00:04, 73.36it/s]Measuring inference for batch_size=1:  69%|██████▉   | 688/1000 [00:09<00:04, 73.34it/s]Measuring inference for batch_size=1:  70%|██████▉   | 696/1000 [00:09<00:04, 73.37it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [00:09<00:04, 73.30it/s]Measuring inference for batch_size=1:  71%|███████   | 712/1000 [00:09<00:03, 73.26it/s]Measuring inference for batch_size=1:  72%|███████▏  | 720/1000 [00:09<00:03, 73.26it/s]Measuring inference for batch_size=1:  73%|███████▎  | 728/1000 [00:09<00:03, 73.32it/s]Measuring inference for batch_size=1:  74%|███████▎  | 736/1000 [00:10<00:03, 73.35it/s]Measuring inference for batch_size=1:  74%|███████▍  | 744/1000 [00:10<00:03, 73.39it/s]Measuring inference for batch_size=1:  75%|███████▌  | 752/1000 [00:10<00:03, 73.42it/s]Measuring inference for batch_size=1:  76%|███████▌  | 760/1000 [00:10<00:03, 73.43it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [00:10<00:03, 73.42it/s]Measuring inference for batch_size=1:  78%|███████▊  | 776/1000 [00:10<00:03, 73.40it/s]Measuring inference for batch_size=1:  78%|███████▊  | 784/1000 [00:10<00:02, 73.32it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [00:10<00:02, 73.31it/s]Measuring inference for batch_size=1:  80%|████████  | 800/1000 [00:10<00:02, 73.28it/s]Measuring inference for batch_size=1:  81%|████████  | 808/1000 [00:11<00:02, 73.31it/s]Measuring inference for batch_size=1:  82%|████████▏ | 816/1000 [00:11<00:02, 73.29it/s]Measuring inference for batch_size=1:  82%|████████▏ | 824/1000 [00:11<00:02, 73.32it/s]Measuring inference for batch_size=1:  83%|████████▎ | 832/1000 [00:11<00:02, 73.32it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:11<00:02, 73.31it/s]Measuring inference for batch_size=1:  85%|████████▍ | 848/1000 [00:11<00:02, 73.29it/s]Measuring inference for batch_size=1:  86%|████████▌ | 856/1000 [00:11<00:01, 73.29it/s]Measuring inference for batch_size=1:  86%|████████▋ | 864/1000 [00:11<00:01, 73.27it/s]Measuring inference for batch_size=1:  87%|████████▋ | 872/1000 [00:11<00:01, 73.24it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [00:12<00:01, 73.26it/s]Measuring inference for batch_size=1:  89%|████████▉ | 888/1000 [00:12<00:01, 73.31it/s]Measuring inference for batch_size=1:  90%|████████▉ | 896/1000 [00:12<00:01, 73.34it/s]Measuring inference for batch_size=1:  90%|█████████ | 904/1000 [00:12<00:01, 73.38it/s]Measuring inference for batch_size=1:  91%|█████████ | 912/1000 [00:12<00:01, 73.40it/s]Measuring inference for batch_size=1:  92%|█████████▏| 920/1000 [00:12<00:01, 73.41it/s]Measuring inference for batch_size=1:  93%|█████████▎| 928/1000 [00:12<00:00, 73.36it/s]Measuring inference for batch_size=1:  94%|█████████▎| 936/1000 [00:12<00:00, 73.32it/s]Measuring inference for batch_size=1:  94%|█████████▍| 944/1000 [00:12<00:00, 73.30it/s]Measuring inference for batch_size=1:  95%|█████████▌| 952/1000 [00:13<00:00, 73.32it/s]Measuring inference for batch_size=1:  96%|█████████▌| 960/1000 [00:13<00:00, 73.30it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [00:13<00:00, 73.33it/s]Measuring inference for batch_size=1:  98%|█████████▊| 976/1000 [00:13<00:00, 73.31it/s]Measuring inference for batch_size=1:  98%|█████████▊| 984/1000 [00:13<00:00, 73.34it/s]Measuring inference for batch_size=1:  99%|█████████▉| 992/1000 [00:13<00:00, 73.32it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 73.33it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 73.21it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=16:   8%|▊         | 8/100 [00:00<00:01, 72.27it/s]Warming up with batch_size=16:  16%|█▌        | 16/100 [00:00<00:01, 72.42it/s]Warming up with batch_size=16:  24%|██▍       | 24/100 [00:00<00:01, 72.44it/s]Warming up with batch_size=16:  32%|███▏      | 32/100 [00:00<00:00, 72.46it/s]Warming up with batch_size=16:  40%|████      | 40/100 [00:00<00:00, 72.48it/s]Warming up with batch_size=16:  48%|████▊     | 48/100 [00:00<00:00, 72.51it/s]Warming up with batch_size=16:  56%|█████▌    | 56/100 [00:00<00:00, 72.53it/s]Warming up with batch_size=16:  64%|██████▍   | 64/100 [00:00<00:00, 72.49it/s]Warming up with batch_size=16:  72%|███████▏  | 72/100 [00:00<00:00, 72.48it/s]Warming up with batch_size=16:  80%|████████  | 80/100 [00:01<00:00, 72.51it/s]Warming up with batch_size=16:  88%|████████▊ | 88/100 [00:01<00:00, 72.51it/s]Warming up with batch_size=16:  96%|█████████▌| 96/100 [00:01<00:00, 72.55it/s]Warming up with batch_size=16: 100%|██████████| 100/100 [00:01<00:00, 72.50it/s]
STAGE:2024-02-23 09:57:50 177168:177168 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:57:50 177168:177168 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:57:50 177168:177168 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=16:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=16:   1%|          | 8/1000 [00:00<00:13, 71.51it/s]Measuring inference for batch_size=16:   2%|▏         | 16/1000 [00:00<00:13, 71.94it/s]Measuring inference for batch_size=16:   2%|▏         | 24/1000 [00:00<00:13, 71.99it/s]Measuring inference for batch_size=16:   3%|▎         | 32/1000 [00:00<00:13, 72.02it/s]Measuring inference for batch_size=16:   4%|▍         | 40/1000 [00:00<00:13, 71.98it/s]Measuring inference for batch_size=16:   5%|▍         | 48/1000 [00:00<00:13, 72.03it/s]Measuring inference for batch_size=16:   6%|▌         | 56/1000 [00:00<00:13, 72.05it/s]Measuring inference for batch_size=16:   6%|▋         | 64/1000 [00:00<00:12, 72.07it/s]Measuring inference for batch_size=16:   7%|▋         | 72/1000 [00:00<00:12, 72.07it/s]Measuring inference for batch_size=16:   8%|▊         | 80/1000 [00:01<00:12, 72.10it/s]Measuring inference for batch_size=16:   9%|▉         | 88/1000 [00:01<00:12, 72.09it/s]Measuring inference for batch_size=16:  10%|▉         | 96/1000 [00:01<00:12, 72.08it/s]Measuring inference for batch_size=16:  10%|█         | 104/1000 [00:01<00:12, 72.06it/s]Measuring inference for batch_size=16:  11%|█         | 112/1000 [00:01<00:12, 72.07it/s]Measuring inference for batch_size=16:  12%|█▏        | 120/1000 [00:01<00:12, 72.07it/s]Measuring inference for batch_size=16:  13%|█▎        | 128/1000 [00:01<00:12, 72.06it/s]Measuring inference for batch_size=16:  14%|█▎        | 136/1000 [00:01<00:11, 72.06it/s]Measuring inference for batch_size=16:  14%|█▍        | 144/1000 [00:01<00:11, 72.04it/s]Measuring inference for batch_size=16:  15%|█▌        | 152/1000 [00:02<00:11, 72.00it/s]Measuring inference for batch_size=16:  16%|█▌        | 160/1000 [00:02<00:11, 72.01it/s]Measuring inference for batch_size=16:  17%|█▋        | 168/1000 [00:02<00:11, 72.00it/s]Measuring inference for batch_size=16:  18%|█▊        | 176/1000 [00:02<00:11, 71.99it/s]Measuring inference for batch_size=16:  18%|█▊        | 184/1000 [00:02<00:11, 71.98it/s]Measuring inference for batch_size=16:  19%|█▉        | 192/1000 [00:02<00:11, 71.97it/s]Measuring inference for batch_size=16:  20%|██        | 200/1000 [00:02<00:11, 71.98it/s]Measuring inference for batch_size=16:  21%|██        | 208/1000 [00:02<00:11, 71.98it/s]Measuring inference for batch_size=16:  22%|██▏       | 216/1000 [00:02<00:10, 71.96it/s]Measuring inference for batch_size=16:  22%|██▏       | 224/1000 [00:03<00:10, 71.93it/s]Measuring inference for batch_size=16:  23%|██▎       | 232/1000 [00:03<00:10, 71.94it/s]Measuring inference for batch_size=16:  24%|██▍       | 240/1000 [00:03<00:10, 71.98it/s]Measuring inference for batch_size=16:  25%|██▍       | 248/1000 [00:03<00:10, 72.03it/s]Measuring inference for batch_size=16:  26%|██▌       | 256/1000 [00:03<00:10, 72.06it/s]Measuring inference for batch_size=16:  26%|██▋       | 264/1000 [00:03<00:10, 72.04it/s]Measuring inference for batch_size=16:  27%|██▋       | 272/1000 [00:03<00:10, 72.06it/s]Measuring inference for batch_size=16:  28%|██▊       | 280/1000 [00:03<00:09, 72.03it/s]Measuring inference for batch_size=16:  29%|██▉       | 288/1000 [00:03<00:09, 72.04it/s]Measuring inference for batch_size=16:  30%|██▉       | 296/1000 [00:04<00:09, 72.03it/s]Measuring inference for batch_size=16:  30%|███       | 304/1000 [00:04<00:09, 72.06it/s]Measuring inference for batch_size=16:  31%|███       | 312/1000 [00:04<00:09, 72.04it/s]Measuring inference for batch_size=16:  32%|███▏      | 320/1000 [00:04<00:09, 72.02it/s]Measuring inference for batch_size=16:  33%|███▎      | 328/1000 [00:04<00:09, 71.98it/s]Measuring inference for batch_size=16:  34%|███▎      | 336/1000 [00:04<00:09, 71.97it/s]Measuring inference for batch_size=16:  34%|███▍      | 344/1000 [00:04<00:09, 71.94it/s]Measuring inference for batch_size=16:  35%|███▌      | 352/1000 [00:04<00:09, 71.95it/s]Measuring inference for batch_size=16:  36%|███▌      | 360/1000 [00:04<00:08, 71.94it/s]Measuring inference for batch_size=16:  37%|███▋      | 368/1000 [00:05<00:08, 71.92it/s]Measuring inference for batch_size=16:  38%|███▊      | 376/1000 [00:05<00:08, 71.91it/s]Measuring inference for batch_size=16:  38%|███▊      | 384/1000 [00:05<00:08, 71.90it/s]Measuring inference for batch_size=16:  39%|███▉      | 392/1000 [00:05<00:08, 71.88it/s]Measuring inference for batch_size=16:  40%|████      | 400/1000 [00:05<00:08, 71.87it/s]Measuring inference for batch_size=16:  41%|████      | 408/1000 [00:05<00:08, 71.87it/s]Measuring inference for batch_size=16:  42%|████▏     | 416/1000 [00:05<00:08, 71.85it/s]Measuring inference for batch_size=16:  42%|████▏     | 424/1000 [00:05<00:08, 71.92it/s]Measuring inference for batch_size=16:  43%|████▎     | 432/1000 [00:06<00:07, 71.96it/s]Measuring inference for batch_size=16:  44%|████▍     | 440/1000 [00:06<00:07, 71.99it/s]Measuring inference for batch_size=16:  45%|████▍     | 448/1000 [00:06<00:07, 72.01it/s]Measuring inference for batch_size=16:  46%|████▌     | 456/1000 [00:06<00:07, 71.96it/s]Measuring inference for batch_size=16:  46%|████▋     | 464/1000 [00:06<00:07, 71.94it/s]Measuring inference for batch_size=16:  47%|████▋     | 472/1000 [00:06<00:07, 71.93it/s]Measuring inference for batch_size=16:  48%|████▊     | 480/1000 [00:06<00:07, 71.93it/s]Measuring inference for batch_size=16:  49%|████▉     | 488/1000 [00:06<00:07, 71.92it/s]Measuring inference for batch_size=16:  50%|████▉     | 496/1000 [00:06<00:07, 71.95it/s]Measuring inference for batch_size=16:  50%|█████     | 504/1000 [00:07<00:06, 71.95it/s]Measuring inference for batch_size=16:  51%|█████     | 512/1000 [00:07<00:06, 71.99it/s]Measuring inference for batch_size=16:  52%|█████▏    | 520/1000 [00:07<00:06, 71.97it/s]Measuring inference for batch_size=16:  53%|█████▎    | 528/1000 [00:07<00:06, 72.02it/s]Measuring inference for batch_size=16:  54%|█████▎    | 536/1000 [00:07<00:06, 72.02it/s]Measuring inference for batch_size=16:  54%|█████▍    | 544/1000 [00:07<00:06, 72.02it/s]Measuring inference for batch_size=16:  55%|█████▌    | 552/1000 [00:07<00:06, 72.00it/s]Measuring inference for batch_size=16:  56%|█████▌    | 560/1000 [00:07<00:06, 72.00it/s]Measuring inference for batch_size=16:  57%|█████▋    | 568/1000 [00:07<00:05, 72.03it/s]Measuring inference for batch_size=16:  58%|█████▊    | 576/1000 [00:08<00:05, 72.07it/s]Measuring inference for batch_size=16:  58%|█████▊    | 584/1000 [00:08<00:05, 72.10it/s]Measuring inference for batch_size=16:  59%|█████▉    | 592/1000 [00:08<00:05, 72.10it/s]Measuring inference for batch_size=16:  60%|██████    | 600/1000 [00:08<00:05, 72.07it/s]Measuring inference for batch_size=16:  61%|██████    | 608/1000 [00:08<00:05, 72.00it/s]Measuring inference for batch_size=16:  62%|██████▏   | 616/1000 [00:08<00:05, 71.98it/s]Measuring inference for batch_size=16:  62%|██████▏   | 624/1000 [00:08<00:05, 72.04it/s]Measuring inference for batch_size=16:  63%|██████▎   | 632/1000 [00:08<00:05, 72.02it/s]Measuring inference for batch_size=16:  64%|██████▍   | 640/1000 [00:08<00:04, 72.10it/s]Measuring inference for batch_size=16:  65%|██████▍   | 648/1000 [00:08<00:04, 72.11it/s]Measuring inference for batch_size=16:  66%|██████▌   | 656/1000 [00:09<00:04, 72.14it/s]Measuring inference for batch_size=16:  66%|██████▋   | 664/1000 [00:09<00:04, 72.14it/s]Measuring inference for batch_size=16:  67%|██████▋   | 672/1000 [00:09<00:04, 72.11it/s]Measuring inference for batch_size=16:  68%|██████▊   | 680/1000 [00:09<00:04, 72.09it/s]Measuring inference for batch_size=16:  69%|██████▉   | 688/1000 [00:09<00:04, 72.08it/s]Measuring inference for batch_size=16:  70%|██████▉   | 696/1000 [00:09<00:04, 72.08it/s]Measuring inference for batch_size=16:  70%|███████   | 704/1000 [00:09<00:04, 72.08it/s]Measuring inference for batch_size=16:  71%|███████   | 712/1000 [00:09<00:03, 72.09it/s]Measuring inference for batch_size=16:  72%|███████▏  | 720/1000 [00:09<00:03, 72.08it/s]Measuring inference for batch_size=16:  73%|███████▎  | 728/1000 [00:10<00:03, 72.09it/s]Measuring inference for batch_size=16:  74%|███████▎  | 736/1000 [00:10<00:03, 72.09it/s]Measuring inference for batch_size=16:  74%|███████▍  | 744/1000 [00:10<00:03, 72.05it/s]Measuring inference for batch_size=16:  75%|███████▌  | 752/1000 [00:10<00:03, 72.03it/s]Measuring inference for batch_size=16:  76%|███████▌  | 760/1000 [00:10<00:03, 72.00it/s]Measuring inference for batch_size=16:  77%|███████▋  | 768/1000 [00:10<00:03, 72.03it/s]Measuring inference for batch_size=16:  78%|███████▊  | 776/1000 [00:10<00:03, 72.00it/s]Measuring inference for batch_size=16:  78%|███████▊  | 784/1000 [00:10<00:02, 72.00it/s]Measuring inference for batch_size=16:  79%|███████▉  | 792/1000 [00:10<00:02, 71.99it/s]Measuring inference for batch_size=16:  80%|████████  | 800/1000 [00:11<00:02, 71.98it/s]Measuring inference for batch_size=16:  81%|████████  | 808/1000 [00:11<00:02, 71.99it/s]Measuring inference for batch_size=16:  82%|████████▏ | 816/1000 [00:11<00:02, 71.99it/s]Measuring inference for batch_size=16:  82%|████████▏ | 824/1000 [00:11<00:02, 72.00it/s]Measuring inference for batch_size=16:  83%|████████▎ | 832/1000 [00:11<00:02, 72.03it/s]Measuring inference for batch_size=16:  84%|████████▍ | 840/1000 [00:11<00:02, 72.05it/s]Measuring inference for batch_size=16:  85%|████████▍ | 848/1000 [00:11<00:02, 72.08it/s]Measuring inference for batch_size=16:  86%|████████▌ | 856/1000 [00:11<00:01, 72.06it/s]Measuring inference for batch_size=16:  86%|████████▋ | 864/1000 [00:11<00:01, 72.07it/s]Measuring inference for batch_size=16:  87%|████████▋ | 872/1000 [00:12<00:01, 72.04it/s]Measuring inference for batch_size=16:  88%|████████▊ | 880/1000 [00:12<00:01, 72.01it/s]Measuring inference for batch_size=16:  89%|████████▉ | 888/1000 [00:12<00:01, 71.98it/s]Measuring inference for batch_size=16:  90%|████████▉ | 896/1000 [00:12<00:01, 71.99it/s]Measuring inference for batch_size=16:  90%|█████████ | 904/1000 [00:12<00:01, 71.93it/s]Measuring inference for batch_size=16:  91%|█████████ | 912/1000 [00:12<00:01, 71.90it/s]Measuring inference for batch_size=16:  92%|█████████▏| 920/1000 [00:12<00:01, 71.89it/s]Measuring inference for batch_size=16:  93%|█████████▎| 928/1000 [00:12<00:01, 71.90it/s]Measuring inference for batch_size=16:  94%|█████████▎| 936/1000 [00:12<00:00, 71.92it/s]Measuring inference for batch_size=16:  94%|█████████▍| 944/1000 [00:13<00:00, 71.93it/s]Measuring inference for batch_size=16:  95%|█████████▌| 952/1000 [00:13<00:00, 71.91it/s]Measuring inference for batch_size=16:  96%|█████████▌| 960/1000 [00:13<00:00, 71.92it/s]Measuring inference for batch_size=16:  97%|█████████▋| 968/1000 [00:13<00:00, 71.95it/s]Measuring inference for batch_size=16:  98%|█████████▊| 976/1000 [00:13<00:00, 71.97it/s]Measuring inference for batch_size=16:  98%|█████████▊| 984/1000 [00:13<00:00, 71.95it/s]Measuring inference for batch_size=16:  99%|█████████▉| 992/1000 [00:13<00:00, 71.96it/s]Measuring inference for batch_size=16: 100%|██████████| 1000/1000 [00:13<00:00, 71.95it/s]Measuring inference for batch_size=16: 100%|██████████| 1000/1000 [00:13<00:00, 72.00it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 11580687848
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.53 GB
    total: 31.28 GB
    used: 3.29 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 374.76 MB
    max_inference_bytes: 392962560
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
  batch_size_16:
    max_inference: 378.48 MB
    max_inference_bytes: 396866048
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
params: 60192808
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 93.551 us +/- 5.105 us [91.076 us, 213.861 us]
        batches_per_second: 10.71 K +/- 411.14 [4.68 K, 10.98 K]
      metrics:
        batches_per_second_max: 10979.853403141362
        batches_per_second_mean: 10710.313544485833
        batches_per_second_min: 4675.924191750279
        batches_per_second_std: 411.1417035858933
        seconds_per_batch_max: 0.00021386146545410156
        seconds_per_batch_mean: 9.35513973236084e-05
        seconds_per_batch_min: 9.107589721679688e-05
        seconds_per_batch_std: 5.105456011343599e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.357 us +/- 0.662 us [22.650 us, 30.994 us]
        batches_per_second: 42.84 K +/- 1.04 K [32.26 K, 44.15 K]
      metrics:
        batches_per_second_max: 44150.56842105263
        batches_per_second_mean: 42843.711082076996
        batches_per_second_min: 32263.876923076925
        batches_per_second_std: 1041.7028870417823
        seconds_per_batch_max: 3.0994415283203125e-05
        seconds_per_batch_mean: 2.335667610168457e-05
        seconds_per_batch_min: 2.2649765014648438e-05
        seconds_per_batch_std: 6.621762177268445e-07
    on_device_inference:
      human_readable:
        batch_latency: -13526513.181 us +/- 39.940 ms [-14246399.879 us, -13430335.999
          us]
        batches_per_second: -0.07 +/- 0.00 [-0.07, -0.07]
      metrics:
        batches_per_second_max: -0.07019317220219819
        batches_per_second_mean: -0.07392951644718161
        batches_per_second_min: -0.074458300976913
        batches_per_second_std: 0.00021485311430313443
        seconds_per_batch_max: -13.430335998535156
        seconds_per_batch_mean: -13.526513180732728
        seconds_per_batch_min: -14.246399879455566
        seconds_per_batch_std: 0.039939908552589956
    total:
      human_readable:
        batch_latency: 13.651 ms +/- 43.094 us [13.552 ms, 14.509 ms]
        batches_per_second: 73.26 +/- 0.23 [68.92, 73.79]
      metrics:
        batches_per_second_max: 73.78752001125908
        batches_per_second_mean: 73.25598772582609
        batches_per_second_min: 68.92404772077431
        batches_per_second_std: 0.2260106445665885
        seconds_per_batch_max: 0.014508724212646484
        seconds_per_batch_mean: 0.013650894165039062
        seconds_per_batch_min: 0.013552427291870117
        seconds_per_batch_std: 4.309416971430749e-05
  batch_size_16:
    cpu_to_gpu:
      human_readable:
        batch_latency: 144.474 us +/- 5.437 us [141.621 us, 274.420 us]
        batches_per_second: 6.93 K +/- 193.18 [3.64 K, 7.06 K]
      metrics:
        batches_per_second_max: 7061.117845117845
        batches_per_second_mean: 6928.6132906041385
        batches_per_second_min: 3644.05212858384
        batches_per_second_std: 193.18003124757482
        seconds_per_batch_max: 0.00027441978454589844
        seconds_per_batch_mean: 0.00014447426795959474
        seconds_per_batch_min: 0.00014162063598632812
        seconds_per_batch_std: 5.436633922638657e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.686 us +/- 0.648 us [22.650 us, 31.233 us]
        batches_per_second: 42.25 K +/- 1.02 K [32.02 K, 44.15 K]
      metrics:
        batches_per_second_max: 44150.56842105263
        batches_per_second_mean: 42246.52309624079
        batches_per_second_min: 32017.58778625954
        batches_per_second_std: 1021.8512388334702
        seconds_per_batch_max: 3.123283386230469e-05
        seconds_per_batch_mean: 2.3686170578002928e-05
        seconds_per_batch_min: 2.2649765014648438e-05
        seconds_per_batch_std: 6.478053044634874e-07
    on_device_inference:
      human_readable:
        batch_latency: -13706926.247 us +/- 36.776 ms [-14364128.113 us, -13594304.085
          us]
        batches_per_second: -0.07 +/- 0.00 [-0.07, -0.07]
      metrics:
        batches_per_second_max: -0.06961786974799959
        batches_per_second_mean: -0.07295633439614263
        batches_per_second_min: -0.07356022005714481
        batches_per_second_std: 0.00019296687834116866
        seconds_per_batch_max: -13.594304084777832
        seconds_per_batch_mean: -13.706926246643066
        seconds_per_batch_min: -14.364128112792969
        seconds_per_batch_std: 0.036775683255265285
    total:
      human_readable:
        batch_latency: 13.880 ms +/- 39.901 us [13.766 ms, 14.674 ms]
        batches_per_second: 72.04 +/- 0.20 [68.15, 72.64]
      metrics:
        batches_per_second_max: 72.64247735499403
        batches_per_second_mean: 72.04450613978726
        batches_per_second_min: 68.14798446715518
        batches_per_second_std: 0.20276803683140812
        seconds_per_batch_max: 0.014673948287963867
        seconds_per_batch_mean: 0.013880421161651611
        seconds_per_batch_min: 0.013766050338745117
        seconds_per_batch_std: 3.990067916582004e-05


#####
baseline-baseline-py-id - Run 2
2024-02-23 09:58:45
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.64it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.64it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  60.19 M, 100.000% Params, 11.58 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.016% Params, 118.01 MMac, 1.019% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.000% Params, 1.61 MMac, 0.014% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.007% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.007% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.359% Params, 680.39 MMac, 5.875% MACs, 
    (0): Bottleneck(
      75.01 k, 0.125% Params, 236.43 MMac, 2.042% MACs, 
      (conv1): Conv2d(4.1 k, 0.007% Params, 12.85 MMac, 0.111% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.028% Params, 52.99 MMac, 0.458% MACs, 
        (0): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    2.34 M, 3.887% Params, 1.92 GMac, 16.555% MACs, 
    (0): Bottleneck(
      379.39 k, 0.630% Params, 376.02 MMac, 3.247% MACs, 
      (conv1): Conv2d(32.77 k, 0.054% Params, 102.76 MMac, 0.887% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 802.82 KMac, 0.007% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.008% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.219% Params, 103.56 MMac, 0.894% MACs, 
        (0): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (4): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (5): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (6): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (7): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    40.61 M, 67.473% Params, 8.05 GMac, 69.501% MACs, 
    (0): Bottleneck(
      1.51 M, 2.513% Params, 374.26 MMac, 3.232% MACs, 
      (conv1): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 401.41 KMac, 0.003% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.004% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 0.874% Params, 103.16 MMac, 0.891% MACs, 
        (0): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (6): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (7): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (8): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (9): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (10): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (11): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (12): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (13): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (14): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (15): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (16): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (17): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (18): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (19): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (20): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (21): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (22): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (23): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (24): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (25): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (26): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (27): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (28): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (29): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (30): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (31): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (32): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (33): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (34): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (35): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 24.861% Params, 811.02 MMac, 7.003% MACs, 
    (0): Bottleneck(
      6.04 M, 10.034% Params, 373.38 MMac, 3.224% MACs, 
      (conv1): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 200.7 KMac, 0.002% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.002% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 3.491% Params, 102.96 MMac, 0.889% MACs, 
        (0): Conv2d(2.1 M, 3.484% Params, 102.76 MMac, 0.887% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.001% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 3.404% Params, 2.05 MMac, 0.018% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  10%|█         | 10/100 [00:00<00:00, 96.95it/s]Warming up with batch_size=1:  20%|██        | 20/100 [00:00<00:00, 96.92it/s]Warming up with batch_size=1:  30%|███       | 30/100 [00:00<00:00, 96.98it/s]Warming up with batch_size=1:  40%|████      | 40/100 [00:00<00:00, 97.17it/s]Warming up with batch_size=1:  50%|█████     | 50/100 [00:00<00:00, 97.25it/s]Warming up with batch_size=1:  60%|██████    | 60/100 [00:00<00:00, 97.27it/s]Warming up with batch_size=1:  70%|███████   | 70/100 [00:00<00:00, 97.26it/s]Warming up with batch_size=1:  80%|████████  | 80/100 [00:00<00:00, 97.28it/s]Warming up with batch_size=1:  90%|█████████ | 90/100 [00:00<00:00, 97.27it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:01<00:00, 97.27it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:01<00:00, 97.21it/s]
STAGE:2024-02-23 09:58:13 177214:177214 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:58:13 177214:177214 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:58:13 177214:177214 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   1%|          | 8/1000 [00:00<00:13, 72.55it/s]Measuring inference for batch_size=1:   2%|▏         | 16/1000 [00:00<00:13, 72.92it/s]Measuring inference for batch_size=1:   2%|▏         | 24/1000 [00:00<00:13, 73.01it/s]Measuring inference for batch_size=1:   3%|▎         | 32/1000 [00:00<00:13, 73.03it/s]Measuring inference for batch_size=1:   4%|▍         | 40/1000 [00:00<00:13, 73.01it/s]Measuring inference for batch_size=1:   5%|▍         | 48/1000 [00:00<00:13, 72.98it/s]Measuring inference for batch_size=1:   6%|▌         | 56/1000 [00:00<00:12, 72.99it/s]Measuring inference for batch_size=1:   6%|▋         | 64/1000 [00:00<00:12, 73.00it/s]Measuring inference for batch_size=1:   7%|▋         | 72/1000 [00:00<00:12, 73.00it/s]Measuring inference for batch_size=1:   8%|▊         | 80/1000 [00:01<00:12, 73.01it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:01<00:12, 72.99it/s]Measuring inference for batch_size=1:  10%|▉         | 96/1000 [00:01<00:12, 73.01it/s]Measuring inference for batch_size=1:  10%|█         | 104/1000 [00:01<00:12, 73.02it/s]Measuring inference for batch_size=1:  11%|█         | 112/1000 [00:01<00:12, 73.03it/s]Measuring inference for batch_size=1:  12%|█▏        | 120/1000 [00:01<00:12, 73.06it/s]Measuring inference for batch_size=1:  13%|█▎        | 128/1000 [00:01<00:11, 73.06it/s]Measuring inference for batch_size=1:  14%|█▎        | 136/1000 [00:01<00:11, 73.04it/s]Measuring inference for batch_size=1:  14%|█▍        | 144/1000 [00:01<00:11, 73.03it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:02<00:11, 73.03it/s]Measuring inference for batch_size=1:  16%|█▌        | 160/1000 [00:02<00:11, 73.03it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:02<00:11, 73.02it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:02<00:11, 73.04it/s]Measuring inference for batch_size=1:  18%|█▊        | 184/1000 [00:02<00:11, 73.10it/s]Measuring inference for batch_size=1:  19%|█▉        | 192/1000 [00:02<00:11, 73.16it/s]Measuring inference for batch_size=1:  20%|██        | 200/1000 [00:02<00:10, 73.19it/s]Measuring inference for batch_size=1:  21%|██        | 208/1000 [00:02<00:10, 73.17it/s]Measuring inference for batch_size=1:  22%|██▏       | 216/1000 [00:02<00:10, 73.18it/s]Measuring inference for batch_size=1:  22%|██▏       | 224/1000 [00:03<00:10, 73.20it/s]Measuring inference for batch_size=1:  23%|██▎       | 232/1000 [00:03<00:10, 73.21it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:03<00:10, 73.23it/s]Measuring inference for batch_size=1:  25%|██▍       | 248/1000 [00:03<00:10, 73.23it/s]Measuring inference for batch_size=1:  26%|██▌       | 256/1000 [00:03<00:10, 73.20it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:03<00:10, 73.16it/s]Measuring inference for batch_size=1:  27%|██▋       | 272/1000 [00:03<00:09, 73.13it/s]Measuring inference for batch_size=1:  28%|██▊       | 280/1000 [00:03<00:09, 73.14it/s]Measuring inference for batch_size=1:  29%|██▉       | 288/1000 [00:03<00:09, 73.15it/s]Measuring inference for batch_size=1:  30%|██▉       | 296/1000 [00:04<00:09, 73.14it/s]Measuring inference for batch_size=1:  30%|███       | 304/1000 [00:04<00:09, 73.14it/s]Measuring inference for batch_size=1:  31%|███       | 312/1000 [00:04<00:09, 73.08it/s]Measuring inference for batch_size=1:  32%|███▏      | 320/1000 [00:04<00:09, 73.09it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:04<00:09, 73.11it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:04<00:09, 73.12it/s]Measuring inference for batch_size=1:  34%|███▍      | 344/1000 [00:04<00:08, 73.13it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:04<00:08, 73.14it/s]Measuring inference for batch_size=1:  36%|███▌      | 360/1000 [00:04<00:08, 73.15it/s]Measuring inference for batch_size=1:  37%|███▋      | 368/1000 [00:05<00:08, 73.14it/s]Measuring inference for batch_size=1:  38%|███▊      | 376/1000 [00:05<00:08, 73.13it/s]Measuring inference for batch_size=1:  38%|███▊      | 384/1000 [00:05<00:08, 73.13it/s]Measuring inference for batch_size=1:  39%|███▉      | 392/1000 [00:05<00:08, 73.08it/s]Measuring inference for batch_size=1:  40%|████      | 400/1000 [00:05<00:08, 73.06it/s]Measuring inference for batch_size=1:  41%|████      | 408/1000 [00:05<00:08, 73.03it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:05<00:07, 73.00it/s]Measuring inference for batch_size=1:  42%|████▏     | 424/1000 [00:05<00:07, 72.99it/s]Measuring inference for batch_size=1:  43%|████▎     | 432/1000 [00:05<00:07, 72.97it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:06<00:07, 72.96it/s]Measuring inference for batch_size=1:  45%|████▍     | 448/1000 [00:06<00:07, 72.94it/s]Measuring inference for batch_size=1:  46%|████▌     | 456/1000 [00:06<00:07, 72.93it/s]Measuring inference for batch_size=1:  46%|████▋     | 464/1000 [00:06<00:07, 72.94it/s]Measuring inference for batch_size=1:  47%|████▋     | 472/1000 [00:06<00:07, 73.02it/s]Measuring inference for batch_size=1:  48%|████▊     | 480/1000 [00:06<00:07, 73.04it/s]Measuring inference for batch_size=1:  49%|████▉     | 488/1000 [00:06<00:07, 73.08it/s]Measuring inference for batch_size=1:  50%|████▉     | 496/1000 [00:06<00:06, 73.07it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:06<00:06, 73.05it/s]Measuring inference for batch_size=1:  51%|█████     | 512/1000 [00:07<00:06, 73.08it/s]Measuring inference for batch_size=1:  52%|█████▏    | 520/1000 [00:07<00:06, 73.08it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:07<00:06, 73.07it/s]Measuring inference for batch_size=1:  54%|█████▎    | 536/1000 [00:07<00:06, 73.09it/s]Measuring inference for batch_size=1:  54%|█████▍    | 544/1000 [00:07<00:06, 73.03it/s]Measuring inference for batch_size=1:  55%|█████▌    | 552/1000 [00:07<00:06, 73.03it/s]Measuring inference for batch_size=1:  56%|█████▌    | 560/1000 [00:07<00:06, 73.02it/s]Measuring inference for batch_size=1:  57%|█████▋    | 568/1000 [00:07<00:05, 73.07it/s]Measuring inference for batch_size=1:  58%|█████▊    | 576/1000 [00:07<00:05, 73.04it/s]Measuring inference for batch_size=1:  58%|█████▊    | 584/1000 [00:07<00:05, 73.04it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [00:08<00:05, 73.05it/s]Measuring inference for batch_size=1:  60%|██████    | 600/1000 [00:08<00:05, 73.02it/s]Measuring inference for batch_size=1:  61%|██████    | 608/1000 [00:08<00:05, 73.01it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:08<00:05, 73.00it/s]Measuring inference for batch_size=1:  62%|██████▏   | 624/1000 [00:08<00:05, 72.98it/s]Measuring inference for batch_size=1:  63%|██████▎   | 632/1000 [00:08<00:05, 72.96it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [00:08<00:04, 72.96it/s]Measuring inference for batch_size=1:  65%|██████▍   | 648/1000 [00:08<00:04, 73.01it/s]Measuring inference for batch_size=1:  66%|██████▌   | 656/1000 [00:08<00:04, 72.96it/s]Measuring inference for batch_size=1:  66%|██████▋   | 664/1000 [00:09<00:04, 72.98it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:09<00:04, 72.96it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [00:09<00:04, 73.06it/s]Measuring inference for batch_size=1:  69%|██████▉   | 688/1000 [00:09<00:04, 73.02it/s]Measuring inference for batch_size=1:  70%|██████▉   | 696/1000 [00:09<00:04, 73.03it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [00:09<00:04, 72.99it/s]Measuring inference for batch_size=1:  71%|███████   | 712/1000 [00:09<00:03, 73.04it/s]Measuring inference for batch_size=1:  72%|███████▏  | 720/1000 [00:09<00:03, 73.05it/s]Measuring inference for batch_size=1:  73%|███████▎  | 728/1000 [00:09<00:03, 73.05it/s]Measuring inference for batch_size=1:  74%|███████▎  | 736/1000 [00:10<00:03, 73.04it/s]Measuring inference for batch_size=1:  74%|███████▍  | 744/1000 [00:10<00:03, 73.01it/s]Measuring inference for batch_size=1:  75%|███████▌  | 752/1000 [00:10<00:03, 72.98it/s]Measuring inference for batch_size=1:  76%|███████▌  | 760/1000 [00:10<00:03, 73.01it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [00:10<00:03, 73.00it/s]Measuring inference for batch_size=1:  78%|███████▊  | 776/1000 [00:10<00:03, 73.01it/s]Measuring inference for batch_size=1:  78%|███████▊  | 784/1000 [00:10<00:02, 73.03it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [00:10<00:02, 73.08it/s]Measuring inference for batch_size=1:  80%|████████  | 800/1000 [00:10<00:02, 73.10it/s]Measuring inference for batch_size=1:  81%|████████  | 808/1000 [00:11<00:02, 73.12it/s]Measuring inference for batch_size=1:  82%|████████▏ | 816/1000 [00:11<00:02, 73.13it/s]Measuring inference for batch_size=1:  82%|████████▏ | 824/1000 [00:11<00:02, 73.14it/s]Measuring inference for batch_size=1:  83%|████████▎ | 832/1000 [00:11<00:02, 73.09it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:11<00:02, 73.08it/s]Measuring inference for batch_size=1:  85%|████████▍ | 848/1000 [00:11<00:02, 73.06it/s]Measuring inference for batch_size=1:  86%|████████▌ | 856/1000 [00:11<00:01, 73.05it/s]Measuring inference for batch_size=1:  86%|████████▋ | 864/1000 [00:11<00:01, 73.08it/s]Measuring inference for batch_size=1:  87%|████████▋ | 872/1000 [00:11<00:01, 73.08it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [00:12<00:01, 73.06it/s]Measuring inference for batch_size=1:  89%|████████▉ | 888/1000 [00:12<00:01, 73.06it/s]Measuring inference for batch_size=1:  90%|████████▉ | 896/1000 [00:12<00:01, 73.07it/s]Measuring inference for batch_size=1:  90%|█████████ | 904/1000 [00:12<00:01, 73.05it/s]Measuring inference for batch_size=1:  91%|█████████ | 912/1000 [00:12<00:01, 73.07it/s]Measuring inference for batch_size=1:  92%|█████████▏| 920/1000 [00:12<00:01, 73.09it/s]Measuring inference for batch_size=1:  93%|█████████▎| 928/1000 [00:12<00:00, 73.10it/s]Measuring inference for batch_size=1:  94%|█████████▎| 936/1000 [00:12<00:00, 73.14it/s]Measuring inference for batch_size=1:  94%|█████████▍| 944/1000 [00:12<00:00, 73.12it/s]Measuring inference for batch_size=1:  95%|█████████▌| 952/1000 [00:13<00:00, 73.12it/s]Measuring inference for batch_size=1:  96%|█████████▌| 960/1000 [00:13<00:00, 73.11it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [00:13<00:00, 73.15it/s]Measuring inference for batch_size=1:  98%|█████████▊| 976/1000 [00:13<00:00, 73.15it/s]Measuring inference for batch_size=1:  98%|█████████▊| 984/1000 [00:13<00:00, 73.15it/s]Measuring inference for batch_size=1:  99%|█████████▉| 992/1000 [00:13<00:00, 73.10it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 73.08it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 73.06it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=16:   8%|▊         | 8/100 [00:00<00:01, 72.51it/s]Warming up with batch_size=16:  16%|█▌        | 16/100 [00:00<00:01, 72.64it/s]Warming up with batch_size=16:  24%|██▍       | 24/100 [00:00<00:01, 72.68it/s]Warming up with batch_size=16:  32%|███▏      | 32/100 [00:00<00:00, 72.67it/s]Warming up with batch_size=16:  40%|████      | 40/100 [00:00<00:00, 72.72it/s]Warming up with batch_size=16:  48%|████▊     | 48/100 [00:00<00:00, 72.70it/s]Warming up with batch_size=16:  56%|█████▌    | 56/100 [00:00<00:00, 72.71it/s]Warming up with batch_size=16:  64%|██████▍   | 64/100 [00:00<00:00, 72.73it/s]Warming up with batch_size=16:  72%|███████▏  | 72/100 [00:00<00:00, 72.74it/s]Warming up with batch_size=16:  80%|████████  | 80/100 [00:01<00:00, 72.75it/s]Warming up with batch_size=16:  88%|████████▊ | 88/100 [00:01<00:00, 72.75it/s]Warming up with batch_size=16:  96%|█████████▌| 96/100 [00:01<00:00, 72.72it/s]Warming up with batch_size=16: 100%|██████████| 100/100 [00:01<00:00, 72.71it/s]
STAGE:2024-02-23 09:58:29 177214:177214 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:58:29 177214:177214 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:58:29 177214:177214 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=16:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=16:   1%|          | 8/1000 [00:00<00:13, 71.42it/s]Measuring inference for batch_size=16:   2%|▏         | 16/1000 [00:00<00:13, 71.63it/s]Measuring inference for batch_size=16:   2%|▏         | 24/1000 [00:00<00:13, 71.67it/s]Measuring inference for batch_size=16:   3%|▎         | 32/1000 [00:00<00:13, 71.75it/s]Measuring inference for batch_size=16:   4%|▍         | 40/1000 [00:00<00:13, 71.79it/s]Measuring inference for batch_size=16:   5%|▍         | 48/1000 [00:00<00:13, 71.81it/s]Measuring inference for batch_size=16:   6%|▌         | 56/1000 [00:00<00:13, 71.84it/s]Measuring inference for batch_size=16:   6%|▋         | 64/1000 [00:00<00:13, 71.83it/s]Measuring inference for batch_size=16:   7%|▋         | 72/1000 [00:01<00:12, 71.85it/s]Measuring inference for batch_size=16:   8%|▊         | 80/1000 [00:01<00:12, 71.87it/s]Measuring inference for batch_size=16:   9%|▉         | 88/1000 [00:01<00:12, 71.90it/s]Measuring inference for batch_size=16:  10%|▉         | 96/1000 [00:01<00:12, 71.88it/s]Measuring inference for batch_size=16:  10%|█         | 104/1000 [00:01<00:12, 71.88it/s]Measuring inference for batch_size=16:  11%|█         | 112/1000 [00:01<00:12, 71.89it/s]Measuring inference for batch_size=16:  12%|█▏        | 120/1000 [00:01<00:12, 71.91it/s]Measuring inference for batch_size=16:  13%|█▎        | 128/1000 [00:01<00:12, 71.94it/s]Measuring inference for batch_size=16:  14%|█▎        | 136/1000 [00:01<00:12, 71.96it/s]Measuring inference for batch_size=16:  14%|█▍        | 144/1000 [00:02<00:11, 71.91it/s]Measuring inference for batch_size=16:  15%|█▌        | 152/1000 [00:02<00:11, 71.92it/s]Measuring inference for batch_size=16:  16%|█▌        | 160/1000 [00:02<00:11, 71.89it/s]Measuring inference for batch_size=16:  17%|█▋        | 168/1000 [00:02<00:11, 71.89it/s]Measuring inference for batch_size=16:  18%|█▊        | 176/1000 [00:02<00:11, 71.88it/s]Measuring inference for batch_size=16:  18%|█▊        | 184/1000 [00:02<00:11, 71.91it/s]Measuring inference for batch_size=16:  19%|█▉        | 192/1000 [00:02<00:11, 71.92it/s]Measuring inference for batch_size=16:  20%|██        | 200/1000 [00:02<00:11, 71.94it/s]Measuring inference for batch_size=16:  21%|██        | 208/1000 [00:02<00:11, 71.92it/s]Measuring inference for batch_size=16:  22%|██▏       | 216/1000 [00:03<00:10, 71.92it/s]Measuring inference for batch_size=16:  22%|██▏       | 224/1000 [00:03<00:10, 71.94it/s]Measuring inference for batch_size=16:  23%|██▎       | 232/1000 [00:03<00:10, 71.99it/s]Measuring inference for batch_size=16:  24%|██▍       | 240/1000 [00:03<00:10, 72.02it/s]Measuring inference for batch_size=16:  25%|██▍       | 248/1000 [00:03<00:10, 72.11it/s]Measuring inference for batch_size=16:  26%|██▌       | 256/1000 [00:03<00:10, 72.10it/s]Measuring inference for batch_size=16:  26%|██▋       | 264/1000 [00:03<00:10, 72.12it/s]Measuring inference for batch_size=16:  27%|██▋       | 272/1000 [00:03<00:10, 72.10it/s]Measuring inference for batch_size=16:  28%|██▊       | 280/1000 [00:03<00:09, 72.13it/s]Measuring inference for batch_size=16:  29%|██▉       | 288/1000 [00:04<00:09, 72.10it/s]Measuring inference for batch_size=16:  30%|██▉       | 296/1000 [00:04<00:09, 72.09it/s]Measuring inference for batch_size=16:  30%|███       | 304/1000 [00:04<00:09, 72.05it/s]Measuring inference for batch_size=16:  31%|███       | 312/1000 [00:04<00:09, 72.02it/s]Measuring inference for batch_size=16:  32%|███▏      | 320/1000 [00:04<00:09, 72.02it/s]Measuring inference for batch_size=16:  33%|███▎      | 328/1000 [00:04<00:09, 72.04it/s]Measuring inference for batch_size=16:  34%|███▎      | 336/1000 [00:04<00:09, 72.04it/s]Measuring inference for batch_size=16:  34%|███▍      | 344/1000 [00:04<00:09, 72.04it/s]Measuring inference for batch_size=16:  35%|███▌      | 352/1000 [00:04<00:08, 72.05it/s]Measuring inference for batch_size=16:  36%|███▌      | 360/1000 [00:05<00:08, 72.06it/s]Measuring inference for batch_size=16:  37%|███▋      | 368/1000 [00:05<00:08, 72.06it/s]Measuring inference for batch_size=16:  38%|███▊      | 376/1000 [00:05<00:08, 72.11it/s]Measuring inference for batch_size=16:  38%|███▊      | 384/1000 [00:05<00:08, 72.12it/s]Measuring inference for batch_size=16:  39%|███▉      | 392/1000 [00:05<00:08, 72.20it/s]Measuring inference for batch_size=16:  40%|████      | 400/1000 [00:05<00:08, 72.20it/s]Measuring inference for batch_size=16:  41%|████      | 408/1000 [00:05<00:08, 72.18it/s]Measuring inference for batch_size=16:  42%|████▏     | 416/1000 [00:05<00:08, 72.17it/s]Measuring inference for batch_size=16:  42%|████▏     | 424/1000 [00:05<00:07, 72.16it/s]Measuring inference for batch_size=16:  43%|████▎     | 432/1000 [00:06<00:07, 72.14it/s]Measuring inference for batch_size=16:  44%|████▍     | 440/1000 [00:06<00:07, 72.09it/s]Measuring inference for batch_size=16:  45%|████▍     | 448/1000 [00:06<00:07, 72.00it/s]Measuring inference for batch_size=16:  46%|████▌     | 456/1000 [00:06<00:07, 71.98it/s]Measuring inference for batch_size=16:  46%|████▋     | 464/1000 [00:06<00:07, 72.01it/s]Measuring inference for batch_size=16:  47%|████▋     | 472/1000 [00:06<00:07, 72.02it/s]Measuring inference for batch_size=16:  48%|████▊     | 480/1000 [00:06<00:07, 72.00it/s]Measuring inference for batch_size=16:  49%|████▉     | 488/1000 [00:06<00:07, 72.02it/s]Measuring inference for batch_size=16:  50%|████▉     | 496/1000 [00:06<00:07, 71.96it/s]Measuring inference for batch_size=16:  50%|█████     | 504/1000 [00:07<00:06, 71.98it/s]Measuring inference for batch_size=16:  51%|█████     | 512/1000 [00:07<00:06, 72.01it/s]Measuring inference for batch_size=16:  52%|█████▏    | 520/1000 [00:07<00:06, 72.02it/s]Measuring inference for batch_size=16:  53%|█████▎    | 528/1000 [00:07<00:06, 72.05it/s]Measuring inference for batch_size=16:  54%|█████▎    | 536/1000 [00:07<00:06, 72.14it/s]Measuring inference for batch_size=16:  54%|█████▍    | 544/1000 [00:07<00:06, 72.17it/s]Measuring inference for batch_size=16:  55%|█████▌    | 552/1000 [00:07<00:06, 72.20it/s]Measuring inference for batch_size=16:  56%|█████▌    | 560/1000 [00:07<00:06, 72.14it/s]Measuring inference for batch_size=16:  57%|█████▋    | 568/1000 [00:07<00:05, 72.14it/s]Measuring inference for batch_size=16:  58%|█████▊    | 576/1000 [00:07<00:05, 72.12it/s]Measuring inference for batch_size=16:  58%|█████▊    | 584/1000 [00:08<00:05, 72.11it/s]Measuring inference for batch_size=16:  59%|█████▉    | 592/1000 [00:08<00:05, 72.05it/s]Measuring inference for batch_size=16:  60%|██████    | 600/1000 [00:08<00:05, 72.06it/s]Measuring inference for batch_size=16:  61%|██████    | 608/1000 [00:08<00:05, 72.05it/s]Measuring inference for batch_size=16:  62%|██████▏   | 616/1000 [00:08<00:05, 72.03it/s]Measuring inference for batch_size=16:  62%|██████▏   | 624/1000 [00:08<00:05, 72.02it/s]Measuring inference for batch_size=16:  63%|██████▎   | 632/1000 [00:08<00:05, 72.00it/s]Measuring inference for batch_size=16:  64%|██████▍   | 640/1000 [00:08<00:05, 71.97it/s]Measuring inference for batch_size=16:  65%|██████▍   | 648/1000 [00:08<00:04, 72.01it/s]Measuring inference for batch_size=16:  66%|██████▌   | 656/1000 [00:09<00:04, 72.00it/s]Measuring inference for batch_size=16:  66%|██████▋   | 664/1000 [00:09<00:04, 72.04it/s]Measuring inference for batch_size=16:  67%|██████▋   | 672/1000 [00:09<00:04, 72.05it/s]Measuring inference for batch_size=16:  68%|██████▊   | 680/1000 [00:09<00:04, 72.07it/s]Measuring inference for batch_size=16:  69%|██████▉   | 688/1000 [00:09<00:04, 72.04it/s]Measuring inference for batch_size=16:  70%|██████▉   | 696/1000 [00:09<00:04, 72.06it/s]Measuring inference for batch_size=16:  70%|███████   | 704/1000 [00:09<00:04, 72.06it/s]Measuring inference for batch_size=16:  71%|███████   | 712/1000 [00:09<00:03, 72.07it/s]Measuring inference for batch_size=16:  72%|███████▏  | 720/1000 [00:09<00:03, 72.02it/s]Measuring inference for batch_size=16:  73%|███████▎  | 728/1000 [00:10<00:03, 71.97it/s]Measuring inference for batch_size=16:  74%|███████▎  | 736/1000 [00:10<00:03, 71.91it/s]Measuring inference for batch_size=16:  74%|███████▍  | 744/1000 [00:10<00:03, 71.90it/s]Measuring inference for batch_size=16:  75%|███████▌  | 752/1000 [00:10<00:03, 71.88it/s]Measuring inference for batch_size=16:  76%|███████▌  | 760/1000 [00:10<00:03, 71.87it/s]Measuring inference for batch_size=16:  77%|███████▋  | 768/1000 [00:10<00:03, 71.89it/s]Measuring inference for batch_size=16:  78%|███████▊  | 776/1000 [00:10<00:03, 71.92it/s]Measuring inference for batch_size=16:  78%|███████▊  | 784/1000 [00:10<00:03, 71.90it/s]Measuring inference for batch_size=16:  79%|███████▉  | 792/1000 [00:11<00:02, 71.90it/s]Measuring inference for batch_size=16:  80%|████████  | 800/1000 [00:11<00:02, 71.94it/s]Measuring inference for batch_size=16:  81%|████████  | 808/1000 [00:11<00:02, 71.99it/s]Measuring inference for batch_size=16:  82%|████████▏ | 816/1000 [00:11<00:02, 72.00it/s]Measuring inference for batch_size=16:  82%|████████▏ | 824/1000 [00:11<00:02, 72.05it/s]Measuring inference for batch_size=16:  83%|████████▎ | 832/1000 [00:11<00:02, 72.07it/s]Measuring inference for batch_size=16:  84%|████████▍ | 840/1000 [00:11<00:02, 72.11it/s]Measuring inference for batch_size=16:  85%|████████▍ | 848/1000 [00:11<00:02, 72.09it/s]Measuring inference for batch_size=16:  86%|████████▌ | 856/1000 [00:11<00:01, 72.13it/s]Measuring inference for batch_size=16:  86%|████████▋ | 864/1000 [00:11<00:01, 72.09it/s]Measuring inference for batch_size=16:  87%|████████▋ | 872/1000 [00:12<00:01, 72.07it/s]Measuring inference for batch_size=16:  88%|████████▊ | 880/1000 [00:12<00:01, 72.01it/s]Measuring inference for batch_size=16:  89%|████████▉ | 888/1000 [00:12<00:01, 72.02it/s]Measuring inference for batch_size=16:  90%|████████▉ | 896/1000 [00:12<00:01, 72.03it/s]Measuring inference for batch_size=16:  90%|█████████ | 904/1000 [00:12<00:01, 72.01it/s]Measuring inference for batch_size=16:  91%|█████████ | 912/1000 [00:12<00:01, 71.95it/s]Measuring inference for batch_size=16:  92%|█████████▏| 920/1000 [00:12<00:01, 71.95it/s]Measuring inference for batch_size=16:  93%|█████████▎| 928/1000 [00:12<00:01, 71.93it/s]Measuring inference for batch_size=16:  94%|█████████▎| 936/1000 [00:12<00:00, 71.95it/s]Measuring inference for batch_size=16:  94%|█████████▍| 944/1000 [00:13<00:00, 71.94it/s]Measuring inference for batch_size=16:  95%|█████████▌| 952/1000 [00:13<00:00, 71.99it/s]Measuring inference for batch_size=16:  96%|█████████▌| 960/1000 [00:13<00:00, 72.02it/s]Measuring inference for batch_size=16:  97%|█████████▋| 968/1000 [00:13<00:00, 72.07it/s]Measuring inference for batch_size=16:  98%|█████████▊| 976/1000 [00:13<00:00, 72.10it/s]Measuring inference for batch_size=16:  98%|█████████▊| 984/1000 [00:13<00:00, 72.13it/s]Measuring inference for batch_size=16:  99%|█████████▉| 992/1000 [00:13<00:00, 72.09it/s]Measuring inference for batch_size=16: 100%|██████████| 1000/1000 [00:13<00:00, 72.14it/s]Measuring inference for batch_size=16: 100%|██████████| 1000/1000 [00:13<00:00, 72.01it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 11580687848
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.53 GB
    total: 31.28 GB
    used: 3.29 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 374.76 MB
    max_inference_bytes: 392962560
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
  batch_size_16:
    max_inference: 378.48 MB
    max_inference_bytes: 396866048
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
params: 60192808
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 93.918 us +/- 5.313 us [91.314 us, 221.968 us]
        batches_per_second: 10.67 K +/- 413.03 [4.51 K, 10.95 K]
      metrics:
        batches_per_second_max: 10951.185378590078
        batches_per_second_mean: 10669.33482120589
        batches_per_second_min: 4505.160042964554
        batches_per_second_std: 413.0296781794273
        seconds_per_batch_max: 0.0002219676971435547
        seconds_per_batch_mean: 9.39178466796875e-05
        seconds_per_batch_min: 9.131431579589844e-05
        seconds_per_batch_std: 5.3131921369961595e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.447 us +/- 0.623 us [22.650 us, 32.663 us]
        batches_per_second: 42.68 K +/- 958.41 [30.62 K, 44.15 K]
      metrics:
        batches_per_second_max: 44150.56842105263
        batches_per_second_mean: 42675.46186032678
        batches_per_second_min: 30615.357664233576
        batches_per_second_std: 958.410595255576
        seconds_per_batch_max: 3.266334533691406e-05
        seconds_per_batch_mean: 2.344655990600586e-05
        seconds_per_batch_min: 2.2649765014648438e-05
        seconds_per_batch_std: 6.22954305648348e-07
    on_device_inference:
      human_readable:
        batch_latency: -13553825.852 us +/- 37.090 ms [-14255359.650 us, -13452223.778
          us]
        batches_per_second: -0.07 +/- 0.00 [-0.07, -0.07]
      metrics:
        batches_per_second_max: -0.07014905443118559
        batches_per_second_mean: -0.07378044910839897
        batches_per_second_min: -0.07433715172449337
        batches_per_second_std: 0.00019845310582753604
        seconds_per_batch_max: -13.452223777770996
        seconds_per_batch_mean: -13.553825852394104
        seconds_per_batch_min: -14.255359649658203
        seconds_per_batch_std: 0.037090304170777216
    total:
      human_readable:
        batch_latency: 13.679 ms +/- 40.432 us [13.575 ms, 14.526 ms]
        batches_per_second: 73.11 +/- 0.21 [68.84, 73.66]
      metrics:
        batches_per_second_max: 73.66311315618469
        batches_per_second_mean: 73.10511328302222
        batches_per_second_min: 68.84259593605357
        batches_per_second_std: 0.21069235246809645
        seconds_per_batch_max: 0.014525890350341797
        seconds_per_batch_mean: 0.013679050207138062
        seconds_per_batch_min: 0.013575315475463867
        seconds_per_batch_std: 4.043237342694234e-05
  batch_size_16:
    cpu_to_gpu:
      human_readable:
        batch_latency: 145.158 us +/- 5.723 us [142.336 us, 284.672 us]
        batches_per_second: 6.90 K +/- 196.06 [3.51 K, 7.03 K]
      metrics:
        batches_per_second_max: 7025.634840871022
        batches_per_second_mean: 6896.405724492579
        batches_per_second_min: 3512.817420435511
        batches_per_second_std: 196.06102264131383
        seconds_per_batch_max: 0.0002846717834472656
        seconds_per_batch_mean: 0.0001451582908630371
        seconds_per_batch_min: 0.0001423358917236328
        seconds_per_batch_std: 5.72273009898545e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.499 us +/- 0.648 us [22.411 us, 30.994 us]
        batches_per_second: 42.58 K +/- 1.04 K [32.26 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 42583.46837796324
        batches_per_second_min: 32263.876923076925
        batches_per_second_std: 1037.626614773136
        seconds_per_batch_max: 3.0994415283203125e-05
        seconds_per_batch_mean: 2.3499011993408203e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 6.484878412935743e-07
    on_device_inference:
      human_readable:
        batch_latency: -13704922.169 us +/- 41.475 ms [-14374815.941 us, -13588928.223
          us]
        batches_per_second: -0.07 +/- 0.00 [-0.07, -0.07]
      metrics:
        batches_per_second_max: -0.06956610812370419
        batches_per_second_mean: -0.07296714544405158
        batches_per_second_min: -0.07358932092471737
        batches_per_second_std: 0.00021823236353558085
        seconds_per_batch_max: -13.58892822265625
        seconds_per_batch_mean: -13.70492216873169
        seconds_per_batch_min: -14.374815940856934
        seconds_per_batch_std: 0.041474949753524504
    total:
      human_readable:
        batch_latency: 13.879 ms +/- 44.617 us [13.761 ms, 14.694 ms]
        batches_per_second: 72.05 +/- 0.23 [68.06, 72.67]
      metrics:
        batches_per_second_max: 72.67016650207044
        batches_per_second_mean: 72.05224587135686
        batches_per_second_min: 68.05510214015673
        batches_per_second_std: 0.22746009021068253
        seconds_per_batch_max: 0.014693975448608398
        seconds_per_batch_mean: 0.013878958702087402
        seconds_per_batch_min: 0.013760805130004883
        seconds_per_batch_std: 4.4616586032973696e-05


#####
baseline-baseline-py-id - Run 3
2024-02-23 09:59:20
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.84it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.83it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  60.19 M, 100.000% Params, 11.58 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.016% Params, 118.01 MMac, 1.019% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.000% Params, 1.61 MMac, 0.014% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.007% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.007% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.359% Params, 680.39 MMac, 5.875% MACs, 
    (0): Bottleneck(
      75.01 k, 0.125% Params, 236.43 MMac, 2.042% MACs, 
      (conv1): Conv2d(4.1 k, 0.007% Params, 12.85 MMac, 0.111% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.028% Params, 52.99 MMac, 0.458% MACs, 
        (0): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    2.34 M, 3.887% Params, 1.92 GMac, 16.555% MACs, 
    (0): Bottleneck(
      379.39 k, 0.630% Params, 376.02 MMac, 3.247% MACs, 
      (conv1): Conv2d(32.77 k, 0.054% Params, 102.76 MMac, 0.887% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 802.82 KMac, 0.007% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.008% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.219% Params, 103.56 MMac, 0.894% MACs, 
        (0): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (4): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (5): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (6): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (7): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    40.61 M, 67.473% Params, 8.05 GMac, 69.501% MACs, 
    (0): Bottleneck(
      1.51 M, 2.513% Params, 374.26 MMac, 3.232% MACs, 
      (conv1): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 401.41 KMac, 0.003% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.004% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 0.874% Params, 103.16 MMac, 0.891% MACs, 
        (0): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (6): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (7): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (8): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (9): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (10): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (11): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (12): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (13): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (14): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (15): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (16): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (17): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (18): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (19): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (20): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (21): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (22): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (23): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (24): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (25): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (26): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (27): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (28): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (29): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (30): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (31): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (32): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (33): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (34): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (35): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 24.861% Params, 811.02 MMac, 7.003% MACs, 
    (0): Bottleneck(
      6.04 M, 10.034% Params, 373.38 MMac, 3.224% MACs, 
      (conv1): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 200.7 KMac, 0.002% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.002% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 3.491% Params, 102.96 MMac, 0.889% MACs, 
        (0): Conv2d(2.1 M, 3.484% Params, 102.76 MMac, 0.887% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.001% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 3.404% Params, 2.05 MMac, 0.018% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  11%|█         | 11/100 [00:00<00:00, 100.65it/s]Warming up with batch_size=1:  22%|██▏       | 22/100 [00:00<00:00, 100.66it/s]Warming up with batch_size=1:  33%|███▎      | 33/100 [00:00<00:00, 100.86it/s]Warming up with batch_size=1:  44%|████▍     | 44/100 [00:00<00:00, 100.99it/s]Warming up with batch_size=1:  55%|█████▌    | 55/100 [00:00<00:00, 101.05it/s]Warming up with batch_size=1:  66%|██████▌   | 66/100 [00:00<00:00, 101.10it/s]Warming up with batch_size=1:  77%|███████▋  | 77/100 [00:00<00:00, 101.11it/s]Warming up with batch_size=1:  88%|████████▊ | 88/100 [00:00<00:00, 101.14it/s]Warming up with batch_size=1:  99%|█████████▉| 99/100 [00:00<00:00, 101.18it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 101.05it/s]
STAGE:2024-02-23 09:58:51 177260:177260 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:58:51 177260:177260 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:58:51 177260:177260 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   1%|          | 8/1000 [00:00<00:13, 75.37it/s]Measuring inference for batch_size=1:   2%|▏         | 16/1000 [00:00<00:12, 75.76it/s]Measuring inference for batch_size=1:   2%|▏         | 24/1000 [00:00<00:12, 75.92it/s]Measuring inference for batch_size=1:   3%|▎         | 32/1000 [00:00<00:12, 75.99it/s]Measuring inference for batch_size=1:   4%|▍         | 40/1000 [00:00<00:12, 76.05it/s]Measuring inference for batch_size=1:   5%|▍         | 48/1000 [00:00<00:12, 76.07it/s]Measuring inference for batch_size=1:   6%|▌         | 56/1000 [00:00<00:12, 76.03it/s]Measuring inference for batch_size=1:   6%|▋         | 64/1000 [00:00<00:12, 76.07it/s]Measuring inference for batch_size=1:   7%|▋         | 72/1000 [00:00<00:12, 76.11it/s]Measuring inference for batch_size=1:   8%|▊         | 80/1000 [00:01<00:12, 76.11it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:01<00:11, 76.10it/s]Measuring inference for batch_size=1:  10%|▉         | 96/1000 [00:01<00:11, 76.12it/s]Measuring inference for batch_size=1:  10%|█         | 104/1000 [00:01<00:11, 76.09it/s]Measuring inference for batch_size=1:  11%|█         | 112/1000 [00:01<00:11, 76.10it/s]Measuring inference for batch_size=1:  12%|█▏        | 120/1000 [00:01<00:11, 76.13it/s]Measuring inference for batch_size=1:  13%|█▎        | 128/1000 [00:01<00:11, 76.15it/s]Measuring inference for batch_size=1:  14%|█▎        | 136/1000 [00:01<00:11, 76.18it/s]Measuring inference for batch_size=1:  14%|█▍        | 144/1000 [00:01<00:11, 76.21it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:01<00:11, 76.21it/s]Measuring inference for batch_size=1:  16%|█▌        | 160/1000 [00:02<00:11, 76.22it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:02<00:10, 76.24it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:02<00:10, 76.21it/s]Measuring inference for batch_size=1:  18%|█▊        | 184/1000 [00:02<00:10, 76.21it/s]Measuring inference for batch_size=1:  19%|█▉        | 192/1000 [00:02<00:10, 76.23it/s]Measuring inference for batch_size=1:  20%|██        | 200/1000 [00:02<00:10, 76.23it/s]Measuring inference for batch_size=1:  21%|██        | 208/1000 [00:02<00:10, 76.22it/s]Measuring inference for batch_size=1:  22%|██▏       | 216/1000 [00:02<00:10, 76.18it/s]Measuring inference for batch_size=1:  22%|██▏       | 224/1000 [00:02<00:10, 76.19it/s]Measuring inference for batch_size=1:  23%|██▎       | 232/1000 [00:03<00:10, 76.19it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:03<00:09, 76.21it/s]Measuring inference for batch_size=1:  25%|██▍       | 248/1000 [00:03<00:09, 76.21it/s]Measuring inference for batch_size=1:  26%|██▌       | 256/1000 [00:03<00:09, 76.22it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:03<00:09, 76.21it/s]Measuring inference for batch_size=1:  27%|██▋       | 272/1000 [00:03<00:09, 76.16it/s]Measuring inference for batch_size=1:  28%|██▊       | 280/1000 [00:03<00:09, 76.09it/s]Measuring inference for batch_size=1:  29%|██▉       | 288/1000 [00:03<00:09, 76.05it/s]Measuring inference for batch_size=1:  30%|██▉       | 296/1000 [00:03<00:09, 76.03it/s]Measuring inference for batch_size=1:  30%|███       | 304/1000 [00:03<00:09, 76.00it/s]Measuring inference for batch_size=1:  31%|███       | 312/1000 [00:04<00:09, 75.99it/s]Measuring inference for batch_size=1:  32%|███▏      | 320/1000 [00:04<00:08, 75.99it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:04<00:08, 76.00it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:04<00:08, 76.00it/s]Measuring inference for batch_size=1:  34%|███▍      | 344/1000 [00:04<00:08, 75.96it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:04<00:08, 75.94it/s]Measuring inference for batch_size=1:  36%|███▌      | 360/1000 [00:04<00:08, 75.95it/s]Measuring inference for batch_size=1:  37%|███▋      | 368/1000 [00:04<00:08, 75.95it/s]Measuring inference for batch_size=1:  38%|███▊      | 376/1000 [00:04<00:08, 75.94it/s]Measuring inference for batch_size=1:  38%|███▊      | 384/1000 [00:05<00:08, 75.96it/s]Measuring inference for batch_size=1:  39%|███▉      | 392/1000 [00:05<00:08, 75.97it/s]Measuring inference for batch_size=1:  40%|████      | 400/1000 [00:05<00:07, 76.01it/s]Measuring inference for batch_size=1:  41%|████      | 408/1000 [00:05<00:07, 76.06it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:05<00:07, 76.10it/s]Measuring inference for batch_size=1:  42%|████▏     | 424/1000 [00:05<00:07, 76.09it/s]Measuring inference for batch_size=1:  43%|████▎     | 432/1000 [00:05<00:07, 76.09it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:05<00:07, 76.13it/s]Measuring inference for batch_size=1:  45%|████▍     | 448/1000 [00:05<00:07, 76.15it/s]Measuring inference for batch_size=1:  46%|████▌     | 456/1000 [00:05<00:07, 76.13it/s]Measuring inference for batch_size=1:  46%|████▋     | 464/1000 [00:06<00:07, 76.13it/s]Measuring inference for batch_size=1:  47%|████▋     | 472/1000 [00:06<00:06, 76.15it/s]Measuring inference for batch_size=1:  48%|████▊     | 480/1000 [00:06<00:06, 76.14it/s]Measuring inference for batch_size=1:  49%|████▉     | 488/1000 [00:06<00:06, 76.15it/s]Measuring inference for batch_size=1:  50%|████▉     | 496/1000 [00:06<00:06, 76.12it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:06<00:06, 76.08it/s]Measuring inference for batch_size=1:  51%|█████     | 512/1000 [00:06<00:06, 76.08it/s]Measuring inference for batch_size=1:  52%|█████▏    | 520/1000 [00:06<00:06, 76.11it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:06<00:06, 76.11it/s]Measuring inference for batch_size=1:  54%|█████▎    | 536/1000 [00:07<00:06, 76.10it/s]Measuring inference for batch_size=1:  54%|█████▍    | 544/1000 [00:07<00:05, 76.07it/s]Measuring inference for batch_size=1:  55%|█████▌    | 552/1000 [00:07<00:05, 76.09it/s]Measuring inference for batch_size=1:  56%|█████▌    | 560/1000 [00:07<00:05, 76.12it/s]Measuring inference for batch_size=1:  57%|█████▋    | 568/1000 [00:07<00:05, 76.14it/s]Measuring inference for batch_size=1:  58%|█████▊    | 576/1000 [00:07<00:05, 76.15it/s]Measuring inference for batch_size=1:  58%|█████▊    | 584/1000 [00:07<00:05, 76.16it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [00:07<00:05, 76.17it/s]Measuring inference for batch_size=1:  60%|██████    | 600/1000 [00:07<00:05, 76.18it/s]Measuring inference for batch_size=1:  61%|██████    | 608/1000 [00:07<00:05, 76.15it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:08<00:05, 76.15it/s]Measuring inference for batch_size=1:  62%|██████▏   | 624/1000 [00:08<00:04, 76.14it/s]Measuring inference for batch_size=1:  63%|██████▎   | 632/1000 [00:08<00:04, 76.15it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [00:08<00:04, 76.17it/s]Measuring inference for batch_size=1:  65%|██████▍   | 648/1000 [00:08<00:04, 76.16it/s]Measuring inference for batch_size=1:  66%|██████▌   | 656/1000 [00:08<00:04, 76.14it/s]Measuring inference for batch_size=1:  66%|██████▋   | 664/1000 [00:08<00:04, 76.12it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:08<00:04, 76.10it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [00:08<00:04, 76.13it/s]Measuring inference for batch_size=1:  69%|██████▉   | 688/1000 [00:09<00:04, 76.14it/s]Measuring inference for batch_size=1:  70%|██████▉   | 696/1000 [00:09<00:03, 76.11it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [00:09<00:03, 76.11it/s]Measuring inference for batch_size=1:  71%|███████   | 712/1000 [00:09<00:03, 76.11it/s]Measuring inference for batch_size=1:  72%|███████▏  | 720/1000 [00:09<00:03, 76.10it/s]Measuring inference for batch_size=1:  73%|███████▎  | 728/1000 [00:09<00:03, 76.13it/s]Measuring inference for batch_size=1:  74%|███████▎  | 736/1000 [00:09<00:03, 76.12it/s]Measuring inference for batch_size=1:  74%|███████▍  | 744/1000 [00:09<00:03, 76.16it/s]Measuring inference for batch_size=1:  75%|███████▌  | 752/1000 [00:09<00:03, 76.16it/s]Measuring inference for batch_size=1:  76%|███████▌  | 760/1000 [00:09<00:03, 76.19it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [00:10<00:03, 76.18it/s]Measuring inference for batch_size=1:  78%|███████▊  | 776/1000 [00:10<00:02, 76.17it/s]Measuring inference for batch_size=1:  78%|███████▊  | 784/1000 [00:10<00:02, 76.13it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [00:10<00:02, 76.15it/s]Measuring inference for batch_size=1:  80%|████████  | 800/1000 [00:10<00:02, 76.12it/s]Measuring inference for batch_size=1:  81%|████████  | 808/1000 [00:10<00:02, 76.12it/s]Measuring inference for batch_size=1:  82%|████████▏ | 816/1000 [00:10<00:02, 76.13it/s]Measuring inference for batch_size=1:  82%|████████▏ | 824/1000 [00:10<00:02, 76.15it/s]Measuring inference for batch_size=1:  83%|████████▎ | 832/1000 [00:10<00:02, 76.17it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:11<00:02, 76.20it/s]Measuring inference for batch_size=1:  85%|████████▍ | 848/1000 [00:11<00:01, 76.18it/s]Measuring inference for batch_size=1:  86%|████████▌ | 856/1000 [00:11<00:01, 76.22it/s]Measuring inference for batch_size=1:  86%|████████▋ | 864/1000 [00:11<00:01, 76.22it/s]Measuring inference for batch_size=1:  87%|████████▋ | 872/1000 [00:11<00:01, 76.25it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [00:11<00:01, 76.24it/s]Measuring inference for batch_size=1:  89%|████████▉ | 888/1000 [00:11<00:01, 76.27it/s]Measuring inference for batch_size=1:  90%|████████▉ | 896/1000 [00:11<00:01, 76.28it/s]Measuring inference for batch_size=1:  90%|█████████ | 904/1000 [00:11<00:01, 76.31it/s]Measuring inference for batch_size=1:  91%|█████████ | 912/1000 [00:11<00:01, 76.32it/s]Measuring inference for batch_size=1:  92%|█████████▏| 920/1000 [00:12<00:01, 76.31it/s]Measuring inference for batch_size=1:  93%|█████████▎| 928/1000 [00:12<00:00, 76.27it/s]Measuring inference for batch_size=1:  94%|█████████▎| 936/1000 [00:12<00:00, 76.29it/s]Measuring inference for batch_size=1:  94%|█████████▍| 944/1000 [00:12<00:00, 76.30it/s]Measuring inference for batch_size=1:  95%|█████████▌| 952/1000 [00:12<00:00, 76.31it/s]Measuring inference for batch_size=1:  96%|█████████▌| 960/1000 [00:12<00:00, 76.32it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [00:12<00:00, 76.28it/s]Measuring inference for batch_size=1:  98%|█████████▊| 976/1000 [00:12<00:00, 76.28it/s]Measuring inference for batch_size=1:  98%|█████████▊| 984/1000 [00:12<00:00, 76.29it/s]Measuring inference for batch_size=1:  99%|█████████▉| 992/1000 [00:13<00:00, 76.26it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 76.25it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 76.14it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=16:   8%|▊         | 8/100 [00:00<00:01, 75.08it/s]Warming up with batch_size=16:  16%|█▌        | 16/100 [00:00<00:01, 75.23it/s]Warming up with batch_size=16:  24%|██▍       | 24/100 [00:00<00:01, 75.24it/s]Warming up with batch_size=16:  32%|███▏      | 32/100 [00:00<00:00, 75.30it/s]Warming up with batch_size=16:  40%|████      | 40/100 [00:00<00:00, 75.34it/s]Warming up with batch_size=16:  48%|████▊     | 48/100 [00:00<00:00, 75.39it/s]Warming up with batch_size=16:  56%|█████▌    | 56/100 [00:00<00:00, 75.41it/s]Warming up with batch_size=16:  64%|██████▍   | 64/100 [00:00<00:00, 75.45it/s]Warming up with batch_size=16:  72%|███████▏  | 72/100 [00:00<00:00, 75.44it/s]Warming up with batch_size=16:  80%|████████  | 80/100 [00:01<00:00, 75.47it/s]Warming up with batch_size=16:  88%|████████▊ | 88/100 [00:01<00:00, 75.45it/s]Warming up with batch_size=16:  96%|█████████▌| 96/100 [00:01<00:00, 75.38it/s]Warming up with batch_size=16: 100%|██████████| 100/100 [00:01<00:00, 75.37it/s]
STAGE:2024-02-23 09:59:06 177260:177260 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:59:06 177260:177260 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:59:06 177260:177260 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=16:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=16:   1%|          | 8/1000 [00:00<00:13, 74.15it/s]Measuring inference for batch_size=16:   2%|▏         | 16/1000 [00:00<00:13, 74.38it/s]Measuring inference for batch_size=16:   2%|▏         | 24/1000 [00:00<00:13, 74.57it/s]Measuring inference for batch_size=16:   3%|▎         | 32/1000 [00:00<00:12, 74.63it/s]Measuring inference for batch_size=16:   4%|▍         | 40/1000 [00:00<00:12, 74.67it/s]Measuring inference for batch_size=16:   5%|▍         | 48/1000 [00:00<00:12, 74.75it/s]Measuring inference for batch_size=16:   6%|▌         | 56/1000 [00:00<00:12, 74.77it/s]Measuring inference for batch_size=16:   6%|▋         | 64/1000 [00:00<00:12, 74.82it/s]Measuring inference for batch_size=16:   7%|▋         | 72/1000 [00:00<00:12, 74.87it/s]Measuring inference for batch_size=16:   8%|▊         | 80/1000 [00:01<00:12, 74.87it/s]Measuring inference for batch_size=16:   9%|▉         | 88/1000 [00:01<00:12, 74.88it/s]Measuring inference for batch_size=16:  10%|▉         | 96/1000 [00:01<00:12, 74.88it/s]Measuring inference for batch_size=16:  10%|█         | 104/1000 [00:01<00:11, 74.92it/s]Measuring inference for batch_size=16:  11%|█         | 112/1000 [00:01<00:11, 74.93it/s]Measuring inference for batch_size=16:  12%|█▏        | 120/1000 [00:01<00:11, 74.92it/s]Measuring inference for batch_size=16:  13%|█▎        | 128/1000 [00:01<00:11, 74.89it/s]Measuring inference for batch_size=16:  14%|█▎        | 136/1000 [00:01<00:11, 74.87it/s]Measuring inference for batch_size=16:  14%|█▍        | 144/1000 [00:01<00:11, 74.86it/s]Measuring inference for batch_size=16:  15%|█▌        | 152/1000 [00:02<00:11, 74.87it/s]Measuring inference for batch_size=16:  16%|█▌        | 160/1000 [00:02<00:11, 74.91it/s]Measuring inference for batch_size=16:  17%|█▋        | 168/1000 [00:02<00:11, 74.92it/s]Measuring inference for batch_size=16:  18%|█▊        | 176/1000 [00:02<00:10, 74.93it/s]Measuring inference for batch_size=16:  18%|█▊        | 184/1000 [00:02<00:10, 74.92it/s]Measuring inference for batch_size=16:  19%|█▉        | 192/1000 [00:02<00:10, 74.91it/s]Measuring inference for batch_size=16:  20%|██        | 200/1000 [00:02<00:10, 74.94it/s]Measuring inference for batch_size=16:  21%|██        | 208/1000 [00:02<00:10, 74.94it/s]Measuring inference for batch_size=16:  22%|██▏       | 216/1000 [00:02<00:10, 74.96it/s]Measuring inference for batch_size=16:  22%|██▏       | 224/1000 [00:02<00:10, 74.98it/s]Measuring inference for batch_size=16:  23%|██▎       | 232/1000 [00:03<00:10, 74.99it/s]Measuring inference for batch_size=16:  24%|██▍       | 240/1000 [00:03<00:10, 74.97it/s]Measuring inference for batch_size=16:  25%|██▍       | 248/1000 [00:03<00:10, 74.94it/s]Measuring inference for batch_size=16:  26%|██▌       | 256/1000 [00:03<00:09, 74.93it/s]Measuring inference for batch_size=16:  26%|██▋       | 264/1000 [00:03<00:09, 74.96it/s]Measuring inference for batch_size=16:  27%|██▋       | 272/1000 [00:03<00:09, 74.92it/s]Measuring inference for batch_size=16:  28%|██▊       | 280/1000 [00:03<00:09, 74.96it/s]Measuring inference for batch_size=16:  29%|██▉       | 288/1000 [00:03<00:09, 74.95it/s]Measuring inference for batch_size=16:  30%|██▉       | 296/1000 [00:03<00:09, 74.94it/s]Measuring inference for batch_size=16:  30%|███       | 304/1000 [00:04<00:09, 74.94it/s]Measuring inference for batch_size=16:  31%|███       | 312/1000 [00:04<00:09, 74.94it/s]Measuring inference for batch_size=16:  32%|███▏      | 320/1000 [00:04<00:09, 74.92it/s]Measuring inference for batch_size=16:  33%|███▎      | 328/1000 [00:04<00:08, 74.91it/s]Measuring inference for batch_size=16:  34%|███▎      | 336/1000 [00:04<00:08, 74.91it/s]Measuring inference for batch_size=16:  34%|███▍      | 344/1000 [00:04<00:08, 74.94it/s]Measuring inference for batch_size=16:  35%|███▌      | 352/1000 [00:04<00:08, 74.93it/s]Measuring inference for batch_size=16:  36%|███▌      | 360/1000 [00:04<00:08, 74.93it/s]Measuring inference for batch_size=16:  37%|███▋      | 368/1000 [00:04<00:08, 74.92it/s]Measuring inference for batch_size=16:  38%|███▊      | 376/1000 [00:05<00:08, 74.90it/s]Measuring inference for batch_size=16:  38%|███▊      | 384/1000 [00:05<00:08, 74.87it/s]Measuring inference for batch_size=16:  39%|███▉      | 392/1000 [00:05<00:08, 74.87it/s]Measuring inference for batch_size=16:  40%|████      | 400/1000 [00:05<00:08, 74.86it/s]Measuring inference for batch_size=16:  41%|████      | 408/1000 [00:05<00:07, 74.82it/s]Measuring inference for batch_size=16:  42%|████▏     | 416/1000 [00:05<00:07, 74.84it/s]Measuring inference for batch_size=16:  42%|████▏     | 424/1000 [00:05<00:07, 74.85it/s]Measuring inference for batch_size=16:  43%|████▎     | 432/1000 [00:05<00:07, 74.88it/s]Measuring inference for batch_size=16:  44%|████▍     | 440/1000 [00:05<00:07, 74.87it/s]Measuring inference for batch_size=16:  45%|████▍     | 448/1000 [00:05<00:07, 74.86it/s]Measuring inference for batch_size=16:  46%|████▌     | 456/1000 [00:06<00:07, 74.89it/s]Measuring inference for batch_size=16:  46%|████▋     | 464/1000 [00:06<00:07, 74.92it/s]Measuring inference for batch_size=16:  47%|████▋     | 472/1000 [00:06<00:07, 74.92it/s]Measuring inference for batch_size=16:  48%|████▊     | 480/1000 [00:06<00:06, 74.95it/s]Measuring inference for batch_size=16:  49%|████▉     | 488/1000 [00:06<00:06, 74.97it/s]Measuring inference for batch_size=16:  50%|████▉     | 496/1000 [00:06<00:06, 75.00it/s]Measuring inference for batch_size=16:  50%|█████     | 504/1000 [00:06<00:06, 75.00it/s]Measuring inference for batch_size=16:  51%|█████     | 512/1000 [00:06<00:06, 74.98it/s]Measuring inference for batch_size=16:  52%|█████▏    | 520/1000 [00:06<00:06, 75.01it/s]Measuring inference for batch_size=16:  53%|█████▎    | 528/1000 [00:07<00:06, 75.03it/s]Measuring inference for batch_size=16:  54%|█████▎    | 536/1000 [00:07<00:06, 75.00it/s]Measuring inference for batch_size=16:  54%|█████▍    | 544/1000 [00:07<00:06, 74.99it/s]Measuring inference for batch_size=16:  55%|█████▌    | 552/1000 [00:07<00:05, 75.00it/s]Measuring inference for batch_size=16:  56%|█████▌    | 560/1000 [00:07<00:05, 74.98it/s]Measuring inference for batch_size=16:  57%|█████▋    | 568/1000 [00:07<00:05, 74.95it/s]Measuring inference for batch_size=16:  58%|█████▊    | 576/1000 [00:07<00:05, 74.97it/s]Measuring inference for batch_size=16:  58%|█████▊    | 584/1000 [00:07<00:05, 74.98it/s]Measuring inference for batch_size=16:  59%|█████▉    | 592/1000 [00:07<00:05, 74.99it/s]Measuring inference for batch_size=16:  60%|██████    | 600/1000 [00:08<00:05, 74.98it/s]Measuring inference for batch_size=16:  61%|██████    | 608/1000 [00:08<00:05, 74.99it/s]Measuring inference for batch_size=16:  62%|██████▏   | 616/1000 [00:08<00:05, 74.97it/s]Measuring inference for batch_size=16:  62%|██████▏   | 624/1000 [00:08<00:05, 74.96it/s]Measuring inference for batch_size=16:  63%|██████▎   | 632/1000 [00:08<00:04, 74.93it/s]Measuring inference for batch_size=16:  64%|██████▍   | 640/1000 [00:08<00:04, 74.93it/s]Measuring inference for batch_size=16:  65%|██████▍   | 648/1000 [00:08<00:04, 74.95it/s]Measuring inference for batch_size=16:  66%|██████▌   | 656/1000 [00:08<00:04, 74.93it/s]Measuring inference for batch_size=16:  66%|██████▋   | 664/1000 [00:08<00:04, 74.91it/s]Measuring inference for batch_size=16:  67%|██████▋   | 672/1000 [00:08<00:04, 74.87it/s]Measuring inference for batch_size=16:  68%|██████▊   | 680/1000 [00:09<00:04, 74.85it/s]Measuring inference for batch_size=16:  69%|██████▉   | 688/1000 [00:09<00:04, 74.87it/s]Measuring inference for batch_size=16:  70%|██████▉   | 696/1000 [00:09<00:04, 74.88it/s]Measuring inference for batch_size=16:  70%|███████   | 704/1000 [00:09<00:03, 74.85it/s]Measuring inference for batch_size=16:  71%|███████   | 712/1000 [00:09<00:03, 74.87it/s]Measuring inference for batch_size=16:  72%|███████▏  | 720/1000 [00:09<00:03, 74.91it/s]Measuring inference for batch_size=16:  73%|███████▎  | 728/1000 [00:09<00:03, 74.92it/s]Measuring inference for batch_size=16:  74%|███████▎  | 736/1000 [00:09<00:03, 74.93it/s]Measuring inference for batch_size=16:  74%|███████▍  | 744/1000 [00:09<00:03, 74.92it/s]Measuring inference for batch_size=16:  75%|███████▌  | 752/1000 [00:10<00:03, 74.92it/s]Measuring inference for batch_size=16:  76%|███████▌  | 760/1000 [00:10<00:03, 74.95it/s]Measuring inference for batch_size=16:  77%|███████▋  | 768/1000 [00:10<00:03, 74.95it/s]Measuring inference for batch_size=16:  78%|███████▊  | 776/1000 [00:10<00:02, 74.95it/s]Measuring inference for batch_size=16:  78%|███████▊  | 784/1000 [00:10<00:02, 74.95it/s]Measuring inference for batch_size=16:  79%|███████▉  | 792/1000 [00:10<00:02, 74.94it/s]Measuring inference for batch_size=16:  80%|████████  | 800/1000 [00:10<00:02, 74.93it/s]Measuring inference for batch_size=16:  81%|████████  | 808/1000 [00:10<00:02, 74.88it/s]Measuring inference for batch_size=16:  82%|████████▏ | 816/1000 [00:10<00:02, 74.84it/s]Measuring inference for batch_size=16:  82%|████████▏ | 824/1000 [00:11<00:02, 74.81it/s]Measuring inference for batch_size=16:  83%|████████▎ | 832/1000 [00:11<00:02, 74.78it/s]Measuring inference for batch_size=16:  84%|████████▍ | 840/1000 [00:11<00:02, 74.78it/s]Measuring inference for batch_size=16:  85%|████████▍ | 848/1000 [00:11<00:02, 74.77it/s]Measuring inference for batch_size=16:  86%|████████▌ | 856/1000 [00:11<00:01, 74.75it/s]Measuring inference for batch_size=16:  86%|████████▋ | 864/1000 [00:11<00:01, 74.75it/s]Measuring inference for batch_size=16:  87%|████████▋ | 872/1000 [00:11<00:01, 74.77it/s]Measuring inference for batch_size=16:  88%|████████▊ | 880/1000 [00:11<00:01, 74.74it/s]Measuring inference for batch_size=16:  89%|████████▉ | 888/1000 [00:11<00:01, 74.77it/s]Measuring inference for batch_size=16:  90%|████████▉ | 896/1000 [00:11<00:01, 74.83it/s]Measuring inference for batch_size=16:  90%|█████████ | 904/1000 [00:12<00:01, 74.83it/s]Measuring inference for batch_size=16:  91%|█████████ | 912/1000 [00:12<00:01, 74.87it/s]Measuring inference for batch_size=16:  92%|█████████▏| 920/1000 [00:12<00:01, 74.89it/s]Measuring inference for batch_size=16:  93%|█████████▎| 928/1000 [00:12<00:00, 74.91it/s]Measuring inference for batch_size=16:  94%|█████████▎| 936/1000 [00:12<00:00, 74.88it/s]Measuring inference for batch_size=16:  94%|█████████▍| 944/1000 [00:12<00:00, 74.90it/s]Measuring inference for batch_size=16:  95%|█████████▌| 952/1000 [00:12<00:00, 74.90it/s]Measuring inference for batch_size=16:  96%|█████████▌| 960/1000 [00:12<00:00, 74.90it/s]Measuring inference for batch_size=16:  97%|█████████▋| 968/1000 [00:12<00:00, 74.88it/s]Measuring inference for batch_size=16:  98%|█████████▊| 976/1000 [00:13<00:00, 74.91it/s]Measuring inference for batch_size=16:  98%|█████████▊| 984/1000 [00:13<00:00, 74.88it/s]Measuring inference for batch_size=16:  99%|█████████▉| 992/1000 [00:13<00:00, 74.89it/s]Measuring inference for batch_size=16: 100%|██████████| 1000/1000 [00:13<00:00, 74.90it/s]Measuring inference for batch_size=16: 100%|██████████| 1000/1000 [00:13<00:00, 74.90it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 11580687848
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.52 GB
    total: 31.28 GB
    used: 3.29 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 374.76 MB
    max_inference_bytes: 392962560
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
  batch_size_16:
    max_inference: 378.48 MB
    max_inference_bytes: 396866048
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
params: 60192808
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 91.773 us +/- 5.000 us [89.645 us, 219.107 us]
        batches_per_second: 10.92 K +/- 387.30 [4.56 K, 11.16 K]
      metrics:
        batches_per_second_max: 11155.063829787234
        batches_per_second_mean: 10915.92224406831
        batches_per_second_min: 4563.986942328618
        batches_per_second_std: 387.29700101479796
        seconds_per_batch_max: 0.00021910667419433594
        seconds_per_batch_mean: 9.177327156066895e-05
        seconds_per_batch_min: 8.96453857421875e-05
        seconds_per_batch_std: 5.000140537066174e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 22.529 us +/- 0.577 us [21.696 us, 28.849 us]
        batches_per_second: 44.41 K +/- 976.52 [34.66 K, 46.09 K]
      metrics:
        batches_per_second_max: 46091.25274725275
        batches_per_second_mean: 44412.82348024371
        batches_per_second_min: 34663.669421487604
        batches_per_second_std: 976.5161405339306
        seconds_per_batch_max: 2.8848648071289062e-05
        seconds_per_batch_mean: 2.2528648376464845e-05
        seconds_per_batch_min: 2.1696090698242188e-05
        seconds_per_batch_std: 5.768515719250171e-07
    on_device_inference:
      human_readable:
        batch_latency: -13005847.910 us +/- 38.714 ms [-13760992.050 us, -12920576.096
          us]
        batches_per_second: -0.08 +/- 0.00 [-0.08, -0.07]
      metrics:
        batches_per_second_max: -0.07266917939884872
        batches_per_second_mean: -0.07688915610255387
        batches_per_second_min: -0.07739592976368975
        batches_per_second_std: 0.00022403579748704174
        seconds_per_batch_max: -12.920576095581055
        seconds_per_batch_mean: -13.005847909927368
        seconds_per_batch_min: -13.760992050170898
        seconds_per_batch_std: 0.03871392763145658
    total:
      human_readable:
        batch_latency: 13.126 ms +/- 42.141 us [13.040 ms, 14.025 ms]
        batches_per_second: 76.18 +/- 0.24 [71.30, 76.69]
      metrics:
        batches_per_second_max: 76.6895341183354
        batches_per_second_mean: 76.18467292793687
        batches_per_second_min: 71.30259757921937
        batches_per_second_std: 0.23745629363608917
        seconds_per_batch_max: 0.014024734497070312
        seconds_per_batch_mean: 0.013126131057739257
        seconds_per_batch_min: 0.013039588928222656
        seconds_per_batch_std: 4.2141178025567265e-05
  batch_size_16:
    cpu_to_gpu:
      human_readable:
        batch_latency: 142.694 us +/- 5.127 us [140.190 us, 276.089 us]
        batches_per_second: 7.01 K +/- 174.45 [3.62 K, 7.13 K]
      metrics:
        batches_per_second_max: 7133.170068027211
        batches_per_second_mean: 7014.006426537213
        batches_per_second_min: 3622.0241796200344
        batches_per_second_std: 174.45227011965557
        seconds_per_batch_max: 0.0002760887145996094
        seconds_per_batch_mean: 0.00014269351959228516
        seconds_per_batch_min: 0.00014019012451171875
        seconds_per_batch_std: 5.1270306459310945e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 22.777 us +/- 0.465 us [21.935 us, 29.325 us]
        batches_per_second: 43.92 K +/- 813.45 [34.10 K, 45.59 K]
      metrics:
        batches_per_second_max: 45590.260869565216
        batches_per_second_mean: 43920.30986004125
        batches_per_second_min: 34100.03252032521
        batches_per_second_std: 813.4457669504935
        seconds_per_batch_max: 2.9325485229492188e-05
        seconds_per_batch_mean: 2.2777080535888673e-05
        seconds_per_batch_min: 2.193450927734375e-05
        seconds_per_batch_std: 4.6542604321760626e-07
    on_device_inference:
      human_readable:
        batch_latency: -13173402.481 us +/- 35.811 ms [-13856863.976 us, -13110431.671
          us]
        batches_per_second: -0.08 +/- 0.00 [-0.08, -0.07]
      metrics:
        batches_per_second_max: -0.07216640083689063
        batches_per_second_mean: -0.07591108315243011
        batches_per_second_min: -0.07627513914748542
        batches_per_second_std: 0.0002025373773364891
        seconds_per_batch_max: -13.110431671142578
        seconds_per_batch_mean: -13.173402481079101
        seconds_per_batch_min: -13.856863975524902
        seconds_per_batch_std: 0.03581146519466533
    total:
      human_readable:
        batch_latency: 13.344 ms +/- 38.864 us [13.280 ms, 14.168 ms]
        batches_per_second: 74.94 +/- 0.21 [70.58, 75.30]
      metrics:
        batches_per_second_max: 75.30303955187705
        batches_per_second_mean: 74.93976355445027
        batches_per_second_min: 70.58147244425747
        batches_per_second_std: 0.21249551715146878
        seconds_per_batch_max: 0.014168024063110352
        seconds_per_batch_mean: 0.013344160795211791
        seconds_per_batch_min: 0.01327967643737793
        seconds_per_batch_std: 3.88635162062616e-05


