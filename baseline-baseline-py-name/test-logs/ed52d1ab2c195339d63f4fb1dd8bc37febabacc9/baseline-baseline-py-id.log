#####
baseline-baseline-py-id - Run 1
2024-02-23 10:02:44
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.47it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.47it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 274.15it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:00<00:00, 274.62it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:00<00:00, 275.86it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 275.83it/s]
STAGE:2024-02-23 10:02:32 177589:177589 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 10:02:32 177589:177589 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 10:02:32 177589:177589 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:00<00:04, 207.55it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:00<00:04, 208.04it/s]Measuring inference for batch_size=1:   6%|▋         | 63/1000 [00:00<00:04, 208.04it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:00<00:04, 208.16it/s]Measuring inference for batch_size=1:  10%|█         | 105/1000 [00:00<00:04, 208.23it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:00<00:04, 208.20it/s]Measuring inference for batch_size=1:  15%|█▍        | 147/1000 [00:00<00:04, 208.34it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:00<00:03, 208.38it/s]Measuring inference for batch_size=1:  19%|█▉        | 189/1000 [00:00<00:03, 208.37it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:01<00:03, 208.39it/s]Measuring inference for batch_size=1:  23%|██▎       | 231/1000 [00:01<00:03, 208.45it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:01<00:03, 208.49it/s]Measuring inference for batch_size=1:  27%|██▋       | 273/1000 [00:01<00:03, 208.59it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:01<00:03, 208.59it/s]Measuring inference for batch_size=1:  32%|███▏      | 315/1000 [00:01<00:03, 208.51it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:01<00:03, 208.51it/s]Measuring inference for batch_size=1:  36%|███▌      | 357/1000 [00:01<00:03, 208.38it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:01<00:02, 208.51it/s]Measuring inference for batch_size=1:  40%|███▉      | 399/1000 [00:01<00:02, 208.60it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:02<00:02, 208.62it/s]Measuring inference for batch_size=1:  44%|████▍     | 441/1000 [00:02<00:02, 208.50it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:02<00:02, 208.43it/s]Measuring inference for batch_size=1:  48%|████▊     | 483/1000 [00:02<00:02, 208.50it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:02<00:02, 208.61it/s]Measuring inference for batch_size=1:  52%|█████▎    | 525/1000 [00:02<00:02, 208.75it/s]Measuring inference for batch_size=1:  55%|█████▍    | 546/1000 [00:02<00:02, 208.78it/s]Measuring inference for batch_size=1:  57%|█████▋    | 567/1000 [00:02<00:02, 208.85it/s]Measuring inference for batch_size=1:  59%|█████▉    | 588/1000 [00:02<00:01, 208.93it/s]Measuring inference for batch_size=1:  61%|██████    | 609/1000 [00:02<00:01, 208.87it/s]Measuring inference for batch_size=1:  63%|██████▎   | 630/1000 [00:03<00:01, 208.87it/s]Measuring inference for batch_size=1:  65%|██████▌   | 651/1000 [00:03<00:01, 208.73it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:03<00:01, 208.71it/s]Measuring inference for batch_size=1:  69%|██████▉   | 693/1000 [00:03<00:01, 208.75it/s]Measuring inference for batch_size=1:  71%|███████▏  | 714/1000 [00:03<00:01, 208.83it/s]Measuring inference for batch_size=1:  74%|███████▎  | 735/1000 [00:03<00:01, 208.88it/s]Measuring inference for batch_size=1:  76%|███████▌  | 756/1000 [00:03<00:01, 208.93it/s]Measuring inference for batch_size=1:  78%|███████▊  | 777/1000 [00:03<00:01, 209.02it/s]Measuring inference for batch_size=1:  80%|███████▉  | 798/1000 [00:03<00:00, 209.00it/s]Measuring inference for batch_size=1:  82%|████████▏ | 819/1000 [00:03<00:00, 208.92it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:04<00:00, 208.93it/s]Measuring inference for batch_size=1:  86%|████████▌ | 861/1000 [00:04<00:00, 208.77it/s]Measuring inference for batch_size=1:  88%|████████▊ | 882/1000 [00:04<00:00, 208.90it/s]Measuring inference for batch_size=1:  90%|█████████ | 903/1000 [00:04<00:00, 209.06it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [00:04<00:00, 209.07it/s]Measuring inference for batch_size=1:  94%|█████████▍| 945/1000 [00:04<00:00, 209.02it/s]Measuring inference for batch_size=1:  97%|█████████▋| 966/1000 [00:04<00:00, 209.01it/s]Measuring inference for batch_size=1:  99%|█████████▊| 987/1000 [00:04<00:00, 209.06it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 208.69it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=512:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=512:  21%|██        | 21/100 [00:00<00:00, 203.18it/s]Warming up with batch_size=512:  42%|████▏     | 42/100 [00:00<00:00, 203.38it/s]Warming up with batch_size=512:  63%|██████▎   | 63/100 [00:00<00:00, 203.39it/s]Warming up with batch_size=512:  84%|████████▍ | 84/100 [00:00<00:00, 203.39it/s]Warming up with batch_size=512: 100%|██████████| 100/100 [00:00<00:00, 203.38it/s]
STAGE:2024-02-23 10:02:37 177589:177589 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 10:02:37 177589:177589 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 10:02:37 177589:177589 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=512:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=512:   2%|▏         | 20/1000 [00:00<00:04, 199.75it/s]Measuring inference for batch_size=512:   4%|▍         | 41/1000 [00:00<00:04, 200.18it/s]Measuring inference for batch_size=512:   6%|▌         | 62/1000 [00:00<00:04, 200.46it/s]Measuring inference for batch_size=512:   8%|▊         | 83/1000 [00:00<00:04, 200.59it/s]Measuring inference for batch_size=512:  10%|█         | 104/1000 [00:00<00:04, 200.72it/s]Measuring inference for batch_size=512:  12%|█▎        | 125/1000 [00:00<00:04, 200.67it/s]Measuring inference for batch_size=512:  15%|█▍        | 146/1000 [00:00<00:04, 200.65it/s]Measuring inference for batch_size=512:  17%|█▋        | 167/1000 [00:00<00:04, 200.80it/s]Measuring inference for batch_size=512:  19%|█▉        | 188/1000 [00:00<00:04, 200.84it/s]Measuring inference for batch_size=512:  21%|██        | 209/1000 [00:01<00:03, 200.77it/s]Measuring inference for batch_size=512:  23%|██▎       | 230/1000 [00:01<00:03, 200.78it/s]Measuring inference for batch_size=512:  25%|██▌       | 251/1000 [00:01<00:03, 200.76it/s]Measuring inference for batch_size=512:  27%|██▋       | 272/1000 [00:01<00:03, 200.77it/s]Measuring inference for batch_size=512:  29%|██▉       | 293/1000 [00:01<00:03, 200.85it/s]Measuring inference for batch_size=512:  31%|███▏      | 314/1000 [00:01<00:03, 200.78it/s]Measuring inference for batch_size=512:  34%|███▎      | 335/1000 [00:01<00:03, 200.69it/s]Measuring inference for batch_size=512:  36%|███▌      | 356/1000 [00:01<00:03, 200.62it/s]Measuring inference for batch_size=512:  38%|███▊      | 377/1000 [00:01<00:03, 200.63it/s]Measuring inference for batch_size=512:  40%|███▉      | 398/1000 [00:01<00:02, 200.69it/s]Measuring inference for batch_size=512:  42%|████▏     | 419/1000 [00:02<00:02, 200.81it/s]Measuring inference for batch_size=512:  44%|████▍     | 440/1000 [00:02<00:02, 200.86it/s]Measuring inference for batch_size=512:  46%|████▌     | 461/1000 [00:02<00:02, 200.93it/s]Measuring inference for batch_size=512:  48%|████▊     | 482/1000 [00:02<00:02, 200.96it/s]Measuring inference for batch_size=512:  50%|█████     | 503/1000 [00:02<00:02, 200.92it/s]Measuring inference for batch_size=512:  52%|█████▏    | 524/1000 [00:02<00:02, 200.83it/s]Measuring inference for batch_size=512:  55%|█████▍    | 545/1000 [00:02<00:02, 200.76it/s]Measuring inference for batch_size=512:  57%|█████▋    | 566/1000 [00:02<00:02, 200.71it/s]Measuring inference for batch_size=512:  59%|█████▊    | 587/1000 [00:02<00:02, 200.77it/s]Measuring inference for batch_size=512:  61%|██████    | 608/1000 [00:03<00:01, 200.79it/s]Measuring inference for batch_size=512:  63%|██████▎   | 629/1000 [00:03<00:01, 200.77it/s]Measuring inference for batch_size=512:  65%|██████▌   | 650/1000 [00:03<00:01, 200.80it/s]Measuring inference for batch_size=512:  67%|██████▋   | 671/1000 [00:03<00:01, 200.80it/s]Measuring inference for batch_size=512:  69%|██████▉   | 692/1000 [00:03<00:01, 200.72it/s]Measuring inference for batch_size=512:  71%|███████▏  | 713/1000 [00:03<00:01, 200.70it/s]Measuring inference for batch_size=512:  73%|███████▎  | 734/1000 [00:03<00:01, 200.56it/s]Measuring inference for batch_size=512:  76%|███████▌  | 755/1000 [00:03<00:01, 200.51it/s]Measuring inference for batch_size=512:  78%|███████▊  | 776/1000 [00:03<00:01, 200.48it/s]Measuring inference for batch_size=512:  80%|███████▉  | 797/1000 [00:03<00:01, 200.54it/s]Measuring inference for batch_size=512:  82%|████████▏ | 818/1000 [00:04<00:00, 200.63it/s]Measuring inference for batch_size=512:  84%|████████▍ | 839/1000 [00:04<00:00, 200.67it/s]Measuring inference for batch_size=512:  86%|████████▌ | 860/1000 [00:04<00:00, 200.68it/s]Measuring inference for batch_size=512:  88%|████████▊ | 881/1000 [00:04<00:00, 200.74it/s]Measuring inference for batch_size=512:  90%|█████████ | 902/1000 [00:04<00:00, 200.75it/s]Measuring inference for batch_size=512:  92%|█████████▏| 923/1000 [00:04<00:00, 200.79it/s]Measuring inference for batch_size=512:  94%|█████████▍| 944/1000 [00:04<00:00, 200.76it/s]Measuring inference for batch_size=512:  96%|█████████▋| 965/1000 [00:04<00:00, 200.72it/s]Measuring inference for batch_size=512:  99%|█████████▊| 986/1000 [00:04<00:00, 200.75it/s]Measuring inference for batch_size=512: 100%|██████████| 1000/1000 [00:04<00:00, 200.72it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.53 GB
    total: 31.28 GB
    used: 3.29 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_512:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 92.061 us +/- 2.981 us [90.599 us, 148.535 us]
        batches_per_second: 10.87 K +/- 292.18 [6.73 K, 11.04 K]
      metrics:
        batches_per_second_max: 11037.642105263158
        batches_per_second_mean: 10871.640237363004
        batches_per_second_min: 6732.430176565008
        batches_per_second_std: 292.17963638345896
        seconds_per_batch_max: 0.00014853477478027344
        seconds_per_batch_mean: 9.206128120422363e-05
        seconds_per_batch_min: 9.059906005859375e-05
        seconds_per_batch_std: 2.981446123863358e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.019 us +/- 0.504 us [22.411 us, 29.087 us]
        batches_per_second: 43.46 K +/- 837.45 [34.38 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 43460.94605957406
        batches_per_second_min: 34379.54098360656
        batches_per_second_std: 837.4452262360034
        seconds_per_batch_max: 2.9087066650390625e-05
        seconds_per_batch_mean: 2.3018836975097657e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 5.040563479497883e-07
    on_device_inference:
      human_readable:
        batch_latency: -4666159.425 us +/- 17.381 ms [-4928127.766 us, -4628191.948
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.22, -0.20]
      metrics:
        batches_per_second_max: -0.20291681700484168
        batches_per_second_mean: -0.21431194368033926
        batches_per_second_min: -0.21606709731339988
        batches_per_second_std: 0.0007881765897304846
        seconds_per_batch_max: -4.628191947937012
        seconds_per_batch_mean: -4.666159425258637
        seconds_per_batch_min: -4.928127765655518
        seconds_per_batch_std: 0.017381095576737622
    total:
      human_readable:
        batch_latency: 4.788 ms +/- 19.016 us [4.748 ms, 5.120 ms]
        batches_per_second: 208.85 +/- 0.81 [195.33, 210.60]
      metrics:
        batches_per_second_max: 210.59971881903996
        batches_per_second_mean: 208.84754557587553
        batches_per_second_min: 195.32920411679783
        batches_per_second_std: 0.8127629280265918
        seconds_per_batch_max: 0.0051195621490478516
        seconds_per_batch_mean: 0.004788255691528321
        seconds_per_batch_min: 0.004748344421386719
        seconds_per_batch_std: 1.9016236574959647e-05
  batch_size_512:
    cpu_to_gpu:
      human_readable:
        batch_latency: 142.811 us +/- 4.270 us [140.905 us, 249.147 us]
        batches_per_second: 7.01 K +/- 153.95 [4.01 K, 7.10 K]
      metrics:
        batches_per_second_max: 7096.961082910321
        batches_per_second_mean: 7006.745950695497
        batches_per_second_min: 4013.688038277512
        batches_per_second_std: 153.9451660882867
        seconds_per_batch_max: 0.0002491474151611328
        seconds_per_batch_mean: 0.00014281058311462402
        seconds_per_batch_min: 0.00014090538024902344
        seconds_per_batch_std: 4.26997570549808e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.281 us +/- 0.489 us [22.411 us, 28.610 us]
        batches_per_second: 42.97 K +/- 822.85 [34.95 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 42971.4229919219
        batches_per_second_min: 34952.53333333333
        batches_per_second_std: 822.845469064524
        seconds_per_batch_max: 2.86102294921875e-05
        seconds_per_batch_mean: 2.3280620574951172e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 4.894660846043838e-07
    on_device_inference:
      human_readable:
        batch_latency: -4806765.894 us +/- 16.989 ms [-5158143.997 us, -4775519.848
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.19]
      metrics:
        batches_per_second_max: -0.19386818214929782
        batches_per_second_mean: -0.2080426090803234
        batches_per_second_min: -0.20940128653136084
        batches_per_second_std: 0.0007136937607787676
        seconds_per_batch_max: -4.775519847869873
        seconds_per_batch_mean: -4.8067658939361575
        seconds_per_batch_min: -5.158143997192383
        seconds_per_batch_std: 0.016988875288388397
    total:
      human_readable:
        batch_latency: 4.978 ms +/- 19.969 us [4.947 ms, 5.442 ms]
        batches_per_second: 200.87 +/- 0.77 [183.76, 202.13]
      metrics:
        batches_per_second_max: 202.1253915473953
        batches_per_second_mean: 200.86937737124623
        batches_per_second_min: 183.75921139101862
        batches_per_second_std: 0.7688342270492201
        seconds_per_batch_max: 0.005441904067993164
        seconds_per_batch_mean: 0.00497843599319458
        seconds_per_batch_min: 0.0049474239349365234
        seconds_per_batch_std: 1.9968970837926922e-05


#####
baseline-baseline-py-id - Run 2
2024-02-23 10:03:03
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.96it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.95it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 275.20it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:00<00:00, 275.83it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:00<00:00, 276.99it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 276.91it/s]
STAGE:2024-02-23 10:02:50 177635:177635 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 10:02:50 177635:177635 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 10:02:50 177635:177635 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:00<00:04, 209.28it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:00<00:04, 209.64it/s]Measuring inference for batch_size=1:   6%|▋         | 64/1000 [00:00<00:04, 209.94it/s]Measuring inference for batch_size=1:   9%|▊         | 86/1000 [00:00<00:04, 210.04it/s]Measuring inference for batch_size=1:  11%|█         | 108/1000 [00:00<00:04, 210.10it/s]Measuring inference for batch_size=1:  13%|█▎        | 130/1000 [00:00<00:04, 210.18it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:00<00:04, 210.24it/s]Measuring inference for batch_size=1:  17%|█▋        | 174/1000 [00:00<00:03, 210.31it/s]Measuring inference for batch_size=1:  20%|█▉        | 196/1000 [00:00<00:03, 210.22it/s]Measuring inference for batch_size=1:  22%|██▏       | 218/1000 [00:01<00:03, 210.17it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:01<00:03, 210.17it/s]Measuring inference for batch_size=1:  26%|██▌       | 262/1000 [00:01<00:03, 210.21it/s]Measuring inference for batch_size=1:  28%|██▊       | 284/1000 [00:01<00:03, 210.17it/s]Measuring inference for batch_size=1:  31%|███       | 306/1000 [00:01<00:03, 210.19it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:01<00:03, 210.18it/s]Measuring inference for batch_size=1:  35%|███▌      | 350/1000 [00:01<00:03, 210.16it/s]Measuring inference for batch_size=1:  37%|███▋      | 372/1000 [00:01<00:02, 210.15it/s]Measuring inference for batch_size=1:  39%|███▉      | 394/1000 [00:01<00:02, 210.20it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:01<00:02, 210.28it/s]Measuring inference for batch_size=1:  44%|████▍     | 438/1000 [00:02<00:02, 210.33it/s]Measuring inference for batch_size=1:  46%|████▌     | 460/1000 [00:02<00:02, 210.41it/s]Measuring inference for batch_size=1:  48%|████▊     | 482/1000 [00:02<00:02, 210.39it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:02<00:02, 210.28it/s]Measuring inference for batch_size=1:  53%|█████▎    | 526/1000 [00:02<00:02, 210.31it/s]Measuring inference for batch_size=1:  55%|█████▍    | 548/1000 [00:02<00:02, 210.34it/s]Measuring inference for batch_size=1:  57%|█████▋    | 570/1000 [00:02<00:02, 210.37it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [00:02<00:01, 210.40it/s]Measuring inference for batch_size=1:  61%|██████▏   | 614/1000 [00:02<00:01, 210.34it/s]Measuring inference for batch_size=1:  64%|██████▎   | 636/1000 [00:03<00:01, 210.35it/s]Measuring inference for batch_size=1:  66%|██████▌   | 658/1000 [00:03<00:01, 210.35it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [00:03<00:01, 210.38it/s]Measuring inference for batch_size=1:  70%|███████   | 702/1000 [00:03<00:01, 210.33it/s]Measuring inference for batch_size=1:  72%|███████▏  | 724/1000 [00:03<00:01, 210.29it/s]Measuring inference for batch_size=1:  75%|███████▍  | 746/1000 [00:03<00:01, 210.25it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [00:03<00:01, 210.27it/s]Measuring inference for batch_size=1:  79%|███████▉  | 790/1000 [00:03<00:00, 210.24it/s]Measuring inference for batch_size=1:  81%|████████  | 812/1000 [00:03<00:00, 210.22it/s]Measuring inference for batch_size=1:  83%|████████▎ | 834/1000 [00:03<00:00, 210.23it/s]Measuring inference for batch_size=1:  86%|████████▌ | 856/1000 [00:04<00:00, 210.23it/s]Measuring inference for batch_size=1:  88%|████████▊ | 878/1000 [00:04<00:00, 210.24it/s]Measuring inference for batch_size=1:  90%|█████████ | 900/1000 [00:04<00:00, 210.22it/s]Measuring inference for batch_size=1:  92%|█████████▏| 922/1000 [00:04<00:00, 210.23it/s]Measuring inference for batch_size=1:  94%|█████████▍| 944/1000 [00:04<00:00, 210.29it/s]Measuring inference for batch_size=1:  97%|█████████▋| 966/1000 [00:04<00:00, 210.23it/s]Measuring inference for batch_size=1:  99%|█████████▉| 988/1000 [00:04<00:00, 210.23it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 210.24it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=512:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=512:  21%|██        | 21/100 [00:00<00:00, 204.54it/s]Warming up with batch_size=512:  42%|████▏     | 42/100 [00:00<00:00, 204.77it/s]Warming up with batch_size=512:  63%|██████▎   | 63/100 [00:00<00:00, 204.85it/s]Warming up with batch_size=512:  84%|████████▍ | 84/100 [00:00<00:00, 204.86it/s]Warming up with batch_size=512: 100%|██████████| 100/100 [00:00<00:00, 204.86it/s]
STAGE:2024-02-23 10:02:55 177635:177635 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 10:02:55 177635:177635 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 10:02:55 177635:177635 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=512:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=512:   2%|▏         | 21/1000 [00:00<00:04, 200.93it/s]Measuring inference for batch_size=512:   4%|▍         | 42/1000 [00:00<00:04, 201.68it/s]Measuring inference for batch_size=512:   6%|▋         | 63/1000 [00:00<00:04, 201.90it/s]Measuring inference for batch_size=512:   8%|▊         | 84/1000 [00:00<00:04, 201.91it/s]Measuring inference for batch_size=512:  10%|█         | 105/1000 [00:00<00:04, 201.96it/s]Measuring inference for batch_size=512:  13%|█▎        | 126/1000 [00:00<00:04, 201.94it/s]Measuring inference for batch_size=512:  15%|█▍        | 147/1000 [00:00<00:04, 202.06it/s]Measuring inference for batch_size=512:  17%|█▋        | 168/1000 [00:00<00:04, 201.99it/s]Measuring inference for batch_size=512:  19%|█▉        | 189/1000 [00:00<00:04, 201.91it/s]Measuring inference for batch_size=512:  21%|██        | 210/1000 [00:01<00:03, 201.97it/s]Measuring inference for batch_size=512:  23%|██▎       | 231/1000 [00:01<00:03, 201.91it/s]Measuring inference for batch_size=512:  25%|██▌       | 252/1000 [00:01<00:03, 202.04it/s]Measuring inference for batch_size=512:  27%|██▋       | 273/1000 [00:01<00:03, 202.12it/s]Measuring inference for batch_size=512:  29%|██▉       | 294/1000 [00:01<00:03, 202.14it/s]Measuring inference for batch_size=512:  32%|███▏      | 315/1000 [00:01<00:03, 202.14it/s]Measuring inference for batch_size=512:  34%|███▎      | 336/1000 [00:01<00:03, 202.16it/s]Measuring inference for batch_size=512:  36%|███▌      | 357/1000 [00:01<00:03, 202.18it/s]Measuring inference for batch_size=512:  38%|███▊      | 378/1000 [00:01<00:03, 202.21it/s]Measuring inference for batch_size=512:  40%|███▉      | 399/1000 [00:01<00:02, 202.30it/s]Measuring inference for batch_size=512:  42%|████▏     | 420/1000 [00:02<00:02, 202.30it/s]Measuring inference for batch_size=512:  44%|████▍     | 441/1000 [00:02<00:02, 202.21it/s]Measuring inference for batch_size=512:  46%|████▌     | 462/1000 [00:02<00:02, 202.06it/s]Measuring inference for batch_size=512:  48%|████▊     | 483/1000 [00:02<00:02, 202.03it/s]Measuring inference for batch_size=512:  50%|█████     | 504/1000 [00:02<00:02, 202.04it/s]Measuring inference for batch_size=512:  52%|█████▎    | 525/1000 [00:02<00:02, 202.04it/s]Measuring inference for batch_size=512:  55%|█████▍    | 546/1000 [00:02<00:02, 202.06it/s]Measuring inference for batch_size=512:  57%|█████▋    | 567/1000 [00:02<00:02, 202.01it/s]Measuring inference for batch_size=512:  59%|█████▉    | 588/1000 [00:02<00:02, 201.99it/s]Measuring inference for batch_size=512:  61%|██████    | 609/1000 [00:03<00:01, 202.04it/s]Measuring inference for batch_size=512:  63%|██████▎   | 630/1000 [00:03<00:01, 202.04it/s]Measuring inference for batch_size=512:  65%|██████▌   | 651/1000 [00:03<00:01, 202.10it/s]Measuring inference for batch_size=512:  67%|██████▋   | 672/1000 [00:03<00:01, 202.10it/s]Measuring inference for batch_size=512:  69%|██████▉   | 693/1000 [00:03<00:01, 202.11it/s]Measuring inference for batch_size=512:  71%|███████▏  | 714/1000 [00:03<00:01, 202.08it/s]Measuring inference for batch_size=512:  74%|███████▎  | 735/1000 [00:03<00:01, 202.12it/s]Measuring inference for batch_size=512:  76%|███████▌  | 756/1000 [00:03<00:01, 202.03it/s]Measuring inference for batch_size=512:  78%|███████▊  | 777/1000 [00:03<00:01, 201.96it/s]Measuring inference for batch_size=512:  80%|███████▉  | 798/1000 [00:03<00:01, 201.92it/s]Measuring inference for batch_size=512:  82%|████████▏ | 819/1000 [00:04<00:00, 201.95it/s]Measuring inference for batch_size=512:  84%|████████▍ | 840/1000 [00:04<00:00, 202.03it/s]Measuring inference for batch_size=512:  86%|████████▌ | 861/1000 [00:04<00:00, 202.05it/s]Measuring inference for batch_size=512:  88%|████████▊ | 882/1000 [00:04<00:00, 202.12it/s]Measuring inference for batch_size=512:  90%|█████████ | 903/1000 [00:04<00:00, 202.05it/s]Measuring inference for batch_size=512:  92%|█████████▏| 924/1000 [00:04<00:00, 202.00it/s]Measuring inference for batch_size=512:  94%|█████████▍| 945/1000 [00:04<00:00, 202.02it/s]Measuring inference for batch_size=512:  97%|█████████▋| 966/1000 [00:04<00:00, 202.04it/s]Measuring inference for batch_size=512:  99%|█████████▊| 987/1000 [00:04<00:00, 202.04it/s]Measuring inference for batch_size=512: 100%|██████████| 1000/1000 [00:04<00:00, 202.04it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.53 GB
    total: 31.28 GB
    used: 3.29 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_512:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 91.738 us +/- 2.778 us [90.361 us, 147.820 us]
        batches_per_second: 10.91 K +/- 270.30 [6.77 K, 11.07 K]
      metrics:
        batches_per_second_max: 11066.765171503957
        batches_per_second_mean: 10908.622203142777
        batches_per_second_min: 6765.006451612903
        batches_per_second_std: 270.30464436576506
        seconds_per_batch_max: 0.00014781951904296875
        seconds_per_batch_mean: 9.173822402954102e-05
        seconds_per_batch_min: 9.036064147949219e-05
        seconds_per_batch_std: 2.7775172607926894e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 22.836 us +/- 0.548 us [22.173 us, 30.041 us]
        batches_per_second: 43.81 K +/- 882.84 [33.29 K, 45.10 K]
      metrics:
        batches_per_second_max: 45100.04301075269
        batches_per_second_mean: 43810.70709495521
        batches_per_second_min: 33288.12698412698
        batches_per_second_std: 882.8416314918576
        seconds_per_batch_max: 3.0040740966796875e-05
        seconds_per_batch_mean: 2.283644676208496e-05
        seconds_per_batch_min: 2.2172927856445312e-05
        seconds_per_batch_std: 5.476320104279373e-07
    on_device_inference:
      human_readable:
        batch_latency: -4631837.980 us +/- 12.673 ms [-4857952.118 us, -4604479.790
          us]
        batches_per_second: -0.22 +/- 0.00 [-0.22, -0.21]
      metrics:
        batches_per_second_max: -0.20584805607927237
        batches_per_second_mean: -0.21589860680625284
        batches_per_second_min: -0.2171798000350859
        batches_per_second_std: 0.0005814631254916216
        seconds_per_batch_max: -4.604479789733887
        seconds_per_batch_mean: -4.631837980270386
        seconds_per_batch_min: -4.857952117919922
        seconds_per_batch_std: 0.012672523487052828
    total:
      human_readable:
        batch_latency: 4.753 ms +/- 14.425 us [4.725 ms, 5.047 ms]
        batches_per_second: 210.39 +/- 0.62 [198.15, 211.63]
      metrics:
        batches_per_second_max: 211.6304556233917
        batches_per_second_mean: 210.3916091692802
        batches_per_second_min: 198.15297396891387
        batches_per_second_std: 0.6227358159403421
        seconds_per_batch_max: 0.0050466060638427734
        seconds_per_batch_mean: 0.004753083944320679
        seconds_per_batch_min: 0.004725217819213867
        seconds_per_batch_std: 1.4425455709075053e-05
  batch_size_512:
    cpu_to_gpu:
      human_readable:
        batch_latency: 142.687 us +/- 4.208 us [141.144 us, 247.240 us]
        batches_per_second: 7.01 K +/- 152.67 [4.04 K, 7.08 K]
      metrics:
        batches_per_second_max: 7084.972972972973
        batches_per_second_mean: 7012.736674342973
        batches_per_second_min: 4044.651880424301
        batches_per_second_std: 152.6734925345319
        seconds_per_batch_max: 0.0002472400665283203
        seconds_per_batch_mean: 0.0001426866054534912
        seconds_per_batch_min: 0.000141143798828125
        seconds_per_batch_std: 4.207795182925283e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.055 us +/- 0.582 us [22.173 us, 29.802 us]
        batches_per_second: 43.40 K +/- 958.41 [33.55 K, 45.10 K]
      metrics:
        batches_per_second_max: 45100.04301075269
        batches_per_second_mean: 43398.47291937965
        batches_per_second_min: 33554.432
        batches_per_second_std: 958.4118386974313
        seconds_per_batch_max: 2.9802322387695312e-05
        seconds_per_batch_mean: 2.3055076599121094e-05
        seconds_per_batch_min: 2.2172927856445312e-05
        seconds_per_batch_std: 5.818585762322083e-07
    on_device_inference:
      human_readable:
        batch_latency: -4774714.913 us +/- 15.035 ms [-5106048.107 us, -4747263.908
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.20]
      metrics:
        batches_per_second_max: -0.19584617673308735
        batches_per_second_mean: -0.20943859610031365
        batches_per_second_min: -0.2106476529003286
        batches_per_second_std: 0.0006380475631687939
        seconds_per_batch_max: -4.7472639083862305
        seconds_per_batch_mean: -4.774714913368225
        seconds_per_batch_min: -5.106048107147217
        seconds_per_batch_std: 0.015035229743979285
    total:
      human_readable:
        batch_latency: 4.946 ms +/- 17.996 us [4.918 ms, 5.388 ms]
        batches_per_second: 202.19 +/- 0.70 [185.61, 203.34]
      metrics:
        batches_per_second_max: 203.34047607504726
        batches_per_second_mean: 202.19116863846074
        batches_per_second_min: 185.61331150152674
        batches_per_second_std: 0.6990497180452186
        seconds_per_batch_max: 0.005387544631958008
        seconds_per_batch_mean: 0.004945876598358154
        seconds_per_batch_min: 0.00491786003112793
        seconds_per_batch_std: 1.7995565069505682e-05


#####
baseline-baseline-py-id - Run 3
2024-02-23 10:03:18
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  29%|██▉       | 29/100 [00:00<00:00, 281.74it/s]Warming up with batch_size=1:  58%|█████▊    | 58/100 [00:00<00:00, 284.20it/s]Warming up with batch_size=1:  87%|████████▋ | 87/100 [00:00<00:00, 285.28it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 284.95it/s]
STAGE:2024-02-23 10:03:08 177681:177681 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 10:03:08 177681:177681 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 10:03:08 177681:177681 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 22/1000 [00:00<00:04, 215.66it/s]Measuring inference for batch_size=1:   4%|▍         | 44/1000 [00:00<00:04, 215.94it/s]Measuring inference for batch_size=1:   7%|▋         | 66/1000 [00:00<00:04, 216.09it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:00<00:04, 216.18it/s]Measuring inference for batch_size=1:  11%|█         | 110/1000 [00:00<00:04, 216.30it/s]Measuring inference for batch_size=1:  13%|█▎        | 132/1000 [00:00<00:04, 216.32it/s]Measuring inference for batch_size=1:  15%|█▌        | 154/1000 [00:00<00:03, 216.20it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:00<00:03, 216.25it/s]Measuring inference for batch_size=1:  20%|█▉        | 198/1000 [00:00<00:03, 216.31it/s]Measuring inference for batch_size=1:  22%|██▏       | 220/1000 [00:01<00:03, 216.30it/s]Measuring inference for batch_size=1:  24%|██▍       | 242/1000 [00:01<00:03, 216.26it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:01<00:03, 216.19it/s]Measuring inference for batch_size=1:  29%|██▊       | 286/1000 [00:01<00:03, 216.20it/s]Measuring inference for batch_size=1:  31%|███       | 308/1000 [00:01<00:03, 216.19it/s]Measuring inference for batch_size=1:  33%|███▎      | 330/1000 [00:01<00:03, 216.20it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:01<00:02, 216.18it/s]Measuring inference for batch_size=1:  37%|███▋      | 374/1000 [00:01<00:02, 216.17it/s]Measuring inference for batch_size=1:  40%|███▉      | 396/1000 [00:01<00:02, 216.22it/s]Measuring inference for batch_size=1:  42%|████▏     | 418/1000 [00:01<00:02, 216.30it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:02<00:02, 216.32it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:02<00:02, 216.41it/s]Measuring inference for batch_size=1:  48%|████▊     | 484/1000 [00:02<00:02, 216.54it/s]Measuring inference for batch_size=1:  51%|█████     | 506/1000 [00:02<00:02, 216.55it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:02<00:02, 216.57it/s]Measuring inference for batch_size=1:  55%|█████▌    | 550/1000 [00:02<00:02, 216.57it/s]Measuring inference for batch_size=1:  57%|█████▋    | 572/1000 [00:02<00:01, 216.52it/s]Measuring inference for batch_size=1:  59%|█████▉    | 594/1000 [00:02<00:01, 216.47it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:02<00:01, 216.43it/s]Measuring inference for batch_size=1:  64%|██████▍   | 638/1000 [00:02<00:01, 216.44it/s]Measuring inference for batch_size=1:  66%|██████▌   | 660/1000 [00:03<00:01, 216.34it/s]Measuring inference for batch_size=1:  68%|██████▊   | 682/1000 [00:03<00:01, 216.31it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [00:03<00:01, 216.39it/s]Measuring inference for batch_size=1:  73%|███████▎  | 726/1000 [00:03<00:01, 216.37it/s]Measuring inference for batch_size=1:  75%|███████▍  | 748/1000 [00:03<00:01, 216.39it/s]Measuring inference for batch_size=1:  77%|███████▋  | 770/1000 [00:03<00:01, 216.41it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [00:03<00:00, 216.47it/s]Measuring inference for batch_size=1:  81%|████████▏ | 814/1000 [00:03<00:00, 216.45it/s]Measuring inference for batch_size=1:  84%|████████▎ | 836/1000 [00:03<00:00, 216.42it/s]Measuring inference for batch_size=1:  86%|████████▌ | 858/1000 [00:03<00:00, 216.43it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [00:04<00:00, 216.40it/s]Measuring inference for batch_size=1:  90%|█████████ | 902/1000 [00:04<00:00, 216.39it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [00:04<00:00, 216.37it/s]Measuring inference for batch_size=1:  95%|█████████▍| 946/1000 [00:04<00:00, 216.40it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [00:04<00:00, 216.49it/s]Measuring inference for batch_size=1:  99%|█████████▉| 990/1000 [00:04<00:00, 216.44it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 216.35it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=512:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=512:  21%|██        | 21/100 [00:00<00:00, 209.23it/s]Warming up with batch_size=512:  42%|████▏     | 42/100 [00:00<00:00, 209.64it/s]Warming up with batch_size=512:  63%|██████▎   | 63/100 [00:00<00:00, 209.61it/s]Warming up with batch_size=512:  84%|████████▍ | 84/100 [00:00<00:00, 209.67it/s]Warming up with batch_size=512: 100%|██████████| 100/100 [00:00<00:00, 209.67it/s]
STAGE:2024-02-23 10:03:13 177681:177681 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 10:03:13 177681:177681 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 10:03:13 177681:177681 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=512:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=512:   2%|▏         | 21/1000 [00:00<00:04, 205.44it/s]Measuring inference for batch_size=512:   4%|▍         | 42/1000 [00:00<00:04, 206.09it/s]Measuring inference for batch_size=512:   6%|▋         | 63/1000 [00:00<00:04, 206.27it/s]Measuring inference for batch_size=512:   8%|▊         | 84/1000 [00:00<00:04, 206.42it/s]Measuring inference for batch_size=512:  10%|█         | 105/1000 [00:00<00:04, 206.44it/s]Measuring inference for batch_size=512:  13%|█▎        | 126/1000 [00:00<00:04, 206.33it/s]Measuring inference for batch_size=512:  15%|█▍        | 147/1000 [00:00<00:04, 206.34it/s]Measuring inference for batch_size=512:  17%|█▋        | 168/1000 [00:00<00:04, 206.33it/s]Measuring inference for batch_size=512:  19%|█▉        | 189/1000 [00:00<00:03, 206.21it/s]Measuring inference for batch_size=512:  21%|██        | 210/1000 [00:01<00:03, 206.25it/s]Measuring inference for batch_size=512:  23%|██▎       | 231/1000 [00:01<00:03, 206.21it/s]Measuring inference for batch_size=512:  25%|██▌       | 252/1000 [00:01<00:03, 206.24it/s]Measuring inference for batch_size=512:  27%|██▋       | 273/1000 [00:01<00:03, 206.25it/s]Measuring inference for batch_size=512:  29%|██▉       | 294/1000 [00:01<00:03, 206.33it/s]Measuring inference for batch_size=512:  32%|███▏      | 315/1000 [00:01<00:03, 206.45it/s]Measuring inference for batch_size=512:  34%|███▎      | 336/1000 [00:01<00:03, 206.44it/s]Measuring inference for batch_size=512:  36%|███▌      | 357/1000 [00:01<00:03, 206.41it/s]Measuring inference for batch_size=512:  38%|███▊      | 378/1000 [00:01<00:03, 206.44it/s]Measuring inference for batch_size=512:  40%|███▉      | 399/1000 [00:01<00:02, 206.39it/s]Measuring inference for batch_size=512:  42%|████▏     | 420/1000 [00:02<00:02, 206.26it/s]Measuring inference for batch_size=512:  44%|████▍     | 441/1000 [00:02<00:02, 206.29it/s]Measuring inference for batch_size=512:  46%|████▌     | 462/1000 [00:02<00:02, 206.13it/s]Measuring inference for batch_size=512:  48%|████▊     | 483/1000 [00:02<00:02, 206.17it/s]Measuring inference for batch_size=512:  50%|█████     | 504/1000 [00:02<00:02, 206.12it/s]Measuring inference for batch_size=512:  52%|█████▎    | 525/1000 [00:02<00:02, 206.08it/s]Measuring inference for batch_size=512:  55%|█████▍    | 546/1000 [00:02<00:02, 206.20it/s]Measuring inference for batch_size=512:  57%|█████▋    | 567/1000 [00:02<00:02, 206.06it/s]Measuring inference for batch_size=512:  59%|█████▉    | 588/1000 [00:02<00:01, 206.07it/s]Measuring inference for batch_size=512:  61%|██████    | 609/1000 [00:02<00:01, 206.18it/s]Measuring inference for batch_size=512:  63%|██████▎   | 630/1000 [00:03<00:01, 206.27it/s]Measuring inference for batch_size=512:  65%|██████▌   | 651/1000 [00:03<00:01, 206.34it/s]Measuring inference for batch_size=512:  67%|██████▋   | 672/1000 [00:03<00:01, 206.45it/s]Measuring inference for batch_size=512:  69%|██████▉   | 693/1000 [00:03<00:01, 206.53it/s]Measuring inference for batch_size=512:  71%|███████▏  | 714/1000 [00:03<00:01, 206.54it/s]Measuring inference for batch_size=512:  74%|███████▎  | 735/1000 [00:03<00:01, 206.54it/s]Measuring inference for batch_size=512:  76%|███████▌  | 756/1000 [00:03<00:01, 206.51it/s]Measuring inference for batch_size=512:  78%|███████▊  | 777/1000 [00:03<00:01, 206.45it/s]Measuring inference for batch_size=512:  80%|███████▉  | 798/1000 [00:03<00:00, 206.50it/s]Measuring inference for batch_size=512:  82%|████████▏ | 819/1000 [00:03<00:00, 206.45it/s]Measuring inference for batch_size=512:  84%|████████▍ | 840/1000 [00:04<00:00, 206.42it/s]Measuring inference for batch_size=512:  86%|████████▌ | 861/1000 [00:04<00:00, 206.39it/s]Measuring inference for batch_size=512:  88%|████████▊ | 882/1000 [00:04<00:00, 206.26it/s]Measuring inference for batch_size=512:  90%|█████████ | 903/1000 [00:04<00:00, 206.27it/s]Measuring inference for batch_size=512:  92%|█████████▏| 924/1000 [00:04<00:00, 206.30it/s]Measuring inference for batch_size=512:  94%|█████████▍| 945/1000 [00:04<00:00, 206.31it/s]Measuring inference for batch_size=512:  97%|█████████▋| 966/1000 [00:04<00:00, 206.33it/s]Measuring inference for batch_size=512:  99%|█████████▊| 987/1000 [00:04<00:00, 206.30it/s]Measuring inference for batch_size=512: 100%|██████████| 1000/1000 [00:04<00:00, 206.32it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.53 GB
    total: 31.28 GB
    used: 3.29 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_512:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 91.177 us +/- 3.047 us [89.407 us, 143.766 us]
        batches_per_second: 10.98 K +/- 309.61 [6.96 K, 11.18 K]
      metrics:
        batches_per_second_max: 11184.810666666666
        batches_per_second_mean: 10977.926717280418
        batches_per_second_min: 6955.728026533997
        batches_per_second_std: 309.6146202460889
        seconds_per_batch_max: 0.0001437664031982422
        seconds_per_batch_mean: 9.117674827575684e-05
        seconds_per_batch_min: 8.940696716308594e-05
        seconds_per_batch_std: 3.0471915769763683e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 22.488 us +/- 0.734 us [21.696 us, 29.087 us]
        batches_per_second: 44.51 K +/- 1.25 K [34.38 K, 46.09 K]
      metrics:
        batches_per_second_max: 46091.25274725275
        batches_per_second_mean: 44509.01188431194
        batches_per_second_min: 34379.54098360656
        batches_per_second_std: 1248.5625195334467
        seconds_per_batch_max: 2.9087066650390625e-05
        seconds_per_batch_mean: 2.2487878799438476e-05
        seconds_per_batch_min: 2.1696090698242188e-05
        seconds_per_batch_std: 7.339250006266605e-07
    on_device_inference:
      human_readable:
        batch_latency: -4498942.914 us +/- 14.735 ms [-4747615.814 us, -4462143.898
          us]
        batches_per_second: -0.22 +/- 0.00 [-0.22, -0.21]
      metrics:
        batches_per_second_max: -0.21063203913996845
        batches_per_second_mean: -0.22227678377047658
        batches_per_second_min: -0.22410751935766057
        batches_per_second_std: 0.0007169725128493862
        seconds_per_batch_max: -4.462143898010254
        seconds_per_batch_mean: -4.498942914485931
        seconds_per_batch_min: -4.747615814208984
        seconds_per_batch_std: 0.014734790715484718
    total:
      human_readable:
        batch_latency: 4.619 ms +/- 16.296 us [4.580 ms, 4.932 ms]
        batches_per_second: 216.51 +/- 0.75 [202.77, 218.34]
      metrics:
        batches_per_second_max: 218.33961478396668
        batches_per_second_mean: 216.5129751618
        batches_per_second_min: 202.77031665458063
        batches_per_second_std: 0.745911537769995
        seconds_per_batch_max: 0.00493168830871582
        seconds_per_batch_mean: 0.0046187169551849365
        seconds_per_batch_min: 0.004580020904541016
        seconds_per_batch_std: 1.6295862251873494e-05
  batch_size_512:
    cpu_to_gpu:
      human_readable:
        batch_latency: 141.950 us +/- 4.117 us [139.952 us, 246.048 us]
        batches_per_second: 7.05 K +/- 149.51 [4.06 K, 7.15 K]
      metrics:
        batches_per_second_max: 7145.321976149915
        batches_per_second_mean: 7048.922611748619
        batches_per_second_min: 4064.248062015504
        batches_per_second_std: 149.50501489014135
        seconds_per_batch_max: 0.0002460479736328125
        seconds_per_batch_mean: 0.0001419503688812256
        seconds_per_batch_min: 0.0001399517059326172
        seconds_per_batch_std: 4.116670197738872e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 22.650 us +/- 0.631 us [21.696 us, 30.518 us]
        batches_per_second: 44.18 K +/- 1.07 K [32.77 K, 46.09 K]
      metrics:
        batches_per_second_max: 46091.25274725275
        batches_per_second_mean: 44179.675528237116
        batches_per_second_min: 32768.0
        batches_per_second_std: 1067.4213671340287
        seconds_per_batch_max: 3.0517578125e-05
        seconds_per_batch_mean: 2.265000343322754e-05
        seconds_per_batch_min: 2.1696090698242188e-05
        seconds_per_batch_std: 6.307511646944272e-07
    on_device_inference:
      human_readable:
        batch_latency: -4673422.944 us +/- 16.355 ms [-4999296.188 us, -4637216.091
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.22, -0.20]
      metrics:
        batches_per_second_max: -0.20002815642918487
        batches_per_second_mean: -0.21397847468849962
        batches_per_second_min: -0.21564662511785412
        batches_per_second_std: 0.0007287592093425265
        seconds_per_batch_max: -4.637216091156006
        seconds_per_batch_mean: -4.673422944068909
        seconds_per_batch_min: -4.999296188354492
        seconds_per_batch_std: 0.016355482551477583
    total:
      human_readable:
        batch_latency: 4.843 ms +/- 19.016 us [4.808 ms, 5.279 ms]
        batches_per_second: 206.47 +/- 0.78 [189.44, 207.98]
      metrics:
        batches_per_second_max: 207.97857886646503
        batches_per_second_mean: 206.47195164332956
        batches_per_second_min: 189.43606883157943
        batches_per_second_std: 0.7753724042617968
        seconds_per_batch_max: 0.005278825759887695
        seconds_per_batch_mean: 0.004843344211578369
        seconds_per_batch_min: 0.004808187484741211
        seconds_per_batch_std: 1.901645812752394e-05


