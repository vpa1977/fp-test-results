#####
baseline-baseline-py-id - Run 1
2024-02-23 10:08:06
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.77it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  60.19 M, 100.000% Params, 11.58 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.016% Params, 118.01 MMac, 1.019% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.000% Params, 1.61 MMac, 0.014% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.007% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.007% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.359% Params, 680.39 MMac, 5.875% MACs, 
    (0): Bottleneck(
      75.01 k, 0.125% Params, 236.43 MMac, 2.042% MACs, 
      (conv1): Conv2d(4.1 k, 0.007% Params, 12.85 MMac, 0.111% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.028% Params, 52.99 MMac, 0.458% MACs, 
        (0): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    2.34 M, 3.887% Params, 1.92 GMac, 16.555% MACs, 
    (0): Bottleneck(
      379.39 k, 0.630% Params, 376.02 MMac, 3.247% MACs, 
      (conv1): Conv2d(32.77 k, 0.054% Params, 102.76 MMac, 0.887% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 802.82 KMac, 0.007% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.008% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.219% Params, 103.56 MMac, 0.894% MACs, 
        (0): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (4): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (5): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (6): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (7): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    40.61 M, 67.473% Params, 8.05 GMac, 69.501% MACs, 
    (0): Bottleneck(
      1.51 M, 2.513% Params, 374.26 MMac, 3.232% MACs, 
      (conv1): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 401.41 KMac, 0.003% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.004% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 0.874% Params, 103.16 MMac, 0.891% MACs, 
        (0): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (6): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (7): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (8): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (9): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (10): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (11): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (12): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (13): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (14): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (15): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (16): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (17): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (18): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (19): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (20): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (21): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (22): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (23): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (24): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (25): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (26): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (27): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (28): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (29): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (30): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (31): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (32): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (33): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (34): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (35): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 24.861% Params, 811.02 MMac, 7.003% MACs, 
    (0): Bottleneck(
      6.04 M, 10.034% Params, 373.38 MMac, 3.224% MACs, 
      (conv1): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 200.7 KMac, 0.002% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.002% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 3.491% Params, 102.96 MMac, 0.889% MACs, 
        (0): Conv2d(2.1 M, 3.484% Params, 102.76 MMac, 0.887% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.001% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 3.404% Params, 2.05 MMac, 0.018% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  10%|█         | 10/100 [00:00<00:00, 99.42it/s]Warming up with batch_size=1:  20%|██        | 20/100 [00:00<00:00, 99.68it/s]Warming up with batch_size=1:  31%|███       | 31/100 [00:00<00:00, 99.89it/s]Warming up with batch_size=1:  42%|████▏     | 42/100 [00:00<00:00, 100.05it/s]Warming up with batch_size=1:  53%|█████▎    | 53/100 [00:00<00:00, 100.14it/s]Warming up with batch_size=1:  64%|██████▍   | 64/100 [00:00<00:00, 100.19it/s]Warming up with batch_size=1:  75%|███████▌  | 75/100 [00:00<00:00, 100.20it/s]Warming up with batch_size=1:  86%|████████▌ | 86/100 [00:00<00:00, 100.22it/s]Warming up with batch_size=1:  97%|█████████▋| 97/100 [00:00<00:00, 100.21it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 100.10it/s]
STAGE:2024-02-23 10:07:35 178019:178019 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 10:07:35 178019:178019 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 10:07:35 178019:178019 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   1%|          | 8/1000 [00:00<00:13, 74.84it/s]Measuring inference for batch_size=1:   2%|▏         | 16/1000 [00:00<00:13, 75.15it/s]Measuring inference for batch_size=1:   2%|▏         | 24/1000 [00:00<00:12, 75.26it/s]Measuring inference for batch_size=1:   3%|▎         | 32/1000 [00:00<00:12, 75.29it/s]Measuring inference for batch_size=1:   4%|▍         | 40/1000 [00:00<00:12, 75.34it/s]Measuring inference for batch_size=1:   5%|▍         | 48/1000 [00:00<00:12, 75.29it/s]Measuring inference for batch_size=1:   6%|▌         | 56/1000 [00:00<00:12, 75.33it/s]Measuring inference for batch_size=1:   6%|▋         | 64/1000 [00:00<00:12, 75.38it/s]Measuring inference for batch_size=1:   7%|▋         | 72/1000 [00:00<00:12, 75.41it/s]Measuring inference for batch_size=1:   8%|▊         | 80/1000 [00:01<00:12, 75.41it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:01<00:12, 75.37it/s]Measuring inference for batch_size=1:  10%|▉         | 96/1000 [00:01<00:11, 75.40it/s]Measuring inference for batch_size=1:  10%|█         | 104/1000 [00:01<00:11, 75.40it/s]Measuring inference for batch_size=1:  11%|█         | 112/1000 [00:01<00:11, 75.43it/s]Measuring inference for batch_size=1:  12%|█▏        | 120/1000 [00:01<00:11, 75.43it/s]Measuring inference for batch_size=1:  13%|█▎        | 128/1000 [00:01<00:11, 75.44it/s]Measuring inference for batch_size=1:  14%|█▎        | 136/1000 [00:01<00:11, 75.44it/s]Measuring inference for batch_size=1:  14%|█▍        | 144/1000 [00:01<00:11, 75.44it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:02<00:11, 75.43it/s]Measuring inference for batch_size=1:  16%|█▌        | 160/1000 [00:02<00:11, 75.42it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:02<00:11, 75.40it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:02<00:10, 75.41it/s]Measuring inference for batch_size=1:  18%|█▊        | 184/1000 [00:02<00:10, 75.41it/s]Measuring inference for batch_size=1:  19%|█▉        | 192/1000 [00:02<00:10, 75.40it/s]Measuring inference for batch_size=1:  20%|██        | 200/1000 [00:02<00:10, 75.38it/s]Measuring inference for batch_size=1:  21%|██        | 208/1000 [00:02<00:10, 75.40it/s]Measuring inference for batch_size=1:  22%|██▏       | 216/1000 [00:02<00:10, 75.39it/s]Measuring inference for batch_size=1:  22%|██▏       | 224/1000 [00:02<00:10, 75.40it/s]Measuring inference for batch_size=1:  23%|██▎       | 232/1000 [00:03<00:10, 75.45it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:03<00:10, 75.44it/s]Measuring inference for batch_size=1:  25%|██▍       | 248/1000 [00:03<00:09, 75.45it/s]Measuring inference for batch_size=1:  26%|██▌       | 256/1000 [00:03<00:09, 75.42it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:03<00:09, 75.40it/s]Measuring inference for batch_size=1:  27%|██▋       | 272/1000 [00:03<00:09, 75.36it/s]Measuring inference for batch_size=1:  28%|██▊       | 280/1000 [00:03<00:09, 75.34it/s]Measuring inference for batch_size=1:  29%|██▉       | 288/1000 [00:03<00:09, 75.37it/s]Measuring inference for batch_size=1:  30%|██▉       | 296/1000 [00:03<00:09, 75.38it/s]Measuring inference for batch_size=1:  30%|███       | 304/1000 [00:04<00:09, 75.42it/s]Measuring inference for batch_size=1:  31%|███       | 312/1000 [00:04<00:09, 75.44it/s]Measuring inference for batch_size=1:  32%|███▏      | 320/1000 [00:04<00:09, 75.41it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:04<00:08, 75.43it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:04<00:08, 75.41it/s]Measuring inference for batch_size=1:  34%|███▍      | 344/1000 [00:04<00:08, 75.41it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:04<00:08, 75.45it/s]Measuring inference for batch_size=1:  36%|███▌      | 360/1000 [00:04<00:08, 75.46it/s]Measuring inference for batch_size=1:  37%|███▋      | 368/1000 [00:04<00:08, 75.41it/s]Measuring inference for batch_size=1:  38%|███▊      | 376/1000 [00:04<00:08, 75.38it/s]Measuring inference for batch_size=1:  38%|███▊      | 384/1000 [00:05<00:08, 75.38it/s]Measuring inference for batch_size=1:  39%|███▉      | 392/1000 [00:05<00:08, 75.39it/s]Measuring inference for batch_size=1:  40%|████      | 400/1000 [00:05<00:07, 75.37it/s]Measuring inference for batch_size=1:  41%|████      | 408/1000 [00:05<00:07, 75.34it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:05<00:07, 75.36it/s]Measuring inference for batch_size=1:  42%|████▏     | 424/1000 [00:05<00:07, 75.33it/s]Measuring inference for batch_size=1:  43%|████▎     | 432/1000 [00:05<00:07, 75.30it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:05<00:07, 75.31it/s]Measuring inference for batch_size=1:  45%|████▍     | 448/1000 [00:05<00:07, 75.32it/s]Measuring inference for batch_size=1:  46%|████▌     | 456/1000 [00:06<00:07, 75.33it/s]Measuring inference for batch_size=1:  46%|████▋     | 464/1000 [00:06<00:07, 75.36it/s]Measuring inference for batch_size=1:  47%|████▋     | 472/1000 [00:06<00:07, 75.42it/s]Measuring inference for batch_size=1:  48%|████▊     | 480/1000 [00:06<00:06, 75.38it/s]Measuring inference for batch_size=1:  49%|████▉     | 488/1000 [00:06<00:06, 75.40it/s]Measuring inference for batch_size=1:  50%|████▉     | 496/1000 [00:06<00:06, 75.41it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:06<00:06, 75.44it/s]Measuring inference for batch_size=1:  51%|█████     | 512/1000 [00:06<00:06, 75.49it/s]Measuring inference for batch_size=1:  52%|█████▏    | 520/1000 [00:06<00:06, 75.51it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:07<00:06, 75.52it/s]Measuring inference for batch_size=1:  54%|█████▎    | 536/1000 [00:07<00:06, 75.51it/s]Measuring inference for batch_size=1:  54%|█████▍    | 544/1000 [00:07<00:06, 75.50it/s]Measuring inference for batch_size=1:  55%|█████▌    | 552/1000 [00:07<00:05, 75.51it/s]Measuring inference for batch_size=1:  56%|█████▌    | 560/1000 [00:07<00:05, 75.48it/s]Measuring inference for batch_size=1:  57%|█████▋    | 568/1000 [00:07<00:05, 75.46it/s]Measuring inference for batch_size=1:  58%|█████▊    | 576/1000 [00:07<00:05, 75.45it/s]Measuring inference for batch_size=1:  58%|█████▊    | 584/1000 [00:07<00:05, 75.45it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [00:07<00:05, 75.47it/s]Measuring inference for batch_size=1:  60%|██████    | 600/1000 [00:07<00:05, 75.48it/s]Measuring inference for batch_size=1:  61%|██████    | 608/1000 [00:08<00:05, 75.51it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:08<00:05, 75.51it/s]Measuring inference for batch_size=1:  62%|██████▏   | 624/1000 [00:08<00:04, 75.52it/s]Measuring inference for batch_size=1:  63%|██████▎   | 632/1000 [00:08<00:04, 75.53it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [00:08<00:04, 75.52it/s]Measuring inference for batch_size=1:  65%|██████▍   | 648/1000 [00:08<00:04, 75.49it/s]Measuring inference for batch_size=1:  66%|██████▌   | 656/1000 [00:08<00:04, 75.50it/s]Measuring inference for batch_size=1:  66%|██████▋   | 664/1000 [00:08<00:04, 75.52it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:08<00:04, 75.53it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [00:09<00:04, 75.51it/s]Measuring inference for batch_size=1:  69%|██████▉   | 688/1000 [00:09<00:04, 75.50it/s]Measuring inference for batch_size=1:  70%|██████▉   | 696/1000 [00:09<00:04, 75.49it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [00:09<00:03, 75.52it/s]Measuring inference for batch_size=1:  71%|███████   | 712/1000 [00:09<00:03, 75.50it/s]Measuring inference for batch_size=1:  72%|███████▏  | 720/1000 [00:09<00:03, 75.49it/s]Measuring inference for batch_size=1:  73%|███████▎  | 728/1000 [00:09<00:03, 75.51it/s]Measuring inference for batch_size=1:  74%|███████▎  | 736/1000 [00:09<00:03, 75.53it/s]Measuring inference for batch_size=1:  74%|███████▍  | 744/1000 [00:09<00:03, 75.53it/s]Measuring inference for batch_size=1:  75%|███████▌  | 752/1000 [00:09<00:03, 75.55it/s]Measuring inference for batch_size=1:  76%|███████▌  | 760/1000 [00:10<00:03, 75.56it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [00:10<00:03, 75.57it/s]Measuring inference for batch_size=1:  78%|███████▊  | 776/1000 [00:10<00:02, 75.58it/s]Measuring inference for batch_size=1:  78%|███████▊  | 784/1000 [00:10<00:02, 75.59it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [00:10<00:02, 75.60it/s]Measuring inference for batch_size=1:  80%|████████  | 800/1000 [00:10<00:02, 75.62it/s]Measuring inference for batch_size=1:  81%|████████  | 808/1000 [00:10<00:02, 75.63it/s]Measuring inference for batch_size=1:  82%|████████▏ | 816/1000 [00:10<00:02, 75.62it/s]Measuring inference for batch_size=1:  82%|████████▏ | 824/1000 [00:10<00:02, 75.64it/s]Measuring inference for batch_size=1:  83%|████████▎ | 832/1000 [00:11<00:02, 75.65it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:11<00:02, 75.66it/s]Measuring inference for batch_size=1:  85%|████████▍ | 848/1000 [00:11<00:02, 75.67it/s]Measuring inference for batch_size=1:  86%|████████▌ | 856/1000 [00:11<00:01, 75.70it/s]Measuring inference for batch_size=1:  86%|████████▋ | 864/1000 [00:11<00:01, 75.72it/s]Measuring inference for batch_size=1:  87%|████████▋ | 872/1000 [00:11<00:01, 75.67it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [00:11<00:01, 75.66it/s]Measuring inference for batch_size=1:  89%|████████▉ | 888/1000 [00:11<00:01, 75.63it/s]Measuring inference for batch_size=1:  90%|████████▉ | 896/1000 [00:11<00:01, 75.60it/s]Measuring inference for batch_size=1:  90%|█████████ | 904/1000 [00:11<00:01, 75.60it/s]Measuring inference for batch_size=1:  91%|█████████ | 912/1000 [00:12<00:01, 75.59it/s]Measuring inference for batch_size=1:  92%|█████████▏| 920/1000 [00:12<00:01, 75.60it/s]Measuring inference for batch_size=1:  93%|█████████▎| 928/1000 [00:12<00:00, 75.58it/s]Measuring inference for batch_size=1:  94%|█████████▎| 936/1000 [00:12<00:00, 75.59it/s]Measuring inference for batch_size=1:  94%|█████████▍| 944/1000 [00:12<00:00, 75.61it/s]Measuring inference for batch_size=1:  95%|█████████▌| 952/1000 [00:12<00:00, 75.58it/s]Measuring inference for batch_size=1:  96%|█████████▌| 960/1000 [00:12<00:00, 75.55it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [00:12<00:00, 75.56it/s]Measuring inference for batch_size=1:  98%|█████████▊| 976/1000 [00:12<00:00, 75.56it/s]Measuring inference for batch_size=1:  98%|█████████▊| 984/1000 [00:13<00:00, 75.57it/s]Measuring inference for batch_size=1:  99%|█████████▉| 992/1000 [00:13<00:00, 75.54it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 75.50it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 75.47it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=512:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=512:   8%|▊         | 8/100 [00:00<00:01, 74.32it/s]Warming up with batch_size=512:  16%|█▌        | 16/100 [00:00<00:01, 74.62it/s]Warming up with batch_size=512:  24%|██▍       | 24/100 [00:00<00:01, 74.70it/s]Warming up with batch_size=512:  32%|███▏      | 32/100 [00:00<00:00, 74.73it/s]Warming up with batch_size=512:  40%|████      | 40/100 [00:00<00:00, 74.73it/s]Warming up with batch_size=512:  48%|████▊     | 48/100 [00:00<00:00, 74.73it/s]Warming up with batch_size=512:  56%|█████▌    | 56/100 [00:00<00:00, 74.76it/s]Warming up with batch_size=512:  64%|██████▍   | 64/100 [00:00<00:00, 74.74it/s]Warming up with batch_size=512:  72%|███████▏  | 72/100 [00:00<00:00, 74.74it/s]Warming up with batch_size=512:  80%|████████  | 80/100 [00:01<00:00, 74.74it/s]Warming up with batch_size=512:  88%|████████▊ | 88/100 [00:01<00:00, 74.73it/s]Warming up with batch_size=512:  96%|█████████▌| 96/100 [00:01<00:00, 74.71it/s]Warming up with batch_size=512: 100%|██████████| 100/100 [00:01<00:00, 74.71it/s]
STAGE:2024-02-23 10:07:50 178019:178019 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 10:07:50 178019:178019 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 10:07:50 178019:178019 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=512:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=512:   1%|          | 8/1000 [00:00<00:13, 73.53it/s]Measuring inference for batch_size=512:   2%|▏         | 16/1000 [00:00<00:13, 73.80it/s]Measuring inference for batch_size=512:   2%|▏         | 24/1000 [00:00<00:13, 73.95it/s]Measuring inference for batch_size=512:   3%|▎         | 32/1000 [00:00<00:13, 74.00it/s]Measuring inference for batch_size=512:   4%|▍         | 40/1000 [00:00<00:12, 74.03it/s]Measuring inference for batch_size=512:   5%|▍         | 48/1000 [00:00<00:12, 74.03it/s]Measuring inference for batch_size=512:   6%|▌         | 56/1000 [00:00<00:12, 74.09it/s]Measuring inference for batch_size=512:   6%|▋         | 64/1000 [00:00<00:12, 74.10it/s]Measuring inference for batch_size=512:   7%|▋         | 72/1000 [00:00<00:12, 74.14it/s]Measuring inference for batch_size=512:   8%|▊         | 80/1000 [00:01<00:12, 74.13it/s]Measuring inference for batch_size=512:   9%|▉         | 88/1000 [00:01<00:12, 74.16it/s]Measuring inference for batch_size=512:  10%|▉         | 96/1000 [00:01<00:12, 74.16it/s]Measuring inference for batch_size=512:  10%|█         | 104/1000 [00:01<00:12, 74.15it/s]Measuring inference for batch_size=512:  11%|█         | 112/1000 [00:01<00:11, 74.14it/s]Measuring inference for batch_size=512:  12%|█▏        | 120/1000 [00:01<00:11, 74.14it/s]Measuring inference for batch_size=512:  13%|█▎        | 128/1000 [00:01<00:11, 74.15it/s]Measuring inference for batch_size=512:  14%|█▎        | 136/1000 [00:01<00:11, 74.13it/s]Measuring inference for batch_size=512:  14%|█▍        | 144/1000 [00:01<00:11, 74.16it/s]Measuring inference for batch_size=512:  15%|█▌        | 152/1000 [00:02<00:11, 74.17it/s]Measuring inference for batch_size=512:  16%|█▌        | 160/1000 [00:02<00:11, 74.16it/s]Measuring inference for batch_size=512:  17%|█▋        | 168/1000 [00:02<00:11, 74.14it/s]Measuring inference for batch_size=512:  18%|█▊        | 176/1000 [00:02<00:11, 74.14it/s]Measuring inference for batch_size=512:  18%|█▊        | 184/1000 [00:02<00:10, 74.19it/s]Measuring inference for batch_size=512:  19%|█▉        | 192/1000 [00:02<00:10, 74.18it/s]Measuring inference for batch_size=512:  20%|██        | 200/1000 [00:02<00:10, 74.18it/s]Measuring inference for batch_size=512:  21%|██        | 208/1000 [00:02<00:10, 74.15it/s]Measuring inference for batch_size=512:  22%|██▏       | 216/1000 [00:02<00:10, 74.14it/s]Measuring inference for batch_size=512:  22%|██▏       | 224/1000 [00:03<00:10, 74.09it/s]Measuring inference for batch_size=512:  23%|██▎       | 232/1000 [00:03<00:10, 74.12it/s]Measuring inference for batch_size=512:  24%|██▍       | 240/1000 [00:03<00:10, 74.09it/s]Measuring inference for batch_size=512:  25%|██▍       | 248/1000 [00:03<00:10, 74.11it/s]Measuring inference for batch_size=512:  26%|██▌       | 256/1000 [00:03<00:10, 74.12it/s]Measuring inference for batch_size=512:  26%|██▋       | 264/1000 [00:03<00:09, 74.11it/s]Measuring inference for batch_size=512:  27%|██▋       | 272/1000 [00:03<00:09, 74.11it/s]Measuring inference for batch_size=512:  28%|██▊       | 280/1000 [00:03<00:09, 74.16it/s]Measuring inference for batch_size=512:  29%|██▉       | 288/1000 [00:03<00:09, 74.17it/s]Measuring inference for batch_size=512:  30%|██▉       | 296/1000 [00:03<00:09, 74.14it/s]Measuring inference for batch_size=512:  30%|███       | 304/1000 [00:04<00:09, 74.14it/s]Measuring inference for batch_size=512:  31%|███       | 312/1000 [00:04<00:09, 74.16it/s]Measuring inference for batch_size=512:  32%|███▏      | 320/1000 [00:04<00:09, 74.14it/s]Measuring inference for batch_size=512:  33%|███▎      | 328/1000 [00:04<00:09, 74.14it/s]Measuring inference for batch_size=512:  34%|███▎      | 336/1000 [00:04<00:08, 74.12it/s]Measuring inference for batch_size=512:  34%|███▍      | 344/1000 [00:04<00:08, 74.14it/s]Measuring inference for batch_size=512:  35%|███▌      | 352/1000 [00:04<00:08, 74.16it/s]Measuring inference for batch_size=512:  36%|███▌      | 360/1000 [00:04<00:08, 74.19it/s]Measuring inference for batch_size=512:  37%|███▋      | 368/1000 [00:04<00:08, 74.20it/s]Measuring inference for batch_size=512:  38%|███▊      | 376/1000 [00:05<00:08, 74.22it/s]Measuring inference for batch_size=512:  38%|███▊      | 384/1000 [00:05<00:08, 74.22it/s]Measuring inference for batch_size=512:  39%|███▉      | 392/1000 [00:05<00:08, 74.23it/s]Measuring inference for batch_size=512:  40%|████      | 400/1000 [00:05<00:08, 74.20it/s]Measuring inference for batch_size=512:  41%|████      | 408/1000 [00:05<00:07, 74.22it/s]Measuring inference for batch_size=512:  42%|████▏     | 416/1000 [00:05<00:07, 74.16it/s]Measuring inference for batch_size=512:  42%|████▏     | 424/1000 [00:05<00:07, 74.18it/s]Measuring inference for batch_size=512:  43%|████▎     | 432/1000 [00:05<00:07, 74.17it/s]Measuring inference for batch_size=512:  44%|████▍     | 440/1000 [00:05<00:07, 74.15it/s]Measuring inference for batch_size=512:  45%|████▍     | 448/1000 [00:06<00:07, 74.15it/s]Measuring inference for batch_size=512:  46%|████▌     | 456/1000 [00:06<00:07, 74.17it/s]Measuring inference for batch_size=512:  46%|████▋     | 464/1000 [00:06<00:07, 74.19it/s]Measuring inference for batch_size=512:  47%|████▋     | 472/1000 [00:06<00:07, 74.24it/s]Measuring inference for batch_size=512:  48%|████▊     | 480/1000 [00:06<00:07, 74.23it/s]Measuring inference for batch_size=512:  49%|████▉     | 488/1000 [00:06<00:06, 74.23it/s]Measuring inference for batch_size=512:  50%|████▉     | 496/1000 [00:06<00:06, 74.20it/s]Measuring inference for batch_size=512:  50%|█████     | 504/1000 [00:06<00:06, 74.18it/s]Measuring inference for batch_size=512:  51%|█████     | 512/1000 [00:06<00:06, 74.15it/s]Measuring inference for batch_size=512:  52%|█████▏    | 520/1000 [00:07<00:06, 74.13it/s]Measuring inference for batch_size=512:  53%|█████▎    | 528/1000 [00:07<00:06, 74.18it/s]Measuring inference for batch_size=512:  54%|█████▎    | 536/1000 [00:07<00:06, 74.24it/s]Measuring inference for batch_size=512:  54%|█████▍    | 544/1000 [00:07<00:06, 74.25it/s]Measuring inference for batch_size=512:  55%|█████▌    | 552/1000 [00:07<00:06, 74.28it/s]Measuring inference for batch_size=512:  56%|█████▌    | 560/1000 [00:07<00:05, 74.24it/s]Measuring inference for batch_size=512:  57%|█████▋    | 568/1000 [00:07<00:05, 74.27it/s]Measuring inference for batch_size=512:  58%|█████▊    | 576/1000 [00:07<00:05, 74.28it/s]Measuring inference for batch_size=512:  58%|█████▊    | 584/1000 [00:07<00:05, 74.25it/s]Measuring inference for batch_size=512:  59%|█████▉    | 592/1000 [00:07<00:05, 74.22it/s]Measuring inference for batch_size=512:  60%|██████    | 600/1000 [00:08<00:05, 74.20it/s]Measuring inference for batch_size=512:  61%|██████    | 608/1000 [00:08<00:05, 74.17it/s]Measuring inference for batch_size=512:  62%|██████▏   | 616/1000 [00:08<00:05, 74.19it/s]Measuring inference for batch_size=512:  62%|██████▏   | 624/1000 [00:08<00:05, 74.17it/s]Measuring inference for batch_size=512:  63%|██████▎   | 632/1000 [00:08<00:04, 74.20it/s]Measuring inference for batch_size=512:  64%|██████▍   | 640/1000 [00:08<00:04, 74.21it/s]Measuring inference for batch_size=512:  65%|██████▍   | 648/1000 [00:08<00:04, 74.19it/s]Measuring inference for batch_size=512:  66%|██████▌   | 656/1000 [00:08<00:04, 74.20it/s]Measuring inference for batch_size=512:  66%|██████▋   | 664/1000 [00:08<00:04, 74.20it/s]Measuring inference for batch_size=512:  67%|██████▋   | 672/1000 [00:09<00:04, 74.21it/s]Measuring inference for batch_size=512:  68%|██████▊   | 680/1000 [00:09<00:04, 74.19it/s]Measuring inference for batch_size=512:  69%|██████▉   | 688/1000 [00:09<00:04, 74.18it/s]Measuring inference for batch_size=512:  70%|██████▉   | 696/1000 [00:09<00:04, 74.18it/s]Measuring inference for batch_size=512:  70%|███████   | 704/1000 [00:09<00:03, 74.12it/s]Measuring inference for batch_size=512:  71%|███████   | 712/1000 [00:09<00:03, 74.12it/s]Measuring inference for batch_size=512:  72%|███████▏  | 720/1000 [00:09<00:03, 74.12it/s]Measuring inference for batch_size=512:  73%|███████▎  | 728/1000 [00:09<00:03, 74.11it/s]Measuring inference for batch_size=512:  74%|███████▎  | 736/1000 [00:09<00:03, 74.13it/s]Measuring inference for batch_size=512:  74%|███████▍  | 744/1000 [00:10<00:03, 74.11it/s]Measuring inference for batch_size=512:  75%|███████▌  | 752/1000 [00:10<00:03, 74.09it/s]Measuring inference for batch_size=512:  76%|███████▌  | 760/1000 [00:10<00:03, 74.06it/s]Measuring inference for batch_size=512:  77%|███████▋  | 768/1000 [00:10<00:03, 74.09it/s]Measuring inference for batch_size=512:  78%|███████▊  | 776/1000 [00:10<00:03, 74.12it/s]Measuring inference for batch_size=512:  78%|███████▊  | 784/1000 [00:10<00:02, 74.08it/s]Measuring inference for batch_size=512:  79%|███████▉  | 792/1000 [00:10<00:02, 74.08it/s]Measuring inference for batch_size=512:  80%|████████  | 800/1000 [00:10<00:02, 74.04it/s]Measuring inference for batch_size=512:  81%|████████  | 808/1000 [00:10<00:02, 74.05it/s]Measuring inference for batch_size=512:  82%|████████▏ | 816/1000 [00:11<00:02, 74.05it/s]Measuring inference for batch_size=512:  82%|████████▏ | 824/1000 [00:11<00:02, 74.06it/s]Measuring inference for batch_size=512:  83%|████████▎ | 832/1000 [00:11<00:02, 74.06it/s]Measuring inference for batch_size=512:  84%|████████▍ | 840/1000 [00:11<00:02, 74.06it/s]Measuring inference for batch_size=512:  85%|████████▍ | 848/1000 [00:11<00:02, 74.08it/s]Measuring inference for batch_size=512:  86%|████████▌ | 856/1000 [00:11<00:01, 74.08it/s]Measuring inference for batch_size=512:  86%|████████▋ | 864/1000 [00:11<00:01, 74.07it/s]Measuring inference for batch_size=512:  87%|████████▋ | 872/1000 [00:11<00:01, 74.09it/s]Measuring inference for batch_size=512:  88%|████████▊ | 880/1000 [00:11<00:01, 74.07it/s]Measuring inference for batch_size=512:  89%|████████▉ | 888/1000 [00:11<00:01, 74.07it/s]Measuring inference for batch_size=512:  90%|████████▉ | 896/1000 [00:12<00:01, 74.10it/s]Measuring inference for batch_size=512:  90%|█████████ | 904/1000 [00:12<00:01, 74.09it/s]Measuring inference for batch_size=512:  91%|█████████ | 912/1000 [00:12<00:01, 74.04it/s]Measuring inference for batch_size=512:  92%|█████████▏| 920/1000 [00:12<00:01, 74.03it/s]Measuring inference for batch_size=512:  93%|█████████▎| 928/1000 [00:12<00:00, 74.03it/s]Measuring inference for batch_size=512:  94%|█████████▎| 936/1000 [00:12<00:00, 74.07it/s]Measuring inference for batch_size=512:  94%|█████████▍| 944/1000 [00:12<00:00, 74.07it/s]Measuring inference for batch_size=512:  95%|█████████▌| 952/1000 [00:12<00:00, 74.08it/s]Measuring inference for batch_size=512:  96%|█████████▌| 960/1000 [00:12<00:00, 74.08it/s]Measuring inference for batch_size=512:  97%|█████████▋| 968/1000 [00:13<00:00, 74.07it/s]Measuring inference for batch_size=512:  98%|█████████▊| 976/1000 [00:13<00:00, 74.06it/s]Measuring inference for batch_size=512:  98%|█████████▊| 984/1000 [00:13<00:00, 74.08it/s]Measuring inference for batch_size=512:  99%|█████████▉| 992/1000 [00:13<00:00, 74.03it/s]Measuring inference for batch_size=512: 100%|██████████| 1000/1000 [00:13<00:00, 74.03it/s]Measuring inference for batch_size=512: 100%|██████████| 1000/1000 [00:13<00:00, 74.13it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 11580687848
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.52 GB
    total: 31.28 GB
    used: 3.29 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 374.76 MB
    max_inference_bytes: 392962560
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
  batch_size_512:
    max_inference: 378.48 MB
    max_inference_bytes: 396866048
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
params: 60192808
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 92.087 us +/- 4.894 us [89.884 us, 214.815 us]
        batches_per_second: 10.88 K +/- 384.81 [4.66 K, 11.13 K]
      metrics:
        batches_per_second_max: 11125.474801061007
        batches_per_second_mean: 10878.285528036435
        batches_per_second_min: 4655.165371809101
        batches_per_second_std: 384.8072544071125
        seconds_per_batch_max: 0.0002148151397705078
        seconds_per_batch_mean: 9.20870304107666e-05
        seconds_per_batch_min: 8.988380432128906e-05
        seconds_per_batch_std: 4.893885703753369e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 22.699 us +/- 0.593 us [21.935 us, 29.325 us]
        batches_per_second: 44.08 K +/- 992.24 [34.10 K, 45.59 K]
      metrics:
        batches_per_second_max: 45590.260869565216
        batches_per_second_mean: 44079.926000033454
        batches_per_second_min: 34100.03252032521
        batches_per_second_std: 992.240388489636
        seconds_per_batch_max: 2.9325485229492188e-05
        seconds_per_batch_mean: 2.2699356079101562e-05
        seconds_per_batch_min: 2.193450927734375e-05
        seconds_per_batch_std: 5.93405403733507e-07
    on_device_inference:
      human_readable:
        batch_latency: -13121268.672 us +/- 39.042 ms [-13843104.362 us, -13046463.966
          us]
        batches_per_second: -0.08 +/- 0.00 [-0.08, -0.07]
      metrics:
        batches_per_second_max: -0.07223813198358973
        batches_per_second_mean: -0.07621280507670576
        batches_per_second_min: -0.07664912136941768
        batches_per_second_std: 0.00022269474235152328
        seconds_per_batch_max: -13.046463966369629
        seconds_per_batch_mean: -13.12126867198944
        seconds_per_batch_min: -13.843104362487793
        seconds_per_batch_std: 0.03904158812004155
    total:
      human_readable:
        batch_latency: 13.242 ms +/- 42.031 us [13.166 ms, 14.103 ms]
        batches_per_second: 75.52 +/- 0.23 [70.91, 75.95]
      metrics:
        batches_per_second_max: 75.9548722406331
        batches_per_second_mean: 75.51546281627105
        batches_per_second_min: 70.90722207195023
        batches_per_second_std: 0.23357042853511278
        seconds_per_batch_max: 0.014102935791015625
        seconds_per_batch_mean: 0.013242450952529907
        seconds_per_batch_min: 0.013165712356567383
        seconds_per_batch_std: 4.203053992843956e-05
  batch_size_512:
    cpu_to_gpu:
      human_readable:
        batch_latency: 142.203 us +/- 5.229 us [139.952 us, 283.003 us]
        batches_per_second: 7.04 K +/- 171.30 [3.53 K, 7.15 K]
      metrics:
        batches_per_second_max: 7145.321976149915
        batches_per_second_mean: 7038.182912420504
        batches_per_second_min: 3533.5332771693343
        batches_per_second_std: 171.29589926958738
        seconds_per_batch_max: 0.0002830028533935547
        seconds_per_batch_mean: 0.00014220309257507324
        seconds_per_batch_min: 0.0001399517059326172
        seconds_per_batch_std: 5.2287034471142155e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 22.945 us +/- 0.648 us [22.173 us, 34.332 us]
        batches_per_second: 43.61 K +/- 1.04 K [29.13 K, 45.10 K]
      metrics:
        batches_per_second_max: 45100.04301075269
        batches_per_second_mean: 43611.28882518704
        batches_per_second_min: 29127.11111111111
        batches_per_second_std: 1043.0004553898546
        seconds_per_batch_max: 3.4332275390625e-05
        seconds_per_batch_mean: 2.2945165634155274e-05
        seconds_per_batch_min: 2.2172927856445312e-05
        seconds_per_batch_std: 6.483880380149327e-07
    on_device_inference:
      human_readable:
        batch_latency: -13310946.693 us +/- 37.581 ms [-13974592.209 us, -13233695.984
          us]
        batches_per_second: -0.08 +/- 0.00 [-0.08, -0.07]
      metrics:
        batches_per_second_max: -0.07155843870462476
        batches_per_second_mean: -0.07512672579117634
        batches_per_second_min: -0.07556467983075892
        batches_per_second_std: 0.00020872593305796367
        seconds_per_batch_max: -13.233695983886719
        seconds_per_batch_mean: -13.31094669342041
        seconds_per_batch_min: -13.974592208862305
        seconds_per_batch_std: 0.03758081013791982
    total:
      human_readable:
        batch_latency: 13.481 ms +/- 40.646 us [13.402 ms, 14.292 ms]
        batches_per_second: 74.18 +/- 0.22 [69.97, 74.62]
      metrics:
        batches_per_second_max: 74.61581157048317
        batches_per_second_mean: 74.1765004160829
        batches_per_second_min: 69.96920510467929
        batches_per_second_std: 0.21841663718899643
        seconds_per_batch_max: 0.014292001724243164
        seconds_per_batch_mean: 0.013481478214263916
        seconds_per_batch_min: 0.013401985168457031
        seconds_per_batch_std: 4.064588667203249e-05


#####
baseline-baseline-py-id - Run 2
2024-02-23 10:08:44
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.67it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.67it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  60.19 M, 100.000% Params, 11.58 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.016% Params, 118.01 MMac, 1.019% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.000% Params, 1.61 MMac, 0.014% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.007% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.007% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.359% Params, 680.39 MMac, 5.875% MACs, 
    (0): Bottleneck(
      75.01 k, 0.125% Params, 236.43 MMac, 2.042% MACs, 
      (conv1): Conv2d(4.1 k, 0.007% Params, 12.85 MMac, 0.111% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.028% Params, 52.99 MMac, 0.458% MACs, 
        (0): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    2.34 M, 3.887% Params, 1.92 GMac, 16.555% MACs, 
    (0): Bottleneck(
      379.39 k, 0.630% Params, 376.02 MMac, 3.247% MACs, 
      (conv1): Conv2d(32.77 k, 0.054% Params, 102.76 MMac, 0.887% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 802.82 KMac, 0.007% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.008% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.219% Params, 103.56 MMac, 0.894% MACs, 
        (0): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (4): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (5): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (6): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (7): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    40.61 M, 67.473% Params, 8.05 GMac, 69.501% MACs, 
    (0): Bottleneck(
      1.51 M, 2.513% Params, 374.26 MMac, 3.232% MACs, 
      (conv1): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 401.41 KMac, 0.003% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.004% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 0.874% Params, 103.16 MMac, 0.891% MACs, 
        (0): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (6): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (7): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (8): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (9): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (10): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (11): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (12): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (13): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (14): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (15): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (16): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (17): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (18): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (19): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (20): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (21): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (22): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (23): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (24): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (25): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (26): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (27): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (28): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (29): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (30): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (31): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (32): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (33): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (34): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (35): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 24.861% Params, 811.02 MMac, 7.003% MACs, 
    (0): Bottleneck(
      6.04 M, 10.034% Params, 373.38 MMac, 3.224% MACs, 
      (conv1): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 200.7 KMac, 0.002% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.002% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 3.491% Params, 102.96 MMac, 0.889% MACs, 
        (0): Conv2d(2.1 M, 3.484% Params, 102.76 MMac, 0.887% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.001% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 3.404% Params, 2.05 MMac, 0.018% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  10%|█         | 10/100 [00:00<00:00, 95.44it/s]Warming up with batch_size=1:  20%|██        | 20/100 [00:00<00:00, 95.80it/s]Warming up with batch_size=1:  30%|███       | 30/100 [00:00<00:00, 95.95it/s]Warming up with batch_size=1:  40%|████      | 40/100 [00:00<00:00, 96.07it/s]Warming up with batch_size=1:  50%|█████     | 50/100 [00:00<00:00, 96.12it/s]Warming up with batch_size=1:  60%|██████    | 60/100 [00:00<00:00, 96.11it/s]Warming up with batch_size=1:  70%|███████   | 70/100 [00:00<00:00, 96.17it/s]Warming up with batch_size=1:  80%|████████  | 80/100 [00:00<00:00, 96.15it/s]Warming up with batch_size=1:  90%|█████████ | 90/100 [00:00<00:00, 96.10it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:01<00:00, 96.06it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:01<00:00, 96.05it/s]
STAGE:2024-02-23 10:08:12 178065:178065 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 10:08:12 178065:178065 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 10:08:12 178065:178065 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   1%|          | 8/1000 [00:00<00:13, 71.78it/s]Measuring inference for batch_size=1:   2%|▏         | 16/1000 [00:00<00:13, 72.09it/s]Measuring inference for batch_size=1:   2%|▏         | 24/1000 [00:00<00:13, 72.25it/s]Measuring inference for batch_size=1:   3%|▎         | 32/1000 [00:00<00:13, 72.30it/s]Measuring inference for batch_size=1:   4%|▍         | 40/1000 [00:00<00:13, 72.34it/s]Measuring inference for batch_size=1:   5%|▍         | 48/1000 [00:00<00:13, 72.36it/s]Measuring inference for batch_size=1:   6%|▌         | 56/1000 [00:00<00:13, 72.37it/s]Measuring inference for batch_size=1:   6%|▋         | 64/1000 [00:00<00:12, 72.38it/s]Measuring inference for batch_size=1:   7%|▋         | 72/1000 [00:00<00:12, 72.40it/s]Measuring inference for batch_size=1:   8%|▊         | 80/1000 [00:01<00:12, 72.40it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:01<00:12, 72.42it/s]Measuring inference for batch_size=1:  10%|▉         | 96/1000 [00:01<00:12, 72.45it/s]Measuring inference for batch_size=1:  10%|█         | 104/1000 [00:01<00:12, 72.41it/s]Measuring inference for batch_size=1:  11%|█         | 112/1000 [00:01<00:12, 72.36it/s]Measuring inference for batch_size=1:  12%|█▏        | 120/1000 [00:01<00:12, 72.38it/s]Measuring inference for batch_size=1:  13%|█▎        | 128/1000 [00:01<00:12, 72.35it/s]Measuring inference for batch_size=1:  14%|█▎        | 136/1000 [00:01<00:11, 72.32it/s]Measuring inference for batch_size=1:  14%|█▍        | 144/1000 [00:01<00:11, 72.35it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:02<00:11, 72.34it/s]Measuring inference for batch_size=1:  16%|█▌        | 160/1000 [00:02<00:11, 72.35it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:02<00:11, 72.36it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:02<00:11, 72.34it/s]Measuring inference for batch_size=1:  18%|█▊        | 184/1000 [00:02<00:11, 72.33it/s]Measuring inference for batch_size=1:  19%|█▉        | 192/1000 [00:02<00:11, 72.33it/s]Measuring inference for batch_size=1:  20%|██        | 200/1000 [00:02<00:11, 72.35it/s]Measuring inference for batch_size=1:  21%|██        | 208/1000 [00:02<00:10, 72.32it/s]Measuring inference for batch_size=1:  22%|██▏       | 216/1000 [00:02<00:10, 72.34it/s]Measuring inference for batch_size=1:  22%|██▏       | 224/1000 [00:03<00:10, 72.36it/s]Measuring inference for batch_size=1:  23%|██▎       | 232/1000 [00:03<00:10, 72.35it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:03<00:10, 72.38it/s]Measuring inference for batch_size=1:  25%|██▍       | 248/1000 [00:03<00:10, 72.36it/s]Measuring inference for batch_size=1:  26%|██▌       | 256/1000 [00:03<00:10, 72.36it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:03<00:10, 72.37it/s]Measuring inference for batch_size=1:  27%|██▋       | 272/1000 [00:03<00:10, 72.34it/s]Measuring inference for batch_size=1:  28%|██▊       | 280/1000 [00:03<00:09, 72.34it/s]Measuring inference for batch_size=1:  29%|██▉       | 288/1000 [00:03<00:09, 72.31it/s]Measuring inference for batch_size=1:  30%|██▉       | 296/1000 [00:04<00:09, 72.30it/s]Measuring inference for batch_size=1:  30%|███       | 304/1000 [00:04<00:09, 72.31it/s]Measuring inference for batch_size=1:  31%|███       | 312/1000 [00:04<00:09, 72.32it/s]Measuring inference for batch_size=1:  32%|███▏      | 320/1000 [00:04<00:09, 72.30it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:04<00:09, 72.29it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:04<00:09, 72.31it/s]Measuring inference for batch_size=1:  34%|███▍      | 344/1000 [00:04<00:09, 72.31it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:04<00:08, 72.30it/s]Measuring inference for batch_size=1:  36%|███▌      | 360/1000 [00:04<00:08, 72.31it/s]Measuring inference for batch_size=1:  37%|███▋      | 368/1000 [00:05<00:08, 72.32it/s]Measuring inference for batch_size=1:  38%|███▊      | 376/1000 [00:05<00:08, 72.34it/s]Measuring inference for batch_size=1:  38%|███▊      | 384/1000 [00:05<00:08, 72.35it/s]Measuring inference for batch_size=1:  39%|███▉      | 392/1000 [00:05<00:08, 72.33it/s]Measuring inference for batch_size=1:  40%|████      | 400/1000 [00:05<00:08, 72.35it/s]Measuring inference for batch_size=1:  41%|████      | 408/1000 [00:05<00:08, 72.34it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:05<00:08, 72.34it/s]Measuring inference for batch_size=1:  42%|████▏     | 424/1000 [00:05<00:07, 72.38it/s]Measuring inference for batch_size=1:  43%|████▎     | 432/1000 [00:05<00:07, 72.42it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:06<00:07, 72.45it/s]Measuring inference for batch_size=1:  45%|████▍     | 448/1000 [00:06<00:07, 72.46it/s]Measuring inference for batch_size=1:  46%|████▌     | 456/1000 [00:06<00:07, 72.42it/s]Measuring inference for batch_size=1:  46%|████▋     | 464/1000 [00:06<00:07, 72.42it/s]Measuring inference for batch_size=1:  47%|████▋     | 472/1000 [00:06<00:07, 72.43it/s]Measuring inference for batch_size=1:  48%|████▊     | 480/1000 [00:06<00:07, 72.44it/s]Measuring inference for batch_size=1:  49%|████▉     | 488/1000 [00:06<00:07, 72.45it/s]Measuring inference for batch_size=1:  50%|████▉     | 496/1000 [00:06<00:06, 72.41it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:06<00:06, 72.42it/s]Measuring inference for batch_size=1:  51%|█████     | 512/1000 [00:07<00:06, 72.41it/s]Measuring inference for batch_size=1:  52%|█████▏    | 520/1000 [00:07<00:06, 72.46it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:07<00:06, 72.44it/s]Measuring inference for batch_size=1:  54%|█████▎    | 536/1000 [00:07<00:06, 72.45it/s]Measuring inference for batch_size=1:  54%|█████▍    | 544/1000 [00:07<00:06, 72.43it/s]Measuring inference for batch_size=1:  55%|█████▌    | 552/1000 [00:07<00:06, 72.44it/s]Measuring inference for batch_size=1:  56%|█████▌    | 560/1000 [00:07<00:06, 72.38it/s]Measuring inference for batch_size=1:  57%|█████▋    | 568/1000 [00:07<00:05, 72.45it/s]Measuring inference for batch_size=1:  58%|█████▊    | 576/1000 [00:07<00:05, 72.44it/s]Measuring inference for batch_size=1:  58%|█████▊    | 584/1000 [00:08<00:05, 72.46it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [00:08<00:05, 72.43it/s]Measuring inference for batch_size=1:  60%|██████    | 600/1000 [00:08<00:05, 72.45it/s]Measuring inference for batch_size=1:  61%|██████    | 608/1000 [00:08<00:05, 72.43it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:08<00:05, 72.42it/s]Measuring inference for batch_size=1:  62%|██████▏   | 624/1000 [00:08<00:05, 72.40it/s]Measuring inference for batch_size=1:  63%|██████▎   | 632/1000 [00:08<00:05, 72.44it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [00:08<00:04, 72.40it/s]Measuring inference for batch_size=1:  65%|██████▍   | 648/1000 [00:08<00:04, 72.41it/s]Measuring inference for batch_size=1:  66%|██████▌   | 656/1000 [00:09<00:04, 72.40it/s]Measuring inference for batch_size=1:  66%|██████▋   | 664/1000 [00:09<00:04, 72.43it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:09<00:04, 72.39it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [00:09<00:04, 72.40it/s]Measuring inference for batch_size=1:  69%|██████▉   | 688/1000 [00:09<00:04, 72.38it/s]Measuring inference for batch_size=1:  70%|██████▉   | 696/1000 [00:09<00:04, 72.42it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [00:09<00:04, 72.42it/s]Measuring inference for batch_size=1:  71%|███████   | 712/1000 [00:09<00:03, 72.47it/s]Measuring inference for batch_size=1:  72%|███████▏  | 720/1000 [00:09<00:03, 72.47it/s]Measuring inference for batch_size=1:  73%|███████▎  | 728/1000 [00:10<00:03, 72.49it/s]Measuring inference for batch_size=1:  74%|███████▎  | 736/1000 [00:10<00:03, 72.46it/s]Measuring inference for batch_size=1:  74%|███████▍  | 744/1000 [00:10<00:03, 72.46it/s]Measuring inference for batch_size=1:  75%|███████▌  | 752/1000 [00:10<00:03, 72.44it/s]Measuring inference for batch_size=1:  76%|███████▌  | 760/1000 [00:10<00:03, 72.43it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [00:10<00:03, 72.38it/s]Measuring inference for batch_size=1:  78%|███████▊  | 776/1000 [00:10<00:03, 72.42it/s]Measuring inference for batch_size=1:  78%|███████▊  | 784/1000 [00:10<00:02, 72.42it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [00:10<00:02, 72.45it/s]Measuring inference for batch_size=1:  80%|████████  | 800/1000 [00:11<00:02, 72.42it/s]Measuring inference for batch_size=1:  81%|████████  | 808/1000 [00:11<00:02, 72.40it/s]Measuring inference for batch_size=1:  82%|████████▏ | 816/1000 [00:11<00:02, 72.37it/s]Measuring inference for batch_size=1:  82%|████████▏ | 824/1000 [00:11<00:02, 72.39it/s]Measuring inference for batch_size=1:  83%|████████▎ | 832/1000 [00:11<00:02, 72.37it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:11<00:02, 72.40it/s]Measuring inference for batch_size=1:  85%|████████▍ | 848/1000 [00:11<00:02, 72.37it/s]Measuring inference for batch_size=1:  86%|████████▌ | 856/1000 [00:11<00:01, 72.40it/s]Measuring inference for batch_size=1:  86%|████████▋ | 864/1000 [00:11<00:01, 72.41it/s]Measuring inference for batch_size=1:  87%|████████▋ | 872/1000 [00:12<00:01, 72.41it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [00:12<00:01, 72.39it/s]Measuring inference for batch_size=1:  89%|████████▉ | 888/1000 [00:12<00:01, 72.42it/s]Measuring inference for batch_size=1:  90%|████████▉ | 896/1000 [00:12<00:01, 72.42it/s]Measuring inference for batch_size=1:  90%|█████████ | 904/1000 [00:12<00:01, 72.41it/s]Measuring inference for batch_size=1:  91%|█████████ | 912/1000 [00:12<00:01, 72.41it/s]Measuring inference for batch_size=1:  92%|█████████▏| 920/1000 [00:12<00:01, 72.44it/s]Measuring inference for batch_size=1:  93%|█████████▎| 928/1000 [00:12<00:00, 72.42it/s]Measuring inference for batch_size=1:  94%|█████████▎| 936/1000 [00:12<00:00, 72.42it/s]Measuring inference for batch_size=1:  94%|█████████▍| 944/1000 [00:13<00:00, 72.41it/s]Measuring inference for batch_size=1:  95%|█████████▌| 952/1000 [00:13<00:00, 72.44it/s]Measuring inference for batch_size=1:  96%|█████████▌| 960/1000 [00:13<00:00, 72.41it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [00:13<00:00, 72.44it/s]Measuring inference for batch_size=1:  98%|█████████▊| 976/1000 [00:13<00:00, 72.42it/s]Measuring inference for batch_size=1:  98%|█████████▊| 984/1000 [00:13<00:00, 72.45it/s]Measuring inference for batch_size=1:  99%|█████████▉| 992/1000 [00:13<00:00, 72.44it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 72.44it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 72.39it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=512:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=512:   8%|▊         | 8/100 [00:00<00:01, 71.68it/s]Warming up with batch_size=512:  16%|█▌        | 16/100 [00:00<00:01, 71.77it/s]Warming up with batch_size=512:  24%|██▍       | 24/100 [00:00<00:01, 71.87it/s]Warming up with batch_size=512:  32%|███▏      | 32/100 [00:00<00:00, 71.92it/s]Warming up with batch_size=512:  40%|████      | 40/100 [00:00<00:00, 71.91it/s]Warming up with batch_size=512:  48%|████▊     | 48/100 [00:00<00:00, 71.89it/s]Warming up with batch_size=512:  56%|█████▌    | 56/100 [00:00<00:00, 71.91it/s]Warming up with batch_size=512:  64%|██████▍   | 64/100 [00:00<00:00, 71.89it/s]Warming up with batch_size=512:  72%|███████▏  | 72/100 [00:01<00:00, 71.94it/s]Warming up with batch_size=512:  80%|████████  | 80/100 [00:01<00:00, 71.97it/s]Warming up with batch_size=512:  88%|████████▊ | 88/100 [00:01<00:00, 71.97it/s]Warming up with batch_size=512:  96%|█████████▌| 96/100 [00:01<00:00, 71.91it/s]Warming up with batch_size=512: 100%|██████████| 100/100 [00:01<00:00, 71.90it/s]
STAGE:2024-02-23 10:08:28 178065:178065 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 10:08:28 178065:178065 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 10:08:28 178065:178065 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=512:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=512:   1%|          | 8/1000 [00:00<00:13, 70.88it/s]Measuring inference for batch_size=512:   2%|▏         | 16/1000 [00:00<00:13, 71.13it/s]Measuring inference for batch_size=512:   2%|▏         | 24/1000 [00:00<00:13, 71.21it/s]Measuring inference for batch_size=512:   3%|▎         | 32/1000 [00:00<00:13, 71.29it/s]Measuring inference for batch_size=512:   4%|▍         | 40/1000 [00:00<00:13, 71.29it/s]Measuring inference for batch_size=512:   5%|▍         | 48/1000 [00:00<00:13, 71.32it/s]Measuring inference for batch_size=512:   6%|▌         | 56/1000 [00:00<00:13, 71.34it/s]Measuring inference for batch_size=512:   6%|▋         | 64/1000 [00:00<00:13, 71.33it/s]Measuring inference for batch_size=512:   7%|▋         | 72/1000 [00:01<00:13, 71.32it/s]Measuring inference for batch_size=512:   8%|▊         | 80/1000 [00:01<00:12, 71.33it/s]Measuring inference for batch_size=512:   9%|▉         | 88/1000 [00:01<00:12, 71.31it/s]Measuring inference for batch_size=512:  10%|▉         | 96/1000 [00:01<00:12, 71.32it/s]Measuring inference for batch_size=512:  10%|█         | 104/1000 [00:01<00:12, 71.32it/s]Measuring inference for batch_size=512:  11%|█         | 112/1000 [00:01<00:12, 71.31it/s]Measuring inference for batch_size=512:  12%|█▏        | 120/1000 [00:01<00:12, 71.32it/s]Measuring inference for batch_size=512:  13%|█▎        | 128/1000 [00:01<00:12, 71.33it/s]Measuring inference for batch_size=512:  14%|█▎        | 136/1000 [00:01<00:12, 71.34it/s]Measuring inference for batch_size=512:  14%|█▍        | 144/1000 [00:02<00:11, 71.35it/s]Measuring inference for batch_size=512:  15%|█▌        | 152/1000 [00:02<00:11, 71.38it/s]Measuring inference for batch_size=512:  16%|█▌        | 160/1000 [00:02<00:11, 71.37it/s]Measuring inference for batch_size=512:  17%|█▋        | 168/1000 [00:02<00:11, 71.37it/s]Measuring inference for batch_size=512:  18%|█▊        | 176/1000 [00:02<00:11, 71.36it/s]Measuring inference for batch_size=512:  18%|█▊        | 184/1000 [00:02<00:11, 71.36it/s]Measuring inference for batch_size=512:  19%|█▉        | 192/1000 [00:02<00:11, 71.39it/s]Measuring inference for batch_size=512:  20%|██        | 200/1000 [00:02<00:11, 71.40it/s]Measuring inference for batch_size=512:  21%|██        | 208/1000 [00:02<00:11, 71.41it/s]Measuring inference for batch_size=512:  22%|██▏       | 216/1000 [00:03<00:10, 71.39it/s]Measuring inference for batch_size=512:  22%|██▏       | 224/1000 [00:03<00:10, 71.40it/s]Measuring inference for batch_size=512:  23%|██▎       | 232/1000 [00:03<00:10, 71.38it/s]Measuring inference for batch_size=512:  24%|██▍       | 240/1000 [00:03<00:10, 71.39it/s]Measuring inference for batch_size=512:  25%|██▍       | 248/1000 [00:03<00:10, 71.41it/s]Measuring inference for batch_size=512:  26%|██▌       | 256/1000 [00:03<00:10, 71.40it/s]Measuring inference for batch_size=512:  26%|██▋       | 264/1000 [00:03<00:10, 71.42it/s]Measuring inference for batch_size=512:  27%|██▋       | 272/1000 [00:03<00:10, 71.40it/s]Measuring inference for batch_size=512:  28%|██▊       | 280/1000 [00:03<00:10, 71.41it/s]Measuring inference for batch_size=512:  29%|██▉       | 288/1000 [00:04<00:09, 71.37it/s]Measuring inference for batch_size=512:  30%|██▉       | 296/1000 [00:04<00:09, 71.36it/s]Measuring inference for batch_size=512:  30%|███       | 304/1000 [00:04<00:09, 71.31it/s]Measuring inference for batch_size=512:  31%|███       | 312/1000 [00:04<00:09, 71.33it/s]Measuring inference for batch_size=512:  32%|███▏      | 320/1000 [00:04<00:09, 71.35it/s]Measuring inference for batch_size=512:  33%|███▎      | 328/1000 [00:04<00:09, 71.36it/s]Measuring inference for batch_size=512:  34%|███▎      | 336/1000 [00:04<00:09, 71.38it/s]Measuring inference for batch_size=512:  34%|███▍      | 344/1000 [00:04<00:09, 71.37it/s]Measuring inference for batch_size=512:  35%|███▌      | 352/1000 [00:04<00:09, 71.39it/s]Measuring inference for batch_size=512:  36%|███▌      | 360/1000 [00:05<00:08, 71.37it/s]Measuring inference for batch_size=512:  37%|███▋      | 368/1000 [00:05<00:08, 71.37it/s]Measuring inference for batch_size=512:  38%|███▊      | 376/1000 [00:05<00:08, 71.38it/s]Measuring inference for batch_size=512:  38%|███▊      | 384/1000 [00:05<00:08, 71.38it/s]Measuring inference for batch_size=512:  39%|███▉      | 392/1000 [00:05<00:08, 71.37it/s]Measuring inference for batch_size=512:  40%|████      | 400/1000 [00:05<00:08, 71.34it/s]Measuring inference for batch_size=512:  41%|████      | 408/1000 [00:05<00:08, 71.31it/s]Measuring inference for batch_size=512:  42%|████▏     | 416/1000 [00:05<00:08, 71.31it/s]Measuring inference for batch_size=512:  42%|████▏     | 424/1000 [00:05<00:08, 71.30it/s]Measuring inference for batch_size=512:  43%|████▎     | 432/1000 [00:06<00:07, 71.27it/s]Measuring inference for batch_size=512:  44%|████▍     | 440/1000 [00:06<00:07, 71.25it/s]Measuring inference for batch_size=512:  45%|████▍     | 448/1000 [00:06<00:07, 71.27it/s]Measuring inference for batch_size=512:  46%|████▌     | 456/1000 [00:06<00:07, 71.28it/s]Measuring inference for batch_size=512:  46%|████▋     | 464/1000 [00:06<00:07, 71.28it/s]Measuring inference for batch_size=512:  47%|████▋     | 472/1000 [00:06<00:07, 71.26it/s]Measuring inference for batch_size=512:  48%|████▊     | 480/1000 [00:06<00:07, 71.28it/s]Measuring inference for batch_size=512:  49%|████▉     | 488/1000 [00:06<00:07, 71.29it/s]Measuring inference for batch_size=512:  50%|████▉     | 496/1000 [00:06<00:07, 71.31it/s]Measuring inference for batch_size=512:  50%|█████     | 504/1000 [00:07<00:06, 71.33it/s]Measuring inference for batch_size=512:  51%|█████     | 512/1000 [00:07<00:06, 71.31it/s]Measuring inference for batch_size=512:  52%|█████▏    | 520/1000 [00:07<00:06, 71.30it/s]Measuring inference for batch_size=512:  53%|█████▎    | 528/1000 [00:07<00:06, 71.33it/s]Measuring inference for batch_size=512:  54%|█████▎    | 536/1000 [00:07<00:06, 71.31it/s]Measuring inference for batch_size=512:  54%|█████▍    | 544/1000 [00:07<00:06, 71.31it/s]Measuring inference for batch_size=512:  55%|█████▌    | 552/1000 [00:07<00:06, 71.29it/s]Measuring inference for batch_size=512:  56%|█████▌    | 560/1000 [00:07<00:06, 71.31it/s]Measuring inference for batch_size=512:  57%|█████▋    | 568/1000 [00:07<00:06, 71.33it/s]Measuring inference for batch_size=512:  58%|█████▊    | 576/1000 [00:08<00:05, 71.32it/s]Measuring inference for batch_size=512:  58%|█████▊    | 584/1000 [00:08<00:05, 71.34it/s]Measuring inference for batch_size=512:  59%|█████▉    | 592/1000 [00:08<00:05, 71.34it/s]Measuring inference for batch_size=512:  60%|██████    | 600/1000 [00:08<00:05, 71.35it/s]Measuring inference for batch_size=512:  61%|██████    | 608/1000 [00:08<00:05, 71.35it/s]Measuring inference for batch_size=512:  62%|██████▏   | 616/1000 [00:08<00:05, 71.34it/s]Measuring inference for batch_size=512:  62%|██████▏   | 624/1000 [00:08<00:05, 71.36it/s]Measuring inference for batch_size=512:  63%|██████▎   | 632/1000 [00:08<00:05, 71.36it/s]Measuring inference for batch_size=512:  64%|██████▍   | 640/1000 [00:08<00:05, 71.36it/s]Measuring inference for batch_size=512:  65%|██████▍   | 648/1000 [00:09<00:04, 71.33it/s]Measuring inference for batch_size=512:  66%|██████▌   | 656/1000 [00:09<00:04, 71.34it/s]Measuring inference for batch_size=512:  66%|██████▋   | 664/1000 [00:09<00:04, 71.34it/s]Measuring inference for batch_size=512:  67%|██████▋   | 672/1000 [00:09<00:04, 71.38it/s]Measuring inference for batch_size=512:  68%|██████▊   | 680/1000 [00:09<00:04, 71.37it/s]Measuring inference for batch_size=512:  69%|██████▉   | 688/1000 [00:09<00:04, 71.35it/s]Measuring inference for batch_size=512:  70%|██████▉   | 696/1000 [00:09<00:04, 71.36it/s]Measuring inference for batch_size=512:  70%|███████   | 704/1000 [00:09<00:04, 71.36it/s]Measuring inference for batch_size=512:  71%|███████   | 712/1000 [00:09<00:04, 71.36it/s]Measuring inference for batch_size=512:  72%|███████▏  | 720/1000 [00:10<00:03, 71.37it/s]Measuring inference for batch_size=512:  73%|███████▎  | 728/1000 [00:10<00:03, 71.37it/s]Measuring inference for batch_size=512:  74%|███████▎  | 736/1000 [00:10<00:03, 71.33it/s]Measuring inference for batch_size=512:  74%|███████▍  | 744/1000 [00:10<00:03, 71.32it/s]Measuring inference for batch_size=512:  75%|███████▌  | 752/1000 [00:10<00:03, 71.31it/s]Measuring inference for batch_size=512:  76%|███████▌  | 760/1000 [00:10<00:03, 71.28it/s]Measuring inference for batch_size=512:  77%|███████▋  | 768/1000 [00:10<00:03, 71.27it/s]Measuring inference for batch_size=512:  78%|███████▊  | 776/1000 [00:10<00:03, 71.27it/s]Measuring inference for batch_size=512:  78%|███████▊  | 784/1000 [00:10<00:03, 71.29it/s]Measuring inference for batch_size=512:  79%|███████▉  | 792/1000 [00:11<00:02, 71.32it/s]Measuring inference for batch_size=512:  80%|████████  | 800/1000 [00:11<00:02, 71.31it/s]Measuring inference for batch_size=512:  81%|████████  | 808/1000 [00:11<00:02, 71.31it/s]Measuring inference for batch_size=512:  82%|████████▏ | 816/1000 [00:11<00:02, 71.30it/s]Measuring inference for batch_size=512:  82%|████████▏ | 824/1000 [00:11<00:02, 71.30it/s]Measuring inference for batch_size=512:  83%|████████▎ | 832/1000 [00:11<00:02, 71.33it/s]Measuring inference for batch_size=512:  84%|████████▍ | 840/1000 [00:11<00:02, 71.31it/s]Measuring inference for batch_size=512:  85%|████████▍ | 848/1000 [00:11<00:02, 71.31it/s]Measuring inference for batch_size=512:  86%|████████▌ | 856/1000 [00:12<00:02, 71.31it/s]Measuring inference for batch_size=512:  86%|████████▋ | 864/1000 [00:12<00:01, 71.30it/s]Measuring inference for batch_size=512:  87%|████████▋ | 872/1000 [00:12<00:01, 71.29it/s]Measuring inference for batch_size=512:  88%|████████▊ | 880/1000 [00:12<00:01, 71.31it/s]Measuring inference for batch_size=512:  89%|████████▉ | 888/1000 [00:12<00:01, 71.31it/s]Measuring inference for batch_size=512:  90%|████████▉ | 896/1000 [00:12<00:01, 71.31it/s]Measuring inference for batch_size=512:  90%|█████████ | 904/1000 [00:12<00:01, 71.33it/s]Measuring inference for batch_size=512:  91%|█████████ | 912/1000 [00:12<00:01, 71.36it/s]Measuring inference for batch_size=512:  92%|█████████▏| 920/1000 [00:12<00:01, 71.38it/s]Measuring inference for batch_size=512:  93%|█████████▎| 928/1000 [00:13<00:01, 71.37it/s]Measuring inference for batch_size=512:  94%|█████████▎| 936/1000 [00:13<00:00, 71.37it/s]Measuring inference for batch_size=512:  94%|█████████▍| 944/1000 [00:13<00:00, 71.34it/s]Measuring inference for batch_size=512:  95%|█████████▌| 952/1000 [00:13<00:00, 71.31it/s]Measuring inference for batch_size=512:  96%|█████████▌| 960/1000 [00:13<00:00, 71.30it/s]Measuring inference for batch_size=512:  97%|█████████▋| 968/1000 [00:13<00:00, 71.33it/s]Measuring inference for batch_size=512:  98%|█████████▊| 976/1000 [00:13<00:00, 71.33it/s]Measuring inference for batch_size=512:  98%|█████████▊| 984/1000 [00:13<00:00, 71.30it/s]Measuring inference for batch_size=512:  99%|█████████▉| 992/1000 [00:13<00:00, 71.27it/s]Measuring inference for batch_size=512: 100%|██████████| 1000/1000 [00:14<00:00, 71.28it/s]Measuring inference for batch_size=512: 100%|██████████| 1000/1000 [00:14<00:00, 71.33it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 11580687848
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.53 GB
    total: 31.28 GB
    used: 3.29 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 374.76 MB
    max_inference_bytes: 392962560
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
  batch_size_512:
    max_inference: 378.48 MB
    max_inference_bytes: 396866048
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
params: 60192808
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 93.325 us +/- 4.928 us [91.076 us, 214.100 us]
        batches_per_second: 10.73 K +/- 386.27 [4.67 K, 10.98 K]
      metrics:
        batches_per_second_max: 10979.853403141362
        batches_per_second_mean: 10734.255812105112
        batches_per_second_min: 4670.71714922049
        batches_per_second_std: 386.265752951651
        seconds_per_batch_max: 0.00021409988403320312
        seconds_per_batch_mean: 9.332513809204102e-05
        seconds_per_batch_min: 9.107589721679688e-05
        seconds_per_batch_std: 4.9276203118747976e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.255 us +/- 0.604 us [22.411 us, 30.041 us]
        batches_per_second: 43.03 K +/- 944.79 [33.29 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 43026.18682725158
        batches_per_second_min: 33288.12698412698
        batches_per_second_std: 944.7869798531755
        seconds_per_batch_max: 3.0040740966796875e-05
        seconds_per_batch_mean: 2.3254871368408202e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 6.043126075986808e-07
    on_device_inference:
      human_readable:
        batch_latency: -13681980.771 us +/- 33.296 ms [-14419039.726 us, -13620991.707
          us]
        batches_per_second: -0.07 +/- 0.00 [-0.07, -0.07]
      metrics:
        batches_per_second_max: -0.06935274602087284
        batches_per_second_mean: -0.07308925437839543
        batches_per_second_min: -0.07341609344767723
        batches_per_second_std: 0.00017344083451906566
        seconds_per_batch_max: -13.620991706848145
        seconds_per_batch_mean: -13.681980771064758
        seconds_per_batch_min: -14.419039726257324
        seconds_per_batch_std: 0.03329626192555469
    total:
      human_readable:
        batch_latency: 13.806 ms +/- 36.693 us [13.744 ms, 14.681 ms]
        batches_per_second: 72.43 +/- 0.19 [68.12, 72.76]
      metrics:
        batches_per_second_max: 72.75714682209271
        batches_per_second_mean: 72.4311802773211
        batches_per_second_min: 68.11588930752241
        batches_per_second_std: 0.18604418399775013
        seconds_per_batch_max: 0.014680862426757812
        seconds_per_batch_mean: 0.013806303024291993
        seconds_per_batch_min: 0.013744354248046875
        seconds_per_batch_std: 3.669267182225567e-05
  batch_size_512:
    cpu_to_gpu:
      human_readable:
        batch_latency: 144.382 us +/- 5.524 us [141.859 us, 286.818 us]
        batches_per_second: 6.93 K +/- 182.29 [3.49 K, 7.05 K]
      metrics:
        batches_per_second_max: 7049.250420168068
        batches_per_second_mean: 6932.683086282676
        batches_per_second_min: 3486.536990856193
        batches_per_second_std: 182.29088324164562
        seconds_per_batch_max: 0.0002868175506591797
        seconds_per_batch_mean: 0.0001443824768066406
        seconds_per_batch_min: 0.0001418590545654297
        seconds_per_batch_std: 5.524230733472173e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.341 us +/- 0.426 us [22.411 us, 29.802 us]
        batches_per_second: 42.86 K +/- 726.17 [33.55 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 42856.37394832961
        batches_per_second_min: 33554.432
        batches_per_second_std: 726.1712596109411
        seconds_per_batch_max: 2.9802322387695312e-05
        seconds_per_batch_mean: 2.3340940475463867e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 4.261493842927508e-07
    on_device_inference:
      human_readable:
        batch_latency: -13837895.406 us +/- 32.743 ms [-14518431.664 us, -13780703.545
          us]
        batches_per_second: -0.07 +/- 0.00 [-0.07, -0.07]
      metrics:
        batches_per_second_max: -0.06887796307318356
        batches_per_second_mean: -0.07226572048830951
        batches_per_second_min: -0.07256523563999318
        batches_per_second_std: 0.00016745825222100067
        seconds_per_batch_max: -13.7807035446167
        seconds_per_batch_mean: -13.837895405769348
        seconds_per_batch_min: -14.518431663513184
        seconds_per_batch_std: 0.032742540764502844
    total:
      human_readable:
        batch_latency: 14.011 ms +/- 36.122 us [13.953 ms, 14.842 ms]
        batches_per_second: 71.37 +/- 0.18 [67.38, 71.67]
      metrics:
        batches_per_second_max: 71.66687740281931
        batches_per_second_mean: 71.37250055714976
        batches_per_second_min: 67.37837751004017
        batches_per_second_std: 0.17853926384218766
        seconds_per_batch_max: 0.014841556549072266
        seconds_per_batch_mean: 0.014011088848114014
        seconds_per_batch_min: 0.013953447341918945
        seconds_per_batch_std: 3.612195005710479e-05


#####
baseline-baseline-py-id - Run 3
2024-02-23 10:09:21
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  60.19 M, 100.000% Params, 11.58 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.016% Params, 118.01 MMac, 1.019% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.000% Params, 1.61 MMac, 0.014% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.007% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.007% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.359% Params, 680.39 MMac, 5.875% MACs, 
    (0): Bottleneck(
      75.01 k, 0.125% Params, 236.43 MMac, 2.042% MACs, 
      (conv1): Conv2d(4.1 k, 0.007% Params, 12.85 MMac, 0.111% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.028% Params, 52.99 MMac, 0.458% MACs, 
        (0): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    2.34 M, 3.887% Params, 1.92 GMac, 16.555% MACs, 
    (0): Bottleneck(
      379.39 k, 0.630% Params, 376.02 MMac, 3.247% MACs, 
      (conv1): Conv2d(32.77 k, 0.054% Params, 102.76 MMac, 0.887% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 802.82 KMac, 0.007% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.008% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.219% Params, 103.56 MMac, 0.894% MACs, 
        (0): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (4): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (5): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (6): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (7): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    40.61 M, 67.473% Params, 8.05 GMac, 69.501% MACs, 
    (0): Bottleneck(
      1.51 M, 2.513% Params, 374.26 MMac, 3.232% MACs, 
      (conv1): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 401.41 KMac, 0.003% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.004% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 0.874% Params, 103.16 MMac, 0.891% MACs, 
        (0): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (6): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (7): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (8): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (9): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (10): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (11): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (12): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (13): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (14): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (15): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (16): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (17): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (18): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (19): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (20): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (21): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (22): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (23): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (24): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (25): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (26): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (27): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (28): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (29): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (30): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (31): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (32): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (33): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (34): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (35): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 24.861% Params, 811.02 MMac, 7.003% MACs, 
    (0): Bottleneck(
      6.04 M, 10.034% Params, 373.38 MMac, 3.224% MACs, 
      (conv1): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 200.7 KMac, 0.002% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.002% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 3.491% Params, 102.96 MMac, 0.889% MACs, 
        (0): Conv2d(2.1 M, 3.484% Params, 102.76 MMac, 0.887% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.001% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 3.404% Params, 2.05 MMac, 0.018% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  10%|█         | 10/100 [00:00<00:00, 96.17it/s]Warming up with batch_size=1:  20%|██        | 20/100 [00:00<00:00, 96.33it/s]Warming up with batch_size=1:  30%|███       | 30/100 [00:00<00:00, 96.44it/s]Warming up with batch_size=1:  40%|████      | 40/100 [00:00<00:00, 96.49it/s]Warming up with batch_size=1:  50%|█████     | 50/100 [00:00<00:00, 96.52it/s]Warming up with batch_size=1:  60%|██████    | 60/100 [00:00<00:00, 96.50it/s]Warming up with batch_size=1:  70%|███████   | 70/100 [00:00<00:00, 96.49it/s]Warming up with batch_size=1:  80%|████████  | 80/100 [00:00<00:00, 96.50it/s]Warming up with batch_size=1:  90%|█████████ | 90/100 [00:00<00:00, 96.50it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:01<00:00, 96.49it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:01<00:00, 96.47it/s]
STAGE:2024-02-23 10:08:51 178111:178111 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 10:08:51 178111:178111 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 10:08:51 178111:178111 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   1%|          | 8/1000 [00:00<00:13, 72.11it/s]Measuring inference for batch_size=1:   2%|▏         | 16/1000 [00:00<00:13, 72.46it/s]Measuring inference for batch_size=1:   2%|▏         | 24/1000 [00:00<00:13, 72.52it/s]Measuring inference for batch_size=1:   3%|▎         | 32/1000 [00:00<00:13, 72.55it/s]Measuring inference for batch_size=1:   4%|▍         | 40/1000 [00:00<00:13, 72.65it/s]Measuring inference for batch_size=1:   5%|▍         | 48/1000 [00:00<00:13, 72.65it/s]Measuring inference for batch_size=1:   6%|▌         | 56/1000 [00:00<00:12, 72.68it/s]Measuring inference for batch_size=1:   6%|▋         | 64/1000 [00:00<00:12, 72.67it/s]Measuring inference for batch_size=1:   7%|▋         | 72/1000 [00:00<00:12, 72.68it/s]Measuring inference for batch_size=1:   8%|▊         | 80/1000 [00:01<00:12, 72.69it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:01<00:12, 72.68it/s]Measuring inference for batch_size=1:  10%|▉         | 96/1000 [00:01<00:12, 72.67it/s]Measuring inference for batch_size=1:  10%|█         | 104/1000 [00:01<00:12, 72.70it/s]Measuring inference for batch_size=1:  11%|█         | 112/1000 [00:01<00:12, 72.69it/s]Measuring inference for batch_size=1:  12%|█▏        | 120/1000 [00:01<00:12, 72.72it/s]Measuring inference for batch_size=1:  13%|█▎        | 128/1000 [00:01<00:11, 72.72it/s]Measuring inference for batch_size=1:  14%|█▎        | 136/1000 [00:01<00:11, 72.73it/s]Measuring inference for batch_size=1:  14%|█▍        | 144/1000 [00:01<00:11, 72.73it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:02<00:11, 72.72it/s]Measuring inference for batch_size=1:  16%|█▌        | 160/1000 [00:02<00:11, 72.73it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:02<00:11, 72.73it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:02<00:11, 72.76it/s]Measuring inference for batch_size=1:  18%|█▊        | 184/1000 [00:02<00:11, 72.77it/s]Measuring inference for batch_size=1:  19%|█▉        | 192/1000 [00:02<00:11, 72.78it/s]Measuring inference for batch_size=1:  20%|██        | 200/1000 [00:02<00:10, 72.80it/s]Measuring inference for batch_size=1:  21%|██        | 208/1000 [00:02<00:10, 72.80it/s]Measuring inference for batch_size=1:  22%|██▏       | 216/1000 [00:02<00:10, 72.81it/s]Measuring inference for batch_size=1:  22%|██▏       | 224/1000 [00:03<00:10, 72.82it/s]Measuring inference for batch_size=1:  23%|██▎       | 232/1000 [00:03<00:10, 72.82it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:03<00:10, 72.86it/s]Measuring inference for batch_size=1:  25%|██▍       | 248/1000 [00:03<00:10, 72.89it/s]Measuring inference for batch_size=1:  26%|██▌       | 256/1000 [00:03<00:10, 72.88it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:03<00:10, 72.89it/s]Measuring inference for batch_size=1:  27%|██▋       | 272/1000 [00:03<00:09, 72.87it/s]Measuring inference for batch_size=1:  28%|██▊       | 280/1000 [00:03<00:09, 72.85it/s]Measuring inference for batch_size=1:  29%|██▉       | 288/1000 [00:03<00:09, 72.83it/s]Measuring inference for batch_size=1:  30%|██▉       | 296/1000 [00:04<00:09, 72.88it/s]Measuring inference for batch_size=1:  30%|███       | 304/1000 [00:04<00:09, 72.90it/s]Measuring inference for batch_size=1:  31%|███       | 312/1000 [00:04<00:09, 72.89it/s]Measuring inference for batch_size=1:  32%|███▏      | 320/1000 [00:04<00:09, 72.90it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:04<00:09, 72.89it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:04<00:09, 72.88it/s]Measuring inference for batch_size=1:  34%|███▍      | 344/1000 [00:04<00:09, 72.87it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:04<00:08, 72.85it/s]Measuring inference for batch_size=1:  36%|███▌      | 360/1000 [00:04<00:08, 72.83it/s]Measuring inference for batch_size=1:  37%|███▋      | 368/1000 [00:05<00:08, 72.80it/s]Measuring inference for batch_size=1:  38%|███▊      | 376/1000 [00:05<00:08, 72.75it/s]Measuring inference for batch_size=1:  38%|███▊      | 384/1000 [00:05<00:08, 72.73it/s]Measuring inference for batch_size=1:  39%|███▉      | 392/1000 [00:05<00:08, 72.72it/s]Measuring inference for batch_size=1:  40%|████      | 400/1000 [00:05<00:08, 72.72it/s]Measuring inference for batch_size=1:  41%|████      | 408/1000 [00:05<00:08, 72.72it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:05<00:08, 72.73it/s]Measuring inference for batch_size=1:  42%|████▏     | 424/1000 [00:05<00:07, 72.79it/s]Measuring inference for batch_size=1:  43%|████▎     | 432/1000 [00:05<00:07, 72.83it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:06<00:07, 72.85it/s]Measuring inference for batch_size=1:  45%|████▍     | 448/1000 [00:06<00:07, 72.90it/s]Measuring inference for batch_size=1:  46%|████▌     | 456/1000 [00:06<00:07, 72.92it/s]Measuring inference for batch_size=1:  46%|████▋     | 464/1000 [00:06<00:07, 72.93it/s]Measuring inference for batch_size=1:  47%|████▋     | 472/1000 [00:06<00:07, 72.96it/s]Measuring inference for batch_size=1:  48%|████▊     | 480/1000 [00:06<00:07, 72.97it/s]Measuring inference for batch_size=1:  49%|████▉     | 488/1000 [00:06<00:07, 72.93it/s]Measuring inference for batch_size=1:  50%|████▉     | 496/1000 [00:06<00:06, 72.90it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:06<00:06, 72.88it/s]Measuring inference for batch_size=1:  51%|█████     | 512/1000 [00:07<00:06, 72.85it/s]Measuring inference for batch_size=1:  52%|█████▏    | 520/1000 [00:07<00:06, 72.84it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:07<00:06, 72.82it/s]Measuring inference for batch_size=1:  54%|█████▎    | 536/1000 [00:07<00:06, 72.83it/s]Measuring inference for batch_size=1:  54%|█████▍    | 544/1000 [00:07<00:06, 72.83it/s]Measuring inference for batch_size=1:  55%|█████▌    | 552/1000 [00:07<00:06, 72.84it/s]Measuring inference for batch_size=1:  56%|█████▌    | 560/1000 [00:07<00:06, 72.82it/s]Measuring inference for batch_size=1:  57%|█████▋    | 568/1000 [00:07<00:05, 72.80it/s]Measuring inference for batch_size=1:  58%|█████▊    | 576/1000 [00:07<00:05, 72.78it/s]Measuring inference for batch_size=1:  58%|█████▊    | 584/1000 [00:08<00:05, 72.78it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [00:08<00:05, 72.77it/s]Measuring inference for batch_size=1:  60%|██████    | 600/1000 [00:08<00:05, 72.78it/s]Measuring inference for batch_size=1:  61%|██████    | 608/1000 [00:08<00:05, 72.79it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:08<00:05, 72.78it/s]Measuring inference for batch_size=1:  62%|██████▏   | 624/1000 [00:08<00:05, 72.80it/s]Measuring inference for batch_size=1:  63%|██████▎   | 632/1000 [00:08<00:05, 72.80it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [00:08<00:04, 72.79it/s]Measuring inference for batch_size=1:  65%|██████▍   | 648/1000 [00:08<00:04, 72.82it/s]Measuring inference for batch_size=1:  66%|██████▌   | 656/1000 [00:09<00:04, 72.83it/s]Measuring inference for batch_size=1:  66%|██████▋   | 664/1000 [00:09<00:04, 72.86it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:09<00:04, 72.85it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [00:09<00:04, 72.84it/s]Measuring inference for batch_size=1:  69%|██████▉   | 688/1000 [00:09<00:04, 72.83it/s]Measuring inference for batch_size=1:  70%|██████▉   | 696/1000 [00:09<00:04, 72.86it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [00:09<00:04, 72.86it/s]Measuring inference for batch_size=1:  71%|███████   | 712/1000 [00:09<00:03, 72.87it/s]Measuring inference for batch_size=1:  72%|███████▏  | 720/1000 [00:09<00:03, 72.85it/s]Measuring inference for batch_size=1:  73%|███████▎  | 728/1000 [00:10<00:03, 72.84it/s]Measuring inference for batch_size=1:  74%|███████▎  | 736/1000 [00:10<00:03, 72.84it/s]Measuring inference for batch_size=1:  74%|███████▍  | 744/1000 [00:10<00:03, 72.86it/s]Measuring inference for batch_size=1:  75%|███████▌  | 752/1000 [00:10<00:03, 72.84it/s]Measuring inference for batch_size=1:  76%|███████▌  | 760/1000 [00:10<00:03, 72.84it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [00:10<00:03, 72.83it/s]Measuring inference for batch_size=1:  78%|███████▊  | 776/1000 [00:10<00:03, 72.87it/s]Measuring inference for batch_size=1:  78%|███████▊  | 784/1000 [00:10<00:02, 72.86it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [00:10<00:02, 72.88it/s]Measuring inference for batch_size=1:  80%|████████  | 800/1000 [00:10<00:02, 72.82it/s]Measuring inference for batch_size=1:  81%|████████  | 808/1000 [00:11<00:02, 72.83it/s]Measuring inference for batch_size=1:  82%|████████▏ | 816/1000 [00:11<00:02, 72.81it/s]Measuring inference for batch_size=1:  82%|████████▏ | 824/1000 [00:11<00:02, 72.80it/s]Measuring inference for batch_size=1:  83%|████████▎ | 832/1000 [00:11<00:02, 72.78it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:11<00:02, 72.80it/s]Measuring inference for batch_size=1:  85%|████████▍ | 848/1000 [00:11<00:02, 72.84it/s]Measuring inference for batch_size=1:  86%|████████▌ | 856/1000 [00:11<00:01, 72.83it/s]Measuring inference for batch_size=1:  86%|████████▋ | 864/1000 [00:11<00:01, 72.80it/s]Measuring inference for batch_size=1:  87%|████████▋ | 872/1000 [00:11<00:01, 72.51it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [00:12<00:01, 72.34it/s]Measuring inference for batch_size=1:  89%|████████▉ | 888/1000 [00:12<00:01, 72.48it/s]Measuring inference for batch_size=1:  90%|████████▉ | 896/1000 [00:12<00:01, 72.58it/s]Measuring inference for batch_size=1:  90%|█████████ | 904/1000 [00:12<00:01, 72.63it/s]Measuring inference for batch_size=1:  91%|█████████ | 912/1000 [00:12<00:01, 72.67it/s]Measuring inference for batch_size=1:  92%|█████████▏| 920/1000 [00:12<00:01, 72.70it/s]Measuring inference for batch_size=1:  93%|█████████▎| 928/1000 [00:12<00:00, 72.67it/s]Measuring inference for batch_size=1:  94%|█████████▎| 936/1000 [00:12<00:00, 72.72it/s]Measuring inference for batch_size=1:  94%|█████████▍| 944/1000 [00:12<00:00, 72.72it/s]Measuring inference for batch_size=1:  95%|█████████▌| 952/1000 [00:13<00:00, 72.75it/s]Measuring inference for batch_size=1:  96%|█████████▌| 960/1000 [00:13<00:00, 72.70it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [00:13<00:00, 72.72it/s]Measuring inference for batch_size=1:  98%|█████████▊| 976/1000 [00:13<00:00, 72.71it/s]Measuring inference for batch_size=1:  98%|█████████▊| 984/1000 [00:13<00:00, 72.76it/s]Measuring inference for batch_size=1:  99%|█████████▉| 992/1000 [00:13<00:00, 72.74it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 72.74it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 72.78it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=512:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=512:   8%|▊         | 8/100 [00:00<00:01, 71.85it/s]Warming up with batch_size=512:  16%|█▌        | 16/100 [00:00<00:01, 72.07it/s]Warming up with batch_size=512:  24%|██▍       | 24/100 [00:00<00:01, 72.11it/s]Warming up with batch_size=512:  32%|███▏      | 32/100 [00:00<00:00, 72.10it/s]Warming up with batch_size=512:  40%|████      | 40/100 [00:00<00:00, 72.08it/s]Warming up with batch_size=512:  48%|████▊     | 48/100 [00:00<00:00, 72.13it/s]Warming up with batch_size=512:  56%|█████▌    | 56/100 [00:00<00:00, 72.14it/s]Warming up with batch_size=512:  64%|██████▍   | 64/100 [00:00<00:00, 72.14it/s]Warming up with batch_size=512:  72%|███████▏  | 72/100 [00:00<00:00, 72.16it/s]Warming up with batch_size=512:  80%|████████  | 80/100 [00:01<00:00, 72.16it/s]Warming up with batch_size=512:  88%|████████▊ | 88/100 [00:01<00:00, 72.16it/s]Warming up with batch_size=512:  96%|█████████▌| 96/100 [00:01<00:00, 72.13it/s]Warming up with batch_size=512: 100%|██████████| 100/100 [00:01<00:00, 72.12it/s]
STAGE:2024-02-23 10:09:06 178111:178111 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 10:09:06 178111:178111 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 10:09:06 178111:178111 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=512:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=512:   1%|          | 8/1000 [00:00<00:13, 71.00it/s]Measuring inference for batch_size=512:   2%|▏         | 16/1000 [00:00<00:13, 71.34it/s]Measuring inference for batch_size=512:   2%|▏         | 24/1000 [00:00<00:13, 71.38it/s]Measuring inference for batch_size=512:   3%|▎         | 32/1000 [00:00<00:13, 71.38it/s]Measuring inference for batch_size=512:   4%|▍         | 40/1000 [00:00<00:13, 71.36it/s]Measuring inference for batch_size=512:   5%|▍         | 48/1000 [00:00<00:13, 71.43it/s]Measuring inference for batch_size=512:   6%|▌         | 56/1000 [00:00<00:13, 71.48it/s]Measuring inference for batch_size=512:   6%|▋         | 64/1000 [00:00<00:13, 71.47it/s]Measuring inference for batch_size=512:   7%|▋         | 72/1000 [00:01<00:12, 71.47it/s]Measuring inference for batch_size=512:   8%|▊         | 80/1000 [00:01<00:12, 71.50it/s]Measuring inference for batch_size=512:   9%|▉         | 88/1000 [00:01<00:12, 71.48it/s]Measuring inference for batch_size=512:  10%|▉         | 96/1000 [00:01<00:12, 71.47it/s]Measuring inference for batch_size=512:  10%|█         | 104/1000 [00:01<00:12, 71.48it/s]Measuring inference for batch_size=512:  11%|█         | 112/1000 [00:01<00:12, 71.45it/s]Measuring inference for batch_size=512:  12%|█▏        | 120/1000 [00:01<00:12, 71.47it/s]Measuring inference for batch_size=512:  13%|█▎        | 128/1000 [00:01<00:12, 70.83it/s]Measuring inference for batch_size=512:  14%|█▎        | 136/1000 [00:01<00:12, 70.09it/s]Measuring inference for batch_size=512:  14%|█▍        | 144/1000 [00:02<00:12, 69.69it/s]Measuring inference for batch_size=512:  15%|█▌        | 152/1000 [00:02<00:12, 70.22it/s]Measuring inference for batch_size=512:  16%|█▌        | 160/1000 [00:02<00:11, 70.60it/s]Measuring inference for batch_size=512:  17%|█▋        | 168/1000 [00:02<00:11, 70.85it/s]Measuring inference for batch_size=512:  18%|█▊        | 176/1000 [00:02<00:11, 71.02it/s]Measuring inference for batch_size=512:  18%|█▊        | 184/1000 [00:02<00:11, 71.16it/s]Measuring inference for batch_size=512:  19%|█▉        | 192/1000 [00:02<00:11, 71.24it/s]Measuring inference for batch_size=512:  20%|██        | 200/1000 [00:02<00:11, 71.28it/s]Measuring inference for batch_size=512:  21%|██        | 208/1000 [00:02<00:11, 71.31it/s]Measuring inference for batch_size=512:  22%|██▏       | 216/1000 [00:03<00:10, 71.31it/s]Measuring inference for batch_size=512:  22%|██▏       | 224/1000 [00:03<00:10, 71.32it/s]Measuring inference for batch_size=512:  23%|██▎       | 232/1000 [00:03<00:10, 71.35it/s]Measuring inference for batch_size=512:  24%|██▍       | 240/1000 [00:03<00:10, 71.38it/s]Measuring inference for batch_size=512:  25%|██▍       | 248/1000 [00:03<00:10, 71.44it/s]Measuring inference for batch_size=512:  26%|██▌       | 256/1000 [00:03<00:10, 71.45it/s]Measuring inference for batch_size=512:  26%|██▋       | 264/1000 [00:03<00:10, 71.46it/s]Measuring inference for batch_size=512:  27%|██▋       | 272/1000 [00:03<00:10, 71.43it/s]Measuring inference for batch_size=512:  28%|██▊       | 280/1000 [00:03<00:10, 71.45it/s]Measuring inference for batch_size=512:  29%|██▉       | 288/1000 [00:04<00:09, 71.47it/s]Measuring inference for batch_size=512:  30%|██▉       | 296/1000 [00:04<00:09, 71.47it/s]Measuring inference for batch_size=512:  30%|███       | 304/1000 [00:04<00:09, 71.52it/s]Measuring inference for batch_size=512:  31%|███       | 312/1000 [00:04<00:09, 71.52it/s]Measuring inference for batch_size=512:  32%|███▏      | 320/1000 [00:04<00:09, 71.50it/s]Measuring inference for batch_size=512:  33%|███▎      | 328/1000 [00:04<00:09, 71.51it/s]Measuring inference for batch_size=512:  34%|███▎      | 336/1000 [00:04<00:09, 71.50it/s]Measuring inference for batch_size=512:  34%|███▍      | 344/1000 [00:04<00:09, 71.53it/s]Measuring inference for batch_size=512:  35%|███▌      | 352/1000 [00:04<00:09, 71.55it/s]Measuring inference for batch_size=512:  36%|███▌      | 360/1000 [00:05<00:08, 71.53it/s]Measuring inference for batch_size=512:  37%|███▋      | 368/1000 [00:05<00:08, 71.51it/s]Measuring inference for batch_size=512:  38%|███▊      | 376/1000 [00:05<00:08, 71.51it/s]Measuring inference for batch_size=512:  38%|███▊      | 384/1000 [00:05<00:08, 71.47it/s]Measuring inference for batch_size=512:  39%|███▉      | 392/1000 [00:05<00:08, 71.47it/s]Measuring inference for batch_size=512:  40%|████      | 400/1000 [00:05<00:08, 71.46it/s]Measuring inference for batch_size=512:  41%|████      | 408/1000 [00:05<00:08, 71.45it/s]Measuring inference for batch_size=512:  42%|████▏     | 416/1000 [00:05<00:08, 71.45it/s]Measuring inference for batch_size=512:  42%|████▏     | 424/1000 [00:05<00:08, 71.47it/s]Measuring inference for batch_size=512:  43%|████▎     | 432/1000 [00:06<00:07, 71.48it/s]Measuring inference for batch_size=512:  44%|████▍     | 440/1000 [00:06<00:07, 71.47it/s]Measuring inference for batch_size=512:  45%|████▍     | 448/1000 [00:06<00:07, 71.48it/s]Measuring inference for batch_size=512:  46%|████▌     | 456/1000 [00:06<00:07, 71.46it/s]Measuring inference for batch_size=512:  46%|████▋     | 464/1000 [00:06<00:07, 71.44it/s]Measuring inference for batch_size=512:  47%|████▋     | 472/1000 [00:06<00:07, 71.40it/s]Measuring inference for batch_size=512:  48%|████▊     | 480/1000 [00:06<00:07, 71.43it/s]Measuring inference for batch_size=512:  49%|████▉     | 488/1000 [00:06<00:07, 71.47it/s]Measuring inference for batch_size=512:  50%|████▉     | 496/1000 [00:06<00:07, 71.47it/s]Measuring inference for batch_size=512:  50%|█████     | 504/1000 [00:07<00:06, 71.49it/s]Measuring inference for batch_size=512:  51%|█████     | 512/1000 [00:07<00:06, 71.49it/s]Measuring inference for batch_size=512:  52%|█████▏    | 520/1000 [00:07<00:06, 71.46it/s]Measuring inference for batch_size=512:  53%|█████▎    | 528/1000 [00:07<00:06, 71.43it/s]Measuring inference for batch_size=512:  54%|█████▎    | 536/1000 [00:07<00:06, 71.39it/s]Measuring inference for batch_size=512:  54%|█████▍    | 544/1000 [00:07<00:06, 71.38it/s]Measuring inference for batch_size=512:  55%|█████▌    | 552/1000 [00:07<00:06, 71.39it/s]Measuring inference for batch_size=512:  56%|█████▌    | 560/1000 [00:07<00:06, 71.40it/s]Measuring inference for batch_size=512:  57%|█████▋    | 568/1000 [00:07<00:06, 71.37it/s]Measuring inference for batch_size=512:  58%|█████▊    | 576/1000 [00:08<00:05, 71.35it/s]Measuring inference for batch_size=512:  58%|█████▊    | 584/1000 [00:08<00:05, 71.35it/s]Measuring inference for batch_size=512:  59%|█████▉    | 592/1000 [00:08<00:05, 71.35it/s]Measuring inference for batch_size=512:  60%|██████    | 600/1000 [00:08<00:05, 71.38it/s]Measuring inference for batch_size=512:  61%|██████    | 608/1000 [00:08<00:05, 71.41it/s]Measuring inference for batch_size=512:  62%|██████▏   | 616/1000 [00:08<00:05, 71.43it/s]Measuring inference for batch_size=512:  62%|██████▏   | 624/1000 [00:08<00:05, 71.45it/s]Measuring inference for batch_size=512:  63%|██████▎   | 632/1000 [00:08<00:05, 71.46it/s]Measuring inference for batch_size=512:  64%|██████▍   | 640/1000 [00:08<00:05, 71.47it/s]Measuring inference for batch_size=512:  65%|██████▍   | 648/1000 [00:09<00:04, 71.47it/s]Measuring inference for batch_size=512:  66%|██████▌   | 656/1000 [00:09<00:04, 71.47it/s]Measuring inference for batch_size=512:  66%|██████▋   | 664/1000 [00:09<00:04, 71.46it/s]Measuring inference for batch_size=512:  67%|██████▋   | 672/1000 [00:09<00:04, 71.45it/s]Measuring inference for batch_size=512:  68%|██████▊   | 680/1000 [00:09<00:04, 71.44it/s]Measuring inference for batch_size=512:  69%|██████▉   | 688/1000 [00:09<00:04, 71.43it/s]Measuring inference for batch_size=512:  70%|██████▉   | 696/1000 [00:09<00:04, 71.42it/s]Measuring inference for batch_size=512:  70%|███████   | 704/1000 [00:09<00:04, 71.41it/s]Measuring inference for batch_size=512:  71%|███████   | 712/1000 [00:09<00:04, 71.41it/s]Measuring inference for batch_size=512:  72%|███████▏  | 720/1000 [00:10<00:03, 71.41it/s]Measuring inference for batch_size=512:  73%|███████▎  | 728/1000 [00:10<00:03, 71.39it/s]Measuring inference for batch_size=512:  74%|███████▎  | 736/1000 [00:10<00:03, 71.37it/s]Measuring inference for batch_size=512:  74%|███████▍  | 744/1000 [00:10<00:03, 71.35it/s]Measuring inference for batch_size=512:  75%|███████▌  | 752/1000 [00:10<00:03, 71.32it/s]Measuring inference for batch_size=512:  76%|███████▌  | 760/1000 [00:10<00:03, 71.32it/s]Measuring inference for batch_size=512:  77%|███████▋  | 768/1000 [00:10<00:03, 71.39it/s]Measuring inference for batch_size=512:  78%|███████▊  | 776/1000 [00:10<00:03, 71.40it/s]Measuring inference for batch_size=512:  78%|███████▊  | 784/1000 [00:10<00:03, 71.43it/s]Measuring inference for batch_size=512:  79%|███████▉  | 792/1000 [00:11<00:02, 71.44it/s]Measuring inference for batch_size=512:  80%|████████  | 800/1000 [00:11<00:02, 71.46it/s]Measuring inference for batch_size=512:  81%|████████  | 808/1000 [00:11<00:02, 71.51it/s]Measuring inference for batch_size=512:  82%|████████▏ | 816/1000 [00:11<00:02, 71.51it/s]Measuring inference for batch_size=512:  82%|████████▏ | 824/1000 [00:11<00:02, 71.50it/s]Measuring inference for batch_size=512:  83%|████████▎ | 832/1000 [00:11<00:02, 71.47it/s]Measuring inference for batch_size=512:  84%|████████▍ | 840/1000 [00:11<00:02, 71.45it/s]Measuring inference for batch_size=512:  85%|████████▍ | 848/1000 [00:11<00:02, 71.44it/s]Measuring inference for batch_size=512:  86%|████████▌ | 856/1000 [00:11<00:02, 71.47it/s]Measuring inference for batch_size=512:  86%|████████▋ | 864/1000 [00:12<00:01, 71.48it/s]Measuring inference for batch_size=512:  87%|████████▋ | 872/1000 [00:12<00:01, 71.48it/s]Measuring inference for batch_size=512:  88%|████████▊ | 880/1000 [00:12<00:01, 71.48it/s]Measuring inference for batch_size=512:  89%|████████▉ | 888/1000 [00:12<00:01, 71.45it/s]Measuring inference for batch_size=512:  90%|████████▉ | 896/1000 [00:12<00:01, 71.47it/s]Measuring inference for batch_size=512:  90%|█████████ | 904/1000 [00:12<00:01, 71.48it/s]Measuring inference for batch_size=512:  91%|█████████ | 912/1000 [00:12<00:01, 71.48it/s]Measuring inference for batch_size=512:  92%|█████████▏| 920/1000 [00:12<00:01, 71.49it/s]Measuring inference for batch_size=512:  93%|█████████▎| 928/1000 [00:13<00:01, 71.46it/s]Measuring inference for batch_size=512:  94%|█████████▎| 936/1000 [00:13<00:00, 71.49it/s]Measuring inference for batch_size=512:  94%|█████████▍| 944/1000 [00:13<00:00, 71.53it/s]Measuring inference for batch_size=512:  95%|█████████▌| 952/1000 [00:13<00:00, 71.52it/s]Measuring inference for batch_size=512:  96%|█████████▌| 960/1000 [00:13<00:00, 71.50it/s]Measuring inference for batch_size=512:  97%|█████████▋| 968/1000 [00:13<00:00, 71.49it/s]Measuring inference for batch_size=512:  98%|█████████▊| 976/1000 [00:13<00:00, 71.48it/s]Measuring inference for batch_size=512:  98%|█████████▊| 984/1000 [00:13<00:00, 70.88it/s]Measuring inference for batch_size=512:  99%|█████████▉| 992/1000 [00:13<00:00, 70.20it/s]Measuring inference for batch_size=512: 100%|██████████| 1000/1000 [00:14<00:00, 69.73it/s]Measuring inference for batch_size=512: 100%|██████████| 1000/1000 [00:14<00:00, 71.32it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 11580687848
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.52 GB
    total: 31.28 GB
    used: 3.29 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 374.76 MB
    max_inference_bytes: 392962560
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
  batch_size_512:
    max_inference: 378.48 MB
    max_inference_bytes: 396866048
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
params: 60192808
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 93.347 us +/- 5.206 us [90.837 us, 221.491 us]
        batches_per_second: 10.73 K +/- 402.91 [4.51 K, 11.01 K]
      metrics:
        batches_per_second_max: 11008.671916010499
        batches_per_second_mean: 10733.54967780117
        batches_per_second_min: 4514.858988159311
        batches_per_second_std: 402.9104911473688
        seconds_per_batch_max: 0.00022149085998535156
        seconds_per_batch_mean: 9.334707260131836e-05
        seconds_per_batch_min: 9.083747863769531e-05
        seconds_per_batch_std: 5.205837457774243e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.407 us +/- 0.632 us [22.650 us, 30.756 us]
        batches_per_second: 42.75 K +/- 1.00 K [32.51 K, 44.15 K]
      metrics:
        batches_per_second_max: 44150.56842105263
        batches_per_second_mean: 42748.7449202991
        batches_per_second_min: 32513.98449612403
        batches_per_second_std: 1000.1519506440179
        seconds_per_batch_max: 3.075599670410156e-05
        seconds_per_batch_mean: 2.3407220840454103e-05
        seconds_per_batch_min: 2.2649765014648438e-05
        seconds_per_batch_std: 6.319518648055656e-07
    on_device_inference:
      human_readable:
        batch_latency: -13607073.210 us +/- 41.778 ms [-14310400.009 us, -13526047.707
          us]
        batches_per_second: -0.07 +/- 0.00 [-0.07, -0.07]
      metrics:
        batches_per_second_max: -0.06987924861361222
        batches_per_second_mean: -0.07349186985914928
        batches_per_second_min: -0.07393142636276202
        batches_per_second_std: 0.00022154218101764464
        seconds_per_batch_max: -13.526047706604004
        seconds_per_batch_mean: -13.607073209762573
        seconds_per_batch_min: -14.310400009155273
        seconds_per_batch_std: 0.041777564989219355
    total:
      human_readable:
        batch_latency: 13.732 ms +/- 44.769 us [13.649 ms, 14.579 ms]
        batches_per_second: 72.82 +/- 0.23 [68.59, 73.27]
      metrics:
        batches_per_second_max: 73.2655114589156
        batches_per_second_mean: 72.82464814346132
        batches_per_second_min: 68.59266043043108
        batches_per_second_std: 0.23163589102988544
        seconds_per_batch_max: 0.014578819274902344
        seconds_per_batch_mean: 0.013731756925582886
        seconds_per_batch_min: 0.01364898681640625
        seconds_per_batch_std: 4.476885176315542e-05
  batch_size_512:
    cpu_to_gpu:
      human_readable:
        batch_latency: 144.814 us +/- 5.644 us [141.621 us, 274.420 us]
        batches_per_second: 6.91 K +/- 203.90 [3.64 K, 7.06 K]
      metrics:
        batches_per_second_max: 7061.117845117845
        batches_per_second_mean: 6913.041176963272
        batches_per_second_min: 3644.05212858384
        batches_per_second_std: 203.90370160302086
        seconds_per_batch_max: 0.00027441978454589844
        seconds_per_batch_mean: 0.00014481401443481445
        seconds_per_batch_min: 0.00014162063598632812
        seconds_per_batch_std: 5.6443238989504095e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.689 us +/- 0.827 us [22.650 us, 31.471 us]
        batches_per_second: 42.26 K +/- 1.27 K [31.78 K, 44.15 K]
      metrics:
        batches_per_second_max: 44150.56842105263
        batches_per_second_mean: 42257.6515765692
        batches_per_second_min: 31775.030303030304
        batches_per_second_std: 1266.8246863696952
        seconds_per_batch_max: 3.147125244140625e-05
        seconds_per_batch_mean: 2.3689031600952147e-05
        seconds_per_batch_min: 2.2649765014648438e-05
        seconds_per_batch_std: 8.271482054478083e-07
    on_device_inference:
      human_readable:
        batch_latency: -13838601.943 us +/- 121.937 ms [-14530143.738 us, -13725472.450
          us]
        batches_per_second: -0.07 +/- 0.00 [-0.07, -0.07]
      metrics:
        batches_per_second_max: -0.06882244374492975
        batches_per_second_mean: -0.07226704637581875
        batches_per_second_min: -0.07285723705498555
        batches_per_second_std: 0.0006141841223065915
        seconds_per_batch_max: -13.725472450256348
        seconds_per_batch_mean: -13.838601943016052
        seconds_per_batch_min: -14.530143737792969
        seconds_per_batch_std: 0.12193718278602218
    total:
      human_readable:
        batch_latency: 14.013 ms +/- 123.326 us [13.897 ms, 14.787 ms]
        batches_per_second: 71.37 +/- 0.61 [67.63, 71.96]
      metrics:
        batches_per_second_max: 71.95704163735869
        batches_per_second_mean: 71.36915779061333
        batches_per_second_min: 67.62606816935926
        batches_per_second_std: 0.6057842024533435
        seconds_per_batch_max: 0.01478719711303711
        seconds_per_batch_mean: 0.014012701511383056
        seconds_per_batch_min: 0.013897180557250977
        seconds_per_batch_std: 0.00012332620968496619


