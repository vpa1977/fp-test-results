#####
baseline-baseline-py-id - Run 1
2024-02-23 06:57:18
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  8.42it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  8.41it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  60.19 M, 100.000% Params, 11.58 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.016% Params, 118.01 MMac, 1.019% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.000% Params, 1.61 MMac, 0.014% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.007% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.007% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.359% Params, 680.39 MMac, 5.875% MACs, 
    (0): Bottleneck(
      75.01 k, 0.125% Params, 236.43 MMac, 2.042% MACs, 
      (conv1): Conv2d(4.1 k, 0.007% Params, 12.85 MMac, 0.111% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.028% Params, 52.99 MMac, 0.458% MACs, 
        (0): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    2.34 M, 3.887% Params, 1.92 GMac, 16.555% MACs, 
    (0): Bottleneck(
      379.39 k, 0.630% Params, 376.02 MMac, 3.247% MACs, 
      (conv1): Conv2d(32.77 k, 0.054% Params, 102.76 MMac, 0.887% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 802.82 KMac, 0.007% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.008% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.219% Params, 103.56 MMac, 0.894% MACs, 
        (0): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (4): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (5): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (6): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (7): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    40.61 M, 67.473% Params, 8.05 GMac, 69.501% MACs, 
    (0): Bottleneck(
      1.51 M, 2.513% Params, 374.26 MMac, 3.232% MACs, 
      (conv1): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 401.41 KMac, 0.003% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.004% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 0.874% Params, 103.16 MMac, 0.891% MACs, 
        (0): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (6): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (7): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (8): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (9): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (10): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (11): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (12): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (13): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (14): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (15): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (16): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (17): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (18): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (19): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (20): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (21): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (22): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (23): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (24): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (25): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (26): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (27): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (28): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (29): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (30): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (31): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (32): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (33): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (34): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (35): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 24.861% Params, 811.02 MMac, 7.003% MACs, 
    (0): Bottleneck(
      6.04 M, 10.034% Params, 373.38 MMac, 3.224% MACs, 
      (conv1): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 200.7 KMac, 0.002% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.002% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 3.491% Params, 102.96 MMac, 0.889% MACs, 
        (0): Conv2d(2.1 M, 3.484% Params, 102.76 MMac, 0.887% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.001% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 3.404% Params, 2.05 MMac, 0.018% MACs, in_features=2048, out_features=1000, bias=True)
)Measurement of allocated memory is only available on CUDA devices

Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:   2%|▏         | 2/100 [00:00<00:07, 12.79it/s]Warming up with batch_size=1:   4%|▍         | 4/100 [00:00<00:07, 12.79it/s]Warming up with batch_size=1:   6%|▌         | 6/100 [00:00<00:07, 12.79it/s]Warming up with batch_size=1:   8%|▊         | 8/100 [00:00<00:07, 12.78it/s]Warming up with batch_size=1:  10%|█         | 10/100 [00:00<00:07, 12.77it/s]Warming up with batch_size=1:  12%|█▏        | 12/100 [00:00<00:06, 12.76it/s]Warming up with batch_size=1:  14%|█▍        | 14/100 [00:01<00:06, 12.76it/s]Warming up with batch_size=1:  16%|█▌        | 16/100 [00:01<00:06, 12.75it/s]Warming up with batch_size=1:  18%|█▊        | 18/100 [00:01<00:06, 12.75it/s]Warming up with batch_size=1:  20%|██        | 20/100 [00:01<00:06, 12.75it/s]Warming up with batch_size=1:  22%|██▏       | 22/100 [00:01<00:06, 12.74it/s]Warming up with batch_size=1:  24%|██▍       | 24/100 [00:01<00:05, 12.74it/s]Warming up with batch_size=1:  26%|██▌       | 26/100 [00:02<00:05, 12.75it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:02<00:05, 12.74it/s]Warming up with batch_size=1:  30%|███       | 30/100 [00:02<00:05, 12.75it/s]Warming up with batch_size=1:  32%|███▏      | 32/100 [00:02<00:05, 12.75it/s]Warming up with batch_size=1:  34%|███▍      | 34/100 [00:02<00:05, 12.75it/s]Warming up with batch_size=1:  36%|███▌      | 36/100 [00:02<00:05, 12.75it/s]Warming up with batch_size=1:  38%|███▊      | 38/100 [00:02<00:04, 12.75it/s]Warming up with batch_size=1:  40%|████      | 40/100 [00:03<00:04, 12.74it/s]Warming up with batch_size=1:  42%|████▏     | 42/100 [00:03<00:04, 12.74it/s]Warming up with batch_size=1:  44%|████▍     | 44/100 [00:03<00:04, 12.74it/s]Warming up with batch_size=1:  46%|████▌     | 46/100 [00:03<00:04, 12.75it/s]Warming up with batch_size=1:  48%|████▊     | 48/100 [00:03<00:04, 12.75it/s]Warming up with batch_size=1:  50%|█████     | 50/100 [00:03<00:03, 12.74it/s]Warming up with batch_size=1:  52%|█████▏    | 52/100 [00:04<00:03, 12.74it/s]Warming up with batch_size=1:  54%|█████▍    | 54/100 [00:04<00:03, 12.75it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:04<00:03, 12.74it/s]Warming up with batch_size=1:  58%|█████▊    | 58/100 [00:04<00:03, 12.74it/s]Warming up with batch_size=1:  60%|██████    | 60/100 [00:04<00:03, 12.74it/s]Warming up with batch_size=1:  62%|██████▏   | 62/100 [00:04<00:02, 12.74it/s]Warming up with batch_size=1:  64%|██████▍   | 64/100 [00:05<00:02, 12.74it/s]Warming up with batch_size=1:  66%|██████▌   | 66/100 [00:05<00:02, 12.74it/s]Warming up with batch_size=1:  68%|██████▊   | 68/100 [00:05<00:02, 12.74it/s]Warming up with batch_size=1:  70%|███████   | 70/100 [00:05<00:02, 12.75it/s]Warming up with batch_size=1:  72%|███████▏  | 72/100 [00:05<00:02, 12.74it/s]Warming up with batch_size=1:  74%|███████▍  | 74/100 [00:05<00:02, 12.74it/s]Warming up with batch_size=1:  76%|███████▌  | 76/100 [00:05<00:01, 12.74it/s]Warming up with batch_size=1:  78%|███████▊  | 78/100 [00:06<00:01, 12.74it/s]Warming up with batch_size=1:  80%|████████  | 80/100 [00:06<00:01, 12.74it/s]Warming up with batch_size=1:  82%|████████▏ | 82/100 [00:06<00:01, 12.74it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:06<00:01, 12.74it/s]Warming up with batch_size=1:  86%|████████▌ | 86/100 [00:06<00:01, 12.75it/s]Warming up with batch_size=1:  88%|████████▊ | 88/100 [00:06<00:00, 12.75it/s]Warming up with batch_size=1:  90%|█████████ | 90/100 [00:07<00:00, 12.75it/s]Warming up with batch_size=1:  92%|█████████▏| 92/100 [00:07<00:00, 12.75it/s]Warming up with batch_size=1:  94%|█████████▍| 94/100 [00:07<00:00, 12.75it/s]Warming up with batch_size=1:  96%|█████████▌| 96/100 [00:07<00:00, 12.75it/s]Warming up with batch_size=1:  98%|█████████▊| 98/100 [00:07<00:00, 12.75it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:07<00:00, 12.75it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:07<00:00, 12.75it/s]
STAGE:2024-02-23 06:55:57 173336:173336 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 06:55:57 173336:173336 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 06:55:57 173336:173336 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   0%|          | 2/1000 [00:00<01:18, 12.76it/s]Measuring inference for batch_size=1:   0%|          | 4/1000 [00:00<01:18, 12.75it/s]Measuring inference for batch_size=1:   1%|          | 6/1000 [00:00<01:17, 12.75it/s]Measuring inference for batch_size=1:   1%|          | 8/1000 [00:00<01:17, 12.75it/s]Measuring inference for batch_size=1:   1%|          | 10/1000 [00:00<01:17, 12.75it/s]Measuring inference for batch_size=1:   1%|          | 12/1000 [00:00<01:17, 12.75it/s]Measuring inference for batch_size=1:   1%|▏         | 14/1000 [00:01<01:17, 12.75it/s]Measuring inference for batch_size=1:   2%|▏         | 16/1000 [00:01<01:17, 12.75it/s]Measuring inference for batch_size=1:   2%|▏         | 18/1000 [00:01<01:17, 12.75it/s]Measuring inference for batch_size=1:   2%|▏         | 20/1000 [00:01<01:16, 12.74it/s]Measuring inference for batch_size=1:   2%|▏         | 22/1000 [00:01<01:16, 12.74it/s]Measuring inference for batch_size=1:   2%|▏         | 24/1000 [00:01<01:16, 12.75it/s]Measuring inference for batch_size=1:   3%|▎         | 26/1000 [00:02<01:16, 12.75it/s]Measuring inference for batch_size=1:   3%|▎         | 28/1000 [00:02<01:16, 12.75it/s]Measuring inference for batch_size=1:   3%|▎         | 30/1000 [00:02<01:16, 12.74it/s]Measuring inference for batch_size=1:   3%|▎         | 32/1000 [00:02<01:15, 12.74it/s]Measuring inference for batch_size=1:   3%|▎         | 34/1000 [00:02<01:15, 12.75it/s]Measuring inference for batch_size=1:   4%|▎         | 36/1000 [00:02<01:15, 12.75it/s]Measuring inference for batch_size=1:   4%|▍         | 38/1000 [00:02<01:15, 12.75it/s]Measuring inference for batch_size=1:   4%|▍         | 40/1000 [00:03<01:15, 12.74it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:03<01:15, 12.75it/s]Measuring inference for batch_size=1:   4%|▍         | 44/1000 [00:03<01:14, 12.75it/s]Measuring inference for batch_size=1:   5%|▍         | 46/1000 [00:03<01:14, 12.75it/s]Measuring inference for batch_size=1:   5%|▍         | 48/1000 [00:03<01:14, 12.75it/s]Measuring inference for batch_size=1:   5%|▌         | 50/1000 [00:03<01:14, 12.75it/s]Measuring inference for batch_size=1:   5%|▌         | 52/1000 [00:04<01:14, 12.76it/s]Measuring inference for batch_size=1:   5%|▌         | 54/1000 [00:04<01:14, 12.76it/s]Measuring inference for batch_size=1:   6%|▌         | 56/1000 [00:04<01:14, 12.76it/s]Measuring inference for batch_size=1:   6%|▌         | 58/1000 [00:04<01:13, 12.76it/s]Measuring inference for batch_size=1:   6%|▌         | 60/1000 [00:04<01:13, 12.76it/s]Measuring inference for batch_size=1:   6%|▌         | 62/1000 [00:04<01:13, 12.75it/s]Measuring inference for batch_size=1:   6%|▋         | 64/1000 [00:05<01:13, 12.74it/s]Measuring inference for batch_size=1:   7%|▋         | 66/1000 [00:05<01:13, 12.74it/s]Measuring inference for batch_size=1:   7%|▋         | 68/1000 [00:05<01:13, 12.74it/s]Measuring inference for batch_size=1:   7%|▋         | 70/1000 [00:05<01:12, 12.75it/s]Measuring inference for batch_size=1:   7%|▋         | 72/1000 [00:05<01:12, 12.75it/s]Measuring inference for batch_size=1:   7%|▋         | 74/1000 [00:05<01:12, 12.74it/s]Measuring inference for batch_size=1:   8%|▊         | 76/1000 [00:05<01:12, 12.75it/s]Measuring inference for batch_size=1:   8%|▊         | 78/1000 [00:06<01:12, 12.75it/s]Measuring inference for batch_size=1:   8%|▊         | 80/1000 [00:06<01:12, 12.75it/s]Measuring inference for batch_size=1:   8%|▊         | 82/1000 [00:06<01:12, 12.75it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:06<01:11, 12.75it/s]Measuring inference for batch_size=1:   9%|▊         | 86/1000 [00:06<01:11, 12.75it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:06<01:11, 12.75it/s]Measuring inference for batch_size=1:   9%|▉         | 90/1000 [00:07<01:11, 12.75it/s]Measuring inference for batch_size=1:   9%|▉         | 92/1000 [00:07<01:11, 12.75it/s]Measuring inference for batch_size=1:   9%|▉         | 94/1000 [00:07<01:11, 12.75it/s]Measuring inference for batch_size=1:  10%|▉         | 96/1000 [00:07<01:10, 12.74it/s]Measuring inference for batch_size=1:  10%|▉         | 98/1000 [00:07<01:10, 12.74it/s]Measuring inference for batch_size=1:  10%|█         | 100/1000 [00:07<01:10, 12.73it/s]Measuring inference for batch_size=1:  10%|█         | 102/1000 [00:08<01:10, 12.73it/s]Measuring inference for batch_size=1:  10%|█         | 104/1000 [00:08<01:10, 12.74it/s]Measuring inference for batch_size=1:  11%|█         | 106/1000 [00:08<01:10, 12.74it/s]Measuring inference for batch_size=1:  11%|█         | 108/1000 [00:08<01:10, 12.74it/s]Measuring inference for batch_size=1:  11%|█         | 110/1000 [00:08<01:09, 12.75it/s]Measuring inference for batch_size=1:  11%|█         | 112/1000 [00:08<01:09, 12.75it/s]Measuring inference for batch_size=1:  11%|█▏        | 114/1000 [00:08<01:09, 12.75it/s]Measuring inference for batch_size=1:  12%|█▏        | 116/1000 [00:09<01:09, 12.75it/s]Measuring inference for batch_size=1:  12%|█▏        | 118/1000 [00:09<01:09, 12.75it/s]Measuring inference for batch_size=1:  12%|█▏        | 120/1000 [00:09<01:08, 12.75it/s]Measuring inference for batch_size=1:  12%|█▏        | 122/1000 [00:09<01:08, 12.75it/s]Measuring inference for batch_size=1:  12%|█▏        | 124/1000 [00:09<01:08, 12.75it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:09<01:08, 12.75it/s]Measuring inference for batch_size=1:  13%|█▎        | 128/1000 [00:10<01:08, 12.75it/s]Measuring inference for batch_size=1:  13%|█▎        | 130/1000 [00:10<01:08, 12.74it/s]Measuring inference for batch_size=1:  13%|█▎        | 132/1000 [00:10<01:08, 12.74it/s]Measuring inference for batch_size=1:  13%|█▎        | 134/1000 [00:10<01:07, 12.74it/s]Measuring inference for batch_size=1:  14%|█▎        | 136/1000 [00:10<01:07, 12.74it/s]Measuring inference for batch_size=1:  14%|█▍        | 138/1000 [00:10<01:07, 12.75it/s]Measuring inference for batch_size=1:  14%|█▍        | 140/1000 [00:10<01:07, 12.74it/s]Measuring inference for batch_size=1:  14%|█▍        | 142/1000 [00:11<01:07, 12.74it/s]Measuring inference for batch_size=1:  14%|█▍        | 144/1000 [00:11<01:07, 12.74it/s]Measuring inference for batch_size=1:  15%|█▍        | 146/1000 [00:11<01:07, 12.74it/s]Measuring inference for batch_size=1:  15%|█▍        | 148/1000 [00:11<01:06, 12.75it/s]Measuring inference for batch_size=1:  15%|█▌        | 150/1000 [00:11<01:06, 12.75it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:11<01:06, 12.76it/s]Measuring inference for batch_size=1:  15%|█▌        | 154/1000 [00:12<01:06, 12.75it/s]Measuring inference for batch_size=1:  16%|█▌        | 156/1000 [00:12<01:06, 12.75it/s]Measuring inference for batch_size=1:  16%|█▌        | 158/1000 [00:12<01:06, 12.75it/s]Measuring inference for batch_size=1:  16%|█▌        | 160/1000 [00:12<01:05, 12.75it/s]Measuring inference for batch_size=1:  16%|█▌        | 162/1000 [00:12<01:05, 12.75it/s]Measuring inference for batch_size=1:  16%|█▋        | 164/1000 [00:12<01:05, 12.75it/s]Measuring inference for batch_size=1:  17%|█▋        | 166/1000 [00:13<01:05, 12.75it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:13<01:05, 12.74it/s]Measuring inference for batch_size=1:  17%|█▋        | 170/1000 [00:13<01:05, 12.74it/s]Measuring inference for batch_size=1:  17%|█▋        | 172/1000 [00:13<01:04, 12.74it/s]Measuring inference for batch_size=1:  17%|█▋        | 174/1000 [00:13<01:04, 12.74it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:13<01:04, 12.74it/s]Measuring inference for batch_size=1:  18%|█▊        | 178/1000 [00:13<01:04, 12.74it/s]Measuring inference for batch_size=1:  18%|█▊        | 180/1000 [00:14<01:04, 12.74it/s]Measuring inference for batch_size=1:  18%|█▊        | 182/1000 [00:14<01:04, 12.73it/s]Measuring inference for batch_size=1:  18%|█▊        | 184/1000 [00:14<01:04, 12.73it/s]Measuring inference for batch_size=1:  19%|█▊        | 186/1000 [00:14<01:03, 12.74it/s]Measuring inference for batch_size=1:  19%|█▉        | 188/1000 [00:14<01:03, 12.73it/s]Measuring inference for batch_size=1:  19%|█▉        | 190/1000 [00:14<01:03, 12.73it/s]Measuring inference for batch_size=1:  19%|█▉        | 192/1000 [00:15<01:03, 12.73it/s]Measuring inference for batch_size=1:  19%|█▉        | 194/1000 [00:15<01:03, 12.73it/s]Measuring inference for batch_size=1:  20%|█▉        | 196/1000 [00:15<01:03, 12.74it/s]Measuring inference for batch_size=1:  20%|█▉        | 198/1000 [00:15<01:02, 12.74it/s]Measuring inference for batch_size=1:  20%|██        | 200/1000 [00:15<01:02, 12.74it/s]Measuring inference for batch_size=1:  20%|██        | 202/1000 [00:15<01:02, 12.74it/s]Measuring inference for batch_size=1:  20%|██        | 204/1000 [00:16<01:02, 12.74it/s]Measuring inference for batch_size=1:  21%|██        | 206/1000 [00:16<01:02, 12.74it/s]Measuring inference for batch_size=1:  21%|██        | 208/1000 [00:16<01:02, 12.74it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:16<01:02, 12.74it/s]Measuring inference for batch_size=1:  21%|██        | 212/1000 [00:16<01:01, 12.74it/s]Measuring inference for batch_size=1:  21%|██▏       | 214/1000 [00:16<01:01, 12.74it/s]Measuring inference for batch_size=1:  22%|██▏       | 216/1000 [00:16<01:01, 12.74it/s]Measuring inference for batch_size=1:  22%|██▏       | 218/1000 [00:17<01:01, 12.74it/s]Measuring inference for batch_size=1:  22%|██▏       | 220/1000 [00:17<01:01, 12.73it/s]Measuring inference for batch_size=1:  22%|██▏       | 222/1000 [00:17<01:01, 12.73it/s]Measuring inference for batch_size=1:  22%|██▏       | 224/1000 [00:17<01:00, 12.74it/s]Measuring inference for batch_size=1:  23%|██▎       | 226/1000 [00:17<01:00, 12.73it/s]Measuring inference for batch_size=1:  23%|██▎       | 228/1000 [00:17<01:00, 12.73it/s]Measuring inference for batch_size=1:  23%|██▎       | 230/1000 [00:18<01:00, 12.74it/s]Measuring inference for batch_size=1:  23%|██▎       | 232/1000 [00:18<01:00, 12.74it/s]Measuring inference for batch_size=1:  23%|██▎       | 234/1000 [00:18<01:00, 12.74it/s]Measuring inference for batch_size=1:  24%|██▎       | 236/1000 [00:18<00:59, 12.73it/s]Measuring inference for batch_size=1:  24%|██▍       | 238/1000 [00:18<00:59, 12.74it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:18<00:59, 12.74it/s]Measuring inference for batch_size=1:  24%|██▍       | 242/1000 [00:18<00:59, 12.74it/s]Measuring inference for batch_size=1:  24%|██▍       | 244/1000 [00:19<00:59, 12.73it/s]Measuring inference for batch_size=1:  25%|██▍       | 246/1000 [00:19<00:59, 12.74it/s]Measuring inference for batch_size=1:  25%|██▍       | 248/1000 [00:19<00:59, 12.74it/s]Measuring inference for batch_size=1:  25%|██▌       | 250/1000 [00:19<00:58, 12.74it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:19<00:58, 12.74it/s]Measuring inference for batch_size=1:  25%|██▌       | 254/1000 [00:19<00:58, 12.74it/s]Measuring inference for batch_size=1:  26%|██▌       | 256/1000 [00:20<00:58, 12.74it/s]Measuring inference for batch_size=1:  26%|██▌       | 258/1000 [00:20<00:58, 12.74it/s]Measuring inference for batch_size=1:  26%|██▌       | 260/1000 [00:20<00:58, 12.75it/s]Measuring inference for batch_size=1:  26%|██▌       | 262/1000 [00:20<00:57, 12.75it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:20<00:57, 12.75it/s]Measuring inference for batch_size=1:  27%|██▋       | 266/1000 [00:20<00:57, 12.75it/s]Measuring inference for batch_size=1:  27%|██▋       | 268/1000 [00:21<00:57, 12.75it/s]Measuring inference for batch_size=1:  27%|██▋       | 270/1000 [00:21<00:57, 12.75it/s]Measuring inference for batch_size=1:  27%|██▋       | 272/1000 [00:21<00:57, 12.74it/s]Measuring inference for batch_size=1:  27%|██▋       | 274/1000 [00:21<00:56, 12.75it/s]Measuring inference for batch_size=1:  28%|██▊       | 276/1000 [00:21<00:56, 12.74it/s]Measuring inference for batch_size=1:  28%|██▊       | 278/1000 [00:21<00:56, 12.74it/s]Measuring inference for batch_size=1:  28%|██▊       | 280/1000 [00:21<00:56, 12.74it/s]Measuring inference for batch_size=1:  28%|██▊       | 282/1000 [00:22<00:56, 12.74it/s]Measuring inference for batch_size=1:  28%|██▊       | 284/1000 [00:22<00:56, 12.74it/s]Measuring inference for batch_size=1:  29%|██▊       | 286/1000 [00:22<00:56, 12.73it/s]Measuring inference for batch_size=1:  29%|██▉       | 288/1000 [00:22<00:55, 12.73it/s]Measuring inference for batch_size=1:  29%|██▉       | 290/1000 [00:22<00:55, 12.73it/s]Measuring inference for batch_size=1:  29%|██▉       | 292/1000 [00:22<00:55, 12.73it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:23<00:55, 12.73it/s]Measuring inference for batch_size=1:  30%|██▉       | 296/1000 [00:23<00:55, 12.74it/s]Measuring inference for batch_size=1:  30%|██▉       | 298/1000 [00:23<00:55, 12.74it/s]Measuring inference for batch_size=1:  30%|███       | 300/1000 [00:23<00:54, 12.75it/s]Measuring inference for batch_size=1:  30%|███       | 302/1000 [00:23<00:54, 12.75it/s]Measuring inference for batch_size=1:  30%|███       | 304/1000 [00:23<00:54, 12.75it/s]Measuring inference for batch_size=1:  31%|███       | 306/1000 [00:24<00:54, 12.75it/s]Measuring inference for batch_size=1:  31%|███       | 308/1000 [00:24<00:54, 12.75it/s]Measuring inference for batch_size=1:  31%|███       | 310/1000 [00:24<00:54, 12.75it/s]Measuring inference for batch_size=1:  31%|███       | 312/1000 [00:24<00:53, 12.74it/s]Measuring inference for batch_size=1:  31%|███▏      | 314/1000 [00:24<00:53, 12.74it/s]Measuring inference for batch_size=1:  32%|███▏      | 316/1000 [00:24<00:53, 12.74it/s]Measuring inference for batch_size=1:  32%|███▏      | 318/1000 [00:24<00:53, 12.74it/s]Measuring inference for batch_size=1:  32%|███▏      | 320/1000 [00:25<00:53, 12.74it/s]Measuring inference for batch_size=1:  32%|███▏      | 322/1000 [00:25<00:53, 12.75it/s]Measuring inference for batch_size=1:  32%|███▏      | 324/1000 [00:25<00:53, 12.74it/s]Measuring inference for batch_size=1:  33%|███▎      | 326/1000 [00:25<00:52, 12.74it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:25<00:52, 12.74it/s]Measuring inference for batch_size=1:  33%|███▎      | 330/1000 [00:25<00:52, 12.74it/s]Measuring inference for batch_size=1:  33%|███▎      | 332/1000 [00:26<00:52, 12.74it/s]Measuring inference for batch_size=1:  33%|███▎      | 334/1000 [00:26<00:52, 12.74it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:26<00:52, 12.74it/s]Measuring inference for batch_size=1:  34%|███▍      | 338/1000 [00:26<00:51, 12.73it/s]Measuring inference for batch_size=1:  34%|███▍      | 340/1000 [00:26<00:51, 12.74it/s]Measuring inference for batch_size=1:  34%|███▍      | 342/1000 [00:26<00:51, 12.74it/s]Measuring inference for batch_size=1:  34%|███▍      | 344/1000 [00:26<00:51, 12.73it/s]Measuring inference for batch_size=1:  35%|███▍      | 346/1000 [00:27<00:51, 12.73it/s]Measuring inference for batch_size=1:  35%|███▍      | 348/1000 [00:27<00:51, 12.73it/s]Measuring inference for batch_size=1:  35%|███▌      | 350/1000 [00:27<00:51, 12.73it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:27<00:50, 12.73it/s]Measuring inference for batch_size=1:  35%|███▌      | 354/1000 [00:27<00:50, 12.73it/s]Measuring inference for batch_size=1:  36%|███▌      | 356/1000 [00:27<00:50, 12.73it/s]Measuring inference for batch_size=1:  36%|███▌      | 358/1000 [00:28<00:50, 12.73it/s]Measuring inference for batch_size=1:  36%|███▌      | 360/1000 [00:28<00:50, 12.74it/s]Measuring inference for batch_size=1:  36%|███▌      | 362/1000 [00:28<00:50, 12.74it/s]Measuring inference for batch_size=1:  36%|███▋      | 364/1000 [00:28<00:49, 12.74it/s]Measuring inference for batch_size=1:  37%|███▋      | 366/1000 [00:28<00:49, 12.74it/s]Measuring inference for batch_size=1:  37%|███▋      | 368/1000 [00:28<00:49, 12.74it/s]Measuring inference for batch_size=1:  37%|███▋      | 370/1000 [00:29<00:49, 12.74it/s]Measuring inference for batch_size=1:  37%|███▋      | 372/1000 [00:29<00:49, 12.74it/s]Measuring inference for batch_size=1:  37%|███▋      | 374/1000 [00:29<00:49, 12.74it/s]Measuring inference for batch_size=1:  38%|███▊      | 376/1000 [00:29<00:48, 12.75it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:29<00:48, 12.74it/s]Measuring inference for batch_size=1:  38%|███▊      | 380/1000 [00:29<00:48, 12.74it/s]Measuring inference for batch_size=1:  38%|███▊      | 382/1000 [00:29<00:48, 12.74it/s]Measuring inference for batch_size=1:  38%|███▊      | 384/1000 [00:30<00:48, 12.74it/s]Measuring inference for batch_size=1:  39%|███▊      | 386/1000 [00:30<00:48, 12.74it/s]Measuring inference for batch_size=1:  39%|███▉      | 388/1000 [00:30<00:48, 12.74it/s]Measuring inference for batch_size=1:  39%|███▉      | 390/1000 [00:30<00:47, 12.74it/s]Measuring inference for batch_size=1:  39%|███▉      | 392/1000 [00:30<00:47, 12.75it/s]Measuring inference for batch_size=1:  39%|███▉      | 394/1000 [00:30<00:47, 12.75it/s]Measuring inference for batch_size=1:  40%|███▉      | 396/1000 [00:31<00:47, 12.74it/s]Measuring inference for batch_size=1:  40%|███▉      | 398/1000 [00:31<00:47, 12.74it/s]Measuring inference for batch_size=1:  40%|████      | 400/1000 [00:31<00:47, 12.74it/s]Measuring inference for batch_size=1:  40%|████      | 402/1000 [00:31<00:46, 12.75it/s]Measuring inference for batch_size=1:  40%|████      | 404/1000 [00:31<00:46, 12.74it/s]Measuring inference for batch_size=1:  41%|████      | 406/1000 [00:31<00:46, 12.74it/s]Measuring inference for batch_size=1:  41%|████      | 408/1000 [00:32<00:46, 12.74it/s]Measuring inference for batch_size=1:  41%|████      | 410/1000 [00:32<00:46, 12.74it/s]Measuring inference for batch_size=1:  41%|████      | 412/1000 [00:32<00:46, 12.74it/s]Measuring inference for batch_size=1:  41%|████▏     | 414/1000 [00:32<00:45, 12.74it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:32<00:45, 12.74it/s]Measuring inference for batch_size=1:  42%|████▏     | 418/1000 [00:32<00:45, 12.74it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:32<00:45, 12.74it/s]Measuring inference for batch_size=1:  42%|████▏     | 422/1000 [00:33<00:45, 12.74it/s]Measuring inference for batch_size=1:  42%|████▏     | 424/1000 [00:33<00:45, 12.74it/s]Measuring inference for batch_size=1:  43%|████▎     | 426/1000 [00:33<00:45, 12.74it/s]Measuring inference for batch_size=1:  43%|████▎     | 428/1000 [00:33<00:44, 12.74it/s]Measuring inference for batch_size=1:  43%|████▎     | 430/1000 [00:33<00:44, 12.74it/s]Measuring inference for batch_size=1:  43%|████▎     | 432/1000 [00:33<00:44, 12.74it/s]Measuring inference for batch_size=1:  43%|████▎     | 434/1000 [00:34<00:44, 12.73it/s]Measuring inference for batch_size=1:  44%|████▎     | 436/1000 [00:34<00:44, 12.73it/s]Measuring inference for batch_size=1:  44%|████▍     | 438/1000 [00:34<00:44, 12.74it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:34<00:43, 12.74it/s]Measuring inference for batch_size=1:  44%|████▍     | 442/1000 [00:34<00:43, 12.74it/s]Measuring inference for batch_size=1:  44%|████▍     | 444/1000 [00:34<00:43, 12.73it/s]Measuring inference for batch_size=1:  45%|████▍     | 446/1000 [00:35<00:43, 12.73it/s]Measuring inference for batch_size=1:  45%|████▍     | 448/1000 [00:35<00:43, 12.73it/s]Measuring inference for batch_size=1:  45%|████▌     | 450/1000 [00:35<00:43, 12.74it/s]Measuring inference for batch_size=1:  45%|████▌     | 452/1000 [00:35<00:43, 12.74it/s]Measuring inference for batch_size=1:  45%|████▌     | 454/1000 [00:35<00:42, 12.75it/s]Measuring inference for batch_size=1:  46%|████▌     | 456/1000 [00:35<00:42, 12.75it/s]Measuring inference for batch_size=1:  46%|████▌     | 458/1000 [00:35<00:42, 12.74it/s]Measuring inference for batch_size=1:  46%|████▌     | 460/1000 [00:36<00:42, 12.74it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:36<00:42, 12.75it/s]Measuring inference for batch_size=1:  46%|████▋     | 464/1000 [00:36<00:42, 12.74it/s]Measuring inference for batch_size=1:  47%|████▋     | 466/1000 [00:36<00:41, 12.74it/s]Measuring inference for batch_size=1:  47%|████▋     | 468/1000 [00:36<00:41, 12.74it/s]Measuring inference for batch_size=1:  47%|████▋     | 470/1000 [00:36<00:41, 12.75it/s]Measuring inference for batch_size=1:  47%|████▋     | 472/1000 [00:37<00:41, 12.75it/s]Measuring inference for batch_size=1:  47%|████▋     | 474/1000 [00:37<00:41, 12.75it/s]Measuring inference for batch_size=1:  48%|████▊     | 476/1000 [00:37<00:41, 12.75it/s]Measuring inference for batch_size=1:  48%|████▊     | 478/1000 [00:37<00:40, 12.74it/s]Measuring inference for batch_size=1:  48%|████▊     | 480/1000 [00:37<00:40, 12.74it/s]Measuring inference for batch_size=1:  48%|████▊     | 482/1000 [00:37<00:40, 12.74it/s]Measuring inference for batch_size=1:  48%|████▊     | 484/1000 [00:37<00:40, 12.74it/s]Measuring inference for batch_size=1:  49%|████▊     | 486/1000 [00:38<00:40, 12.74it/s]Measuring inference for batch_size=1:  49%|████▉     | 488/1000 [00:38<00:40, 12.74it/s]Measuring inference for batch_size=1:  49%|████▉     | 490/1000 [00:38<00:40, 12.74it/s]Measuring inference for batch_size=1:  49%|████▉     | 492/1000 [00:38<00:39, 12.74it/s]Measuring inference for batch_size=1:  49%|████▉     | 494/1000 [00:38<00:39, 12.74it/s]Measuring inference for batch_size=1:  50%|████▉     | 496/1000 [00:38<00:39, 12.75it/s]Measuring inference for batch_size=1:  50%|████▉     | 498/1000 [00:39<00:39, 12.75it/s]Measuring inference for batch_size=1:  50%|█████     | 500/1000 [00:39<00:39, 12.74it/s]Measuring inference for batch_size=1:  50%|█████     | 502/1000 [00:39<00:39, 12.74it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:39<00:38, 12.74it/s]Measuring inference for batch_size=1:  51%|█████     | 506/1000 [00:39<00:38, 12.75it/s]Measuring inference for batch_size=1:  51%|█████     | 508/1000 [00:39<00:38, 12.75it/s]Measuring inference for batch_size=1:  51%|█████     | 510/1000 [00:40<00:38, 12.76it/s]Measuring inference for batch_size=1:  51%|█████     | 512/1000 [00:40<00:38, 12.75it/s]Measuring inference for batch_size=1:  51%|█████▏    | 514/1000 [00:40<00:38, 12.75it/s]Measuring inference for batch_size=1:  52%|█████▏    | 516/1000 [00:40<00:37, 12.75it/s]Measuring inference for batch_size=1:  52%|█████▏    | 518/1000 [00:40<00:37, 12.75it/s]Measuring inference for batch_size=1:  52%|█████▏    | 520/1000 [00:40<00:37, 12.75it/s]Measuring inference for batch_size=1:  52%|█████▏    | 522/1000 [00:40<00:37, 12.75it/s]Measuring inference for batch_size=1:  52%|█████▏    | 524/1000 [00:41<00:37, 12.75it/s]Measuring inference for batch_size=1:  53%|█████▎    | 526/1000 [00:41<00:37, 12.75it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:41<00:37, 12.74it/s]Measuring inference for batch_size=1:  53%|█████▎    | 530/1000 [00:41<00:36, 12.74it/s]Measuring inference for batch_size=1:  53%|█████▎    | 532/1000 [00:41<00:36, 12.74it/s]Measuring inference for batch_size=1:  53%|█████▎    | 534/1000 [00:41<00:36, 12.74it/s]Measuring inference for batch_size=1:  54%|█████▎    | 536/1000 [00:42<00:36, 12.74it/s]Measuring inference for batch_size=1:  54%|█████▍    | 538/1000 [00:42<00:36, 12.75it/s]Measuring inference for batch_size=1:  54%|█████▍    | 540/1000 [00:42<00:36, 12.74it/s]Measuring inference for batch_size=1:  54%|█████▍    | 542/1000 [00:42<00:35, 12.74it/s]Measuring inference for batch_size=1:  54%|█████▍    | 544/1000 [00:42<00:35, 12.74it/s]Measuring inference for batch_size=1:  55%|█████▍    | 546/1000 [00:42<00:35, 12.74it/s]Measuring inference for batch_size=1:  55%|█████▍    | 548/1000 [00:43<00:35, 12.75it/s]Measuring inference for batch_size=1:  55%|█████▌    | 550/1000 [00:43<00:35, 12.74it/s]Measuring inference for batch_size=1:  55%|█████▌    | 552/1000 [00:43<00:35, 12.74it/s]Measuring inference for batch_size=1:  55%|█████▌    | 554/1000 [00:43<00:34, 12.75it/s]Measuring inference for batch_size=1:  56%|█████▌    | 556/1000 [00:43<00:34, 12.75it/s]Measuring inference for batch_size=1:  56%|█████▌    | 558/1000 [00:43<00:34, 12.74it/s]Measuring inference for batch_size=1:  56%|█████▌    | 560/1000 [00:43<00:34, 12.74it/s]Measuring inference for batch_size=1:  56%|█████▌    | 562/1000 [00:44<00:34, 12.74it/s]Measuring inference for batch_size=1:  56%|█████▋    | 564/1000 [00:44<00:34, 12.74it/s]Measuring inference for batch_size=1:  57%|█████▋    | 566/1000 [00:44<00:34, 12.74it/s]Measuring inference for batch_size=1:  57%|█████▋    | 568/1000 [00:44<00:33, 12.74it/s]Measuring inference for batch_size=1:  57%|█████▋    | 570/1000 [00:44<00:33, 12.74it/s]Measuring inference for batch_size=1:  57%|█████▋    | 572/1000 [00:44<00:33, 12.75it/s]Measuring inference for batch_size=1:  57%|█████▋    | 574/1000 [00:45<00:33, 12.75it/s]Measuring inference for batch_size=1:  58%|█████▊    | 576/1000 [00:45<00:33, 12.75it/s]Measuring inference for batch_size=1:  58%|█████▊    | 578/1000 [00:45<00:33, 12.76it/s]Measuring inference for batch_size=1:  58%|█████▊    | 580/1000 [00:45<00:32, 12.75it/s]Measuring inference for batch_size=1:  58%|█████▊    | 582/1000 [00:45<00:32, 12.75it/s]Measuring inference for batch_size=1:  58%|█████▊    | 584/1000 [00:45<00:32, 12.75it/s]Measuring inference for batch_size=1:  59%|█████▊    | 586/1000 [00:45<00:32, 12.75it/s]Measuring inference for batch_size=1:  59%|█████▉    | 588/1000 [00:46<00:32, 12.75it/s]Measuring inference for batch_size=1:  59%|█████▉    | 590/1000 [00:46<00:32, 12.75it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [00:46<00:32, 12.75it/s]Measuring inference for batch_size=1:  59%|█████▉    | 594/1000 [00:46<00:31, 12.74it/s]Measuring inference for batch_size=1:  60%|█████▉    | 596/1000 [00:46<00:31, 12.74it/s]Measuring inference for batch_size=1:  60%|█████▉    | 598/1000 [00:46<00:31, 12.74it/s]Measuring inference for batch_size=1:  60%|██████    | 600/1000 [00:47<00:31, 12.74it/s]Measuring inference for batch_size=1:  60%|██████    | 602/1000 [00:47<00:31, 12.75it/s]Measuring inference for batch_size=1:  60%|██████    | 604/1000 [00:47<00:31, 12.74it/s]Measuring inference for batch_size=1:  61%|██████    | 606/1000 [00:47<00:30, 12.74it/s]Measuring inference for batch_size=1:  61%|██████    | 608/1000 [00:47<00:30, 12.74it/s]Measuring inference for batch_size=1:  61%|██████    | 610/1000 [00:47<00:30, 12.74it/s]Measuring inference for batch_size=1:  61%|██████    | 612/1000 [00:48<00:30, 12.75it/s]Measuring inference for batch_size=1:  61%|██████▏   | 614/1000 [00:48<00:30, 12.74it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:48<00:30, 12.75it/s]Measuring inference for batch_size=1:  62%|██████▏   | 618/1000 [00:48<00:29, 12.75it/s]Measuring inference for batch_size=1:  62%|██████▏   | 620/1000 [00:48<00:29, 12.74it/s]Measuring inference for batch_size=1:  62%|██████▏   | 622/1000 [00:48<00:29, 12.74it/s]Measuring inference for batch_size=1:  62%|██████▏   | 624/1000 [00:48<00:29, 12.75it/s]Measuring inference for batch_size=1:  63%|██████▎   | 626/1000 [00:49<00:29, 12.75it/s]Measuring inference for batch_size=1:  63%|██████▎   | 628/1000 [00:49<00:29, 12.74it/s]Measuring inference for batch_size=1:  63%|██████▎   | 630/1000 [00:49<00:29, 12.75it/s]Measuring inference for batch_size=1:  63%|██████▎   | 632/1000 [00:49<00:28, 12.74it/s]Measuring inference for batch_size=1:  63%|██████▎   | 634/1000 [00:49<00:28, 12.75it/s]Measuring inference for batch_size=1:  64%|██████▎   | 636/1000 [00:49<00:28, 12.75it/s]Measuring inference for batch_size=1:  64%|██████▍   | 638/1000 [00:50<00:28, 12.74it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [00:50<00:28, 12.75it/s]Measuring inference for batch_size=1:  64%|██████▍   | 642/1000 [00:50<00:28, 12.75it/s]Measuring inference for batch_size=1:  64%|██████▍   | 644/1000 [00:50<00:27, 12.75it/s]Measuring inference for batch_size=1:  65%|██████▍   | 646/1000 [00:50<00:27, 12.75it/s]Measuring inference for batch_size=1:  65%|██████▍   | 648/1000 [00:50<00:27, 12.75it/s]Measuring inference for batch_size=1:  65%|██████▌   | 650/1000 [00:51<00:27, 12.75it/s]Measuring inference for batch_size=1:  65%|██████▌   | 652/1000 [00:51<00:27, 12.75it/s]Measuring inference for batch_size=1:  65%|██████▌   | 654/1000 [00:51<00:27, 12.75it/s]Measuring inference for batch_size=1:  66%|██████▌   | 656/1000 [00:51<00:26, 12.75it/s]Measuring inference for batch_size=1:  66%|██████▌   | 658/1000 [00:51<00:26, 12.75it/s]Measuring inference for batch_size=1:  66%|██████▌   | 660/1000 [00:51<00:26, 12.75it/s]Measuring inference for batch_size=1:  66%|██████▌   | 662/1000 [00:51<00:26, 12.75it/s]Measuring inference for batch_size=1:  66%|██████▋   | 664/1000 [00:52<00:26, 12.75it/s]Measuring inference for batch_size=1:  67%|██████▋   | 666/1000 [00:52<00:26, 12.75it/s]Measuring inference for batch_size=1:  67%|██████▋   | 668/1000 [00:52<00:26, 12.74it/s]Measuring inference for batch_size=1:  67%|██████▋   | 670/1000 [00:52<00:25, 12.74it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:52<00:25, 12.74it/s]Measuring inference for batch_size=1:  67%|██████▋   | 674/1000 [00:52<00:25, 12.74it/s]Measuring inference for batch_size=1:  68%|██████▊   | 676/1000 [00:53<00:25, 12.74it/s]Measuring inference for batch_size=1:  68%|██████▊   | 678/1000 [00:53<00:25, 12.74it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [00:53<00:25, 12.74it/s]Measuring inference for batch_size=1:  68%|██████▊   | 682/1000 [00:53<00:24, 12.74it/s]Measuring inference for batch_size=1:  68%|██████▊   | 684/1000 [00:53<00:24, 12.74it/s]Measuring inference for batch_size=1:  69%|██████▊   | 686/1000 [00:53<00:24, 12.74it/s]Measuring inference for batch_size=1:  69%|██████▉   | 688/1000 [00:53<00:24, 12.75it/s]Measuring inference for batch_size=1:  69%|██████▉   | 690/1000 [00:54<00:24, 12.74it/s]Measuring inference for batch_size=1:  69%|██████▉   | 692/1000 [00:54<00:24, 12.75it/s]Measuring inference for batch_size=1:  69%|██████▉   | 694/1000 [00:54<00:24, 12.75it/s]Measuring inference for batch_size=1:  70%|██████▉   | 696/1000 [00:54<00:23, 12.75it/s]Measuring inference for batch_size=1:  70%|██████▉   | 698/1000 [00:54<00:23, 12.75it/s]Measuring inference for batch_size=1:  70%|███████   | 700/1000 [00:54<00:23, 12.75it/s]Measuring inference for batch_size=1:  70%|███████   | 702/1000 [00:55<00:23, 12.75it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [00:55<00:23, 12.74it/s]Measuring inference for batch_size=1:  71%|███████   | 706/1000 [00:55<00:23, 12.74it/s]Measuring inference for batch_size=1:  71%|███████   | 708/1000 [00:55<00:22, 12.74it/s]Measuring inference for batch_size=1:  71%|███████   | 710/1000 [00:55<00:22, 12.75it/s]Measuring inference for batch_size=1:  71%|███████   | 712/1000 [00:55<00:22, 12.74it/s]Measuring inference for batch_size=1:  71%|███████▏  | 714/1000 [00:56<00:22, 12.74it/s]Measuring inference for batch_size=1:  72%|███████▏  | 716/1000 [00:56<00:22, 12.74it/s]Measuring inference for batch_size=1:  72%|███████▏  | 718/1000 [00:56<00:22, 12.74it/s]Measuring inference for batch_size=1:  72%|███████▏  | 720/1000 [00:56<00:21, 12.74it/s]Measuring inference for batch_size=1:  72%|███████▏  | 722/1000 [00:56<00:21, 12.74it/s]Measuring inference for batch_size=1:  72%|███████▏  | 724/1000 [00:56<00:21, 12.74it/s]Measuring inference for batch_size=1:  73%|███████▎  | 726/1000 [00:56<00:21, 12.75it/s]Measuring inference for batch_size=1:  73%|███████▎  | 728/1000 [00:57<00:21, 12.74it/s]Measuring inference for batch_size=1:  73%|███████▎  | 730/1000 [00:57<00:21, 12.74it/s]Measuring inference for batch_size=1:  73%|███████▎  | 732/1000 [00:57<00:21, 12.75it/s]Measuring inference for batch_size=1:  73%|███████▎  | 734/1000 [00:57<00:20, 12.75it/s]Measuring inference for batch_size=1:  74%|███████▎  | 736/1000 [00:57<00:20, 12.74it/s]Measuring inference for batch_size=1:  74%|███████▍  | 738/1000 [00:57<00:20, 12.74it/s]Measuring inference for batch_size=1:  74%|███████▍  | 740/1000 [00:58<00:20, 12.74it/s]Measuring inference for batch_size=1:  74%|███████▍  | 742/1000 [00:58<00:20, 12.74it/s]Measuring inference for batch_size=1:  74%|███████▍  | 744/1000 [00:58<00:20, 12.75it/s]Measuring inference for batch_size=1:  75%|███████▍  | 746/1000 [00:58<00:19, 12.74it/s]Measuring inference for batch_size=1:  75%|███████▍  | 748/1000 [00:58<00:19, 12.74it/s]Measuring inference for batch_size=1:  75%|███████▌  | 750/1000 [00:58<00:19, 12.74it/s]Measuring inference for batch_size=1:  75%|███████▌  | 752/1000 [00:59<00:19, 12.75it/s]Measuring inference for batch_size=1:  75%|███████▌  | 754/1000 [00:59<00:19, 12.74it/s]Measuring inference for batch_size=1:  76%|███████▌  | 756/1000 [00:59<00:19, 12.74it/s]Measuring inference for batch_size=1:  76%|███████▌  | 758/1000 [00:59<00:18, 12.75it/s]Measuring inference for batch_size=1:  76%|███████▌  | 760/1000 [00:59<00:18, 12.75it/s]Measuring inference for batch_size=1:  76%|███████▌  | 762/1000 [00:59<00:18, 12.74it/s]Measuring inference for batch_size=1:  76%|███████▋  | 764/1000 [00:59<00:18, 12.74it/s]Measuring inference for batch_size=1:  77%|███████▋  | 766/1000 [01:00<00:18, 12.74it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [01:00<00:18, 12.74it/s]Measuring inference for batch_size=1:  77%|███████▋  | 770/1000 [01:00<00:18, 12.75it/s]Measuring inference for batch_size=1:  77%|███████▋  | 772/1000 [01:00<00:17, 12.74it/s]Measuring inference for batch_size=1:  77%|███████▋  | 774/1000 [01:00<00:17, 12.74it/s]Measuring inference for batch_size=1:  78%|███████▊  | 776/1000 [01:00<00:17, 12.74it/s]Measuring inference for batch_size=1:  78%|███████▊  | 778/1000 [01:01<00:17, 12.73it/s]Measuring inference for batch_size=1:  78%|███████▊  | 780/1000 [01:01<00:17, 12.74it/s]Measuring inference for batch_size=1:  78%|███████▊  | 782/1000 [01:01<00:17, 12.74it/s]Measuring inference for batch_size=1:  78%|███████▊  | 784/1000 [01:01<00:16, 12.74it/s]Measuring inference for batch_size=1:  79%|███████▊  | 786/1000 [01:01<00:16, 12.74it/s]Measuring inference for batch_size=1:  79%|███████▉  | 788/1000 [01:01<00:16, 12.73it/s]Measuring inference for batch_size=1:  79%|███████▉  | 790/1000 [01:01<00:16, 12.73it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [01:02<00:16, 12.73it/s]Measuring inference for batch_size=1:  79%|███████▉  | 794/1000 [01:02<00:16, 12.74it/s]Measuring inference for batch_size=1:  80%|███████▉  | 796/1000 [01:02<00:16, 12.74it/s]Measuring inference for batch_size=1:  80%|███████▉  | 798/1000 [01:02<00:15, 12.74it/s]Measuring inference for batch_size=1:  80%|████████  | 800/1000 [01:02<00:15, 12.74it/s]Measuring inference for batch_size=1:  80%|████████  | 802/1000 [01:02<00:15, 12.74it/s]Measuring inference for batch_size=1:  80%|████████  | 804/1000 [01:03<00:15, 12.74it/s]Measuring inference for batch_size=1:  81%|████████  | 806/1000 [01:03<00:15, 12.74it/s]Measuring inference for batch_size=1:  81%|████████  | 808/1000 [01:03<00:15, 12.75it/s]Measuring inference for batch_size=1:  81%|████████  | 810/1000 [01:03<00:14, 12.74it/s]Measuring inference for batch_size=1:  81%|████████  | 812/1000 [01:03<00:14, 12.74it/s]Measuring inference for batch_size=1:  81%|████████▏ | 814/1000 [01:03<00:14, 12.74it/s]Measuring inference for batch_size=1:  82%|████████▏ | 816/1000 [01:04<00:14, 12.74it/s]Measuring inference for batch_size=1:  82%|████████▏ | 818/1000 [01:04<00:14, 12.74it/s]Measuring inference for batch_size=1:  82%|████████▏ | 820/1000 [01:04<00:14, 12.75it/s]Measuring inference for batch_size=1:  82%|████████▏ | 822/1000 [01:04<00:13, 12.74it/s]Measuring inference for batch_size=1:  82%|████████▏ | 824/1000 [01:04<00:13, 12.74it/s]Measuring inference for batch_size=1:  83%|████████▎ | 826/1000 [01:04<00:13, 12.74it/s]Measuring inference for batch_size=1:  83%|████████▎ | 828/1000 [01:04<00:13, 12.74it/s]Measuring inference for batch_size=1:  83%|████████▎ | 830/1000 [01:05<00:13, 12.74it/s]Measuring inference for batch_size=1:  83%|████████▎ | 832/1000 [01:05<00:13, 12.74it/s]Measuring inference for batch_size=1:  83%|████████▎ | 834/1000 [01:05<00:13, 12.74it/s]Measuring inference for batch_size=1:  84%|████████▎ | 836/1000 [01:05<00:12, 12.74it/s]Measuring inference for batch_size=1:  84%|████████▍ | 838/1000 [01:05<00:12, 12.74it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [01:05<00:12, 12.74it/s]Measuring inference for batch_size=1:  84%|████████▍ | 842/1000 [01:06<00:12, 12.74it/s]Measuring inference for batch_size=1:  84%|████████▍ | 844/1000 [01:06<00:12, 12.73it/s]Measuring inference for batch_size=1:  85%|████████▍ | 846/1000 [01:06<00:12, 12.74it/s]Measuring inference for batch_size=1:  85%|████████▍ | 848/1000 [01:06<00:11, 12.74it/s]Measuring inference for batch_size=1:  85%|████████▌ | 850/1000 [01:06<00:11, 12.74it/s]Measuring inference for batch_size=1:  85%|████████▌ | 852/1000 [01:06<00:11, 12.74it/s]Measuring inference for batch_size=1:  85%|████████▌ | 854/1000 [01:07<00:11, 12.74it/s]Measuring inference for batch_size=1:  86%|████████▌ | 856/1000 [01:07<00:11, 12.74it/s]Measuring inference for batch_size=1:  86%|████████▌ | 858/1000 [01:07<00:11, 12.73it/s]Measuring inference for batch_size=1:  86%|████████▌ | 860/1000 [01:07<00:10, 12.73it/s]Measuring inference for batch_size=1:  86%|████████▌ | 862/1000 [01:07<00:10, 12.74it/s]Measuring inference for batch_size=1:  86%|████████▋ | 864/1000 [01:07<00:10, 12.74it/s]Measuring inference for batch_size=1:  87%|████████▋ | 866/1000 [01:07<00:10, 12.74it/s]Measuring inference for batch_size=1:  87%|████████▋ | 868/1000 [01:08<00:10, 12.74it/s]Measuring inference for batch_size=1:  87%|████████▋ | 870/1000 [01:08<00:10, 12.74it/s]Measuring inference for batch_size=1:  87%|████████▋ | 872/1000 [01:08<00:10, 12.74it/s]Measuring inference for batch_size=1:  87%|████████▋ | 874/1000 [01:08<00:09, 12.74it/s]Measuring inference for batch_size=1:  88%|████████▊ | 876/1000 [01:08<00:09, 12.74it/s]Measuring inference for batch_size=1:  88%|████████▊ | 878/1000 [01:08<00:09, 12.74it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [01:09<00:09, 12.74it/s]Measuring inference for batch_size=1:  88%|████████▊ | 882/1000 [01:09<00:09, 12.74it/s]Measuring inference for batch_size=1:  88%|████████▊ | 884/1000 [01:09<00:09, 12.74it/s]Measuring inference for batch_size=1:  89%|████████▊ | 886/1000 [01:09<00:08, 12.75it/s]Measuring inference for batch_size=1:  89%|████████▉ | 888/1000 [01:09<00:08, 12.74it/s]Measuring inference for batch_size=1:  89%|████████▉ | 890/1000 [01:09<00:08, 12.74it/s]Measuring inference for batch_size=1:  89%|████████▉ | 892/1000 [01:10<00:08, 12.74it/s]Measuring inference for batch_size=1:  89%|████████▉ | 894/1000 [01:10<00:08, 12.74it/s]Measuring inference for batch_size=1:  90%|████████▉ | 896/1000 [01:10<00:08, 12.74it/s]Measuring inference for batch_size=1:  90%|████████▉ | 898/1000 [01:10<00:08, 12.74it/s]Measuring inference for batch_size=1:  90%|█████████ | 900/1000 [01:10<00:07, 12.73it/s]Measuring inference for batch_size=1:  90%|█████████ | 902/1000 [01:10<00:07, 12.73it/s]Measuring inference for batch_size=1:  90%|█████████ | 904/1000 [01:10<00:07, 12.74it/s]Measuring inference for batch_size=1:  91%|█████████ | 906/1000 [01:11<00:07, 12.74it/s]Measuring inference for batch_size=1:  91%|█████████ | 908/1000 [01:11<00:07, 12.72it/s]Measuring inference for batch_size=1:  91%|█████████ | 910/1000 [01:11<00:07, 12.73it/s]Measuring inference for batch_size=1:  91%|█████████ | 912/1000 [01:11<00:06, 12.74it/s]Measuring inference for batch_size=1:  91%|█████████▏| 914/1000 [01:11<00:06, 12.74it/s]Measuring inference for batch_size=1:  92%|█████████▏| 916/1000 [01:11<00:06, 12.74it/s]Measuring inference for batch_size=1:  92%|█████████▏| 918/1000 [01:12<00:06, 12.74it/s]Measuring inference for batch_size=1:  92%|█████████▏| 920/1000 [01:12<00:06, 12.74it/s]Measuring inference for batch_size=1:  92%|█████████▏| 922/1000 [01:12<00:06, 12.73it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [01:12<00:05, 12.74it/s]Measuring inference for batch_size=1:  93%|█████████▎| 926/1000 [01:12<00:05, 12.74it/s]Measuring inference for batch_size=1:  93%|█████████▎| 928/1000 [01:12<00:05, 12.74it/s]Measuring inference for batch_size=1:  93%|█████████▎| 930/1000 [01:12<00:05, 12.74it/s]Measuring inference for batch_size=1:  93%|█████████▎| 932/1000 [01:13<00:05, 12.74it/s]Measuring inference for batch_size=1:  93%|█████████▎| 934/1000 [01:13<00:05, 12.74it/s]Measuring inference for batch_size=1:  94%|█████████▎| 936/1000 [01:13<00:05, 12.74it/s]Measuring inference for batch_size=1:  94%|█████████▍| 938/1000 [01:13<00:04, 12.75it/s]Measuring inference for batch_size=1:  94%|█████████▍| 940/1000 [01:13<00:04, 12.75it/s]Measuring inference for batch_size=1:  94%|█████████▍| 942/1000 [01:13<00:04, 12.74it/s]Measuring inference for batch_size=1:  94%|█████████▍| 944/1000 [01:14<00:04, 12.74it/s]Measuring inference for batch_size=1:  95%|█████████▍| 946/1000 [01:14<00:04, 12.73it/s]Measuring inference for batch_size=1:  95%|█████████▍| 948/1000 [01:14<00:04, 12.73it/s]Measuring inference for batch_size=1:  95%|█████████▌| 950/1000 [01:14<00:03, 12.73it/s]Measuring inference for batch_size=1:  95%|█████████▌| 952/1000 [01:14<00:03, 12.73it/s]Measuring inference for batch_size=1:  95%|█████████▌| 954/1000 [01:14<00:03, 12.74it/s]Measuring inference for batch_size=1:  96%|█████████▌| 956/1000 [01:15<00:03, 12.73it/s]Measuring inference for batch_size=1:  96%|█████████▌| 958/1000 [01:15<00:03, 12.73it/s]Measuring inference for batch_size=1:  96%|█████████▌| 960/1000 [01:15<00:03, 12.73it/s]Measuring inference for batch_size=1:  96%|█████████▌| 962/1000 [01:15<00:02, 12.73it/s]Measuring inference for batch_size=1:  96%|█████████▋| 964/1000 [01:15<00:02, 12.73it/s]Measuring inference for batch_size=1:  97%|█████████▋| 966/1000 [01:15<00:02, 12.73it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [01:15<00:02, 12.73it/s]Measuring inference for batch_size=1:  97%|█████████▋| 970/1000 [01:16<00:02, 12.73it/s]Measuring inference for batch_size=1:  97%|█████████▋| 972/1000 [01:16<00:02, 12.73it/s]Measuring inference for batch_size=1:  97%|█████████▋| 974/1000 [01:16<00:02, 12.73it/s]Measuring inference for batch_size=1:  98%|█████████▊| 976/1000 [01:16<00:01, 12.73it/s]Measuring inference for batch_size=1:  98%|█████████▊| 978/1000 [01:16<00:01, 12.74it/s]Measuring inference for batch_size=1:  98%|█████████▊| 980/1000 [01:16<00:01, 12.74it/s]Measuring inference for batch_size=1:  98%|█████████▊| 982/1000 [01:17<00:01, 12.73it/s]Measuring inference for batch_size=1:  98%|█████████▊| 984/1000 [01:17<00:01, 12.73it/s]Measuring inference for batch_size=1:  99%|█████████▊| 986/1000 [01:17<00:01, 12.73it/s]Measuring inference for batch_size=1:  99%|█████████▉| 988/1000 [01:17<00:00, 12.73it/s]Measuring inference for batch_size=1:  99%|█████████▉| 990/1000 [01:17<00:00, 12.73it/s]Measuring inference for batch_size=1:  99%|█████████▉| 992/1000 [01:17<00:00, 12.74it/s]Measuring inference for batch_size=1:  99%|█████████▉| 994/1000 [01:18<00:00, 12.74it/s]Measuring inference for batch_size=1: 100%|█████████▉| 996/1000 [01:18<00:00, 12.74it/s]Measuring inference for batch_size=1: 100%|█████████▉| 998/1000 [01:18<00:00, 12.74it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [01:18<00:00, 12.74it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [01:18<00:00, 12.74it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cpu
flops: 11580687848
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.45 GB
    total: 31.28 GB
    used: 3.38 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
params: 60192808
timing:
  batch_size_1:
    on_device_inference:
      human_readable:
        batch_latency: 78.410 ms +/- 97.816 us [78.114 ms, 79.104 ms]
        batches_per_second: 12.75 +/- 0.02 [12.64, 12.80]
      metrics:
        batches_per_second_max: 12.801836200871097
        batches_per_second_mean: 12.753468037191968
        batches_per_second_min: 12.64159428065078
        batches_per_second_std: 0.0159005311575101
        seconds_per_batch_max: 0.07910394668579102
        seconds_per_batch_mean: 0.07841016674041748
        seconds_per_batch_min: 0.07811379432678223
        seconds_per_batch_std: 9.781558520589038e-05


#####
baseline-baseline-py-id - Run 2
2024-02-23 06:58:52
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  9.03it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  60.19 M, 100.000% Params, 11.58 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.016% Params, 118.01 MMac, 1.019% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.000% Params, 1.61 MMac, 0.014% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.007% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.007% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.359% Params, 680.39 MMac, 5.875% MACs, 
    (0): Bottleneck(
      75.01 k, 0.125% Params, 236.43 MMac, 2.042% MACs, 
      (conv1): Conv2d(4.1 k, 0.007% Params, 12.85 MMac, 0.111% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.028% Params, 52.99 MMac, 0.458% MACs, 
        (0): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    2.34 M, 3.887% Params, 1.92 GMac, 16.555% MACs, 
    (0): Bottleneck(
      379.39 k, 0.630% Params, 376.02 MMac, 3.247% MACs, 
      (conv1): Conv2d(32.77 k, 0.054% Params, 102.76 MMac, 0.887% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 802.82 KMac, 0.007% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.008% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.219% Params, 103.56 MMac, 0.894% MACs, 
        (0): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (4): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (5): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (6): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (7): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    40.61 M, 67.473% Params, 8.05 GMac, 69.501% MACs, 
    (0): Bottleneck(
      1.51 M, 2.513% Params, 374.26 MMac, 3.232% MACs, 
      (conv1): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 401.41 KMac, 0.003% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.004% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 0.874% Params, 103.16 MMac, 0.891% MACs, 
        (0): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (6): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (7): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (8): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (9): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (10): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (11): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (12): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (13): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (14): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (15): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (16): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (17): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (18): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (19): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (20): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (21): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (22): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (23): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (24): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (25): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (26): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (27): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (28): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (29): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (30): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (31): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (32): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (33): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (34): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (35): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 24.861% Params, 811.02 MMac, 7.003% MACs, 
    (0): Bottleneck(
      6.04 M, 10.034% Params, 373.38 MMac, 3.224% MACs, 
      (conv1): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 200.7 KMac, 0.002% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.002% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 3.491% Params, 102.96 MMac, 0.889% MACs, 
        (0): Conv2d(2.1 M, 3.484% Params, 102.76 MMac, 0.887% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.001% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 3.404% Params, 2.05 MMac, 0.018% MACs, in_features=2048, out_features=1000, bias=True)
)Measurement of allocated memory is only available on CUDA devices

Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:   2%|▏         | 2/100 [00:00<00:07, 12.70it/s]Warming up with batch_size=1:   4%|▍         | 4/100 [00:00<00:07, 12.70it/s]Warming up with batch_size=1:   6%|▌         | 6/100 [00:00<00:07, 12.69it/s]Warming up with batch_size=1:   8%|▊         | 8/100 [00:00<00:07, 12.70it/s]Warming up with batch_size=1:  10%|█         | 10/100 [00:00<00:07, 12.70it/s]Warming up with batch_size=1:  12%|█▏        | 12/100 [00:00<00:06, 12.69it/s]Warming up with batch_size=1:  14%|█▍        | 14/100 [00:01<00:06, 12.69it/s]Warming up with batch_size=1:  16%|█▌        | 16/100 [00:01<00:06, 12.69it/s]Warming up with batch_size=1:  18%|█▊        | 18/100 [00:01<00:06, 12.69it/s]Warming up with batch_size=1:  20%|██        | 20/100 [00:01<00:06, 12.68it/s]Warming up with batch_size=1:  22%|██▏       | 22/100 [00:01<00:06, 12.69it/s]Warming up with batch_size=1:  24%|██▍       | 24/100 [00:01<00:05, 12.68it/s]Warming up with batch_size=1:  26%|██▌       | 26/100 [00:02<00:05, 12.68it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:02<00:05, 12.68it/s]Warming up with batch_size=1:  30%|███       | 30/100 [00:02<00:05, 12.68it/s]Warming up with batch_size=1:  32%|███▏      | 32/100 [00:02<00:05, 12.68it/s]Warming up with batch_size=1:  34%|███▍      | 34/100 [00:02<00:05, 12.68it/s]Warming up with batch_size=1:  36%|███▌      | 36/100 [00:02<00:05, 12.68it/s]Warming up with batch_size=1:  38%|███▊      | 38/100 [00:02<00:04, 12.68it/s]Warming up with batch_size=1:  40%|████      | 40/100 [00:03<00:04, 12.68it/s]Warming up with batch_size=1:  42%|████▏     | 42/100 [00:03<00:04, 12.68it/s]Warming up with batch_size=1:  44%|████▍     | 44/100 [00:03<00:04, 12.67it/s]Warming up with batch_size=1:  46%|████▌     | 46/100 [00:03<00:04, 12.67it/s]Warming up with batch_size=1:  48%|████▊     | 48/100 [00:03<00:04, 12.67it/s]Warming up with batch_size=1:  50%|█████     | 50/100 [00:03<00:03, 12.67it/s]Warming up with batch_size=1:  52%|█████▏    | 52/100 [00:04<00:03, 12.67it/s]Warming up with batch_size=1:  54%|█████▍    | 54/100 [00:04<00:03, 12.67it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:04<00:03, 12.67it/s]Warming up with batch_size=1:  58%|█████▊    | 58/100 [00:04<00:03, 12.67it/s]Warming up with batch_size=1:  60%|██████    | 60/100 [00:04<00:03, 12.67it/s]Warming up with batch_size=1:  62%|██████▏   | 62/100 [00:04<00:02, 12.67it/s]Warming up with batch_size=1:  64%|██████▍   | 64/100 [00:05<00:02, 12.67it/s]Warming up with batch_size=1:  66%|██████▌   | 66/100 [00:05<00:02, 12.67it/s]Warming up with batch_size=1:  68%|██████▊   | 68/100 [00:05<00:02, 12.67it/s]Warming up with batch_size=1:  70%|███████   | 70/100 [00:05<00:02, 12.67it/s]Warming up with batch_size=1:  72%|███████▏  | 72/100 [00:05<00:02, 12.67it/s]Warming up with batch_size=1:  74%|███████▍  | 74/100 [00:05<00:02, 12.67it/s]Warming up with batch_size=1:  76%|███████▌  | 76/100 [00:05<00:01, 12.67it/s]Warming up with batch_size=1:  78%|███████▊  | 78/100 [00:06<00:01, 12.67it/s]Warming up with batch_size=1:  80%|████████  | 80/100 [00:06<00:01, 12.67it/s]Warming up with batch_size=1:  82%|████████▏ | 82/100 [00:06<00:01, 12.68it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:06<00:01, 12.68it/s]Warming up with batch_size=1:  86%|████████▌ | 86/100 [00:06<00:01, 12.67it/s]Warming up with batch_size=1:  88%|████████▊ | 88/100 [00:06<00:00, 12.67it/s]Warming up with batch_size=1:  90%|█████████ | 90/100 [00:07<00:00, 12.67it/s]Warming up with batch_size=1:  92%|█████████▏| 92/100 [00:07<00:00, 12.67it/s]Warming up with batch_size=1:  94%|█████████▍| 94/100 [00:07<00:00, 12.67it/s]Warming up with batch_size=1:  96%|█████████▌| 96/100 [00:07<00:00, 12.67it/s]Warming up with batch_size=1:  98%|█████████▊| 98/100 [00:07<00:00, 12.67it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:07<00:00, 12.67it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:07<00:00, 12.68it/s]
STAGE:2024-02-23 06:57:31 173391:173391 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 06:57:31 173391:173391 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 06:57:31 173391:173391 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   0%|          | 2/1000 [00:00<01:18, 12.76it/s]Measuring inference for batch_size=1:   0%|          | 4/1000 [00:00<01:18, 12.76it/s]Measuring inference for batch_size=1:   1%|          | 6/1000 [00:00<01:17, 12.76it/s]Measuring inference for batch_size=1:   1%|          | 8/1000 [00:00<01:17, 12.75it/s]Measuring inference for batch_size=1:   1%|          | 10/1000 [00:00<01:17, 12.75it/s]Measuring inference for batch_size=1:   1%|          | 12/1000 [00:00<01:17, 12.74it/s]Measuring inference for batch_size=1:   1%|▏         | 14/1000 [00:01<01:17, 12.74it/s]Measuring inference for batch_size=1:   2%|▏         | 16/1000 [00:01<01:17, 12.74it/s]Measuring inference for batch_size=1:   2%|▏         | 18/1000 [00:01<01:17, 12.73it/s]Measuring inference for batch_size=1:   2%|▏         | 20/1000 [00:01<01:16, 12.74it/s]Measuring inference for batch_size=1:   2%|▏         | 22/1000 [00:01<01:16, 12.73it/s]Measuring inference for batch_size=1:   2%|▏         | 24/1000 [00:01<01:16, 12.73it/s]Measuring inference for batch_size=1:   3%|▎         | 26/1000 [00:02<01:16, 12.73it/s]Measuring inference for batch_size=1:   3%|▎         | 28/1000 [00:02<01:16, 12.73it/s]Measuring inference for batch_size=1:   3%|▎         | 30/1000 [00:02<01:16, 12.73it/s]Measuring inference for batch_size=1:   3%|▎         | 32/1000 [00:02<01:16, 12.73it/s]Measuring inference for batch_size=1:   3%|▎         | 34/1000 [00:02<01:15, 12.74it/s]Measuring inference for batch_size=1:   4%|▎         | 36/1000 [00:02<01:15, 12.73it/s]Measuring inference for batch_size=1:   4%|▍         | 38/1000 [00:02<01:15, 12.74it/s]Measuring inference for batch_size=1:   4%|▍         | 40/1000 [00:03<01:15, 12.73it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:03<01:15, 12.73it/s]Measuring inference for batch_size=1:   4%|▍         | 44/1000 [00:03<01:15, 12.73it/s]Measuring inference for batch_size=1:   5%|▍         | 46/1000 [00:03<01:14, 12.73it/s]Measuring inference for batch_size=1:   5%|▍         | 48/1000 [00:03<01:14, 12.74it/s]Measuring inference for batch_size=1:   5%|▌         | 50/1000 [00:03<01:14, 12.74it/s]Measuring inference for batch_size=1:   5%|▌         | 52/1000 [00:04<01:14, 12.74it/s]Measuring inference for batch_size=1:   5%|▌         | 54/1000 [00:04<01:14, 12.74it/s]Measuring inference for batch_size=1:   6%|▌         | 56/1000 [00:04<01:14, 12.74it/s]Measuring inference for batch_size=1:   6%|▌         | 58/1000 [00:04<01:13, 12.74it/s]Measuring inference for batch_size=1:   6%|▌         | 60/1000 [00:04<01:13, 12.74it/s]Measuring inference for batch_size=1:   6%|▌         | 62/1000 [00:04<01:13, 12.74it/s]Measuring inference for batch_size=1:   6%|▋         | 64/1000 [00:05<01:13, 12.74it/s]Measuring inference for batch_size=1:   7%|▋         | 66/1000 [00:05<01:13, 12.74it/s]Measuring inference for batch_size=1:   7%|▋         | 68/1000 [00:05<01:13, 12.74it/s]Measuring inference for batch_size=1:   7%|▋         | 70/1000 [00:05<01:13, 12.74it/s]Measuring inference for batch_size=1:   7%|▋         | 72/1000 [00:05<01:12, 12.74it/s]Measuring inference for batch_size=1:   7%|▋         | 74/1000 [00:05<01:12, 12.75it/s]Measuring inference for batch_size=1:   8%|▊         | 76/1000 [00:05<01:12, 12.75it/s]Measuring inference for batch_size=1:   8%|▊         | 78/1000 [00:06<01:12, 12.75it/s]Measuring inference for batch_size=1:   8%|▊         | 80/1000 [00:06<01:12, 12.75it/s]Measuring inference for batch_size=1:   8%|▊         | 82/1000 [00:06<01:12, 12.75it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:06<01:11, 12.74it/s]Measuring inference for batch_size=1:   9%|▊         | 86/1000 [00:06<01:11, 12.74it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:06<01:11, 12.74it/s]Measuring inference for batch_size=1:   9%|▉         | 90/1000 [00:07<01:11, 12.74it/s]Measuring inference for batch_size=1:   9%|▉         | 92/1000 [00:07<01:11, 12.75it/s]Measuring inference for batch_size=1:   9%|▉         | 94/1000 [00:07<01:11, 12.75it/s]Measuring inference for batch_size=1:  10%|▉         | 96/1000 [00:07<01:10, 12.75it/s]Measuring inference for batch_size=1:  10%|▉         | 98/1000 [00:07<01:10, 12.75it/s]Measuring inference for batch_size=1:  10%|█         | 100/1000 [00:07<01:10, 12.75it/s]Measuring inference for batch_size=1:  10%|█         | 102/1000 [00:08<01:10, 12.75it/s]Measuring inference for batch_size=1:  10%|█         | 104/1000 [00:08<01:10, 12.75it/s]Measuring inference for batch_size=1:  11%|█         | 106/1000 [00:08<01:10, 12.75it/s]Measuring inference for batch_size=1:  11%|█         | 108/1000 [00:08<01:09, 12.75it/s]Measuring inference for batch_size=1:  11%|█         | 110/1000 [00:08<01:09, 12.75it/s]Measuring inference for batch_size=1:  11%|█         | 112/1000 [00:08<01:09, 12.75it/s]Measuring inference for batch_size=1:  11%|█▏        | 114/1000 [00:08<01:09, 12.75it/s]Measuring inference for batch_size=1:  12%|█▏        | 116/1000 [00:09<01:09, 12.75it/s]Measuring inference for batch_size=1:  12%|█▏        | 118/1000 [00:09<01:09, 12.75it/s]Measuring inference for batch_size=1:  12%|█▏        | 120/1000 [00:09<01:09, 12.75it/s]Measuring inference for batch_size=1:  12%|█▏        | 122/1000 [00:09<01:08, 12.75it/s]Measuring inference for batch_size=1:  12%|█▏        | 124/1000 [00:09<01:08, 12.74it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:09<01:08, 12.74it/s]Measuring inference for batch_size=1:  13%|█▎        | 128/1000 [00:10<01:08, 12.74it/s]Measuring inference for batch_size=1:  13%|█▎        | 130/1000 [00:10<01:08, 12.74it/s]Measuring inference for batch_size=1:  13%|█▎        | 132/1000 [00:10<01:08, 12.74it/s]Measuring inference for batch_size=1:  13%|█▎        | 134/1000 [00:10<01:07, 12.74it/s]Measuring inference for batch_size=1:  14%|█▎        | 136/1000 [00:10<01:07, 12.74it/s]Measuring inference for batch_size=1:  14%|█▍        | 138/1000 [00:10<01:07, 12.74it/s]Measuring inference for batch_size=1:  14%|█▍        | 140/1000 [00:10<01:07, 12.74it/s]Measuring inference for batch_size=1:  14%|█▍        | 142/1000 [00:11<01:07, 12.74it/s]Measuring inference for batch_size=1:  14%|█▍        | 144/1000 [00:11<01:07, 12.74it/s]Measuring inference for batch_size=1:  15%|█▍        | 146/1000 [00:11<01:07, 12.74it/s]Measuring inference for batch_size=1:  15%|█▍        | 148/1000 [00:11<01:06, 12.74it/s]Measuring inference for batch_size=1:  15%|█▌        | 150/1000 [00:11<01:06, 12.74it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:11<01:06, 12.74it/s]Measuring inference for batch_size=1:  15%|█▌        | 154/1000 [00:12<01:06, 12.74it/s]Measuring inference for batch_size=1:  16%|█▌        | 156/1000 [00:12<01:06, 12.74it/s]Measuring inference for batch_size=1:  16%|█▌        | 158/1000 [00:12<01:06, 12.74it/s]Measuring inference for batch_size=1:  16%|█▌        | 160/1000 [00:12<01:05, 12.73it/s]Measuring inference for batch_size=1:  16%|█▌        | 162/1000 [00:12<01:05, 12.73it/s]Measuring inference for batch_size=1:  16%|█▋        | 164/1000 [00:12<01:05, 12.73it/s]Measuring inference for batch_size=1:  17%|█▋        | 166/1000 [00:13<01:05, 12.73it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:13<01:05, 12.74it/s]Measuring inference for batch_size=1:  17%|█▋        | 170/1000 [00:13<01:05, 12.74it/s]Measuring inference for batch_size=1:  17%|█▋        | 172/1000 [00:13<01:05, 12.74it/s]Measuring inference for batch_size=1:  17%|█▋        | 174/1000 [00:13<01:04, 12.74it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:13<01:04, 12.74it/s]Measuring inference for batch_size=1:  18%|█▊        | 178/1000 [00:13<01:04, 12.74it/s]Measuring inference for batch_size=1:  18%|█▊        | 180/1000 [00:14<01:04, 12.74it/s]Measuring inference for batch_size=1:  18%|█▊        | 182/1000 [00:14<01:04, 12.73it/s]Measuring inference for batch_size=1:  18%|█▊        | 184/1000 [00:14<01:04, 12.73it/s]Measuring inference for batch_size=1:  19%|█▊        | 186/1000 [00:14<01:03, 12.74it/s]Measuring inference for batch_size=1:  19%|█▉        | 188/1000 [00:14<01:03, 12.74it/s]Measuring inference for batch_size=1:  19%|█▉        | 190/1000 [00:14<01:03, 12.73it/s]Measuring inference for batch_size=1:  19%|█▉        | 192/1000 [00:15<01:03, 12.73it/s]Measuring inference for batch_size=1:  19%|█▉        | 194/1000 [00:15<01:03, 12.74it/s]Measuring inference for batch_size=1:  20%|█▉        | 196/1000 [00:15<01:03, 12.74it/s]Measuring inference for batch_size=1:  20%|█▉        | 198/1000 [00:15<01:02, 12.74it/s]Measuring inference for batch_size=1:  20%|██        | 200/1000 [00:15<01:02, 12.74it/s]Measuring inference for batch_size=1:  20%|██        | 202/1000 [00:15<01:02, 12.73it/s]Measuring inference for batch_size=1:  20%|██        | 204/1000 [00:16<01:02, 12.73it/s]Measuring inference for batch_size=1:  21%|██        | 206/1000 [00:16<01:02, 12.73it/s]Measuring inference for batch_size=1:  21%|██        | 208/1000 [00:16<01:02, 12.73it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:16<01:02, 12.73it/s]Measuring inference for batch_size=1:  21%|██        | 212/1000 [00:16<01:01, 12.74it/s]Measuring inference for batch_size=1:  21%|██▏       | 214/1000 [00:16<01:01, 12.73it/s]Measuring inference for batch_size=1:  22%|██▏       | 216/1000 [00:16<01:01, 12.73it/s]Measuring inference for batch_size=1:  22%|██▏       | 218/1000 [00:17<01:01, 12.73it/s]Measuring inference for batch_size=1:  22%|██▏       | 220/1000 [00:17<01:01, 12.73it/s]Measuring inference for batch_size=1:  22%|██▏       | 222/1000 [00:17<01:01, 12.73it/s]Measuring inference for batch_size=1:  22%|██▏       | 224/1000 [00:17<01:00, 12.73it/s]Measuring inference for batch_size=1:  23%|██▎       | 226/1000 [00:17<01:00, 12.73it/s]Measuring inference for batch_size=1:  23%|██▎       | 228/1000 [00:17<01:00, 12.73it/s]Measuring inference for batch_size=1:  23%|██▎       | 230/1000 [00:18<01:00, 12.73it/s]Measuring inference for batch_size=1:  23%|██▎       | 232/1000 [00:18<01:00, 12.73it/s]Measuring inference for batch_size=1:  23%|██▎       | 234/1000 [00:18<01:00, 12.73it/s]Measuring inference for batch_size=1:  24%|██▎       | 236/1000 [00:18<01:00, 12.73it/s]Measuring inference for batch_size=1:  24%|██▍       | 238/1000 [00:18<00:59, 12.73it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:18<00:59, 12.73it/s]Measuring inference for batch_size=1:  24%|██▍       | 242/1000 [00:18<00:59, 12.72it/s]Measuring inference for batch_size=1:  24%|██▍       | 244/1000 [00:19<00:59, 12.72it/s]Measuring inference for batch_size=1:  25%|██▍       | 246/1000 [00:19<00:59, 12.72it/s]Measuring inference for batch_size=1:  25%|██▍       | 248/1000 [00:19<00:59, 12.73it/s]Measuring inference for batch_size=1:  25%|██▌       | 250/1000 [00:19<00:58, 12.73it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:19<00:58, 12.73it/s]Measuring inference for batch_size=1:  25%|██▌       | 254/1000 [00:19<00:58, 12.73it/s]Measuring inference for batch_size=1:  26%|██▌       | 256/1000 [00:20<00:58, 12.73it/s]Measuring inference for batch_size=1:  26%|██▌       | 258/1000 [00:20<00:58, 12.73it/s]Measuring inference for batch_size=1:  26%|██▌       | 260/1000 [00:20<00:58, 12.74it/s]Measuring inference for batch_size=1:  26%|██▌       | 262/1000 [00:20<00:57, 12.73it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:20<00:57, 12.73it/s]Measuring inference for batch_size=1:  27%|██▋       | 266/1000 [00:20<00:57, 12.73it/s]Measuring inference for batch_size=1:  27%|██▋       | 268/1000 [00:21<00:57, 12.74it/s]Measuring inference for batch_size=1:  27%|██▋       | 270/1000 [00:21<00:57, 12.73it/s]Measuring inference for batch_size=1:  27%|██▋       | 272/1000 [00:21<00:57, 12.74it/s]Measuring inference for batch_size=1:  27%|██▋       | 274/1000 [00:21<00:56, 12.74it/s]Measuring inference for batch_size=1:  28%|██▊       | 276/1000 [00:21<00:56, 12.73it/s]Measuring inference for batch_size=1:  28%|██▊       | 278/1000 [00:21<00:56, 12.73it/s]Measuring inference for batch_size=1:  28%|██▊       | 280/1000 [00:21<00:56, 12.74it/s]Measuring inference for batch_size=1:  28%|██▊       | 282/1000 [00:22<00:56, 12.74it/s]Measuring inference for batch_size=1:  28%|██▊       | 284/1000 [00:22<00:56, 12.74it/s]Measuring inference for batch_size=1:  29%|██▊       | 286/1000 [00:22<00:56, 12.74it/s]Measuring inference for batch_size=1:  29%|██▉       | 288/1000 [00:22<00:55, 12.73it/s]Measuring inference for batch_size=1:  29%|██▉       | 290/1000 [00:22<00:55, 12.74it/s]Measuring inference for batch_size=1:  29%|██▉       | 292/1000 [00:22<00:55, 12.74it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:23<00:55, 12.74it/s]Measuring inference for batch_size=1:  30%|██▉       | 296/1000 [00:23<00:55, 12.74it/s]Measuring inference for batch_size=1:  30%|██▉       | 298/1000 [00:23<00:55, 12.74it/s]Measuring inference for batch_size=1:  30%|███       | 300/1000 [00:23<00:54, 12.73it/s]Measuring inference for batch_size=1:  30%|███       | 302/1000 [00:23<00:54, 12.74it/s]Measuring inference for batch_size=1:  30%|███       | 304/1000 [00:23<00:54, 12.74it/s]Measuring inference for batch_size=1:  31%|███       | 306/1000 [00:24<00:54, 12.74it/s]Measuring inference for batch_size=1:  31%|███       | 308/1000 [00:24<00:54, 12.74it/s]Measuring inference for batch_size=1:  31%|███       | 310/1000 [00:24<00:54, 12.74it/s]Measuring inference for batch_size=1:  31%|███       | 312/1000 [00:24<00:54, 12.74it/s]Measuring inference for batch_size=1:  31%|███▏      | 314/1000 [00:24<00:53, 12.74it/s]Measuring inference for batch_size=1:  32%|███▏      | 316/1000 [00:24<00:53, 12.74it/s]Measuring inference for batch_size=1:  32%|███▏      | 318/1000 [00:24<00:53, 12.74it/s]Measuring inference for batch_size=1:  32%|███▏      | 320/1000 [00:25<00:53, 12.74it/s]Measuring inference for batch_size=1:  32%|███▏      | 322/1000 [00:25<00:53, 12.74it/s]Measuring inference for batch_size=1:  32%|███▏      | 324/1000 [00:25<00:53, 12.74it/s]Measuring inference for batch_size=1:  33%|███▎      | 326/1000 [00:25<00:52, 12.74it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:25<00:52, 12.74it/s]Measuring inference for batch_size=1:  33%|███▎      | 330/1000 [00:25<00:52, 12.74it/s]Measuring inference for batch_size=1:  33%|███▎      | 332/1000 [00:26<00:52, 12.73it/s]Measuring inference for batch_size=1:  33%|███▎      | 334/1000 [00:26<00:52, 12.73it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:26<00:52, 12.73it/s]Measuring inference for batch_size=1:  34%|███▍      | 338/1000 [00:26<00:52, 12.73it/s]Measuring inference for batch_size=1:  34%|███▍      | 340/1000 [00:26<00:51, 12.73it/s]Measuring inference for batch_size=1:  34%|███▍      | 342/1000 [00:26<00:51, 12.73it/s]Measuring inference for batch_size=1:  34%|███▍      | 344/1000 [00:27<00:51, 12.73it/s]Measuring inference for batch_size=1:  35%|███▍      | 346/1000 [00:27<00:51, 12.73it/s]Measuring inference for batch_size=1:  35%|███▍      | 348/1000 [00:27<00:51, 12.73it/s]Measuring inference for batch_size=1:  35%|███▌      | 350/1000 [00:27<00:51, 12.73it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:27<00:50, 12.74it/s]Measuring inference for batch_size=1:  35%|███▌      | 354/1000 [00:27<00:50, 12.73it/s]Measuring inference for batch_size=1:  36%|███▌      | 356/1000 [00:27<00:50, 12.73it/s]Measuring inference for batch_size=1:  36%|███▌      | 358/1000 [00:28<00:50, 12.73it/s]Measuring inference for batch_size=1:  36%|███▌      | 360/1000 [00:28<00:50, 12.73it/s]Measuring inference for batch_size=1:  36%|███▌      | 362/1000 [00:28<00:50, 12.73it/s]Measuring inference for batch_size=1:  36%|███▋      | 364/1000 [00:28<00:49, 12.73it/s]Measuring inference for batch_size=1:  37%|███▋      | 366/1000 [00:28<00:49, 12.73it/s]Measuring inference for batch_size=1:  37%|███▋      | 368/1000 [00:28<00:49, 12.73it/s]Measuring inference for batch_size=1:  37%|███▋      | 370/1000 [00:29<00:49, 12.73it/s]Measuring inference for batch_size=1:  37%|███▋      | 372/1000 [00:29<00:49, 12.73it/s]Measuring inference for batch_size=1:  37%|███▋      | 374/1000 [00:29<00:49, 12.74it/s]Measuring inference for batch_size=1:  38%|███▊      | 376/1000 [00:29<00:48, 12.74it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:29<00:48, 12.74it/s]Measuring inference for batch_size=1:  38%|███▊      | 380/1000 [00:29<00:48, 12.74it/s]Measuring inference for batch_size=1:  38%|███▊      | 382/1000 [00:29<00:48, 12.74it/s]Measuring inference for batch_size=1:  38%|███▊      | 384/1000 [00:30<00:48, 12.74it/s]Measuring inference for batch_size=1:  39%|███▊      | 386/1000 [00:30<00:48, 12.74it/s]Measuring inference for batch_size=1:  39%|███▉      | 388/1000 [00:30<00:48, 12.73it/s]Measuring inference for batch_size=1:  39%|███▉      | 390/1000 [00:30<00:47, 12.74it/s]Measuring inference for batch_size=1:  39%|███▉      | 392/1000 [00:30<00:47, 12.74it/s]Measuring inference for batch_size=1:  39%|███▉      | 394/1000 [00:30<00:47, 12.74it/s]Measuring inference for batch_size=1:  40%|███▉      | 396/1000 [00:31<00:47, 12.74it/s]Measuring inference for batch_size=1:  40%|███▉      | 398/1000 [00:31<00:47, 12.74it/s]Measuring inference for batch_size=1:  40%|████      | 400/1000 [00:31<00:47, 12.74it/s]Measuring inference for batch_size=1:  40%|████      | 402/1000 [00:31<00:46, 12.74it/s]Measuring inference for batch_size=1:  40%|████      | 404/1000 [00:31<00:46, 12.73it/s]Measuring inference for batch_size=1:  41%|████      | 406/1000 [00:31<00:46, 12.73it/s]Measuring inference for batch_size=1:  41%|████      | 408/1000 [00:32<00:46, 12.73it/s]Measuring inference for batch_size=1:  41%|████      | 410/1000 [00:32<00:46, 12.74it/s]Measuring inference for batch_size=1:  41%|████      | 412/1000 [00:32<00:46, 12.74it/s]Measuring inference for batch_size=1:  41%|████▏     | 414/1000 [00:32<00:45, 12.74it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:32<00:45, 12.74it/s]Measuring inference for batch_size=1:  42%|████▏     | 418/1000 [00:32<00:45, 12.74it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:32<00:45, 12.74it/s]Measuring inference for batch_size=1:  42%|████▏     | 422/1000 [00:33<00:45, 12.74it/s]Measuring inference for batch_size=1:  42%|████▏     | 424/1000 [00:33<00:45, 12.74it/s]Measuring inference for batch_size=1:  43%|████▎     | 426/1000 [00:33<00:45, 12.74it/s]Measuring inference for batch_size=1:  43%|████▎     | 428/1000 [00:33<00:44, 12.74it/s]Measuring inference for batch_size=1:  43%|████▎     | 430/1000 [00:33<00:44, 12.74it/s]Measuring inference for batch_size=1:  43%|████▎     | 432/1000 [00:33<00:44, 12.74it/s]Measuring inference for batch_size=1:  43%|████▎     | 434/1000 [00:34<00:44, 12.74it/s]Measuring inference for batch_size=1:  44%|████▎     | 436/1000 [00:34<00:44, 12.73it/s]Measuring inference for batch_size=1:  44%|████▍     | 438/1000 [00:34<00:44, 12.74it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:34<00:43, 12.74it/s]Measuring inference for batch_size=1:  44%|████▍     | 442/1000 [00:34<00:43, 12.74it/s]Measuring inference for batch_size=1:  44%|████▍     | 444/1000 [00:34<00:43, 12.74it/s]Measuring inference for batch_size=1:  45%|████▍     | 446/1000 [00:35<00:43, 12.75it/s]Measuring inference for batch_size=1:  45%|████▍     | 448/1000 [00:35<00:43, 12.75it/s]Measuring inference for batch_size=1:  45%|████▌     | 450/1000 [00:35<00:43, 12.75it/s]Measuring inference for batch_size=1:  45%|████▌     | 452/1000 [00:35<00:42, 12.75it/s]Measuring inference for batch_size=1:  45%|████▌     | 454/1000 [00:35<00:42, 12.75it/s]Measuring inference for batch_size=1:  46%|████▌     | 456/1000 [00:35<00:42, 12.74it/s]Measuring inference for batch_size=1:  46%|████▌     | 458/1000 [00:35<00:42, 12.73it/s]Measuring inference for batch_size=1:  46%|████▌     | 460/1000 [00:36<00:42, 12.73it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:36<00:42, 12.74it/s]Measuring inference for batch_size=1:  46%|████▋     | 464/1000 [00:36<00:42, 12.74it/s]Measuring inference for batch_size=1:  47%|████▋     | 466/1000 [00:36<00:41, 12.74it/s]Measuring inference for batch_size=1:  47%|████▋     | 468/1000 [00:36<00:41, 12.74it/s]Measuring inference for batch_size=1:  47%|████▋     | 470/1000 [00:36<00:41, 12.74it/s]Measuring inference for batch_size=1:  47%|████▋     | 472/1000 [00:37<00:41, 12.74it/s]Measuring inference for batch_size=1:  47%|████▋     | 474/1000 [00:37<00:41, 12.74it/s]Measuring inference for batch_size=1:  48%|████▊     | 476/1000 [00:37<00:41, 12.74it/s]Measuring inference for batch_size=1:  48%|████▊     | 478/1000 [00:37<00:40, 12.74it/s]Measuring inference for batch_size=1:  48%|████▊     | 480/1000 [00:37<00:40, 12.74it/s]Measuring inference for batch_size=1:  48%|████▊     | 482/1000 [00:37<00:40, 12.74it/s]Measuring inference for batch_size=1:  48%|████▊     | 484/1000 [00:37<00:40, 12.74it/s]Measuring inference for batch_size=1:  49%|████▊     | 486/1000 [00:38<00:40, 12.74it/s]Measuring inference for batch_size=1:  49%|████▉     | 488/1000 [00:38<00:40, 12.73it/s]Measuring inference for batch_size=1:  49%|████▉     | 490/1000 [00:38<00:40, 12.73it/s]Measuring inference for batch_size=1:  49%|████▉     | 492/1000 [00:38<00:39, 12.73it/s]Measuring inference for batch_size=1:  49%|████▉     | 494/1000 [00:38<00:39, 12.73it/s]Measuring inference for batch_size=1:  50%|████▉     | 496/1000 [00:38<00:39, 12.73it/s]Measuring inference for batch_size=1:  50%|████▉     | 498/1000 [00:39<00:39, 12.73it/s]Measuring inference for batch_size=1:  50%|█████     | 500/1000 [00:39<00:39, 12.72it/s]Measuring inference for batch_size=1:  50%|█████     | 502/1000 [00:39<00:39, 12.73it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:39<00:38, 12.73it/s]Measuring inference for batch_size=1:  51%|█████     | 506/1000 [00:39<00:38, 12.73it/s]Measuring inference for batch_size=1:  51%|█████     | 508/1000 [00:39<00:38, 12.73it/s]Measuring inference for batch_size=1:  51%|█████     | 510/1000 [00:40<00:38, 12.74it/s]Measuring inference for batch_size=1:  51%|█████     | 512/1000 [00:40<00:38, 12.73it/s]Measuring inference for batch_size=1:  51%|█████▏    | 514/1000 [00:40<00:38, 12.73it/s]Measuring inference for batch_size=1:  52%|█████▏    | 516/1000 [00:40<00:38, 12.73it/s]Measuring inference for batch_size=1:  52%|█████▏    | 518/1000 [00:40<00:37, 12.73it/s]Measuring inference for batch_size=1:  52%|█████▏    | 520/1000 [00:40<00:37, 12.73it/s]Measuring inference for batch_size=1:  52%|█████▏    | 522/1000 [00:40<00:37, 12.74it/s]Measuring inference for batch_size=1:  52%|█████▏    | 524/1000 [00:41<00:37, 12.73it/s]Measuring inference for batch_size=1:  53%|█████▎    | 526/1000 [00:41<00:37, 12.74it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:41<00:37, 12.74it/s]Measuring inference for batch_size=1:  53%|█████▎    | 530/1000 [00:41<00:36, 12.73it/s]Measuring inference for batch_size=1:  53%|█████▎    | 532/1000 [00:41<00:36, 12.74it/s]Measuring inference for batch_size=1:  53%|█████▎    | 534/1000 [00:41<00:36, 12.74it/s]Measuring inference for batch_size=1:  54%|█████▎    | 536/1000 [00:42<00:36, 12.74it/s]Measuring inference for batch_size=1:  54%|█████▍    | 538/1000 [00:42<00:36, 12.73it/s]Measuring inference for batch_size=1:  54%|█████▍    | 540/1000 [00:42<00:36, 12.74it/s]Measuring inference for batch_size=1:  54%|█████▍    | 542/1000 [00:42<00:35, 12.74it/s]Measuring inference for batch_size=1:  54%|█████▍    | 544/1000 [00:42<00:35, 12.74it/s]Measuring inference for batch_size=1:  55%|█████▍    | 546/1000 [00:42<00:35, 12.75it/s]Measuring inference for batch_size=1:  55%|█████▍    | 548/1000 [00:43<00:35, 12.74it/s]Measuring inference for batch_size=1:  55%|█████▌    | 550/1000 [00:43<00:35, 12.74it/s]Measuring inference for batch_size=1:  55%|█████▌    | 552/1000 [00:43<00:35, 12.74it/s]Measuring inference for batch_size=1:  55%|█████▌    | 554/1000 [00:43<00:35, 12.74it/s]Measuring inference for batch_size=1:  56%|█████▌    | 556/1000 [00:43<00:34, 12.74it/s]Measuring inference for batch_size=1:  56%|█████▌    | 558/1000 [00:43<00:34, 12.73it/s]Measuring inference for batch_size=1:  56%|█████▌    | 560/1000 [00:43<00:34, 12.73it/s]Measuring inference for batch_size=1:  56%|█████▌    | 562/1000 [00:44<00:34, 12.74it/s]Measuring inference for batch_size=1:  56%|█████▋    | 564/1000 [00:44<00:34, 12.74it/s]Measuring inference for batch_size=1:  57%|█████▋    | 566/1000 [00:44<00:34, 12.74it/s]Measuring inference for batch_size=1:  57%|█████▋    | 568/1000 [00:44<00:33, 12.74it/s]Measuring inference for batch_size=1:  57%|█████▋    | 570/1000 [00:44<00:33, 12.74it/s]Measuring inference for batch_size=1:  57%|█████▋    | 572/1000 [00:44<00:33, 12.74it/s]Measuring inference for batch_size=1:  57%|█████▋    | 574/1000 [00:45<00:33, 12.74it/s]Measuring inference for batch_size=1:  58%|█████▊    | 576/1000 [00:45<00:33, 12.74it/s]Measuring inference for batch_size=1:  58%|█████▊    | 578/1000 [00:45<00:33, 12.74it/s]Measuring inference for batch_size=1:  58%|█████▊    | 580/1000 [00:45<00:32, 12.74it/s]Measuring inference for batch_size=1:  58%|█████▊    | 582/1000 [00:45<00:32, 12.74it/s]Measuring inference for batch_size=1:  58%|█████▊    | 584/1000 [00:45<00:32, 12.75it/s]Measuring inference for batch_size=1:  59%|█████▊    | 586/1000 [00:46<00:32, 12.74it/s]Measuring inference for batch_size=1:  59%|█████▉    | 588/1000 [00:46<00:32, 12.74it/s]Measuring inference for batch_size=1:  59%|█████▉    | 590/1000 [00:46<00:32, 12.75it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [00:46<00:32, 12.75it/s]Measuring inference for batch_size=1:  59%|█████▉    | 594/1000 [00:46<00:31, 12.75it/s]Measuring inference for batch_size=1:  60%|█████▉    | 596/1000 [00:46<00:31, 12.74it/s]Measuring inference for batch_size=1:  60%|█████▉    | 598/1000 [00:46<00:31, 12.74it/s]Measuring inference for batch_size=1:  60%|██████    | 600/1000 [00:47<00:31, 12.74it/s]Measuring inference for batch_size=1:  60%|██████    | 602/1000 [00:47<00:31, 12.74it/s]Measuring inference for batch_size=1:  60%|██████    | 604/1000 [00:47<00:31, 12.74it/s]Measuring inference for batch_size=1:  61%|██████    | 606/1000 [00:47<00:30, 12.74it/s]Measuring inference for batch_size=1:  61%|██████    | 608/1000 [00:47<00:30, 12.73it/s]Measuring inference for batch_size=1:  61%|██████    | 610/1000 [00:47<00:30, 12.73it/s]Measuring inference for batch_size=1:  61%|██████    | 612/1000 [00:48<00:30, 12.73it/s]Measuring inference for batch_size=1:  61%|██████▏   | 614/1000 [00:48<00:30, 12.73it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:48<00:30, 12.73it/s]Measuring inference for batch_size=1:  62%|██████▏   | 618/1000 [00:48<00:30, 12.73it/s]Measuring inference for batch_size=1:  62%|██████▏   | 620/1000 [00:48<00:29, 12.73it/s]Measuring inference for batch_size=1:  62%|██████▏   | 622/1000 [00:48<00:29, 12.73it/s]Measuring inference for batch_size=1:  62%|██████▏   | 624/1000 [00:48<00:29, 12.73it/s]Measuring inference for batch_size=1:  63%|██████▎   | 626/1000 [00:49<00:29, 12.72it/s]Measuring inference for batch_size=1:  63%|██████▎   | 628/1000 [00:49<00:29, 12.73it/s]Measuring inference for batch_size=1:  63%|██████▎   | 630/1000 [00:49<00:29, 12.72it/s]Measuring inference for batch_size=1:  63%|██████▎   | 632/1000 [00:49<00:28, 12.73it/s]Measuring inference for batch_size=1:  63%|██████▎   | 634/1000 [00:49<00:28, 12.73it/s]Measuring inference for batch_size=1:  64%|██████▎   | 636/1000 [00:49<00:28, 12.73it/s]Measuring inference for batch_size=1:  64%|██████▍   | 638/1000 [00:50<00:28, 12.73it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [00:50<00:28, 12.73it/s]Measuring inference for batch_size=1:  64%|██████▍   | 642/1000 [00:50<00:28, 12.74it/s]Measuring inference for batch_size=1:  64%|██████▍   | 644/1000 [00:50<00:27, 12.74it/s]Measuring inference for batch_size=1:  65%|██████▍   | 646/1000 [00:50<00:27, 12.74it/s]Measuring inference for batch_size=1:  65%|██████▍   | 648/1000 [00:50<00:27, 12.74it/s]Measuring inference for batch_size=1:  65%|██████▌   | 650/1000 [00:51<00:27, 12.74it/s]Measuring inference for batch_size=1:  65%|██████▌   | 652/1000 [00:51<00:27, 12.74it/s]Measuring inference for batch_size=1:  65%|██████▌   | 654/1000 [00:51<00:27, 12.74it/s]Measuring inference for batch_size=1:  66%|██████▌   | 656/1000 [00:51<00:26, 12.74it/s]Measuring inference for batch_size=1:  66%|██████▌   | 658/1000 [00:51<00:26, 12.74it/s]Measuring inference for batch_size=1:  66%|██████▌   | 660/1000 [00:51<00:26, 12.74it/s]Measuring inference for batch_size=1:  66%|██████▌   | 662/1000 [00:51<00:26, 12.74it/s]Measuring inference for batch_size=1:  66%|██████▋   | 664/1000 [00:52<00:26, 12.74it/s]Measuring inference for batch_size=1:  67%|██████▋   | 666/1000 [00:52<00:26, 12.73it/s]Measuring inference for batch_size=1:  67%|██████▋   | 668/1000 [00:52<00:26, 12.74it/s]Measuring inference for batch_size=1:  67%|██████▋   | 670/1000 [00:52<00:25, 12.74it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:52<00:25, 12.74it/s]Measuring inference for batch_size=1:  67%|██████▋   | 674/1000 [00:52<00:25, 12.74it/s]Measuring inference for batch_size=1:  68%|██████▊   | 676/1000 [00:53<00:25, 12.74it/s]Measuring inference for batch_size=1:  68%|██████▊   | 678/1000 [00:53<00:25, 12.74it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [00:53<00:25, 12.74it/s]Measuring inference for batch_size=1:  68%|██████▊   | 682/1000 [00:53<00:24, 12.74it/s]Measuring inference for batch_size=1:  68%|██████▊   | 684/1000 [00:53<00:24, 12.74it/s]Measuring inference for batch_size=1:  69%|██████▊   | 686/1000 [00:53<00:24, 12.73it/s]Measuring inference for batch_size=1:  69%|██████▉   | 688/1000 [00:54<00:24, 12.74it/s]Measuring inference for batch_size=1:  69%|██████▉   | 690/1000 [00:54<00:24, 12.74it/s]Measuring inference for batch_size=1:  69%|██████▉   | 692/1000 [00:54<00:24, 12.74it/s]Measuring inference for batch_size=1:  69%|██████▉   | 694/1000 [00:54<00:24, 12.74it/s]Measuring inference for batch_size=1:  70%|██████▉   | 696/1000 [00:54<00:23, 12.74it/s]Measuring inference for batch_size=1:  70%|██████▉   | 698/1000 [00:54<00:23, 12.74it/s]Measuring inference for batch_size=1:  70%|███████   | 700/1000 [00:54<00:23, 12.75it/s]Measuring inference for batch_size=1:  70%|███████   | 702/1000 [00:55<00:23, 12.75it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [00:55<00:23, 12.74it/s]Measuring inference for batch_size=1:  71%|███████   | 706/1000 [00:55<00:23, 12.74it/s]Measuring inference for batch_size=1:  71%|███████   | 708/1000 [00:55<00:22, 12.75it/s]Measuring inference for batch_size=1:  71%|███████   | 710/1000 [00:55<00:22, 12.74it/s]Measuring inference for batch_size=1:  71%|███████   | 712/1000 [00:55<00:22, 12.74it/s]Measuring inference for batch_size=1:  71%|███████▏  | 714/1000 [00:56<00:22, 12.74it/s]Measuring inference for batch_size=1:  72%|███████▏  | 716/1000 [00:56<00:22, 12.74it/s]Measuring inference for batch_size=1:  72%|███████▏  | 718/1000 [00:56<00:22, 12.74it/s]Measuring inference for batch_size=1:  72%|███████▏  | 720/1000 [00:56<00:21, 12.74it/s]Measuring inference for batch_size=1:  72%|███████▏  | 722/1000 [00:56<00:21, 12.74it/s]Measuring inference for batch_size=1:  72%|███████▏  | 724/1000 [00:56<00:21, 12.74it/s]Measuring inference for batch_size=1:  73%|███████▎  | 726/1000 [00:56<00:21, 12.74it/s]Measuring inference for batch_size=1:  73%|███████▎  | 728/1000 [00:57<00:21, 12.74it/s]Measuring inference for batch_size=1:  73%|███████▎  | 730/1000 [00:57<00:21, 12.74it/s]Measuring inference for batch_size=1:  73%|███████▎  | 732/1000 [00:57<00:21, 12.74it/s]Measuring inference for batch_size=1:  73%|███████▎  | 734/1000 [00:57<00:20, 12.74it/s]Measuring inference for batch_size=1:  74%|███████▎  | 736/1000 [00:57<00:20, 12.73it/s]Measuring inference for batch_size=1:  74%|███████▍  | 738/1000 [00:57<00:20, 12.74it/s]Measuring inference for batch_size=1:  74%|███████▍  | 740/1000 [00:58<00:20, 12.74it/s]Measuring inference for batch_size=1:  74%|███████▍  | 742/1000 [00:58<00:20, 12.74it/s]Measuring inference for batch_size=1:  74%|███████▍  | 744/1000 [00:58<00:20, 12.74it/s]Measuring inference for batch_size=1:  75%|███████▍  | 746/1000 [00:58<00:19, 12.74it/s]Measuring inference for batch_size=1:  75%|███████▍  | 748/1000 [00:58<00:19, 12.74it/s]Measuring inference for batch_size=1:  75%|███████▌  | 750/1000 [00:58<00:19, 12.73it/s]Measuring inference for batch_size=1:  75%|███████▌  | 752/1000 [00:59<00:19, 12.74it/s]Measuring inference for batch_size=1:  75%|███████▌  | 754/1000 [00:59<00:19, 12.74it/s]Measuring inference for batch_size=1:  76%|███████▌  | 756/1000 [00:59<00:19, 12.73it/s]Measuring inference for batch_size=1:  76%|███████▌  | 758/1000 [00:59<00:19, 12.74it/s]Measuring inference for batch_size=1:  76%|███████▌  | 760/1000 [00:59<00:18, 12.74it/s]Measuring inference for batch_size=1:  76%|███████▌  | 762/1000 [00:59<00:18, 12.73it/s]Measuring inference for batch_size=1:  76%|███████▋  | 764/1000 [00:59<00:18, 12.74it/s]Measuring inference for batch_size=1:  77%|███████▋  | 766/1000 [01:00<00:18, 12.74it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [01:00<00:18, 12.73it/s]Measuring inference for batch_size=1:  77%|███████▋  | 770/1000 [01:00<00:18, 12.73it/s]Measuring inference for batch_size=1:  77%|███████▋  | 772/1000 [01:00<00:17, 12.73it/s]Measuring inference for batch_size=1:  77%|███████▋  | 774/1000 [01:00<00:17, 12.74it/s]Measuring inference for batch_size=1:  78%|███████▊  | 776/1000 [01:00<00:17, 12.74it/s]Measuring inference for batch_size=1:  78%|███████▊  | 778/1000 [01:01<00:17, 12.74it/s]Measuring inference for batch_size=1:  78%|███████▊  | 780/1000 [01:01<00:17, 12.74it/s]Measuring inference for batch_size=1:  78%|███████▊  | 782/1000 [01:01<00:17, 12.75it/s]Measuring inference for batch_size=1:  78%|███████▊  | 784/1000 [01:01<00:16, 12.75it/s]Measuring inference for batch_size=1:  79%|███████▊  | 786/1000 [01:01<00:16, 12.74it/s]Measuring inference for batch_size=1:  79%|███████▉  | 788/1000 [01:01<00:16, 12.74it/s]Measuring inference for batch_size=1:  79%|███████▉  | 790/1000 [01:02<00:16, 12.74it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [01:02<00:16, 12.74it/s]Measuring inference for batch_size=1:  79%|███████▉  | 794/1000 [01:02<00:16, 12.74it/s]Measuring inference for batch_size=1:  80%|███████▉  | 796/1000 [01:02<00:16, 12.74it/s]Measuring inference for batch_size=1:  80%|███████▉  | 798/1000 [01:02<00:15, 12.75it/s]Measuring inference for batch_size=1:  80%|████████  | 800/1000 [01:02<00:15, 12.75it/s]Measuring inference for batch_size=1:  80%|████████  | 802/1000 [01:02<00:15, 12.74it/s]Measuring inference for batch_size=1:  80%|████████  | 804/1000 [01:03<00:15, 12.74it/s]Measuring inference for batch_size=1:  81%|████████  | 806/1000 [01:03<00:15, 12.74it/s]Measuring inference for batch_size=1:  81%|████████  | 808/1000 [01:03<00:15, 12.74it/s]Measuring inference for batch_size=1:  81%|████████  | 810/1000 [01:03<00:14, 12.73it/s]Measuring inference for batch_size=1:  81%|████████  | 812/1000 [01:03<00:14, 12.73it/s]Measuring inference for batch_size=1:  81%|████████▏ | 814/1000 [01:03<00:14, 12.73it/s]Measuring inference for batch_size=1:  82%|████████▏ | 816/1000 [01:04<00:14, 12.73it/s]Measuring inference for batch_size=1:  82%|████████▏ | 818/1000 [01:04<00:14, 12.73it/s]Measuring inference for batch_size=1:  82%|████████▏ | 820/1000 [01:04<00:14, 12.73it/s]Measuring inference for batch_size=1:  82%|████████▏ | 822/1000 [01:04<00:13, 12.73it/s]Measuring inference for batch_size=1:  82%|████████▏ | 824/1000 [01:04<00:13, 12.73it/s]Measuring inference for batch_size=1:  83%|████████▎ | 826/1000 [01:04<00:13, 12.73it/s]Measuring inference for batch_size=1:  83%|████████▎ | 828/1000 [01:05<00:13, 12.73it/s]Measuring inference for batch_size=1:  83%|████████▎ | 830/1000 [01:05<00:13, 12.73it/s]Measuring inference for batch_size=1:  83%|████████▎ | 832/1000 [01:05<00:13, 12.73it/s]Measuring inference for batch_size=1:  83%|████████▎ | 834/1000 [01:05<00:13, 12.73it/s]Measuring inference for batch_size=1:  84%|████████▎ | 836/1000 [01:05<00:12, 12.73it/s]Measuring inference for batch_size=1:  84%|████████▍ | 838/1000 [01:05<00:12, 12.74it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [01:05<00:12, 12.74it/s]Measuring inference for batch_size=1:  84%|████████▍ | 842/1000 [01:06<00:12, 12.74it/s]Measuring inference for batch_size=1:  84%|████████▍ | 844/1000 [01:06<00:12, 12.73it/s]Measuring inference for batch_size=1:  85%|████████▍ | 846/1000 [01:06<00:12, 12.73it/s]Measuring inference for batch_size=1:  85%|████████▍ | 848/1000 [01:06<00:11, 12.73it/s]Measuring inference for batch_size=1:  85%|████████▌ | 850/1000 [01:06<00:11, 12.73it/s]Measuring inference for batch_size=1:  85%|████████▌ | 852/1000 [01:06<00:11, 12.74it/s]Measuring inference for batch_size=1:  85%|████████▌ | 854/1000 [01:07<00:11, 12.73it/s]Measuring inference for batch_size=1:  86%|████████▌ | 856/1000 [01:07<00:11, 12.73it/s]Measuring inference for batch_size=1:  86%|████████▌ | 858/1000 [01:07<00:11, 12.74it/s]Measuring inference for batch_size=1:  86%|████████▌ | 860/1000 [01:07<00:10, 12.74it/s]Measuring inference for batch_size=1:  86%|████████▌ | 862/1000 [01:07<00:10, 12.74it/s]Measuring inference for batch_size=1:  86%|████████▋ | 864/1000 [01:07<00:10, 12.73it/s]Measuring inference for batch_size=1:  87%|████████▋ | 866/1000 [01:07<00:10, 12.73it/s]Measuring inference for batch_size=1:  87%|████████▋ | 868/1000 [01:08<00:10, 12.73it/s]Measuring inference for batch_size=1:  87%|████████▋ | 870/1000 [01:08<00:10, 12.73it/s]Measuring inference for batch_size=1:  87%|████████▋ | 872/1000 [01:08<00:10, 12.73it/s]Measuring inference for batch_size=1:  87%|████████▋ | 874/1000 [01:08<00:09, 12.73it/s]Measuring inference for batch_size=1:  88%|████████▊ | 876/1000 [01:08<00:09, 12.73it/s]Measuring inference for batch_size=1:  88%|████████▊ | 878/1000 [01:08<00:09, 12.73it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [01:09<00:09, 12.73it/s]Measuring inference for batch_size=1:  88%|████████▊ | 882/1000 [01:09<00:09, 12.73it/s]Measuring inference for batch_size=1:  88%|████████▊ | 884/1000 [01:09<00:09, 12.73it/s]Measuring inference for batch_size=1:  89%|████████▊ | 886/1000 [01:09<00:08, 12.73it/s]Measuring inference for batch_size=1:  89%|████████▉ | 888/1000 [01:09<00:08, 12.73it/s]Measuring inference for batch_size=1:  89%|████████▉ | 890/1000 [01:09<00:08, 12.73it/s]Measuring inference for batch_size=1:  89%|████████▉ | 892/1000 [01:10<00:08, 12.72it/s]Measuring inference for batch_size=1:  89%|████████▉ | 894/1000 [01:10<00:08, 12.73it/s]Measuring inference for batch_size=1:  90%|████████▉ | 896/1000 [01:10<00:08, 12.73it/s]Measuring inference for batch_size=1:  90%|████████▉ | 898/1000 [01:10<00:08, 12.73it/s]Measuring inference for batch_size=1:  90%|█████████ | 900/1000 [01:10<00:07, 12.73it/s]Measuring inference for batch_size=1:  90%|█████████ | 902/1000 [01:10<00:07, 12.73it/s]Measuring inference for batch_size=1:  90%|█████████ | 904/1000 [01:10<00:07, 12.74it/s]Measuring inference for batch_size=1:  91%|█████████ | 906/1000 [01:11<00:07, 12.73it/s]Measuring inference for batch_size=1:  91%|█████████ | 908/1000 [01:11<00:07, 12.73it/s]Measuring inference for batch_size=1:  91%|█████████ | 910/1000 [01:11<00:07, 12.73it/s]Measuring inference for batch_size=1:  91%|█████████ | 912/1000 [01:11<00:06, 12.73it/s]Measuring inference for batch_size=1:  91%|█████████▏| 914/1000 [01:11<00:06, 12.73it/s]Measuring inference for batch_size=1:  92%|█████████▏| 916/1000 [01:11<00:06, 12.72it/s]Measuring inference for batch_size=1:  92%|█████████▏| 918/1000 [01:12<00:06, 12.73it/s]Measuring inference for batch_size=1:  92%|█████████▏| 920/1000 [01:12<00:06, 12.73it/s]Measuring inference for batch_size=1:  92%|█████████▏| 922/1000 [01:12<00:06, 12.73it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [01:12<00:05, 12.73it/s]Measuring inference for batch_size=1:  93%|█████████▎| 926/1000 [01:12<00:05, 12.73it/s]Measuring inference for batch_size=1:  93%|█████████▎| 928/1000 [01:12<00:05, 12.73it/s]Measuring inference for batch_size=1:  93%|█████████▎| 930/1000 [01:13<00:05, 12.74it/s]Measuring inference for batch_size=1:  93%|█████████▎| 932/1000 [01:13<00:05, 12.74it/s]Measuring inference for batch_size=1:  93%|█████████▎| 934/1000 [01:13<00:05, 12.74it/s]Measuring inference for batch_size=1:  94%|█████████▎| 936/1000 [01:13<00:05, 12.74it/s]Measuring inference for batch_size=1:  94%|█████████▍| 938/1000 [01:13<00:04, 12.74it/s]Measuring inference for batch_size=1:  94%|█████████▍| 940/1000 [01:13<00:04, 12.74it/s]Measuring inference for batch_size=1:  94%|█████████▍| 942/1000 [01:13<00:04, 12.73it/s]Measuring inference for batch_size=1:  94%|█████████▍| 944/1000 [01:14<00:04, 12.73it/s]Measuring inference for batch_size=1:  95%|█████████▍| 946/1000 [01:14<00:04, 12.73it/s]Measuring inference for batch_size=1:  95%|█████████▍| 948/1000 [01:14<00:04, 12.74it/s]Measuring inference for batch_size=1:  95%|█████████▌| 950/1000 [01:14<00:03, 12.74it/s]Measuring inference for batch_size=1:  95%|█████████▌| 952/1000 [01:14<00:03, 12.74it/s]Measuring inference for batch_size=1:  95%|█████████▌| 954/1000 [01:14<00:03, 12.74it/s]Measuring inference for batch_size=1:  96%|█████████▌| 956/1000 [01:15<00:03, 12.74it/s]Measuring inference for batch_size=1:  96%|█████████▌| 958/1000 [01:15<00:03, 12.74it/s]Measuring inference for batch_size=1:  96%|█████████▌| 960/1000 [01:15<00:03, 12.74it/s]Measuring inference for batch_size=1:  96%|█████████▌| 962/1000 [01:15<00:02, 12.74it/s]Measuring inference for batch_size=1:  96%|█████████▋| 964/1000 [01:15<00:02, 12.74it/s]Measuring inference for batch_size=1:  97%|█████████▋| 966/1000 [01:15<00:02, 12.74it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [01:16<00:02, 12.74it/s]Measuring inference for batch_size=1:  97%|█████████▋| 970/1000 [01:16<00:02, 12.74it/s]Measuring inference for batch_size=1:  97%|█████████▋| 972/1000 [01:16<00:02, 12.74it/s]Measuring inference for batch_size=1:  97%|█████████▋| 974/1000 [01:16<00:02, 12.74it/s]Measuring inference for batch_size=1:  98%|█████████▊| 976/1000 [01:16<00:01, 12.74it/s]Measuring inference for batch_size=1:  98%|█████████▊| 978/1000 [01:16<00:01, 12.73it/s]Measuring inference for batch_size=1:  98%|█████████▊| 980/1000 [01:16<00:01, 12.73it/s]Measuring inference for batch_size=1:  98%|█████████▊| 982/1000 [01:17<00:01, 12.73it/s]Measuring inference for batch_size=1:  98%|█████████▊| 984/1000 [01:17<00:01, 12.73it/s]Measuring inference for batch_size=1:  99%|█████████▊| 986/1000 [01:17<00:01, 12.73it/s]Measuring inference for batch_size=1:  99%|█████████▉| 988/1000 [01:17<00:00, 12.73it/s]Measuring inference for batch_size=1:  99%|█████████▉| 990/1000 [01:17<00:00, 12.73it/s]Measuring inference for batch_size=1:  99%|█████████▉| 992/1000 [01:17<00:00, 12.73it/s]Measuring inference for batch_size=1:  99%|█████████▉| 994/1000 [01:18<00:00, 12.74it/s]Measuring inference for batch_size=1: 100%|█████████▉| 996/1000 [01:18<00:00, 12.74it/s]Measuring inference for batch_size=1: 100%|█████████▉| 998/1000 [01:18<00:00, 12.74it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [01:18<00:00, 12.74it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [01:18<00:00, 12.74it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cpu
flops: 11580687848
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.45 GB
    total: 31.28 GB
    used: 3.38 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
params: 60192808
timing:
  batch_size_1:
    on_device_inference:
      human_readable:
        batch_latency: 78.444 ms +/- 81.446 us [78.151 ms, 78.683 ms]
        batches_per_second: 12.75 +/- 0.01 [12.71, 12.80]
      metrics:
        batches_per_second_max: 12.795821676881634
        batches_per_second_mean: 12.74802966908683
        batches_per_second_min: 12.709280374766301
        batches_per_second_std: 0.013235742519700068
        seconds_per_batch_max: 0.07868266105651855
        seconds_per_batch_mean: 0.0784435794353485
        seconds_per_batch_min: 0.07815051078796387
        seconds_per_batch_std: 8.144554858198803e-05


#####
baseline-baseline-py-id - Run 3
2024-02-23 07:00:25
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  9.12it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  60.19 M, 100.000% Params, 11.58 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.016% Params, 118.01 MMac, 1.019% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.000% Params, 1.61 MMac, 0.014% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.007% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.007% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.359% Params, 680.39 MMac, 5.875% MACs, 
    (0): Bottleneck(
      75.01 k, 0.125% Params, 236.43 MMac, 2.042% MACs, 
      (conv1): Conv2d(4.1 k, 0.007% Params, 12.85 MMac, 0.111% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.028% Params, 52.99 MMac, 0.458% MACs, 
        (0): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    2.34 M, 3.887% Params, 1.92 GMac, 16.555% MACs, 
    (0): Bottleneck(
      379.39 k, 0.630% Params, 376.02 MMac, 3.247% MACs, 
      (conv1): Conv2d(32.77 k, 0.054% Params, 102.76 MMac, 0.887% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 802.82 KMac, 0.007% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.008% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.219% Params, 103.56 MMac, 0.894% MACs, 
        (0): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (4): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (5): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (6): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (7): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    40.61 M, 67.473% Params, 8.05 GMac, 69.501% MACs, 
    (0): Bottleneck(
      1.51 M, 2.513% Params, 374.26 MMac, 3.232% MACs, 
      (conv1): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 401.41 KMac, 0.003% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.004% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 0.874% Params, 103.16 MMac, 0.891% MACs, 
        (0): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (6): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (7): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (8): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (9): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (10): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (11): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (12): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (13): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (14): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (15): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (16): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (17): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (18): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (19): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (20): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (21): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (22): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (23): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (24): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (25): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (26): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (27): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (28): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (29): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (30): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (31): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (32): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (33): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (34): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (35): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 24.861% Params, 811.02 MMac, 7.003% MACs, 
    (0): Bottleneck(
      6.04 M, 10.034% Params, 373.38 MMac, 3.224% MACs, 
      (conv1): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 200.7 KMac, 0.002% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.002% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 3.491% Params, 102.96 MMac, 0.889% MACs, 
        (0): Conv2d(2.1 M, 3.484% Params, 102.76 MMac, 0.887% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.001% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 3.404% Params, 2.05 MMac, 0.018% MACs, in_features=2048, out_features=1000, bias=True)
)Measurement of allocated memory is only available on CUDA devices

Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:   2%|▏         | 2/100 [00:00<00:07, 12.83it/s]Warming up with batch_size=1:   4%|▍         | 4/100 [00:00<00:07, 12.82it/s]Warming up with batch_size=1:   6%|▌         | 6/100 [00:00<00:07, 12.80it/s]Warming up with batch_size=1:   8%|▊         | 8/100 [00:00<00:07, 12.81it/s]Warming up with batch_size=1:  10%|█         | 10/100 [00:00<00:07, 12.80it/s]Warming up with batch_size=1:  12%|█▏        | 12/100 [00:00<00:06, 12.80it/s]Warming up with batch_size=1:  14%|█▍        | 14/100 [00:01<00:06, 12.80it/s]Warming up with batch_size=1:  16%|█▌        | 16/100 [00:01<00:06, 12.80it/s]Warming up with batch_size=1:  18%|█▊        | 18/100 [00:01<00:06, 12.80it/s]Warming up with batch_size=1:  20%|██        | 20/100 [00:01<00:06, 12.79it/s]Warming up with batch_size=1:  22%|██▏       | 22/100 [00:01<00:06, 12.79it/s]Warming up with batch_size=1:  24%|██▍       | 24/100 [00:01<00:05, 12.80it/s]Warming up with batch_size=1:  26%|██▌       | 26/100 [00:02<00:05, 12.80it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:02<00:05, 12.80it/s]Warming up with batch_size=1:  30%|███       | 30/100 [00:02<00:05, 12.79it/s]Warming up with batch_size=1:  32%|███▏      | 32/100 [00:02<00:05, 12.80it/s]Warming up with batch_size=1:  34%|███▍      | 34/100 [00:02<00:05, 12.79it/s]Warming up with batch_size=1:  36%|███▌      | 36/100 [00:02<00:05, 12.79it/s]Warming up with batch_size=1:  38%|███▊      | 38/100 [00:02<00:04, 12.79it/s]Warming up with batch_size=1:  40%|████      | 40/100 [00:03<00:04, 12.79it/s]Warming up with batch_size=1:  42%|████▏     | 42/100 [00:03<00:04, 12.79it/s]Warming up with batch_size=1:  44%|████▍     | 44/100 [00:03<00:04, 12.79it/s]Warming up with batch_size=1:  46%|████▌     | 46/100 [00:03<00:04, 12.79it/s]Warming up with batch_size=1:  48%|████▊     | 48/100 [00:03<00:04, 12.79it/s]Warming up with batch_size=1:  50%|█████     | 50/100 [00:03<00:03, 12.79it/s]Warming up with batch_size=1:  52%|█████▏    | 52/100 [00:04<00:03, 12.79it/s]Warming up with batch_size=1:  54%|█████▍    | 54/100 [00:04<00:03, 12.79it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:04<00:03, 12.79it/s]Warming up with batch_size=1:  58%|█████▊    | 58/100 [00:04<00:03, 12.79it/s]Warming up with batch_size=1:  60%|██████    | 60/100 [00:04<00:03, 12.79it/s]Warming up with batch_size=1:  62%|██████▏   | 62/100 [00:04<00:02, 12.80it/s]Warming up with batch_size=1:  64%|██████▍   | 64/100 [00:05<00:02, 12.80it/s]Warming up with batch_size=1:  66%|██████▌   | 66/100 [00:05<00:02, 12.79it/s]Warming up with batch_size=1:  68%|██████▊   | 68/100 [00:05<00:02, 12.79it/s]Warming up with batch_size=1:  70%|███████   | 70/100 [00:05<00:02, 12.79it/s]Warming up with batch_size=1:  72%|███████▏  | 72/100 [00:05<00:02, 12.79it/s]Warming up with batch_size=1:  74%|███████▍  | 74/100 [00:05<00:02, 12.79it/s]Warming up with batch_size=1:  76%|███████▌  | 76/100 [00:05<00:01, 12.79it/s]Warming up with batch_size=1:  78%|███████▊  | 78/100 [00:06<00:01, 12.79it/s]Warming up with batch_size=1:  80%|████████  | 80/100 [00:06<00:01, 12.79it/s]Warming up with batch_size=1:  82%|████████▏ | 82/100 [00:06<00:01, 12.79it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:06<00:01, 12.80it/s]Warming up with batch_size=1:  86%|████████▌ | 86/100 [00:06<00:01, 12.80it/s]Warming up with batch_size=1:  88%|████████▊ | 88/100 [00:06<00:00, 12.80it/s]Warming up with batch_size=1:  90%|█████████ | 90/100 [00:07<00:00, 12.80it/s]Warming up with batch_size=1:  92%|█████████▏| 92/100 [00:07<00:00, 12.80it/s]Warming up with batch_size=1:  94%|█████████▍| 94/100 [00:07<00:00, 12.80it/s]Warming up with batch_size=1:  96%|█████████▌| 96/100 [00:07<00:00, 12.80it/s]Warming up with batch_size=1:  98%|█████████▊| 98/100 [00:07<00:00, 12.80it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:07<00:00, 12.80it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:07<00:00, 12.79it/s]
STAGE:2024-02-23 06:59:05 173447:173447 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 06:59:05 173447:173447 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 06:59:05 173447:173447 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   0%|          | 2/1000 [00:00<01:19, 12.62it/s]Measuring inference for batch_size=1:   0%|          | 4/1000 [00:00<01:18, 12.65it/s]Measuring inference for batch_size=1:   1%|          | 6/1000 [00:00<01:18, 12.65it/s]Measuring inference for batch_size=1:   1%|          | 8/1000 [00:00<01:18, 12.66it/s]Measuring inference for batch_size=1:   1%|          | 10/1000 [00:00<01:18, 12.66it/s]Measuring inference for batch_size=1:   1%|          | 12/1000 [00:00<01:18, 12.66it/s]Measuring inference for batch_size=1:   1%|▏         | 14/1000 [00:01<01:17, 12.66it/s]Measuring inference for batch_size=1:   2%|▏         | 16/1000 [00:01<01:17, 12.66it/s]Measuring inference for batch_size=1:   2%|▏         | 18/1000 [00:01<01:17, 12.66it/s]Measuring inference for batch_size=1:   2%|▏         | 20/1000 [00:01<01:17, 12.66it/s]Measuring inference for batch_size=1:   2%|▏         | 22/1000 [00:01<01:17, 12.66it/s]Measuring inference for batch_size=1:   2%|▏         | 24/1000 [00:01<01:17, 12.67it/s]Measuring inference for batch_size=1:   3%|▎         | 26/1000 [00:02<01:16, 12.66it/s]Measuring inference for batch_size=1:   3%|▎         | 28/1000 [00:02<01:16, 12.66it/s]Measuring inference for batch_size=1:   3%|▎         | 30/1000 [00:02<01:16, 12.66it/s]Measuring inference for batch_size=1:   3%|▎         | 32/1000 [00:02<01:16, 12.66it/s]Measuring inference for batch_size=1:   3%|▎         | 34/1000 [00:02<01:16, 12.65it/s]Measuring inference for batch_size=1:   4%|▎         | 36/1000 [00:02<01:16, 12.65it/s]Measuring inference for batch_size=1:   4%|▍         | 38/1000 [00:03<01:16, 12.65it/s]Measuring inference for batch_size=1:   4%|▍         | 40/1000 [00:03<01:15, 12.65it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:03<01:15, 12.66it/s]Measuring inference for batch_size=1:   4%|▍         | 44/1000 [00:03<01:15, 12.66it/s]Measuring inference for batch_size=1:   5%|▍         | 46/1000 [00:03<01:15, 12.66it/s]Measuring inference for batch_size=1:   5%|▍         | 48/1000 [00:03<01:15, 12.66it/s]Measuring inference for batch_size=1:   5%|▌         | 50/1000 [00:03<01:15, 12.66it/s]Measuring inference for batch_size=1:   5%|▌         | 52/1000 [00:04<01:14, 12.66it/s]Measuring inference for batch_size=1:   5%|▌         | 54/1000 [00:04<01:14, 12.66it/s]Measuring inference for batch_size=1:   6%|▌         | 56/1000 [00:04<01:14, 12.66it/s]Measuring inference for batch_size=1:   6%|▌         | 58/1000 [00:04<01:14, 12.66it/s]Measuring inference for batch_size=1:   6%|▌         | 60/1000 [00:04<01:14, 12.66it/s]Measuring inference for batch_size=1:   6%|▌         | 62/1000 [00:04<01:14, 12.66it/s]Measuring inference for batch_size=1:   6%|▋         | 64/1000 [00:05<01:13, 12.66it/s]Measuring inference for batch_size=1:   7%|▋         | 66/1000 [00:05<01:13, 12.66it/s]Measuring inference for batch_size=1:   7%|▋         | 68/1000 [00:05<01:13, 12.66it/s]Measuring inference for batch_size=1:   7%|▋         | 70/1000 [00:05<01:13, 12.65it/s]Measuring inference for batch_size=1:   7%|▋         | 72/1000 [00:05<01:13, 12.65it/s]Measuring inference for batch_size=1:   7%|▋         | 74/1000 [00:05<01:13, 12.66it/s]Measuring inference for batch_size=1:   8%|▊         | 76/1000 [00:06<01:13, 12.66it/s]Measuring inference for batch_size=1:   8%|▊         | 78/1000 [00:06<01:12, 12.66it/s]Measuring inference for batch_size=1:   8%|▊         | 80/1000 [00:06<01:12, 12.66it/s]Measuring inference for batch_size=1:   8%|▊         | 82/1000 [00:06<01:12, 12.66it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:06<01:12, 12.66it/s]Measuring inference for batch_size=1:   9%|▊         | 86/1000 [00:06<01:12, 12.66it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:06<01:12, 12.66it/s]Measuring inference for batch_size=1:   9%|▉         | 90/1000 [00:07<01:11, 12.66it/s]Measuring inference for batch_size=1:   9%|▉         | 92/1000 [00:07<01:11, 12.66it/s]Measuring inference for batch_size=1:   9%|▉         | 94/1000 [00:07<01:11, 12.66it/s]Measuring inference for batch_size=1:  10%|▉         | 96/1000 [00:07<01:11, 12.66it/s]Measuring inference for batch_size=1:  10%|▉         | 98/1000 [00:07<01:11, 12.66it/s]Measuring inference for batch_size=1:  10%|█         | 100/1000 [00:07<01:11, 12.66it/s]Measuring inference for batch_size=1:  10%|█         | 102/1000 [00:08<01:10, 12.66it/s]Measuring inference for batch_size=1:  10%|█         | 104/1000 [00:08<01:10, 12.67it/s]Measuring inference for batch_size=1:  11%|█         | 106/1000 [00:08<01:10, 12.67it/s]Measuring inference for batch_size=1:  11%|█         | 108/1000 [00:08<01:10, 12.67it/s]Measuring inference for batch_size=1:  11%|█         | 110/1000 [00:08<01:10, 12.67it/s]Measuring inference for batch_size=1:  11%|█         | 112/1000 [00:08<01:10, 12.67it/s]Measuring inference for batch_size=1:  11%|█▏        | 114/1000 [00:09<01:09, 12.67it/s]Measuring inference for batch_size=1:  12%|█▏        | 116/1000 [00:09<01:09, 12.67it/s]Measuring inference for batch_size=1:  12%|█▏        | 118/1000 [00:09<01:09, 12.67it/s]Measuring inference for batch_size=1:  12%|█▏        | 120/1000 [00:09<01:09, 12.66it/s]Measuring inference for batch_size=1:  12%|█▏        | 122/1000 [00:09<01:09, 12.67it/s]Measuring inference for batch_size=1:  12%|█▏        | 124/1000 [00:09<01:09, 12.66it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:09<01:09, 12.66it/s]Measuring inference for batch_size=1:  13%|█▎        | 128/1000 [00:10<01:08, 12.66it/s]Measuring inference for batch_size=1:  13%|█▎        | 130/1000 [00:10<01:08, 12.67it/s]Measuring inference for batch_size=1:  13%|█▎        | 132/1000 [00:10<01:08, 12.67it/s]Measuring inference for batch_size=1:  13%|█▎        | 134/1000 [00:10<01:08, 12.67it/s]Measuring inference for batch_size=1:  14%|█▎        | 136/1000 [00:10<01:08, 12.66it/s]Measuring inference for batch_size=1:  14%|█▍        | 138/1000 [00:10<01:08, 12.67it/s]Measuring inference for batch_size=1:  14%|█▍        | 140/1000 [00:11<01:07, 12.67it/s]Measuring inference for batch_size=1:  14%|█▍        | 142/1000 [00:11<01:07, 12.68it/s]Measuring inference for batch_size=1:  14%|█▍        | 144/1000 [00:11<01:07, 12.68it/s]Measuring inference for batch_size=1:  15%|█▍        | 146/1000 [00:11<01:07, 12.68it/s]Measuring inference for batch_size=1:  15%|█▍        | 148/1000 [00:11<01:07, 12.68it/s]Measuring inference for batch_size=1:  15%|█▌        | 150/1000 [00:11<01:07, 12.67it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:12<01:06, 12.66it/s]Measuring inference for batch_size=1:  15%|█▌        | 154/1000 [00:12<01:06, 12.67it/s]Measuring inference for batch_size=1:  16%|█▌        | 156/1000 [00:12<01:06, 12.66it/s]Measuring inference for batch_size=1:  16%|█▌        | 158/1000 [00:12<01:06, 12.66it/s]Measuring inference for batch_size=1:  16%|█▌        | 160/1000 [00:12<01:06, 12.66it/s]Measuring inference for batch_size=1:  16%|█▌        | 162/1000 [00:12<01:06, 12.66it/s]Measuring inference for batch_size=1:  16%|█▋        | 164/1000 [00:12<01:06, 12.66it/s]Measuring inference for batch_size=1:  17%|█▋        | 166/1000 [00:13<01:05, 12.66it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:13<01:05, 12.67it/s]Measuring inference for batch_size=1:  17%|█▋        | 170/1000 [00:13<01:05, 12.66it/s]Measuring inference for batch_size=1:  17%|█▋        | 172/1000 [00:13<01:05, 12.67it/s]Measuring inference for batch_size=1:  17%|█▋        | 174/1000 [00:13<01:05, 12.67it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:13<01:05, 12.67it/s]Measuring inference for batch_size=1:  18%|█▊        | 178/1000 [00:14<01:04, 12.66it/s]Measuring inference for batch_size=1:  18%|█▊        | 180/1000 [00:14<01:04, 12.66it/s]Measuring inference for batch_size=1:  18%|█▊        | 182/1000 [00:14<01:04, 12.67it/s]Measuring inference for batch_size=1:  18%|█▊        | 184/1000 [00:14<01:04, 12.67it/s]Measuring inference for batch_size=1:  19%|█▊        | 186/1000 [00:14<01:04, 12.66it/s]Measuring inference for batch_size=1:  19%|█▉        | 188/1000 [00:14<01:04, 12.67it/s]Measuring inference for batch_size=1:  19%|█▉        | 190/1000 [00:15<01:03, 12.67it/s]Measuring inference for batch_size=1:  19%|█▉        | 192/1000 [00:15<01:03, 12.67it/s]Measuring inference for batch_size=1:  19%|█▉        | 194/1000 [00:15<01:03, 12.67it/s]Measuring inference for batch_size=1:  20%|█▉        | 196/1000 [00:15<01:03, 12.67it/s]Measuring inference for batch_size=1:  20%|█▉        | 198/1000 [00:15<01:03, 12.67it/s]Measuring inference for batch_size=1:  20%|██        | 200/1000 [00:15<01:03, 12.66it/s]Measuring inference for batch_size=1:  20%|██        | 202/1000 [00:15<01:02, 12.67it/s]Measuring inference for batch_size=1:  20%|██        | 204/1000 [00:16<01:02, 12.67it/s]Measuring inference for batch_size=1:  21%|██        | 206/1000 [00:16<01:02, 12.66it/s]Measuring inference for batch_size=1:  21%|██        | 208/1000 [00:16<01:02, 12.66it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:16<01:02, 12.66it/s]Measuring inference for batch_size=1:  21%|██        | 212/1000 [00:16<01:02, 12.66it/s]Measuring inference for batch_size=1:  21%|██▏       | 214/1000 [00:16<01:02, 12.66it/s]Measuring inference for batch_size=1:  22%|██▏       | 216/1000 [00:17<01:01, 12.66it/s]Measuring inference for batch_size=1:  22%|██▏       | 218/1000 [00:17<01:01, 12.66it/s]Measuring inference for batch_size=1:  22%|██▏       | 220/1000 [00:17<01:01, 12.66it/s]Measuring inference for batch_size=1:  22%|██▏       | 222/1000 [00:17<01:01, 12.66it/s]Measuring inference for batch_size=1:  22%|██▏       | 224/1000 [00:17<01:01, 12.66it/s]Measuring inference for batch_size=1:  23%|██▎       | 226/1000 [00:17<01:01, 12.66it/s]Measuring inference for batch_size=1:  23%|██▎       | 228/1000 [00:18<01:00, 12.66it/s]Measuring inference for batch_size=1:  23%|██▎       | 230/1000 [00:18<01:00, 12.67it/s]Measuring inference for batch_size=1:  23%|██▎       | 232/1000 [00:18<01:00, 12.66it/s]Measuring inference for batch_size=1:  23%|██▎       | 234/1000 [00:18<01:00, 12.66it/s]Measuring inference for batch_size=1:  24%|██▎       | 236/1000 [00:18<01:00, 12.66it/s]Measuring inference for batch_size=1:  24%|██▍       | 238/1000 [00:18<01:00, 12.66it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:18<01:00, 12.67it/s]Measuring inference for batch_size=1:  24%|██▍       | 242/1000 [00:19<00:59, 12.67it/s]Measuring inference for batch_size=1:  24%|██▍       | 244/1000 [00:19<00:59, 12.67it/s]Measuring inference for batch_size=1:  25%|██▍       | 246/1000 [00:19<00:59, 12.67it/s]Measuring inference for batch_size=1:  25%|██▍       | 248/1000 [00:19<00:59, 12.67it/s]Measuring inference for batch_size=1:  25%|██▌       | 250/1000 [00:19<00:59, 12.66it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:19<00:59, 12.67it/s]Measuring inference for batch_size=1:  25%|██▌       | 254/1000 [00:20<00:58, 12.66it/s]Measuring inference for batch_size=1:  26%|██▌       | 256/1000 [00:20<00:58, 12.66it/s]Measuring inference for batch_size=1:  26%|██▌       | 258/1000 [00:20<00:58, 12.67it/s]Measuring inference for batch_size=1:  26%|██▌       | 260/1000 [00:20<00:58, 12.67it/s]Measuring inference for batch_size=1:  26%|██▌       | 262/1000 [00:20<00:58, 12.66it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:20<00:58, 12.66it/s]Measuring inference for batch_size=1:  27%|██▋       | 266/1000 [00:21<00:57, 12.66it/s]Measuring inference for batch_size=1:  27%|██▋       | 268/1000 [00:21<00:57, 12.66it/s]Measuring inference for batch_size=1:  27%|██▋       | 270/1000 [00:21<00:57, 12.66it/s]Measuring inference for batch_size=1:  27%|██▋       | 272/1000 [00:21<00:57, 12.66it/s]Measuring inference for batch_size=1:  27%|██▋       | 274/1000 [00:21<00:57, 12.66it/s]Measuring inference for batch_size=1:  28%|██▊       | 276/1000 [00:21<00:57, 12.66it/s]Measuring inference for batch_size=1:  28%|██▊       | 278/1000 [00:21<00:57, 12.66it/s]Measuring inference for batch_size=1:  28%|██▊       | 280/1000 [00:22<00:56, 12.66it/s]Measuring inference for batch_size=1:  28%|██▊       | 282/1000 [00:22<00:56, 12.66it/s]Measuring inference for batch_size=1:  28%|██▊       | 284/1000 [00:22<00:56, 12.66it/s]Measuring inference for batch_size=1:  29%|██▊       | 286/1000 [00:22<00:56, 12.65it/s]Measuring inference for batch_size=1:  29%|██▉       | 288/1000 [00:22<00:56, 12.65it/s]Measuring inference for batch_size=1:  29%|██▉       | 290/1000 [00:22<00:56, 12.66it/s]Measuring inference for batch_size=1:  29%|██▉       | 292/1000 [00:23<00:55, 12.66it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:23<00:55, 12.67it/s]Measuring inference for batch_size=1:  30%|██▉       | 296/1000 [00:23<00:55, 12.67it/s]Measuring inference for batch_size=1:  30%|██▉       | 298/1000 [00:23<00:55, 12.67it/s]Measuring inference for batch_size=1:  30%|███       | 300/1000 [00:23<00:55, 12.66it/s]Measuring inference for batch_size=1:  30%|███       | 302/1000 [00:23<00:55, 12.66it/s]Measuring inference for batch_size=1:  30%|███       | 304/1000 [00:24<00:54, 12.66it/s]Measuring inference for batch_size=1:  31%|███       | 306/1000 [00:24<00:54, 12.66it/s]Measuring inference for batch_size=1:  31%|███       | 308/1000 [00:24<00:54, 12.66it/s]Measuring inference for batch_size=1:  31%|███       | 310/1000 [00:24<00:54, 12.66it/s]Measuring inference for batch_size=1:  31%|███       | 312/1000 [00:24<00:54, 12.66it/s]Measuring inference for batch_size=1:  31%|███▏      | 314/1000 [00:24<00:54, 12.66it/s]Measuring inference for batch_size=1:  32%|███▏      | 316/1000 [00:24<00:54, 12.66it/s]Measuring inference for batch_size=1:  32%|███▏      | 318/1000 [00:25<00:53, 12.66it/s]Measuring inference for batch_size=1:  32%|███▏      | 320/1000 [00:25<00:53, 12.66it/s]Measuring inference for batch_size=1:  32%|███▏      | 322/1000 [00:25<00:53, 12.67it/s]Measuring inference for batch_size=1:  32%|███▏      | 324/1000 [00:25<00:53, 12.67it/s]Measuring inference for batch_size=1:  33%|███▎      | 326/1000 [00:25<00:53, 12.67it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:25<00:53, 12.67it/s]Measuring inference for batch_size=1:  33%|███▎      | 330/1000 [00:26<00:52, 12.67it/s]Measuring inference for batch_size=1:  33%|███▎      | 332/1000 [00:26<00:52, 12.66it/s]Measuring inference for batch_size=1:  33%|███▎      | 334/1000 [00:26<00:52, 12.67it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:26<00:52, 12.67it/s]Measuring inference for batch_size=1:  34%|███▍      | 338/1000 [00:26<00:52, 12.67it/s]Measuring inference for batch_size=1:  34%|███▍      | 340/1000 [00:26<00:52, 12.67it/s]Measuring inference for batch_size=1:  34%|███▍      | 342/1000 [00:27<00:51, 12.67it/s]Measuring inference for batch_size=1:  34%|███▍      | 344/1000 [00:27<00:51, 12.66it/s]Measuring inference for batch_size=1:  35%|███▍      | 346/1000 [00:27<00:51, 12.66it/s]Measuring inference for batch_size=1:  35%|███▍      | 348/1000 [00:27<00:51, 12.66it/s]Measuring inference for batch_size=1:  35%|███▌      | 350/1000 [00:27<00:51, 12.66it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:27<00:51, 12.66it/s]Measuring inference for batch_size=1:  35%|███▌      | 354/1000 [00:27<00:51, 12.66it/s]Measuring inference for batch_size=1:  36%|███▌      | 356/1000 [00:28<00:50, 12.66it/s]Measuring inference for batch_size=1:  36%|███▌      | 358/1000 [00:28<00:50, 12.66it/s]Measuring inference for batch_size=1:  36%|███▌      | 360/1000 [00:28<00:50, 12.66it/s]Measuring inference for batch_size=1:  36%|███▌      | 362/1000 [00:28<00:50, 12.66it/s]Measuring inference for batch_size=1:  36%|███▋      | 364/1000 [00:28<00:50, 12.66it/s]Measuring inference for batch_size=1:  37%|███▋      | 366/1000 [00:28<00:50, 12.67it/s]Measuring inference for batch_size=1:  37%|███▋      | 368/1000 [00:29<00:49, 12.66it/s]Measuring inference for batch_size=1:  37%|███▋      | 370/1000 [00:29<00:49, 12.68it/s]Measuring inference for batch_size=1:  37%|███▋      | 372/1000 [00:29<00:49, 12.67it/s]Measuring inference for batch_size=1:  37%|███▋      | 374/1000 [00:29<00:49, 12.67it/s]Measuring inference for batch_size=1:  38%|███▊      | 376/1000 [00:29<00:49, 12.67it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:29<00:49, 12.67it/s]Measuring inference for batch_size=1:  38%|███▊      | 380/1000 [00:30<00:48, 12.67it/s]Measuring inference for batch_size=1:  38%|███▊      | 382/1000 [00:30<00:48, 12.66it/s]Measuring inference for batch_size=1:  38%|███▊      | 384/1000 [00:30<00:48, 12.67it/s]Measuring inference for batch_size=1:  39%|███▊      | 386/1000 [00:30<00:48, 12.67it/s]Measuring inference for batch_size=1:  39%|███▉      | 388/1000 [00:30<00:48, 12.66it/s]Measuring inference for batch_size=1:  39%|███▉      | 390/1000 [00:30<00:48, 12.66it/s]Measuring inference for batch_size=1:  39%|███▉      | 392/1000 [00:30<00:48, 12.66it/s]Measuring inference for batch_size=1:  39%|███▉      | 394/1000 [00:31<00:47, 12.67it/s]Measuring inference for batch_size=1:  40%|███▉      | 396/1000 [00:31<00:47, 12.67it/s]Measuring inference for batch_size=1:  40%|███▉      | 398/1000 [00:31<00:47, 12.67it/s]Measuring inference for batch_size=1:  40%|████      | 400/1000 [00:31<00:47, 12.67it/s]Measuring inference for batch_size=1:  40%|████      | 402/1000 [00:31<00:47, 12.66it/s]Measuring inference for batch_size=1:  40%|████      | 404/1000 [00:31<00:47, 12.66it/s]Measuring inference for batch_size=1:  41%|████      | 406/1000 [00:32<00:46, 12.66it/s]Measuring inference for batch_size=1:  41%|████      | 408/1000 [00:32<00:46, 12.67it/s]Measuring inference for batch_size=1:  41%|████      | 410/1000 [00:32<00:46, 12.66it/s]Measuring inference for batch_size=1:  41%|████      | 412/1000 [00:32<00:46, 12.66it/s]Measuring inference for batch_size=1:  41%|████▏     | 414/1000 [00:32<00:46, 12.66it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:32<00:46, 12.66it/s]Measuring inference for batch_size=1:  42%|████▏     | 418/1000 [00:33<00:45, 12.66it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:33<00:45, 12.66it/s]Measuring inference for batch_size=1:  42%|████▏     | 422/1000 [00:33<00:45, 12.67it/s]Measuring inference for batch_size=1:  42%|████▏     | 424/1000 [00:33<00:45, 12.66it/s]Measuring inference for batch_size=1:  43%|████▎     | 426/1000 [00:33<00:45, 12.66it/s]Measuring inference for batch_size=1:  43%|████▎     | 428/1000 [00:33<00:45, 12.66it/s]Measuring inference for batch_size=1:  43%|████▎     | 430/1000 [00:33<00:45, 12.66it/s]Measuring inference for batch_size=1:  43%|████▎     | 432/1000 [00:34<00:44, 12.66it/s]Measuring inference for batch_size=1:  43%|████▎     | 434/1000 [00:34<00:44, 12.66it/s]Measuring inference for batch_size=1:  44%|████▎     | 436/1000 [00:34<00:44, 12.66it/s]Measuring inference for batch_size=1:  44%|████▍     | 438/1000 [00:34<00:44, 12.66it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:34<00:44, 12.66it/s]Measuring inference for batch_size=1:  44%|████▍     | 442/1000 [00:34<00:44, 12.67it/s]Measuring inference for batch_size=1:  44%|████▍     | 444/1000 [00:35<00:43, 12.66it/s]Measuring inference for batch_size=1:  45%|████▍     | 446/1000 [00:35<00:43, 12.66it/s]Measuring inference for batch_size=1:  45%|████▍     | 448/1000 [00:35<00:43, 12.66it/s]Measuring inference for batch_size=1:  45%|████▌     | 450/1000 [00:35<00:43, 12.66it/s]Measuring inference for batch_size=1:  45%|████▌     | 452/1000 [00:35<00:43, 12.66it/s]Measuring inference for batch_size=1:  45%|████▌     | 454/1000 [00:35<00:43, 12.65it/s]Measuring inference for batch_size=1:  46%|████▌     | 456/1000 [00:36<00:42, 12.65it/s]Measuring inference for batch_size=1:  46%|████▌     | 458/1000 [00:36<00:42, 12.66it/s]Measuring inference for batch_size=1:  46%|████▌     | 460/1000 [00:36<00:42, 12.65it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:36<00:42, 12.65it/s]Measuring inference for batch_size=1:  46%|████▋     | 464/1000 [00:36<00:42, 12.65it/s]Measuring inference for batch_size=1:  47%|████▋     | 466/1000 [00:36<00:42, 12.66it/s]Measuring inference for batch_size=1:  47%|████▋     | 468/1000 [00:36<00:42, 12.66it/s]Measuring inference for batch_size=1:  47%|████▋     | 470/1000 [00:37<00:41, 12.65it/s]Measuring inference for batch_size=1:  47%|████▋     | 472/1000 [00:37<00:41, 12.66it/s]Measuring inference for batch_size=1:  47%|████▋     | 474/1000 [00:37<00:41, 12.66it/s]Measuring inference for batch_size=1:  48%|████▊     | 476/1000 [00:37<00:41, 12.62it/s]Measuring inference for batch_size=1:  48%|████▊     | 478/1000 [00:37<00:41, 12.47it/s]Measuring inference for batch_size=1:  48%|████▊     | 480/1000 [00:37<00:41, 12.53it/s]Measuring inference for batch_size=1:  48%|████▊     | 482/1000 [00:38<00:41, 12.52it/s]Measuring inference for batch_size=1:  48%|████▊     | 484/1000 [00:38<00:41, 12.55it/s]Measuring inference for batch_size=1:  49%|████▊     | 486/1000 [00:38<00:40, 12.59it/s]Measuring inference for batch_size=1:  49%|████▉     | 488/1000 [00:38<00:40, 12.61it/s]Measuring inference for batch_size=1:  49%|████▉     | 490/1000 [00:38<00:40, 12.62it/s]Measuring inference for batch_size=1:  49%|████▉     | 492/1000 [00:38<00:40, 12.64it/s]Measuring inference for batch_size=1:  49%|████▉     | 494/1000 [00:39<00:40, 12.65it/s]Measuring inference for batch_size=1:  50%|████▉     | 496/1000 [00:39<00:39, 12.65it/s]Measuring inference for batch_size=1:  50%|████▉     | 498/1000 [00:39<00:39, 12.65it/s]Measuring inference for batch_size=1:  50%|█████     | 500/1000 [00:39<00:39, 12.65it/s]Measuring inference for batch_size=1:  50%|█████     | 502/1000 [00:39<00:39, 12.66it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:39<00:39, 12.66it/s]Measuring inference for batch_size=1:  51%|█████     | 506/1000 [00:39<00:39, 12.66it/s]Measuring inference for batch_size=1:  51%|█████     | 508/1000 [00:40<00:38, 12.66it/s]Measuring inference for batch_size=1:  51%|█████     | 510/1000 [00:40<00:38, 12.66it/s]Measuring inference for batch_size=1:  51%|█████     | 512/1000 [00:40<00:38, 12.66it/s]Measuring inference for batch_size=1:  51%|█████▏    | 514/1000 [00:40<00:38, 12.66it/s]Measuring inference for batch_size=1:  52%|█████▏    | 516/1000 [00:40<00:38, 12.66it/s]Measuring inference for batch_size=1:  52%|█████▏    | 518/1000 [00:40<00:38, 12.66it/s]Measuring inference for batch_size=1:  52%|█████▏    | 520/1000 [00:41<00:37, 12.66it/s]Measuring inference for batch_size=1:  52%|█████▏    | 522/1000 [00:41<00:37, 12.66it/s]Measuring inference for batch_size=1:  52%|█████▏    | 524/1000 [00:41<00:37, 12.66it/s]Measuring inference for batch_size=1:  53%|█████▎    | 526/1000 [00:41<00:37, 12.66it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:41<00:37, 12.66it/s]Measuring inference for batch_size=1:  53%|█████▎    | 530/1000 [00:41<00:37, 12.66it/s]Measuring inference for batch_size=1:  53%|█████▎    | 532/1000 [00:42<00:36, 12.66it/s]Measuring inference for batch_size=1:  53%|█████▎    | 534/1000 [00:42<00:36, 12.66it/s]Measuring inference for batch_size=1:  54%|█████▎    | 536/1000 [00:42<00:36, 12.66it/s]Measuring inference for batch_size=1:  54%|█████▍    | 538/1000 [00:42<00:36, 12.66it/s]Measuring inference for batch_size=1:  54%|█████▍    | 540/1000 [00:42<00:36, 12.66it/s]Measuring inference for batch_size=1:  54%|█████▍    | 542/1000 [00:42<00:36, 12.66it/s]Measuring inference for batch_size=1:  54%|█████▍    | 544/1000 [00:42<00:36, 12.67it/s]Measuring inference for batch_size=1:  55%|█████▍    | 546/1000 [00:43<00:35, 12.66it/s]Measuring inference for batch_size=1:  55%|█████▍    | 548/1000 [00:43<00:35, 12.66it/s]Measuring inference for batch_size=1:  55%|█████▌    | 550/1000 [00:43<00:35, 12.66it/s]Measuring inference for batch_size=1:  55%|█████▌    | 552/1000 [00:43<00:35, 12.66it/s]Measuring inference for batch_size=1:  55%|█████▌    | 554/1000 [00:43<00:35, 12.66it/s]Measuring inference for batch_size=1:  56%|█████▌    | 556/1000 [00:43<00:35, 12.66it/s]Measuring inference for batch_size=1:  56%|█████▌    | 558/1000 [00:44<00:34, 12.66it/s]Measuring inference for batch_size=1:  56%|█████▌    | 560/1000 [00:44<00:34, 12.66it/s]Measuring inference for batch_size=1:  56%|█████▌    | 562/1000 [00:44<00:34, 12.66it/s]Measuring inference for batch_size=1:  56%|█████▋    | 564/1000 [00:44<00:34, 12.66it/s]Measuring inference for batch_size=1:  57%|█████▋    | 566/1000 [00:44<00:34, 12.66it/s]Measuring inference for batch_size=1:  57%|█████▋    | 568/1000 [00:44<00:34, 12.66it/s]Measuring inference for batch_size=1:  57%|█████▋    | 570/1000 [00:45<00:33, 12.66it/s]Measuring inference for batch_size=1:  57%|█████▋    | 572/1000 [00:45<00:33, 12.66it/s]Measuring inference for batch_size=1:  57%|█████▋    | 574/1000 [00:45<00:33, 12.65it/s]Measuring inference for batch_size=1:  58%|█████▊    | 576/1000 [00:45<00:33, 12.65it/s]Measuring inference for batch_size=1:  58%|█████▊    | 578/1000 [00:45<00:33, 12.66it/s]Measuring inference for batch_size=1:  58%|█████▊    | 580/1000 [00:45<00:33, 12.65it/s]Measuring inference for batch_size=1:  58%|█████▊    | 582/1000 [00:45<00:33, 12.66it/s]Measuring inference for batch_size=1:  58%|█████▊    | 584/1000 [00:46<00:32, 12.66it/s]Measuring inference for batch_size=1:  59%|█████▊    | 586/1000 [00:46<00:32, 12.66it/s]Measuring inference for batch_size=1:  59%|█████▉    | 588/1000 [00:46<00:32, 12.66it/s]Measuring inference for batch_size=1:  59%|█████▉    | 590/1000 [00:46<00:32, 12.67it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [00:46<00:32, 12.66it/s]Measuring inference for batch_size=1:  59%|█████▉    | 594/1000 [00:46<00:32, 12.66it/s]Measuring inference for batch_size=1:  60%|█████▉    | 596/1000 [00:47<00:31, 12.66it/s]Measuring inference for batch_size=1:  60%|█████▉    | 598/1000 [00:47<00:31, 12.66it/s]Measuring inference for batch_size=1:  60%|██████    | 600/1000 [00:47<00:31, 12.66it/s]Measuring inference for batch_size=1:  60%|██████    | 602/1000 [00:47<00:31, 12.66it/s]Measuring inference for batch_size=1:  60%|██████    | 604/1000 [00:47<00:31, 12.66it/s]Measuring inference for batch_size=1:  61%|██████    | 606/1000 [00:47<00:31, 12.66it/s]Measuring inference for batch_size=1:  61%|██████    | 608/1000 [00:48<00:30, 12.66it/s]Measuring inference for batch_size=1:  61%|██████    | 610/1000 [00:48<00:30, 12.66it/s]Measuring inference for batch_size=1:  61%|██████    | 612/1000 [00:48<00:30, 12.66it/s]Measuring inference for batch_size=1:  61%|██████▏   | 614/1000 [00:48<00:30, 12.66it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:48<00:30, 12.66it/s]Measuring inference for batch_size=1:  62%|██████▏   | 618/1000 [00:48<00:30, 12.66it/s]Measuring inference for batch_size=1:  62%|██████▏   | 620/1000 [00:48<00:30, 12.66it/s]Measuring inference for batch_size=1:  62%|██████▏   | 622/1000 [00:49<00:29, 12.67it/s]Measuring inference for batch_size=1:  62%|██████▏   | 624/1000 [00:49<00:29, 12.66it/s]Measuring inference for batch_size=1:  63%|██████▎   | 626/1000 [00:49<00:29, 12.66it/s]Measuring inference for batch_size=1:  63%|██████▎   | 628/1000 [00:49<00:29, 12.66it/s]Measuring inference for batch_size=1:  63%|██████▎   | 630/1000 [00:49<00:29, 12.66it/s]Measuring inference for batch_size=1:  63%|██████▎   | 632/1000 [00:49<00:29, 12.66it/s]Measuring inference for batch_size=1:  63%|██████▎   | 634/1000 [00:50<00:28, 12.67it/s]Measuring inference for batch_size=1:  64%|██████▎   | 636/1000 [00:50<00:28, 12.67it/s]Measuring inference for batch_size=1:  64%|██████▍   | 638/1000 [00:50<00:28, 12.67it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [00:50<00:28, 12.67it/s]Measuring inference for batch_size=1:  64%|██████▍   | 642/1000 [00:50<00:28, 12.67it/s]Measuring inference for batch_size=1:  64%|██████▍   | 644/1000 [00:50<00:28, 12.67it/s]Measuring inference for batch_size=1:  65%|██████▍   | 646/1000 [00:51<00:27, 12.67it/s]Measuring inference for batch_size=1:  65%|██████▍   | 648/1000 [00:51<00:27, 12.67it/s]Measuring inference for batch_size=1:  65%|██████▌   | 650/1000 [00:51<00:27, 12.67it/s]Measuring inference for batch_size=1:  65%|██████▌   | 652/1000 [00:51<00:27, 12.67it/s]Measuring inference for batch_size=1:  65%|██████▌   | 654/1000 [00:51<00:27, 12.67it/s]Measuring inference for batch_size=1:  66%|██████▌   | 656/1000 [00:51<00:27, 12.67it/s]Measuring inference for batch_size=1:  66%|██████▌   | 658/1000 [00:51<00:26, 12.67it/s]Measuring inference for batch_size=1:  66%|██████▌   | 660/1000 [00:52<00:26, 12.67it/s]Measuring inference for batch_size=1:  66%|██████▌   | 662/1000 [00:52<00:26, 12.67it/s]Measuring inference for batch_size=1:  66%|██████▋   | 664/1000 [00:52<00:26, 12.67it/s]Measuring inference for batch_size=1:  67%|██████▋   | 666/1000 [00:52<00:26, 12.66it/s]Measuring inference for batch_size=1:  67%|██████▋   | 668/1000 [00:52<00:26, 12.66it/s]Measuring inference for batch_size=1:  67%|██████▋   | 670/1000 [00:52<00:26, 12.66it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:53<00:25, 12.66it/s]Measuring inference for batch_size=1:  67%|██████▋   | 674/1000 [00:53<00:25, 12.66it/s]Measuring inference for batch_size=1:  68%|██████▊   | 676/1000 [00:53<00:25, 12.66it/s]Measuring inference for batch_size=1:  68%|██████▊   | 678/1000 [00:53<00:25, 12.66it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [00:53<00:25, 12.66it/s]Measuring inference for batch_size=1:  68%|██████▊   | 682/1000 [00:53<00:25, 12.66it/s]Measuring inference for batch_size=1:  68%|██████▊   | 684/1000 [00:54<00:24, 12.66it/s]Measuring inference for batch_size=1:  69%|██████▊   | 686/1000 [00:54<00:24, 12.66it/s]Measuring inference for batch_size=1:  69%|██████▉   | 688/1000 [00:54<00:24, 12.66it/s]Measuring inference for batch_size=1:  69%|██████▉   | 690/1000 [00:54<00:24, 12.66it/s]Measuring inference for batch_size=1:  69%|██████▉   | 692/1000 [00:54<00:24, 12.66it/s]Measuring inference for batch_size=1:  69%|██████▉   | 694/1000 [00:54<00:24, 12.66it/s]Measuring inference for batch_size=1:  70%|██████▉   | 696/1000 [00:54<00:24, 12.66it/s]Measuring inference for batch_size=1:  70%|██████▉   | 698/1000 [00:55<00:23, 12.66it/s]Measuring inference for batch_size=1:  70%|███████   | 700/1000 [00:55<00:23, 12.66it/s]Measuring inference for batch_size=1:  70%|███████   | 702/1000 [00:55<00:23, 12.66it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [00:55<00:23, 12.66it/s]Measuring inference for batch_size=1:  71%|███████   | 706/1000 [00:55<00:23, 12.66it/s]Measuring inference for batch_size=1:  71%|███████   | 708/1000 [00:55<00:23, 12.66it/s]Measuring inference for batch_size=1:  71%|███████   | 710/1000 [00:56<00:22, 12.66it/s]Measuring inference for batch_size=1:  71%|███████   | 712/1000 [00:56<00:22, 12.66it/s]Measuring inference for batch_size=1:  71%|███████▏  | 714/1000 [00:56<00:22, 12.66it/s]Measuring inference for batch_size=1:  72%|███████▏  | 716/1000 [00:56<00:22, 12.66it/s]Measuring inference for batch_size=1:  72%|███████▏  | 718/1000 [00:56<00:22, 12.66it/s]Measuring inference for batch_size=1:  72%|███████▏  | 720/1000 [00:56<00:22, 12.66it/s]Measuring inference for batch_size=1:  72%|███████▏  | 722/1000 [00:57<00:21, 12.66it/s]Measuring inference for batch_size=1:  72%|███████▏  | 724/1000 [00:57<00:21, 12.66it/s]Measuring inference for batch_size=1:  73%|███████▎  | 726/1000 [00:57<00:21, 12.66it/s]Measuring inference for batch_size=1:  73%|███████▎  | 728/1000 [00:57<00:21, 12.66it/s]Measuring inference for batch_size=1:  73%|███████▎  | 730/1000 [00:57<00:21, 12.66it/s]Measuring inference for batch_size=1:  73%|███████▎  | 732/1000 [00:57<00:21, 12.67it/s]Measuring inference for batch_size=1:  73%|███████▎  | 734/1000 [00:57<00:21, 12.66it/s]Measuring inference for batch_size=1:  74%|███████▎  | 736/1000 [00:58<00:20, 12.65it/s]Measuring inference for batch_size=1:  74%|███████▍  | 738/1000 [00:58<00:20, 12.66it/s]Measuring inference for batch_size=1:  74%|███████▍  | 740/1000 [00:58<00:20, 12.66it/s]Measuring inference for batch_size=1:  74%|███████▍  | 742/1000 [00:58<00:20, 12.66it/s]Measuring inference for batch_size=1:  74%|███████▍  | 744/1000 [00:58<00:20, 12.66it/s]Measuring inference for batch_size=1:  75%|███████▍  | 746/1000 [00:58<00:20, 12.67it/s]Measuring inference for batch_size=1:  75%|███████▍  | 748/1000 [00:59<00:19, 12.67it/s]Measuring inference for batch_size=1:  75%|███████▌  | 750/1000 [00:59<00:19, 12.67it/s]Measuring inference for batch_size=1:  75%|███████▌  | 752/1000 [00:59<00:19, 12.67it/s]Measuring inference for batch_size=1:  75%|███████▌  | 754/1000 [00:59<00:19, 12.66it/s]Measuring inference for batch_size=1:  76%|███████▌  | 756/1000 [00:59<00:19, 12.67it/s]Measuring inference for batch_size=1:  76%|███████▌  | 758/1000 [00:59<00:19, 12.66it/s]Measuring inference for batch_size=1:  76%|███████▌  | 760/1000 [01:00<00:18, 12.66it/s]Measuring inference for batch_size=1:  76%|███████▌  | 762/1000 [01:00<00:18, 12.66it/s]Measuring inference for batch_size=1:  76%|███████▋  | 764/1000 [01:00<00:18, 12.66it/s]Measuring inference for batch_size=1:  77%|███████▋  | 766/1000 [01:00<00:18, 12.66it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [01:00<00:18, 12.67it/s]Measuring inference for batch_size=1:  77%|███████▋  | 770/1000 [01:00<00:18, 12.66it/s]Measuring inference for batch_size=1:  77%|███████▋  | 772/1000 [01:00<00:18, 12.66it/s]Measuring inference for batch_size=1:  77%|███████▋  | 774/1000 [01:01<00:17, 12.67it/s]Measuring inference for batch_size=1:  78%|███████▊  | 776/1000 [01:01<00:17, 12.66it/s]Measuring inference for batch_size=1:  78%|███████▊  | 778/1000 [01:01<00:17, 12.66it/s]Measuring inference for batch_size=1:  78%|███████▊  | 780/1000 [01:01<00:17, 12.66it/s]Measuring inference for batch_size=1:  78%|███████▊  | 782/1000 [01:01<00:17, 12.65it/s]Measuring inference for batch_size=1:  78%|███████▊  | 784/1000 [01:01<00:17, 12.65it/s]Measuring inference for batch_size=1:  79%|███████▊  | 786/1000 [01:02<00:16, 12.66it/s]Measuring inference for batch_size=1:  79%|███████▉  | 788/1000 [01:02<00:16, 12.66it/s]Measuring inference for batch_size=1:  79%|███████▉  | 790/1000 [01:02<00:16, 12.66it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [01:02<00:16, 12.66it/s]Measuring inference for batch_size=1:  79%|███████▉  | 794/1000 [01:02<00:16, 12.66it/s]Measuring inference for batch_size=1:  80%|███████▉  | 796/1000 [01:02<00:16, 12.66it/s]Measuring inference for batch_size=1:  80%|███████▉  | 798/1000 [01:03<00:15, 12.66it/s]Measuring inference for batch_size=1:  80%|████████  | 800/1000 [01:03<00:15, 12.66it/s]Measuring inference for batch_size=1:  80%|████████  | 802/1000 [01:03<00:15, 12.66it/s]Measuring inference for batch_size=1:  80%|████████  | 804/1000 [01:03<00:15, 12.66it/s]Measuring inference for batch_size=1:  81%|████████  | 806/1000 [01:03<00:15, 12.66it/s]Measuring inference for batch_size=1:  81%|████████  | 808/1000 [01:03<00:15, 12.65it/s]Measuring inference for batch_size=1:  81%|████████  | 810/1000 [01:03<00:15, 12.65it/s]Measuring inference for batch_size=1:  81%|████████  | 812/1000 [01:04<00:14, 12.65it/s]Measuring inference for batch_size=1:  81%|████████▏ | 814/1000 [01:04<00:14, 12.65it/s]Measuring inference for batch_size=1:  82%|████████▏ | 816/1000 [01:04<00:14, 12.66it/s]Measuring inference for batch_size=1:  82%|████████▏ | 818/1000 [01:04<00:14, 12.66it/s]Measuring inference for batch_size=1:  82%|████████▏ | 820/1000 [01:04<00:14, 12.66it/s]Measuring inference for batch_size=1:  82%|████████▏ | 822/1000 [01:04<00:14, 12.66it/s]Measuring inference for batch_size=1:  82%|████████▏ | 824/1000 [01:05<00:13, 12.65it/s]Measuring inference for batch_size=1:  83%|████████▎ | 826/1000 [01:05<00:13, 12.66it/s]Measuring inference for batch_size=1:  83%|████████▎ | 828/1000 [01:05<00:13, 12.66it/s]Measuring inference for batch_size=1:  83%|████████▎ | 830/1000 [01:05<00:13, 12.67it/s]Measuring inference for batch_size=1:  83%|████████▎ | 832/1000 [01:05<00:13, 12.66it/s]Measuring inference for batch_size=1:  83%|████████▎ | 834/1000 [01:05<00:13, 12.67it/s]Measuring inference for batch_size=1:  84%|████████▎ | 836/1000 [01:06<00:12, 12.66it/s]Measuring inference for batch_size=1:  84%|████████▍ | 838/1000 [01:06<00:12, 12.66it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [01:06<00:12, 12.67it/s]Measuring inference for batch_size=1:  84%|████████▍ | 842/1000 [01:06<00:12, 12.67it/s]Measuring inference for batch_size=1:  84%|████████▍ | 844/1000 [01:06<00:12, 12.66it/s]Measuring inference for batch_size=1:  85%|████████▍ | 846/1000 [01:06<00:12, 12.66it/s]Measuring inference for batch_size=1:  85%|████████▍ | 848/1000 [01:06<00:12, 12.66it/s]Measuring inference for batch_size=1:  85%|████████▌ | 850/1000 [01:07<00:11, 12.66it/s]Measuring inference for batch_size=1:  85%|████████▌ | 852/1000 [01:07<00:11, 12.66it/s]Measuring inference for batch_size=1:  85%|████████▌ | 854/1000 [01:07<00:11, 12.66it/s]Measuring inference for batch_size=1:  86%|████████▌ | 856/1000 [01:07<00:11, 12.65it/s]Measuring inference for batch_size=1:  86%|████████▌ | 858/1000 [01:07<00:11, 12.66it/s]Measuring inference for batch_size=1:  86%|████████▌ | 860/1000 [01:07<00:11, 12.66it/s]Measuring inference for batch_size=1:  86%|████████▌ | 862/1000 [01:08<00:10, 12.65it/s]Measuring inference for batch_size=1:  86%|████████▋ | 864/1000 [01:08<00:10, 12.66it/s]Measuring inference for batch_size=1:  87%|████████▋ | 866/1000 [01:08<00:10, 12.66it/s]Measuring inference for batch_size=1:  87%|████████▋ | 868/1000 [01:08<00:10, 12.66it/s]Measuring inference for batch_size=1:  87%|████████▋ | 870/1000 [01:08<00:10, 12.65it/s]Measuring inference for batch_size=1:  87%|████████▋ | 872/1000 [01:08<00:10, 12.65it/s]Measuring inference for batch_size=1:  87%|████████▋ | 874/1000 [01:09<00:09, 12.65it/s]Measuring inference for batch_size=1:  88%|████████▊ | 876/1000 [01:09<00:09, 12.65it/s]Measuring inference for batch_size=1:  88%|████████▊ | 878/1000 [01:09<00:09, 12.65it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [01:09<00:09, 12.65it/s]Measuring inference for batch_size=1:  88%|████████▊ | 882/1000 [01:09<00:09, 12.65it/s]Measuring inference for batch_size=1:  88%|████████▊ | 884/1000 [01:09<00:09, 12.66it/s]Measuring inference for batch_size=1:  89%|████████▊ | 886/1000 [01:09<00:09, 12.65it/s]Measuring inference for batch_size=1:  89%|████████▉ | 888/1000 [01:10<00:08, 12.66it/s]Measuring inference for batch_size=1:  89%|████████▉ | 890/1000 [01:10<00:08, 12.66it/s]Measuring inference for batch_size=1:  89%|████████▉ | 892/1000 [01:10<00:08, 12.66it/s]Measuring inference for batch_size=1:  89%|████████▉ | 894/1000 [01:10<00:08, 12.66it/s]Measuring inference for batch_size=1:  90%|████████▉ | 896/1000 [01:10<00:08, 12.65it/s]Measuring inference for batch_size=1:  90%|████████▉ | 898/1000 [01:10<00:08, 12.66it/s]Measuring inference for batch_size=1:  90%|█████████ | 900/1000 [01:11<00:07, 12.66it/s]Measuring inference for batch_size=1:  90%|█████████ | 902/1000 [01:11<00:07, 12.66it/s]Measuring inference for batch_size=1:  90%|█████████ | 904/1000 [01:11<00:07, 12.66it/s]Measuring inference for batch_size=1:  91%|█████████ | 906/1000 [01:11<00:07, 12.67it/s]Measuring inference for batch_size=1:  91%|█████████ | 908/1000 [01:11<00:07, 12.67it/s]Measuring inference for batch_size=1:  91%|█████████ | 910/1000 [01:11<00:07, 12.66it/s]Measuring inference for batch_size=1:  91%|█████████ | 912/1000 [01:12<00:06, 12.65it/s]Measuring inference for batch_size=1:  91%|█████████▏| 914/1000 [01:12<00:06, 12.65it/s]Measuring inference for batch_size=1:  92%|█████████▏| 916/1000 [01:12<00:06, 12.66it/s]Measuring inference for batch_size=1:  92%|█████████▏| 918/1000 [01:12<00:06, 12.65it/s]Measuring inference for batch_size=1:  92%|█████████▏| 920/1000 [01:12<00:06, 12.65it/s]Measuring inference for batch_size=1:  92%|█████████▏| 922/1000 [01:12<00:06, 12.65it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [01:12<00:06, 12.65it/s]Measuring inference for batch_size=1:  93%|█████████▎| 926/1000 [01:13<00:05, 12.65it/s]Measuring inference for batch_size=1:  93%|█████████▎| 928/1000 [01:13<00:05, 12.65it/s]Measuring inference for batch_size=1:  93%|█████████▎| 930/1000 [01:13<00:05, 12.65it/s]Measuring inference for batch_size=1:  93%|█████████▎| 932/1000 [01:13<00:05, 12.65it/s]Measuring inference for batch_size=1:  93%|█████████▎| 934/1000 [01:13<00:05, 12.66it/s]Measuring inference for batch_size=1:  94%|█████████▎| 936/1000 [01:13<00:05, 12.66it/s]Measuring inference for batch_size=1:  94%|█████████▍| 938/1000 [01:14<00:04, 12.66it/s]Measuring inference for batch_size=1:  94%|█████████▍| 940/1000 [01:14<00:04, 12.65it/s]Measuring inference for batch_size=1:  94%|█████████▍| 942/1000 [01:14<00:04, 12.66it/s]Measuring inference for batch_size=1:  94%|█████████▍| 944/1000 [01:14<00:04, 12.66it/s]Measuring inference for batch_size=1:  95%|█████████▍| 946/1000 [01:14<00:04, 12.66it/s]Measuring inference for batch_size=1:  95%|█████████▍| 948/1000 [01:14<00:04, 12.66it/s]Measuring inference for batch_size=1:  95%|█████████▌| 950/1000 [01:15<00:03, 12.66it/s]Measuring inference for batch_size=1:  95%|█████████▌| 952/1000 [01:15<00:03, 12.65it/s]Measuring inference for batch_size=1:  95%|█████████▌| 954/1000 [01:15<00:03, 12.65it/s]Measuring inference for batch_size=1:  96%|█████████▌| 956/1000 [01:15<00:03, 12.66it/s]Measuring inference for batch_size=1:  96%|█████████▌| 958/1000 [01:15<00:03, 12.66it/s]Measuring inference for batch_size=1:  96%|█████████▌| 960/1000 [01:15<00:03, 12.66it/s]Measuring inference for batch_size=1:  96%|█████████▌| 962/1000 [01:15<00:03, 12.66it/s]Measuring inference for batch_size=1:  96%|█████████▋| 964/1000 [01:16<00:02, 12.65it/s]Measuring inference for batch_size=1:  97%|█████████▋| 966/1000 [01:16<00:02, 12.66it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [01:16<00:02, 12.66it/s]Measuring inference for batch_size=1:  97%|█████████▋| 970/1000 [01:16<00:02, 12.65it/s]Measuring inference for batch_size=1:  97%|█████████▋| 972/1000 [01:16<00:02, 12.66it/s]Measuring inference for batch_size=1:  97%|█████████▋| 974/1000 [01:16<00:02, 12.66it/s]Measuring inference for batch_size=1:  98%|█████████▊| 976/1000 [01:17<00:01, 12.66it/s]Measuring inference for batch_size=1:  98%|█████████▊| 978/1000 [01:17<00:01, 12.66it/s]Measuring inference for batch_size=1:  98%|█████████▊| 980/1000 [01:17<00:01, 12.66it/s]Measuring inference for batch_size=1:  98%|█████████▊| 982/1000 [01:17<00:01, 12.66it/s]Measuring inference for batch_size=1:  98%|█████████▊| 984/1000 [01:17<00:01, 12.66it/s]Measuring inference for batch_size=1:  99%|█████████▊| 986/1000 [01:17<00:01, 12.66it/s]Measuring inference for batch_size=1:  99%|█████████▉| 988/1000 [01:18<00:00, 12.66it/s]Measuring inference for batch_size=1:  99%|█████████▉| 990/1000 [01:18<00:00, 12.66it/s]Measuring inference for batch_size=1:  99%|█████████▉| 992/1000 [01:18<00:00, 12.67it/s]Measuring inference for batch_size=1:  99%|█████████▉| 994/1000 [01:18<00:00, 12.67it/s]Measuring inference for batch_size=1: 100%|█████████▉| 996/1000 [01:18<00:00, 12.66it/s]Measuring inference for batch_size=1: 100%|█████████▉| 998/1000 [01:18<00:00, 12.67it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [01:18<00:00, 12.67it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [01:18<00:00, 12.66it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cpu
flops: 11580687848
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.44 GB
    total: 31.28 GB
    used: 3.38 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
params: 60192808
timing:
  batch_size_1:
    on_device_inference:
      human_readable:
        batch_latency: 78.920 ms +/- 198.041 us [78.640 ms, 83.416 ms]
        batches_per_second: 12.67 +/- 0.03 [11.99, 12.72]
      metrics:
        batches_per_second_max: 12.716138988179154
        batches_per_second_mean: 12.671102866428603
        batches_per_second_min: 11.988144201720063
        batches_per_second_std: 0.03072171907125225
        seconds_per_batch_max: 0.08341574668884277
        seconds_per_batch_mean: 0.07892020869255066
        seconds_per_batch_min: 0.07864022254943848
        seconds_per_batch_std: 0.0001980407139185396


