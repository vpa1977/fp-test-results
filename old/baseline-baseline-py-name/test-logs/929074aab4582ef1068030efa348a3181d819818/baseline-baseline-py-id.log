#####
baseline-baseline-py-id - Run 1
2024-02-23 09:40:32
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.69it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.68it/s]
Warning: module SiLU is treated as a zero-op.
Warning: module Conv2dNormActivation is treated as a zero-op.
Warning: module StochasticDepth is treated as a zero-op.
Warning: module FusedMBConv is treated as a zero-op.
Warning: module Sigmoid is treated as a zero-op.
Warning: module SqueezeExcitation is treated as a zero-op.
Warning: module MBConv is treated as a zero-op.
Warning: module Dropout is treated as a zero-op.
Warning: module EfficientNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
EfficientNet(
  118.52 M, 100.000% Params, 12.31 GMac, 100.000% MACs, 
  (features): Sequential(
    117.23 M, 98.919% Params, 12.31 GMac, 99.989% MACs, 
    (0): Conv2dNormActivation(
      928, 0.001% Params, 11.64 MMac, 0.095% MACs, 
      (0): Conv2d(864, 0.001% Params, 10.84 MMac, 0.088% MACs, 3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, 0.000% Params, 802.82 KMac, 0.007% MACs, 32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
    )
    (1): Sequential(
      37.12 k, 0.031% Params, 465.63 MMac, 3.782% MACs, 
      (0): FusedMBConv(
        9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
        (block): Sequential(
          9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
          (0): Conv2dNormActivation(
            9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
            (0): Conv2d(9.22 k, 0.008% Params, 115.61 MMac, 0.939% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(64, 0.000% Params, 802.82 KMac, 0.007% MACs, 32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0, mode=row)
      )
      (1): FusedMBConv(
        9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
        (block): Sequential(
          9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
          (0): Conv2dNormActivation(
            9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
            (0): Conv2d(9.22 k, 0.008% Params, 115.61 MMac, 0.939% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(64, 0.000% Params, 802.82 KMac, 0.007% MACs, 32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.002531645569620253, mode=row)
      )
      (2): FusedMBConv(
        9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
        (block): Sequential(
          9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
          (0): Conv2dNormActivation(
            9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
            (0): Conv2d(9.22 k, 0.008% Params, 115.61 MMac, 0.939% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(64, 0.000% Params, 802.82 KMac, 0.007% MACs, 32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.005063291139240506, mode=row)
      )
      (3): FusedMBConv(
        9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
        (block): Sequential(
          9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
          (0): Conv2dNormActivation(
            9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
            (0): Conv2d(9.22 k, 0.008% Params, 115.61 MMac, 0.939% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(64, 0.000% Params, 802.82 KMac, 0.007% MACs, 32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.007594936708860761, mode=row)
      )
    )
    (2): Sequential(
      1.03 M, 0.871% Params, 3.24 GMac, 26.297% MACs, 
      (0): FusedMBConv(
        45.44 k, 0.038% Params, 142.5 MMac, 1.158% MACs, 
        (block): Sequential(
          45.44 k, 0.038% Params, 142.5 MMac, 1.158% MACs, 
          (0): Conv2dNormActivation(
            37.12 k, 0.031% Params, 116.41 MMac, 0.946% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 115.61 MMac, 0.939% MACs, 32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, 0.000% Params, 802.82 KMac, 0.007% MACs, 128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.32 k, 0.007% Params, 26.09 MMac, 0.212% MACs, 
            (0): Conv2d(8.19 k, 0.007% Params, 25.69 MMac, 0.209% MACs, 128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.010126582278481013, mode=row)
      )
      (1): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.012658227848101266, mode=row)
      )
      (2): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.015189873417721522, mode=row)
      )
      (3): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.017721518987341773, mode=row)
      )
      (4): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.020253164556962026, mode=row)
      )
      (5): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.02278481012658228, mode=row)
      )
      (6): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.02531645569620253, mode=row)
      )
    )
    (3): Sequential(
      2.39 M, 2.017% Params, 1.87 GMac, 15.223% MACs, 
      (0): FusedMBConv(
        172.74 k, 0.146% Params, 135.43 MMac, 1.100% MACs, 
        (block): Sequential(
          172.74 k, 0.146% Params, 135.43 MMac, 1.100% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 116.01 MMac, 0.942% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 115.61 MMac, 0.939% MACs, 64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 401.41 KMac, 0.003% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            24.77 k, 0.021% Params, 19.42 MMac, 0.158% MACs, 
            (0): Conv2d(24.58 k, 0.021% Params, 19.27 MMac, 0.157% MACs, 256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.027848101265822787, mode=row)
      )
      (1): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.030379746835443044, mode=row)
      )
      (2): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.03291139240506329, mode=row)
      )
      (3): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.035443037974683546, mode=row)
      )
      (4): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0379746835443038, mode=row)
      )
      (5): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.04050632911392405, mode=row)
      )
      (6): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.04303797468354431, mode=row)
      )
    )
    (4): Sequential(
      3.55 M, 2.998% Params, 585.49 MMac, 4.756% MACs, 
      (0): MBConv(
        134.81 k, 0.114% Params, 44.95 MMac, 0.365% MACs, 
        (block): Sequential(
          134.81 k, 0.114% Params, 44.95 MMac, 0.365% MACs, 
          (0): Conv2dNormActivation(
            37.63 k, 0.032% Params, 29.5 MMac, 0.240% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            4.22 k, 0.004% Params, 827.9 KMac, 0.007% MACs, 
            (0): Conv2d(3.46 k, 0.003% Params, 677.38 KMac, 0.006% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 150.53 KMac, 0.001% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            18.84 k, 0.016% Params, 94.1 KMac, 0.001% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 75.26 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(9.24 k, 0.008% Params, 9.24 KMac, 0.000% MACs, 384, 24, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(9.6 k, 0.008% Params, 9.6 KMac, 0.000% MACs, 24, 384, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            74.11 k, 0.063% Params, 14.53 MMac, 0.118% MACs, 
            (0): Conv2d(73.73 k, 0.062% Params, 14.45 MMac, 0.117% MACs, 384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.04556962025316456, mode=row)
      )
      (1): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.04810126582278482, mode=row)
      )
      (2): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.05063291139240506, mode=row)
      )
      (3): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.053164556962025315, mode=row)
      )
      (4): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.055696202531645575, mode=row)
      )
      (5): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.05822784810126583, mode=row)
      )
      (6): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06075949367088609, mode=row)
      )
      (7): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06329113924050633, mode=row)
      )
      (8): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06582278481012659, mode=row)
      )
      (9): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06835443037974684, mode=row)
      )
    )
    (5): Sequential(
      14.5 M, 12.236% Params, 2.29 GMac, 18.620% MACs, 
      (0): MBConv(
        606.45 k, 0.512% Params, 97.29 MMac, 0.790% MACs, 
        (block): Sequential(
          606.45 k, 0.512% Params, 97.29 MMac, 0.790% MACs, 
          (0): Conv2dNormActivation(
            223.49 k, 0.189% Params, 43.8 MMac, 0.356% MACs, 
            (0): Conv2d(221.18 k, 0.187% Params, 43.35 MMac, 0.352% MACs, 192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.3 k, 0.002% Params, 451.58 KMac, 0.004% MACs, 1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            12.67 k, 0.011% Params, 2.48 MMac, 0.020% MACs, 
            (0): Conv2d(10.37 k, 0.009% Params, 2.03 MMac, 0.017% MACs, 1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(2.3 k, 0.002% Params, 451.58 KMac, 0.004% MACs, 1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            111.79 k, 0.094% Params, 337.58 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 225.79 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(55.34 k, 0.047% Params, 55.34 KMac, 0.000% MACs, 1152, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(56.45 k, 0.048% Params, 56.45 KMac, 0.000% MACs, 48, 1152, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            258.5 k, 0.218% Params, 50.67 MMac, 0.412% MACs, 
            (0): Conv2d(258.05 k, 0.218% Params, 50.58 MMac, 0.411% MACs, 1152, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.07088607594936709, mode=row)
      )
      (1): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.07341772151898734, mode=row)
      )
      (2): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0759493670886076, mode=row)
      )
      (3): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.07848101265822785, mode=row)
      )
      (4): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0810126582278481, mode=row)
      )
      (5): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08354430379746836, mode=row)
      )
      (6): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08607594936708862, mode=row)
      )
      (7): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08860759493670886, mode=row)
      )
      (8): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.09113924050632911, mode=row)
      )
      (9): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.09367088607594937, mode=row)
      )
      (10): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.09620253164556963, mode=row)
      )
      (11): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.09873417721518989, mode=row)
      )
      (12): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.10126582278481013, mode=row)
      )
      (13): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.10379746835443039, mode=row)
      )
      (14): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.10632911392405063, mode=row)
      )
      (15): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.10886075949367088, mode=row)
      )
      (16): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11139240506329115, mode=row)
      )
      (17): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11392405063291139, mode=row)
      )
      (18): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11645569620253166, mode=row)
      )
    )
    (6): Sequential(
      54.87 M, 46.295% Params, 2.22 GMac, 18.003% MACs, 
      (0): MBConv(
        987.32 k, 0.833% Params, 85.8 MMac, 0.697% MACs, 
        (block): Sequential(
          987.32 k, 0.833% Params, 85.8 MMac, 0.697% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 724.42 KMac, 0.006% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 592.7 KMac, 0.005% MACs, 1344, 1344, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 131.71 KMac, 0.001% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 217.78 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 65.86 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            516.86 k, 0.436% Params, 25.33 MMac, 0.206% MACs, 
            (0): Conv2d(516.1 k, 0.435% Params, 25.29 MMac, 0.205% MACs, 1344, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11898734177215191, mode=row)
      )
      (1): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.12151898734177217, mode=row)
      )
      (2): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.12405063291139241, mode=row)
      )
      (3): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.12658227848101267, mode=row)
      )
      (4): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.12911392405063293, mode=row)
      )
      (5): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.13164556962025317, mode=row)
      )
      (6): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.13417721518987344, mode=row)
      )
      (7): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.13670886075949368, mode=row)
      )
      (8): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.13924050632911392, mode=row)
      )
      (9): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.14177215189873418, mode=row)
      )
      (10): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.14430379746835442, mode=row)
      )
      (11): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1468354430379747, mode=row)
      )
      (12): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.14936708860759496, mode=row)
      )
      (13): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1518987341772152, mode=row)
      )
      (14): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.15443037974683546, mode=row)
      )
      (15): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1569620253164557, mode=row)
      )
      (16): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.15949367088607597, mode=row)
      )
      (17): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1620253164556962, mode=row)
      )
      (18): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.16455696202531644, mode=row)
      )
      (19): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1670886075949367, mode=row)
      )
      (20): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.16962025316455698, mode=row)
      )
      (21): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.17215189873417724, mode=row)
      )
      (22): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.17468354430379748, mode=row)
      )
      (23): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.17721518987341772, mode=row)
      )
      (24): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.179746835443038, mode=row)
      )
    )
    (7): Sequential(
      40.03 M, 33.777% Params, 1.59 GMac, 12.886% MACs, 
      (0): MBConv(
        2.84 M, 2.392% Params, 117.69 MMac, 0.956% MACs, 
        (block): Sequential(
          2.84 M, 2.392% Params, 117.69 MMac, 0.956% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            1.48 M, 1.245% Params, 72.32 MMac, 0.587% MACs, 
            (0): Conv2d(1.47 M, 1.244% Params, 72.25 MMac, 0.587% MACs, 2304, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.18227848101265823, mode=row)
      )
      (1): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1848101265822785, mode=row)
      )
      (2): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.18734177215189873, mode=row)
      )
      (3): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.189873417721519, mode=row)
      )
      (4): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.19240506329113927, mode=row)
      )
      (5): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1949367088607595, mode=row)
      )
      (6): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.19746835443037977, mode=row)
      )
    )
    (8): Conv2dNormActivation(
      821.76 k, 0.693% Params, 40.27 MMac, 0.327% MACs, 
      (0): Conv2d(819.2 k, 0.691% Params, 40.14 MMac, 0.326% MACs, 640, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2.56 k, 0.002% Params, 125.44 KMac, 0.001% MACs, 1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 62.72 KMac, 0.001% MACs, output_size=1)
  (classifier): Sequential(
    1.28 M, 1.081% Params, 1.28 MMac, 0.010% MACs, 
    (0): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.4, inplace=True)
    (1): Linear(1.28 M, 1.081% Params, 1.28 MMac, 0.010% MACs, in_features=1280, out_features=1000, bias=True)
  )
)Measurement of allocated memory is only available on CUDA devices

Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:   1%|          | 1/100 [00:00<00:13,  7.34it/s]Warming up with batch_size=1:   2%|▏         | 2/100 [00:00<00:13,  7.33it/s]Warming up with batch_size=1:   3%|▎         | 3/100 [00:00<00:13,  7.32it/s]Warming up with batch_size=1:   4%|▍         | 4/100 [00:00<00:13,  7.32it/s]Warming up with batch_size=1:   5%|▌         | 5/100 [00:00<00:12,  7.31it/s]Warming up with batch_size=1:   6%|▌         | 6/100 [00:00<00:12,  7.31it/s]Warming up with batch_size=1:   7%|▋         | 7/100 [00:00<00:12,  7.32it/s]Warming up with batch_size=1:   8%|▊         | 8/100 [00:01<00:12,  7.31it/s]Warming up with batch_size=1:   9%|▉         | 9/100 [00:01<00:12,  7.31it/s]Warming up with batch_size=1:  10%|█         | 10/100 [00:01<00:12,  7.31it/s]Warming up with batch_size=1:  11%|█         | 11/100 [00:01<00:12,  7.31it/s]Warming up with batch_size=1:  12%|█▏        | 12/100 [00:01<00:12,  7.31it/s]Warming up with batch_size=1:  13%|█▎        | 13/100 [00:01<00:11,  7.31it/s]Warming up with batch_size=1:  14%|█▍        | 14/100 [00:01<00:11,  7.31it/s]Warming up with batch_size=1:  15%|█▌        | 15/100 [00:02<00:11,  7.31it/s]Warming up with batch_size=1:  16%|█▌        | 16/100 [00:02<00:11,  7.31it/s]Warming up with batch_size=1:  17%|█▋        | 17/100 [00:02<00:11,  7.31it/s]Warming up with batch_size=1:  18%|█▊        | 18/100 [00:02<00:11,  7.31it/s]Warming up with batch_size=1:  19%|█▉        | 19/100 [00:02<00:11,  7.31it/s]Warming up with batch_size=1:  20%|██        | 20/100 [00:02<00:10,  7.30it/s]Warming up with batch_size=1:  21%|██        | 21/100 [00:02<00:10,  7.28it/s]Warming up with batch_size=1:  22%|██▏       | 22/100 [00:03<00:10,  7.28it/s]Warming up with batch_size=1:  23%|██▎       | 23/100 [00:03<00:10,  7.28it/s]Warming up with batch_size=1:  24%|██▍       | 24/100 [00:03<00:10,  7.29it/s]Warming up with batch_size=1:  25%|██▌       | 25/100 [00:03<00:10,  7.29it/s]Warming up with batch_size=1:  26%|██▌       | 26/100 [00:03<00:10,  7.28it/s]Warming up with batch_size=1:  27%|██▋       | 27/100 [00:03<00:10,  7.29it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:03<00:09,  7.29it/s]Warming up with batch_size=1:  29%|██▉       | 29/100 [00:03<00:09,  7.29it/s]Warming up with batch_size=1:  30%|███       | 30/100 [00:04<00:09,  7.29it/s]Warming up with batch_size=1:  31%|███       | 31/100 [00:04<00:09,  7.29it/s]Warming up with batch_size=1:  32%|███▏      | 32/100 [00:04<00:09,  7.29it/s]Warming up with batch_size=1:  33%|███▎      | 33/100 [00:04<00:09,  7.29it/s]Warming up with batch_size=1:  34%|███▍      | 34/100 [00:04<00:09,  7.29it/s]Warming up with batch_size=1:  35%|███▌      | 35/100 [00:04<00:08,  7.29it/s]Warming up with batch_size=1:  36%|███▌      | 36/100 [00:04<00:08,  7.29it/s]Warming up with batch_size=1:  37%|███▋      | 37/100 [00:05<00:08,  7.29it/s]Warming up with batch_size=1:  38%|███▊      | 38/100 [00:05<00:08,  7.29it/s]Warming up with batch_size=1:  39%|███▉      | 39/100 [00:05<00:08,  7.28it/s]Warming up with batch_size=1:  40%|████      | 40/100 [00:05<00:08,  7.28it/s]Warming up with batch_size=1:  41%|████      | 41/100 [00:05<00:08,  7.28it/s]Warming up with batch_size=1:  42%|████▏     | 42/100 [00:05<00:07,  7.28it/s]Warming up with batch_size=1:  43%|████▎     | 43/100 [00:05<00:07,  7.28it/s]Warming up with batch_size=1:  44%|████▍     | 44/100 [00:06<00:07,  7.28it/s]Warming up with batch_size=1:  45%|████▌     | 45/100 [00:06<00:07,  7.28it/s]Warming up with batch_size=1:  46%|████▌     | 46/100 [00:06<00:07,  7.28it/s]Warming up with batch_size=1:  47%|████▋     | 47/100 [00:06<00:07,  7.28it/s]Warming up with batch_size=1:  48%|████▊     | 48/100 [00:06<00:07,  7.28it/s]Warming up with batch_size=1:  49%|████▉     | 49/100 [00:06<00:06,  7.29it/s]Warming up with batch_size=1:  50%|█████     | 50/100 [00:06<00:06,  7.29it/s]Warming up with batch_size=1:  51%|█████     | 51/100 [00:06<00:06,  7.29it/s]Warming up with batch_size=1:  52%|█████▏    | 52/100 [00:07<00:06,  7.29it/s]Warming up with batch_size=1:  53%|█████▎    | 53/100 [00:07<00:06,  7.29it/s]Warming up with batch_size=1:  54%|█████▍    | 54/100 [00:07<00:06,  7.29it/s]Warming up with batch_size=1:  55%|█████▌    | 55/100 [00:07<00:06,  7.29it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:07<00:06,  7.30it/s]Warming up with batch_size=1:  57%|█████▋    | 57/100 [00:07<00:05,  7.30it/s]Warming up with batch_size=1:  58%|█████▊    | 58/100 [00:07<00:05,  7.29it/s]Warming up with batch_size=1:  59%|█████▉    | 59/100 [00:08<00:05,  7.29it/s]Warming up with batch_size=1:  60%|██████    | 60/100 [00:08<00:05,  7.29it/s]Warming up with batch_size=1:  61%|██████    | 61/100 [00:08<00:05,  7.29it/s]Warming up with batch_size=1:  62%|██████▏   | 62/100 [00:08<00:05,  7.29it/s]Warming up with batch_size=1:  63%|██████▎   | 63/100 [00:08<00:05,  7.29it/s]Warming up with batch_size=1:  64%|██████▍   | 64/100 [00:08<00:04,  7.29it/s]Warming up with batch_size=1:  65%|██████▌   | 65/100 [00:08<00:04,  7.30it/s]Warming up with batch_size=1:  66%|██████▌   | 66/100 [00:09<00:04,  7.29it/s]Warming up with batch_size=1:  67%|██████▋   | 67/100 [00:09<00:04,  7.30it/s]Warming up with batch_size=1:  68%|██████▊   | 68/100 [00:09<00:04,  7.29it/s]Warming up with batch_size=1:  69%|██████▉   | 69/100 [00:09<00:04,  7.29it/s]Warming up with batch_size=1:  70%|███████   | 70/100 [00:09<00:04,  7.29it/s]Warming up with batch_size=1:  71%|███████   | 71/100 [00:09<00:03,  7.29it/s]Warming up with batch_size=1:  72%|███████▏  | 72/100 [00:09<00:03,  7.29it/s]Warming up with batch_size=1:  73%|███████▎  | 73/100 [00:10<00:03,  7.29it/s]Warming up with batch_size=1:  74%|███████▍  | 74/100 [00:10<00:03,  7.29it/s]Warming up with batch_size=1:  75%|███████▌  | 75/100 [00:10<00:03,  7.29it/s]Warming up with batch_size=1:  76%|███████▌  | 76/100 [00:10<00:03,  7.30it/s]Warming up with batch_size=1:  77%|███████▋  | 77/100 [00:10<00:03,  7.30it/s]Warming up with batch_size=1:  78%|███████▊  | 78/100 [00:10<00:03,  7.30it/s]Warming up with batch_size=1:  79%|███████▉  | 79/100 [00:10<00:02,  7.30it/s]Warming up with batch_size=1:  80%|████████  | 80/100 [00:10<00:02,  7.29it/s]Warming up with batch_size=1:  81%|████████  | 81/100 [00:11<00:02,  7.30it/s]Warming up with batch_size=1:  82%|████████▏ | 82/100 [00:11<00:02,  7.30it/s]Warming up with batch_size=1:  83%|████████▎ | 83/100 [00:11<00:02,  7.30it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:11<00:02,  7.30it/s]Warming up with batch_size=1:  85%|████████▌ | 85/100 [00:11<00:02,  7.31it/s]Warming up with batch_size=1:  86%|████████▌ | 86/100 [00:11<00:01,  7.31it/s]Warming up with batch_size=1:  87%|████████▋ | 87/100 [00:11<00:01,  7.31it/s]Warming up with batch_size=1:  88%|████████▊ | 88/100 [00:12<00:01,  7.31it/s]Warming up with batch_size=1:  89%|████████▉ | 89/100 [00:12<00:01,  7.30it/s]Warming up with batch_size=1:  90%|█████████ | 90/100 [00:12<00:01,  7.30it/s]Warming up with batch_size=1:  91%|█████████ | 91/100 [00:12<00:01,  7.30it/s]Warming up with batch_size=1:  92%|█████████▏| 92/100 [00:12<00:01,  7.30it/s]Warming up with batch_size=1:  93%|█████████▎| 93/100 [00:12<00:00,  7.29it/s]Warming up with batch_size=1:  94%|█████████▍| 94/100 [00:12<00:00,  7.29it/s]Warming up with batch_size=1:  95%|█████████▌| 95/100 [00:13<00:00,  7.29it/s]Warming up with batch_size=1:  96%|█████████▌| 96/100 [00:13<00:00,  7.29it/s]Warming up with batch_size=1:  97%|█████████▋| 97/100 [00:13<00:00,  7.29it/s]Warming up with batch_size=1:  98%|█████████▊| 98/100 [00:13<00:00,  7.30it/s]Warming up with batch_size=1:  99%|█████████▉| 99/100 [00:13<00:00,  7.29it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:13<00:00,  7.29it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:13<00:00,  7.30it/s]
STAGE:2024-02-23 09:34:54 176263:176263 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:34:54 176263:176263 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:34:54 176263:176263 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   0%|          | 1/1000 [00:00<02:17,  7.26it/s]Measuring inference for batch_size=1:   0%|          | 2/1000 [00:00<02:17,  7.28it/s]Measuring inference for batch_size=1:   0%|          | 3/1000 [00:00<02:16,  7.29it/s]Measuring inference for batch_size=1:   0%|          | 4/1000 [00:00<02:16,  7.28it/s]Measuring inference for batch_size=1:   0%|          | 5/1000 [00:00<02:16,  7.28it/s]Measuring inference for batch_size=1:   1%|          | 6/1000 [00:00<02:16,  7.29it/s]Measuring inference for batch_size=1:   1%|          | 7/1000 [00:00<02:16,  7.29it/s]Measuring inference for batch_size=1:   1%|          | 8/1000 [00:01<02:16,  7.29it/s]Measuring inference for batch_size=1:   1%|          | 9/1000 [00:01<02:15,  7.29it/s]Measuring inference for batch_size=1:   1%|          | 10/1000 [00:01<02:15,  7.29it/s]Measuring inference for batch_size=1:   1%|          | 11/1000 [00:01<02:15,  7.30it/s]Measuring inference for batch_size=1:   1%|          | 12/1000 [00:01<02:15,  7.30it/s]Measuring inference for batch_size=1:   1%|▏         | 13/1000 [00:01<02:15,  7.30it/s]Measuring inference for batch_size=1:   1%|▏         | 14/1000 [00:01<02:15,  7.30it/s]Measuring inference for batch_size=1:   2%|▏         | 15/1000 [00:02<02:15,  7.29it/s]Measuring inference for batch_size=1:   2%|▏         | 16/1000 [00:02<02:14,  7.30it/s]Measuring inference for batch_size=1:   2%|▏         | 17/1000 [00:02<02:14,  7.29it/s]Measuring inference for batch_size=1:   2%|▏         | 18/1000 [00:02<02:14,  7.28it/s]Measuring inference for batch_size=1:   2%|▏         | 19/1000 [00:02<02:14,  7.28it/s]Measuring inference for batch_size=1:   2%|▏         | 20/1000 [00:02<02:14,  7.29it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:02<02:14,  7.29it/s]Measuring inference for batch_size=1:   2%|▏         | 22/1000 [00:03<02:14,  7.29it/s]Measuring inference for batch_size=1:   2%|▏         | 23/1000 [00:03<02:14,  7.29it/s]Measuring inference for batch_size=1:   2%|▏         | 24/1000 [00:03<02:13,  7.28it/s]Measuring inference for batch_size=1:   2%|▎         | 25/1000 [00:03<02:13,  7.28it/s]Measuring inference for batch_size=1:   3%|▎         | 26/1000 [00:03<02:13,  7.28it/s]Measuring inference for batch_size=1:   3%|▎         | 27/1000 [00:03<02:13,  7.29it/s]Measuring inference for batch_size=1:   3%|▎         | 28/1000 [00:03<02:13,  7.29it/s]Measuring inference for batch_size=1:   3%|▎         | 29/1000 [00:03<02:13,  7.29it/s]Measuring inference for batch_size=1:   3%|▎         | 30/1000 [00:04<02:13,  7.29it/s]Measuring inference for batch_size=1:   3%|▎         | 31/1000 [00:04<02:13,  7.28it/s]Measuring inference for batch_size=1:   3%|▎         | 32/1000 [00:04<02:12,  7.28it/s]Measuring inference for batch_size=1:   3%|▎         | 33/1000 [00:04<02:12,  7.28it/s]Measuring inference for batch_size=1:   3%|▎         | 34/1000 [00:04<02:12,  7.28it/s]Measuring inference for batch_size=1:   4%|▎         | 35/1000 [00:04<02:12,  7.28it/s]Measuring inference for batch_size=1:   4%|▎         | 36/1000 [00:04<02:12,  7.28it/s]Measuring inference for batch_size=1:   4%|▎         | 37/1000 [00:05<02:12,  7.28it/s]Measuring inference for batch_size=1:   4%|▍         | 38/1000 [00:05<02:12,  7.28it/s]Measuring inference for batch_size=1:   4%|▍         | 39/1000 [00:05<02:11,  7.28it/s]Measuring inference for batch_size=1:   4%|▍         | 40/1000 [00:05<02:11,  7.28it/s]Measuring inference for batch_size=1:   4%|▍         | 41/1000 [00:05<02:11,  7.28it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:05<02:11,  7.28it/s]Measuring inference for batch_size=1:   4%|▍         | 43/1000 [00:05<02:11,  7.28it/s]Measuring inference for batch_size=1:   4%|▍         | 44/1000 [00:06<02:11,  7.29it/s]Measuring inference for batch_size=1:   4%|▍         | 45/1000 [00:06<02:11,  7.29it/s]Measuring inference for batch_size=1:   5%|▍         | 46/1000 [00:06<02:10,  7.29it/s]Measuring inference for batch_size=1:   5%|▍         | 47/1000 [00:06<02:10,  7.28it/s]Measuring inference for batch_size=1:   5%|▍         | 48/1000 [00:06<02:10,  7.28it/s]Measuring inference for batch_size=1:   5%|▍         | 49/1000 [00:06<02:10,  7.29it/s]Measuring inference for batch_size=1:   5%|▌         | 50/1000 [00:06<02:10,  7.28it/s]Measuring inference for batch_size=1:   5%|▌         | 51/1000 [00:06<02:10,  7.28it/s]Measuring inference for batch_size=1:   5%|▌         | 52/1000 [00:07<02:10,  7.28it/s]Measuring inference for batch_size=1:   5%|▌         | 53/1000 [00:07<02:10,  7.28it/s]Measuring inference for batch_size=1:   5%|▌         | 54/1000 [00:07<02:09,  7.28it/s]Measuring inference for batch_size=1:   6%|▌         | 55/1000 [00:07<02:09,  7.28it/s]Measuring inference for batch_size=1:   6%|▌         | 56/1000 [00:07<02:09,  7.28it/s]Measuring inference for batch_size=1:   6%|▌         | 57/1000 [00:07<02:09,  7.29it/s]Measuring inference for batch_size=1:   6%|▌         | 58/1000 [00:07<02:09,  7.28it/s]Measuring inference for batch_size=1:   6%|▌         | 59/1000 [00:08<02:09,  7.28it/s]Measuring inference for batch_size=1:   6%|▌         | 60/1000 [00:08<02:09,  7.29it/s]Measuring inference for batch_size=1:   6%|▌         | 61/1000 [00:08<02:08,  7.28it/s]Measuring inference for batch_size=1:   6%|▌         | 62/1000 [00:08<02:08,  7.28it/s]Measuring inference for batch_size=1:   6%|▋         | 63/1000 [00:08<02:08,  7.28it/s]Measuring inference for batch_size=1:   6%|▋         | 64/1000 [00:08<02:08,  7.28it/s]Measuring inference for batch_size=1:   6%|▋         | 65/1000 [00:08<02:08,  7.29it/s]Measuring inference for batch_size=1:   7%|▋         | 66/1000 [00:09<02:08,  7.29it/s]Measuring inference for batch_size=1:   7%|▋         | 67/1000 [00:09<02:07,  7.29it/s]Measuring inference for batch_size=1:   7%|▋         | 68/1000 [00:09<02:07,  7.29it/s]Measuring inference for batch_size=1:   7%|▋         | 69/1000 [00:09<02:07,  7.29it/s]Measuring inference for batch_size=1:   7%|▋         | 70/1000 [00:09<02:07,  7.29it/s]Measuring inference for batch_size=1:   7%|▋         | 71/1000 [00:09<02:07,  7.29it/s]Measuring inference for batch_size=1:   7%|▋         | 72/1000 [00:09<02:07,  7.28it/s]Measuring inference for batch_size=1:   7%|▋         | 73/1000 [00:10<02:07,  7.28it/s]Measuring inference for batch_size=1:   7%|▋         | 74/1000 [00:10<02:07,  7.28it/s]Measuring inference for batch_size=1:   8%|▊         | 75/1000 [00:10<02:07,  7.28it/s]Measuring inference for batch_size=1:   8%|▊         | 76/1000 [00:10<02:06,  7.28it/s]Measuring inference for batch_size=1:   8%|▊         | 77/1000 [00:10<02:06,  7.28it/s]Measuring inference for batch_size=1:   8%|▊         | 78/1000 [00:10<02:06,  7.29it/s]Measuring inference for batch_size=1:   8%|▊         | 79/1000 [00:10<02:06,  7.29it/s]Measuring inference for batch_size=1:   8%|▊         | 80/1000 [00:10<02:06,  7.28it/s]Measuring inference for batch_size=1:   8%|▊         | 81/1000 [00:11<02:06,  7.28it/s]Measuring inference for batch_size=1:   8%|▊         | 82/1000 [00:11<02:06,  7.28it/s]Measuring inference for batch_size=1:   8%|▊         | 83/1000 [00:11<02:06,  7.28it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:11<02:05,  7.28it/s]Measuring inference for batch_size=1:   8%|▊         | 85/1000 [00:11<02:05,  7.28it/s]Measuring inference for batch_size=1:   9%|▊         | 86/1000 [00:11<02:05,  7.29it/s]Measuring inference for batch_size=1:   9%|▊         | 87/1000 [00:11<02:05,  7.29it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:12<02:05,  7.29it/s]Measuring inference for batch_size=1:   9%|▉         | 89/1000 [00:12<02:04,  7.29it/s]Measuring inference for batch_size=1:   9%|▉         | 90/1000 [00:12<02:04,  7.29it/s]Measuring inference for batch_size=1:   9%|▉         | 91/1000 [00:12<02:04,  7.30it/s]Measuring inference for batch_size=1:   9%|▉         | 92/1000 [00:12<02:04,  7.30it/s]Measuring inference for batch_size=1:   9%|▉         | 93/1000 [00:12<02:04,  7.31it/s]Measuring inference for batch_size=1:   9%|▉         | 94/1000 [00:12<02:04,  7.30it/s]Measuring inference for batch_size=1:  10%|▉         | 95/1000 [00:13<02:03,  7.30it/s]Measuring inference for batch_size=1:  10%|▉         | 96/1000 [00:13<02:03,  7.30it/s]Measuring inference for batch_size=1:  10%|▉         | 97/1000 [00:13<02:03,  7.30it/s]Measuring inference for batch_size=1:  10%|▉         | 98/1000 [00:13<02:03,  7.30it/s]Measuring inference for batch_size=1:  10%|▉         | 99/1000 [00:13<02:03,  7.30it/s]Measuring inference for batch_size=1:  10%|█         | 100/1000 [00:13<02:03,  7.30it/s]Measuring inference for batch_size=1:  10%|█         | 101/1000 [00:13<02:03,  7.29it/s]Measuring inference for batch_size=1:  10%|█         | 102/1000 [00:13<02:03,  7.29it/s]Measuring inference for batch_size=1:  10%|█         | 103/1000 [00:14<02:03,  7.29it/s]Measuring inference for batch_size=1:  10%|█         | 104/1000 [00:14<02:02,  7.29it/s]Measuring inference for batch_size=1:  10%|█         | 105/1000 [00:14<02:02,  7.28it/s]Measuring inference for batch_size=1:  11%|█         | 106/1000 [00:14<02:02,  7.28it/s]Measuring inference for batch_size=1:  11%|█         | 107/1000 [00:14<02:02,  7.28it/s]Measuring inference for batch_size=1:  11%|█         | 108/1000 [00:14<02:02,  7.28it/s]Measuring inference for batch_size=1:  11%|█         | 109/1000 [00:14<02:02,  7.28it/s]Measuring inference for batch_size=1:  11%|█         | 110/1000 [00:15<02:02,  7.28it/s]Measuring inference for batch_size=1:  11%|█         | 111/1000 [00:15<02:02,  7.28it/s]Measuring inference for batch_size=1:  11%|█         | 112/1000 [00:15<02:01,  7.28it/s]Measuring inference for batch_size=1:  11%|█▏        | 113/1000 [00:15<02:01,  7.28it/s]Measuring inference for batch_size=1:  11%|█▏        | 114/1000 [00:15<02:01,  7.28it/s]Measuring inference for batch_size=1:  12%|█▏        | 115/1000 [00:15<02:01,  7.28it/s]Measuring inference for batch_size=1:  12%|█▏        | 116/1000 [00:15<02:01,  7.28it/s]Measuring inference for batch_size=1:  12%|█▏        | 117/1000 [00:16<02:01,  7.28it/s]Measuring inference for batch_size=1:  12%|█▏        | 118/1000 [00:16<02:01,  7.28it/s]Measuring inference for batch_size=1:  12%|█▏        | 119/1000 [00:16<02:00,  7.28it/s]Measuring inference for batch_size=1:  12%|█▏        | 120/1000 [00:16<02:00,  7.28it/s]Measuring inference for batch_size=1:  12%|█▏        | 121/1000 [00:16<02:00,  7.28it/s]Measuring inference for batch_size=1:  12%|█▏        | 122/1000 [00:16<02:00,  7.28it/s]Measuring inference for batch_size=1:  12%|█▏        | 123/1000 [00:16<02:00,  7.29it/s]Measuring inference for batch_size=1:  12%|█▏        | 124/1000 [00:17<02:00,  7.28it/s]Measuring inference for batch_size=1:  12%|█▎        | 125/1000 [00:17<02:00,  7.28it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:17<02:00,  7.28it/s]Measuring inference for batch_size=1:  13%|█▎        | 127/1000 [00:17<01:59,  7.28it/s]Measuring inference for batch_size=1:  13%|█▎        | 128/1000 [00:17<01:59,  7.28it/s]Measuring inference for batch_size=1:  13%|█▎        | 129/1000 [00:17<01:59,  7.28it/s]Measuring inference for batch_size=1:  13%|█▎        | 130/1000 [00:17<01:59,  7.28it/s]Measuring inference for batch_size=1:  13%|█▎        | 131/1000 [00:17<01:59,  7.29it/s]Measuring inference for batch_size=1:  13%|█▎        | 132/1000 [00:18<01:58,  7.29it/s]Measuring inference for batch_size=1:  13%|█▎        | 133/1000 [00:18<01:58,  7.30it/s]Measuring inference for batch_size=1:  13%|█▎        | 134/1000 [00:18<01:58,  7.31it/s]Measuring inference for batch_size=1:  14%|█▎        | 135/1000 [00:18<01:58,  7.30it/s]Measuring inference for batch_size=1:  14%|█▎        | 136/1000 [00:18<01:58,  7.31it/s]Measuring inference for batch_size=1:  14%|█▎        | 137/1000 [00:18<01:58,  7.30it/s]Measuring inference for batch_size=1:  14%|█▍        | 138/1000 [00:18<01:58,  7.30it/s]Measuring inference for batch_size=1:  14%|█▍        | 139/1000 [00:19<01:57,  7.30it/s]Measuring inference for batch_size=1:  14%|█▍        | 140/1000 [00:19<01:57,  7.29it/s]Measuring inference for batch_size=1:  14%|█▍        | 141/1000 [00:19<01:57,  7.29it/s]Measuring inference for batch_size=1:  14%|█▍        | 142/1000 [00:19<01:57,  7.29it/s]Measuring inference for batch_size=1:  14%|█▍        | 143/1000 [00:19<01:57,  7.29it/s]Measuring inference for batch_size=1:  14%|█▍        | 144/1000 [00:19<01:57,  7.29it/s]Measuring inference for batch_size=1:  14%|█▍        | 145/1000 [00:19<01:57,  7.28it/s]Measuring inference for batch_size=1:  15%|█▍        | 146/1000 [00:20<01:57,  7.29it/s]Measuring inference for batch_size=1:  15%|█▍        | 147/1000 [00:20<01:56,  7.29it/s]Measuring inference for batch_size=1:  15%|█▍        | 148/1000 [00:20<01:56,  7.29it/s]Measuring inference for batch_size=1:  15%|█▍        | 149/1000 [00:20<01:56,  7.29it/s]Measuring inference for batch_size=1:  15%|█▌        | 150/1000 [00:20<01:56,  7.29it/s]Measuring inference for batch_size=1:  15%|█▌        | 151/1000 [00:20<01:56,  7.28it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:20<01:56,  7.28it/s]Measuring inference for batch_size=1:  15%|█▌        | 153/1000 [00:20<01:56,  7.29it/s]Measuring inference for batch_size=1:  15%|█▌        | 154/1000 [00:21<01:56,  7.28it/s]Measuring inference for batch_size=1:  16%|█▌        | 155/1000 [00:21<01:55,  7.29it/s]Measuring inference for batch_size=1:  16%|█▌        | 156/1000 [00:21<01:55,  7.28it/s]Measuring inference for batch_size=1:  16%|█▌        | 157/1000 [00:21<01:55,  7.29it/s]Measuring inference for batch_size=1:  16%|█▌        | 158/1000 [00:21<01:55,  7.28it/s]Measuring inference for batch_size=1:  16%|█▌        | 159/1000 [00:21<01:55,  7.29it/s]Measuring inference for batch_size=1:  16%|█▌        | 160/1000 [00:21<01:55,  7.29it/s]Measuring inference for batch_size=1:  16%|█▌        | 161/1000 [00:22<01:55,  7.29it/s]Measuring inference for batch_size=1:  16%|█▌        | 162/1000 [00:22<01:55,  7.29it/s]Measuring inference for batch_size=1:  16%|█▋        | 163/1000 [00:22<01:54,  7.29it/s]Measuring inference for batch_size=1:  16%|█▋        | 164/1000 [00:22<01:54,  7.29it/s]Measuring inference for batch_size=1:  16%|█▋        | 165/1000 [00:22<01:54,  7.28it/s]Measuring inference for batch_size=1:  17%|█▋        | 166/1000 [00:22<01:54,  7.29it/s]Measuring inference for batch_size=1:  17%|█▋        | 167/1000 [00:22<01:54,  7.28it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:23<01:54,  7.27it/s]Measuring inference for batch_size=1:  17%|█▋        | 169/1000 [00:23<01:54,  7.27it/s]Measuring inference for batch_size=1:  17%|█▋        | 170/1000 [00:23<01:54,  7.27it/s]Measuring inference for batch_size=1:  17%|█▋        | 171/1000 [00:23<01:53,  7.28it/s]Measuring inference for batch_size=1:  17%|█▋        | 172/1000 [00:23<01:53,  7.28it/s]Measuring inference for batch_size=1:  17%|█▋        | 173/1000 [00:23<01:53,  7.28it/s]Measuring inference for batch_size=1:  17%|█▋        | 174/1000 [00:23<01:53,  7.28it/s]Measuring inference for batch_size=1:  18%|█▊        | 175/1000 [00:24<01:53,  7.28it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:24<01:53,  7.28it/s]Measuring inference for batch_size=1:  18%|█▊        | 177/1000 [00:24<01:53,  7.28it/s]Measuring inference for batch_size=1:  18%|█▊        | 178/1000 [00:24<01:52,  7.28it/s]Measuring inference for batch_size=1:  18%|█▊        | 179/1000 [00:24<01:52,  7.28it/s]Measuring inference for batch_size=1:  18%|█▊        | 180/1000 [00:24<01:52,  7.28it/s]Measuring inference for batch_size=1:  18%|█▊        | 181/1000 [00:24<01:52,  7.28it/s]Measuring inference for batch_size=1:  18%|█▊        | 182/1000 [00:24<01:52,  7.29it/s]Measuring inference for batch_size=1:  18%|█▊        | 183/1000 [00:25<01:52,  7.28it/s]Measuring inference for batch_size=1:  18%|█▊        | 184/1000 [00:25<01:52,  7.28it/s]Measuring inference for batch_size=1:  18%|█▊        | 185/1000 [00:25<01:51,  7.29it/s]Measuring inference for batch_size=1:  19%|█▊        | 186/1000 [00:25<01:51,  7.28it/s]Measuring inference for batch_size=1:  19%|█▊        | 187/1000 [00:25<01:51,  7.28it/s]Measuring inference for batch_size=1:  19%|█▉        | 188/1000 [00:25<01:51,  7.28it/s]Measuring inference for batch_size=1:  19%|█▉        | 189/1000 [00:25<01:51,  7.28it/s]Measuring inference for batch_size=1:  19%|█▉        | 190/1000 [00:26<01:51,  7.28it/s]Measuring inference for batch_size=1:  19%|█▉        | 191/1000 [00:26<01:51,  7.28it/s]Measuring inference for batch_size=1:  19%|█▉        | 192/1000 [00:26<01:51,  7.28it/s]Measuring inference for batch_size=1:  19%|█▉        | 193/1000 [00:26<01:50,  7.28it/s]Measuring inference for batch_size=1:  19%|█▉        | 194/1000 [00:26<01:50,  7.28it/s]Measuring inference for batch_size=1:  20%|█▉        | 195/1000 [00:26<01:50,  7.28it/s]Measuring inference for batch_size=1:  20%|█▉        | 196/1000 [00:26<01:50,  7.29it/s]Measuring inference for batch_size=1:  20%|█▉        | 197/1000 [00:27<01:50,  7.29it/s]Measuring inference for batch_size=1:  20%|█▉        | 198/1000 [00:27<01:50,  7.29it/s]Measuring inference for batch_size=1:  20%|█▉        | 199/1000 [00:27<01:49,  7.29it/s]Measuring inference for batch_size=1:  20%|██        | 200/1000 [00:27<01:49,  7.29it/s]Measuring inference for batch_size=1:  20%|██        | 201/1000 [00:27<01:49,  7.29it/s]Measuring inference for batch_size=1:  20%|██        | 202/1000 [00:27<01:49,  7.29it/s]Measuring inference for batch_size=1:  20%|██        | 203/1000 [00:27<01:49,  7.29it/s]Measuring inference for batch_size=1:  20%|██        | 204/1000 [00:27<01:49,  7.29it/s]Measuring inference for batch_size=1:  20%|██        | 205/1000 [00:28<01:49,  7.28it/s]Measuring inference for batch_size=1:  21%|██        | 206/1000 [00:28<01:48,  7.29it/s]Measuring inference for batch_size=1:  21%|██        | 207/1000 [00:28<01:48,  7.29it/s]Measuring inference for batch_size=1:  21%|██        | 208/1000 [00:28<01:48,  7.29it/s]Measuring inference for batch_size=1:  21%|██        | 209/1000 [00:28<01:48,  7.29it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:28<01:48,  7.29it/s]Measuring inference for batch_size=1:  21%|██        | 211/1000 [00:28<01:48,  7.28it/s]Measuring inference for batch_size=1:  21%|██        | 212/1000 [00:29<01:48,  7.28it/s]Measuring inference for batch_size=1:  21%|██▏       | 213/1000 [00:29<01:48,  7.28it/s]Measuring inference for batch_size=1:  21%|██▏       | 214/1000 [00:29<01:47,  7.29it/s]Measuring inference for batch_size=1:  22%|██▏       | 215/1000 [00:29<01:47,  7.29it/s]Measuring inference for batch_size=1:  22%|██▏       | 216/1000 [00:29<01:47,  7.28it/s]Measuring inference for batch_size=1:  22%|██▏       | 217/1000 [00:29<01:47,  7.28it/s]Measuring inference for batch_size=1:  22%|██▏       | 218/1000 [00:29<01:47,  7.28it/s]Measuring inference for batch_size=1:  22%|██▏       | 219/1000 [00:30<01:47,  7.27it/s]Measuring inference for batch_size=1:  22%|██▏       | 220/1000 [00:30<01:47,  7.28it/s]Measuring inference for batch_size=1:  22%|██▏       | 221/1000 [00:30<01:46,  7.28it/s]Measuring inference for batch_size=1:  22%|██▏       | 222/1000 [00:30<01:46,  7.28it/s]Measuring inference for batch_size=1:  22%|██▏       | 223/1000 [00:30<01:46,  7.28it/s]Measuring inference for batch_size=1:  22%|██▏       | 224/1000 [00:30<01:46,  7.28it/s]Measuring inference for batch_size=1:  22%|██▎       | 225/1000 [00:30<01:46,  7.28it/s]Measuring inference for batch_size=1:  23%|██▎       | 226/1000 [00:31<01:46,  7.28it/s]Measuring inference for batch_size=1:  23%|██▎       | 227/1000 [00:31<01:46,  7.28it/s]Measuring inference for batch_size=1:  23%|██▎       | 228/1000 [00:31<01:46,  7.28it/s]Measuring inference for batch_size=1:  23%|██▎       | 229/1000 [00:31<01:45,  7.28it/s]Measuring inference for batch_size=1:  23%|██▎       | 230/1000 [00:31<01:45,  7.28it/s]Measuring inference for batch_size=1:  23%|██▎       | 231/1000 [00:31<01:45,  7.28it/s]Measuring inference for batch_size=1:  23%|██▎       | 232/1000 [00:31<01:45,  7.28it/s]Measuring inference for batch_size=1:  23%|██▎       | 233/1000 [00:31<01:45,  7.29it/s]Measuring inference for batch_size=1:  23%|██▎       | 234/1000 [00:32<01:45,  7.29it/s]Measuring inference for batch_size=1:  24%|██▎       | 235/1000 [00:32<01:44,  7.29it/s]Measuring inference for batch_size=1:  24%|██▎       | 236/1000 [00:32<01:44,  7.29it/s]Measuring inference for batch_size=1:  24%|██▎       | 237/1000 [00:32<01:44,  7.30it/s]Measuring inference for batch_size=1:  24%|██▍       | 238/1000 [00:32<01:44,  7.29it/s]Measuring inference for batch_size=1:  24%|██▍       | 239/1000 [00:32<01:44,  7.29it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:32<01:44,  7.29it/s]Measuring inference for batch_size=1:  24%|██▍       | 241/1000 [00:33<01:44,  7.29it/s]Measuring inference for batch_size=1:  24%|██▍       | 242/1000 [00:33<01:43,  7.30it/s]Measuring inference for batch_size=1:  24%|██▍       | 243/1000 [00:33<01:43,  7.29it/s]Measuring inference for batch_size=1:  24%|██▍       | 244/1000 [00:33<01:43,  7.29it/s]Measuring inference for batch_size=1:  24%|██▍       | 245/1000 [00:33<01:43,  7.29it/s]Measuring inference for batch_size=1:  25%|██▍       | 246/1000 [00:33<01:43,  7.27it/s]Measuring inference for batch_size=1:  25%|██▍       | 247/1000 [00:33<01:43,  7.28it/s]Measuring inference for batch_size=1:  25%|██▍       | 248/1000 [00:34<01:43,  7.28it/s]Measuring inference for batch_size=1:  25%|██▍       | 249/1000 [00:34<01:43,  7.28it/s]Measuring inference for batch_size=1:  25%|██▌       | 250/1000 [00:34<01:42,  7.29it/s]Measuring inference for batch_size=1:  25%|██▌       | 251/1000 [00:34<01:42,  7.28it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:34<01:42,  7.28it/s]Measuring inference for batch_size=1:  25%|██▌       | 253/1000 [00:34<01:42,  7.28it/s]Measuring inference for batch_size=1:  25%|██▌       | 254/1000 [00:34<01:42,  7.28it/s]Measuring inference for batch_size=1:  26%|██▌       | 255/1000 [00:35<01:42,  7.28it/s]Measuring inference for batch_size=1:  26%|██▌       | 256/1000 [00:35<01:42,  7.28it/s]Measuring inference for batch_size=1:  26%|██▌       | 257/1000 [00:35<01:42,  7.28it/s]Measuring inference for batch_size=1:  26%|██▌       | 258/1000 [00:35<01:41,  7.29it/s]Measuring inference for batch_size=1:  26%|██▌       | 259/1000 [00:35<01:41,  7.29it/s]Measuring inference for batch_size=1:  26%|██▌       | 260/1000 [00:35<01:41,  7.28it/s]Measuring inference for batch_size=1:  26%|██▌       | 261/1000 [00:35<01:41,  7.28it/s]Measuring inference for batch_size=1:  26%|██▌       | 262/1000 [00:35<01:41,  7.28it/s]Measuring inference for batch_size=1:  26%|██▋       | 263/1000 [00:36<01:41,  7.29it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:36<01:40,  7.29it/s]Measuring inference for batch_size=1:  26%|██▋       | 265/1000 [00:36<01:40,  7.29it/s]Measuring inference for batch_size=1:  27%|██▋       | 266/1000 [00:36<01:40,  7.29it/s]Measuring inference for batch_size=1:  27%|██▋       | 267/1000 [00:36<01:40,  7.28it/s]Measuring inference for batch_size=1:  27%|██▋       | 268/1000 [00:36<01:40,  7.28it/s]Measuring inference for batch_size=1:  27%|██▋       | 269/1000 [00:36<01:40,  7.28it/s]Measuring inference for batch_size=1:  27%|██▋       | 270/1000 [00:37<01:40,  7.28it/s]Measuring inference for batch_size=1:  27%|██▋       | 271/1000 [00:37<01:40,  7.28it/s]Measuring inference for batch_size=1:  27%|██▋       | 272/1000 [00:37<01:39,  7.28it/s]Measuring inference for batch_size=1:  27%|██▋       | 273/1000 [00:37<01:39,  7.28it/s]Measuring inference for batch_size=1:  27%|██▋       | 274/1000 [00:37<01:39,  7.28it/s]Measuring inference for batch_size=1:  28%|██▊       | 275/1000 [00:37<01:39,  7.29it/s]Measuring inference for batch_size=1:  28%|██▊       | 276/1000 [00:37<01:39,  7.28it/s]Measuring inference for batch_size=1:  28%|██▊       | 277/1000 [00:38<01:39,  7.29it/s]Measuring inference for batch_size=1:  28%|██▊       | 278/1000 [00:38<01:39,  7.28it/s]Measuring inference for batch_size=1:  28%|██▊       | 279/1000 [00:38<01:38,  7.28it/s]Measuring inference for batch_size=1:  28%|██▊       | 280/1000 [00:38<01:38,  7.29it/s]Measuring inference for batch_size=1:  28%|██▊       | 281/1000 [00:38<01:38,  7.29it/s]Measuring inference for batch_size=1:  28%|██▊       | 282/1000 [00:38<01:38,  7.29it/s]Measuring inference for batch_size=1:  28%|██▊       | 283/1000 [00:38<01:38,  7.30it/s]Measuring inference for batch_size=1:  28%|██▊       | 284/1000 [00:38<01:38,  7.29it/s]Measuring inference for batch_size=1:  28%|██▊       | 285/1000 [00:39<01:38,  7.29it/s]Measuring inference for batch_size=1:  29%|██▊       | 286/1000 [00:39<01:38,  7.28it/s]Measuring inference for batch_size=1:  29%|██▊       | 287/1000 [00:39<01:37,  7.28it/s]Measuring inference for batch_size=1:  29%|██▉       | 288/1000 [00:39<01:37,  7.29it/s]Measuring inference for batch_size=1:  29%|██▉       | 289/1000 [00:39<01:37,  7.29it/s]Measuring inference for batch_size=1:  29%|██▉       | 290/1000 [00:39<01:37,  7.29it/s]Measuring inference for batch_size=1:  29%|██▉       | 291/1000 [00:39<01:37,  7.29it/s]Measuring inference for batch_size=1:  29%|██▉       | 292/1000 [00:40<01:37,  7.28it/s]Measuring inference for batch_size=1:  29%|██▉       | 293/1000 [00:40<01:37,  7.28it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:40<01:36,  7.28it/s]Measuring inference for batch_size=1:  30%|██▉       | 295/1000 [00:40<01:36,  7.28it/s]Measuring inference for batch_size=1:  30%|██▉       | 296/1000 [00:40<01:36,  7.28it/s]Measuring inference for batch_size=1:  30%|██▉       | 297/1000 [00:40<01:36,  7.28it/s]Measuring inference for batch_size=1:  30%|██▉       | 298/1000 [00:40<01:36,  7.28it/s]Measuring inference for batch_size=1:  30%|██▉       | 299/1000 [00:41<01:36,  7.28it/s]Measuring inference for batch_size=1:  30%|███       | 300/1000 [00:41<01:36,  7.28it/s]Measuring inference for batch_size=1:  30%|███       | 301/1000 [00:41<01:36,  7.28it/s]Measuring inference for batch_size=1:  30%|███       | 302/1000 [00:41<01:35,  7.29it/s]Measuring inference for batch_size=1:  30%|███       | 303/1000 [00:41<01:35,  7.28it/s]Measuring inference for batch_size=1:  30%|███       | 304/1000 [00:41<01:35,  7.29it/s]Measuring inference for batch_size=1:  30%|███       | 305/1000 [00:41<01:35,  7.29it/s]Measuring inference for batch_size=1:  31%|███       | 306/1000 [00:42<01:35,  7.29it/s]Measuring inference for batch_size=1:  31%|███       | 307/1000 [00:42<01:35,  7.29it/s]Measuring inference for batch_size=1:  31%|███       | 308/1000 [00:42<01:34,  7.29it/s]Measuring inference for batch_size=1:  31%|███       | 309/1000 [00:42<01:34,  7.29it/s]Measuring inference for batch_size=1:  31%|███       | 310/1000 [00:42<01:34,  7.29it/s]Measuring inference for batch_size=1:  31%|███       | 311/1000 [00:42<01:34,  7.29it/s]Measuring inference for batch_size=1:  31%|███       | 312/1000 [00:42<01:34,  7.29it/s]Measuring inference for batch_size=1:  31%|███▏      | 313/1000 [00:42<01:34,  7.28it/s]Measuring inference for batch_size=1:  31%|███▏      | 314/1000 [00:43<01:34,  7.28it/s]Measuring inference for batch_size=1:  32%|███▏      | 315/1000 [00:43<01:33,  7.29it/s]Measuring inference for batch_size=1:  32%|███▏      | 316/1000 [00:43<01:33,  7.29it/s]Measuring inference for batch_size=1:  32%|███▏      | 317/1000 [00:43<01:33,  7.29it/s]Measuring inference for batch_size=1:  32%|███▏      | 318/1000 [00:43<01:33,  7.29it/s]Measuring inference for batch_size=1:  32%|███▏      | 319/1000 [00:43<01:33,  7.29it/s]Measuring inference for batch_size=1:  32%|███▏      | 320/1000 [00:43<01:33,  7.29it/s]Measuring inference for batch_size=1:  32%|███▏      | 321/1000 [00:44<01:33,  7.29it/s]Measuring inference for batch_size=1:  32%|███▏      | 322/1000 [00:44<01:33,  7.28it/s]Measuring inference for batch_size=1:  32%|███▏      | 323/1000 [00:44<01:32,  7.28it/s]Measuring inference for batch_size=1:  32%|███▏      | 324/1000 [00:44<01:32,  7.28it/s]Measuring inference for batch_size=1:  32%|███▎      | 325/1000 [00:44<01:32,  7.28it/s]Measuring inference for batch_size=1:  33%|███▎      | 326/1000 [00:44<01:32,  7.28it/s]Measuring inference for batch_size=1:  33%|███▎      | 327/1000 [00:44<01:32,  7.28it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:45<01:32,  7.28it/s]Measuring inference for batch_size=1:  33%|███▎      | 329/1000 [00:45<01:32,  7.28it/s]Measuring inference for batch_size=1:  33%|███▎      | 330/1000 [00:45<01:32,  7.28it/s]Measuring inference for batch_size=1:  33%|███▎      | 331/1000 [00:45<01:31,  7.28it/s]Measuring inference for batch_size=1:  33%|███▎      | 332/1000 [00:45<01:31,  7.28it/s]Measuring inference for batch_size=1:  33%|███▎      | 333/1000 [00:45<01:31,  7.28it/s]Measuring inference for batch_size=1:  33%|███▎      | 334/1000 [00:45<01:31,  7.28it/s]Measuring inference for batch_size=1:  34%|███▎      | 335/1000 [00:45<01:31,  7.28it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:46<01:31,  7.28it/s]Measuring inference for batch_size=1:  34%|███▎      | 337/1000 [00:46<01:30,  7.29it/s]Measuring inference for batch_size=1:  34%|███▍      | 338/1000 [00:46<01:30,  7.29it/s]Measuring inference for batch_size=1:  34%|███▍      | 339/1000 [00:46<01:30,  7.28it/s]Measuring inference for batch_size=1:  34%|███▍      | 340/1000 [00:46<01:30,  7.28it/s]Measuring inference for batch_size=1:  34%|███▍      | 341/1000 [00:46<01:30,  7.28it/s]Measuring inference for batch_size=1:  34%|███▍      | 342/1000 [00:46<01:30,  7.28it/s]Measuring inference for batch_size=1:  34%|███▍      | 343/1000 [00:47<01:30,  7.29it/s]Measuring inference for batch_size=1:  34%|███▍      | 344/1000 [00:47<01:30,  7.29it/s]Measuring inference for batch_size=1:  34%|███▍      | 345/1000 [00:47<01:29,  7.29it/s]Measuring inference for batch_size=1:  35%|███▍      | 346/1000 [00:47<01:29,  7.28it/s]Measuring inference for batch_size=1:  35%|███▍      | 347/1000 [00:47<01:29,  7.28it/s]Measuring inference for batch_size=1:  35%|███▍      | 348/1000 [00:47<01:29,  7.28it/s]Measuring inference for batch_size=1:  35%|███▍      | 349/1000 [00:47<01:29,  7.28it/s]Measuring inference for batch_size=1:  35%|███▌      | 350/1000 [00:48<01:29,  7.28it/s]Measuring inference for batch_size=1:  35%|███▌      | 351/1000 [00:48<01:29,  7.27it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:48<01:29,  7.28it/s]Measuring inference for batch_size=1:  35%|███▌      | 353/1000 [00:48<01:28,  7.27it/s]Measuring inference for batch_size=1:  35%|███▌      | 354/1000 [00:48<01:28,  7.27it/s]Measuring inference for batch_size=1:  36%|███▌      | 355/1000 [00:48<01:28,  7.27it/s]Measuring inference for batch_size=1:  36%|███▌      | 356/1000 [00:48<01:28,  7.28it/s]Measuring inference for batch_size=1:  36%|███▌      | 357/1000 [00:49<01:28,  7.27it/s]Measuring inference for batch_size=1:  36%|███▌      | 358/1000 [00:49<01:28,  7.28it/s]Measuring inference for batch_size=1:  36%|███▌      | 359/1000 [00:49<01:28,  7.27it/s]Measuring inference for batch_size=1:  36%|███▌      | 360/1000 [00:49<01:27,  7.27it/s]Measuring inference for batch_size=1:  36%|███▌      | 361/1000 [00:49<01:27,  7.28it/s]Measuring inference for batch_size=1:  36%|███▌      | 362/1000 [00:49<01:27,  7.29it/s]Measuring inference for batch_size=1:  36%|███▋      | 363/1000 [00:49<01:27,  7.29it/s]Measuring inference for batch_size=1:  36%|███▋      | 364/1000 [00:49<01:27,  7.29it/s]Measuring inference for batch_size=1:  36%|███▋      | 365/1000 [00:50<01:27,  7.29it/s]Measuring inference for batch_size=1:  37%|███▋      | 366/1000 [00:50<01:27,  7.29it/s]Measuring inference for batch_size=1:  37%|███▋      | 367/1000 [00:50<01:26,  7.29it/s]Measuring inference for batch_size=1:  37%|███▋      | 368/1000 [00:50<01:26,  7.29it/s]Measuring inference for batch_size=1:  37%|███▋      | 369/1000 [00:50<01:26,  7.29it/s]Measuring inference for batch_size=1:  37%|███▋      | 370/1000 [00:50<01:26,  7.30it/s]Measuring inference for batch_size=1:  37%|███▋      | 371/1000 [00:50<01:26,  7.30it/s]Measuring inference for batch_size=1:  37%|███▋      | 372/1000 [00:51<01:26,  7.29it/s]Measuring inference for batch_size=1:  37%|███▋      | 373/1000 [00:51<01:25,  7.29it/s]Measuring inference for batch_size=1:  37%|███▋      | 374/1000 [00:51<01:25,  7.29it/s]Measuring inference for batch_size=1:  38%|███▊      | 375/1000 [00:51<01:25,  7.28it/s]Measuring inference for batch_size=1:  38%|███▊      | 376/1000 [00:51<01:25,  7.28it/s]Measuring inference for batch_size=1:  38%|███▊      | 377/1000 [00:51<01:25,  7.28it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:51<01:25,  7.28it/s]Measuring inference for batch_size=1:  38%|███▊      | 379/1000 [00:52<01:25,  7.28it/s]Measuring inference for batch_size=1:  38%|███▊      | 380/1000 [00:52<01:25,  7.28it/s]Measuring inference for batch_size=1:  38%|███▊      | 381/1000 [00:52<01:25,  7.28it/s]Measuring inference for batch_size=1:  38%|███▊      | 382/1000 [00:52<01:24,  7.28it/s]Measuring inference for batch_size=1:  38%|███▊      | 383/1000 [00:52<01:24,  7.28it/s]Measuring inference for batch_size=1:  38%|███▊      | 384/1000 [00:52<01:24,  7.28it/s]Measuring inference for batch_size=1:  38%|███▊      | 385/1000 [00:52<01:24,  7.28it/s]Measuring inference for batch_size=1:  39%|███▊      | 386/1000 [00:52<01:24,  7.28it/s]Measuring inference for batch_size=1:  39%|███▊      | 387/1000 [00:53<01:24,  7.28it/s]Measuring inference for batch_size=1:  39%|███▉      | 388/1000 [00:53<01:24,  7.28it/s]Measuring inference for batch_size=1:  39%|███▉      | 389/1000 [00:53<01:23,  7.27it/s]Measuring inference for batch_size=1:  39%|███▉      | 390/1000 [00:53<01:23,  7.28it/s]Measuring inference for batch_size=1:  39%|███▉      | 391/1000 [00:53<01:23,  7.28it/s]Measuring inference for batch_size=1:  39%|███▉      | 392/1000 [00:53<01:23,  7.28it/s]Measuring inference for batch_size=1:  39%|███▉      | 393/1000 [00:53<01:23,  7.27it/s]Measuring inference for batch_size=1:  39%|███▉      | 394/1000 [00:54<01:23,  7.28it/s]Measuring inference for batch_size=1:  40%|███▉      | 395/1000 [00:54<01:23,  7.28it/s]Measuring inference for batch_size=1:  40%|███▉      | 396/1000 [00:54<01:22,  7.28it/s]Measuring inference for batch_size=1:  40%|███▉      | 397/1000 [00:54<01:22,  7.28it/s]Measuring inference for batch_size=1:  40%|███▉      | 398/1000 [00:54<01:22,  7.28it/s]Measuring inference for batch_size=1:  40%|███▉      | 399/1000 [00:54<01:22,  7.28it/s]Measuring inference for batch_size=1:  40%|████      | 400/1000 [00:54<01:22,  7.28it/s]Measuring inference for batch_size=1:  40%|████      | 401/1000 [00:55<01:22,  7.27it/s]Measuring inference for batch_size=1:  40%|████      | 402/1000 [00:55<01:22,  7.28it/s]Measuring inference for batch_size=1:  40%|████      | 403/1000 [00:55<01:21,  7.28it/s]Measuring inference for batch_size=1:  40%|████      | 404/1000 [00:55<01:21,  7.28it/s]Measuring inference for batch_size=1:  40%|████      | 405/1000 [00:55<01:21,  7.28it/s]Measuring inference for batch_size=1:  41%|████      | 406/1000 [00:55<01:21,  7.28it/s]Measuring inference for batch_size=1:  41%|████      | 407/1000 [00:55<01:21,  7.28it/s]Measuring inference for batch_size=1:  41%|████      | 408/1000 [00:56<01:21,  7.28it/s]Measuring inference for batch_size=1:  41%|████      | 409/1000 [00:56<01:21,  7.28it/s]Measuring inference for batch_size=1:  41%|████      | 410/1000 [00:56<01:20,  7.29it/s]Measuring inference for batch_size=1:  41%|████      | 411/1000 [00:56<01:20,  7.28it/s]Measuring inference for batch_size=1:  41%|████      | 412/1000 [00:56<01:20,  7.28it/s]Measuring inference for batch_size=1:  41%|████▏     | 413/1000 [00:56<01:20,  7.28it/s]Measuring inference for batch_size=1:  41%|████▏     | 414/1000 [00:56<01:20,  7.28it/s]Measuring inference for batch_size=1:  42%|████▏     | 415/1000 [00:56<01:20,  7.28it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:57<01:20,  7.28it/s]Measuring inference for batch_size=1:  42%|████▏     | 417/1000 [00:57<01:20,  7.28it/s]Measuring inference for batch_size=1:  42%|████▏     | 418/1000 [00:57<01:19,  7.28it/s]Measuring inference for batch_size=1:  42%|████▏     | 419/1000 [00:57<01:19,  7.28it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:57<01:19,  7.28it/s]Measuring inference for batch_size=1:  42%|████▏     | 421/1000 [00:57<01:19,  7.28it/s]Measuring inference for batch_size=1:  42%|████▏     | 422/1000 [00:57<01:19,  7.27it/s]Measuring inference for batch_size=1:  42%|████▏     | 423/1000 [00:58<01:19,  7.27it/s]Measuring inference for batch_size=1:  42%|████▏     | 424/1000 [00:58<01:19,  7.27it/s]Measuring inference for batch_size=1:  42%|████▎     | 425/1000 [00:58<01:19,  7.27it/s]Measuring inference for batch_size=1:  43%|████▎     | 426/1000 [00:58<01:18,  7.28it/s]Measuring inference for batch_size=1:  43%|████▎     | 427/1000 [00:58<01:18,  7.27it/s]Measuring inference for batch_size=1:  43%|████▎     | 428/1000 [00:58<01:18,  7.28it/s]Measuring inference for batch_size=1:  43%|████▎     | 429/1000 [00:58<01:18,  7.28it/s]Measuring inference for batch_size=1:  43%|████▎     | 430/1000 [00:59<01:18,  7.28it/s]Measuring inference for batch_size=1:  43%|████▎     | 431/1000 [00:59<01:18,  7.27it/s]Measuring inference for batch_size=1:  43%|████▎     | 432/1000 [00:59<01:18,  7.28it/s]Measuring inference for batch_size=1:  43%|████▎     | 433/1000 [00:59<01:17,  7.28it/s]Measuring inference for batch_size=1:  43%|████▎     | 434/1000 [00:59<01:17,  7.29it/s]Measuring inference for batch_size=1:  44%|████▎     | 435/1000 [00:59<01:17,  7.29it/s]Measuring inference for batch_size=1:  44%|████▎     | 436/1000 [00:59<01:17,  7.28it/s]Measuring inference for batch_size=1:  44%|████▎     | 437/1000 [00:59<01:17,  7.29it/s]Measuring inference for batch_size=1:  44%|████▍     | 438/1000 [01:00<01:17,  7.29it/s]Measuring inference for batch_size=1:  44%|████▍     | 439/1000 [01:00<01:17,  7.29it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [01:00<01:16,  7.29it/s]Measuring inference for batch_size=1:  44%|████▍     | 441/1000 [01:00<01:16,  7.29it/s]Measuring inference for batch_size=1:  44%|████▍     | 442/1000 [01:00<01:16,  7.29it/s]Measuring inference for batch_size=1:  44%|████▍     | 443/1000 [01:00<01:16,  7.29it/s]Measuring inference for batch_size=1:  44%|████▍     | 444/1000 [01:00<01:16,  7.28it/s]Measuring inference for batch_size=1:  44%|████▍     | 445/1000 [01:01<01:16,  7.28it/s]Measuring inference for batch_size=1:  45%|████▍     | 446/1000 [01:01<01:16,  7.28it/s]Measuring inference for batch_size=1:  45%|████▍     | 447/1000 [01:01<01:15,  7.28it/s]Measuring inference for batch_size=1:  45%|████▍     | 448/1000 [01:01<01:15,  7.29it/s]Measuring inference for batch_size=1:  45%|████▍     | 449/1000 [01:01<01:15,  7.29it/s]Measuring inference for batch_size=1:  45%|████▌     | 450/1000 [01:01<01:15,  7.28it/s]Measuring inference for batch_size=1:  45%|████▌     | 451/1000 [01:01<01:15,  7.29it/s]Measuring inference for batch_size=1:  45%|████▌     | 452/1000 [01:02<01:15,  7.28it/s]Measuring inference for batch_size=1:  45%|████▌     | 453/1000 [01:02<01:15,  7.28it/s]Measuring inference for batch_size=1:  45%|████▌     | 454/1000 [01:02<01:14,  7.28it/s]Measuring inference for batch_size=1:  46%|████▌     | 455/1000 [01:02<01:14,  7.28it/s]Measuring inference for batch_size=1:  46%|████▌     | 456/1000 [01:02<01:14,  7.28it/s]Measuring inference for batch_size=1:  46%|████▌     | 457/1000 [01:02<01:14,  7.28it/s]Measuring inference for batch_size=1:  46%|████▌     | 458/1000 [01:02<01:14,  7.28it/s]Measuring inference for batch_size=1:  46%|████▌     | 459/1000 [01:03<01:14,  7.28it/s]Measuring inference for batch_size=1:  46%|████▌     | 460/1000 [01:03<01:14,  7.28it/s]Measuring inference for batch_size=1:  46%|████▌     | 461/1000 [01:03<01:14,  7.28it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [01:03<01:13,  7.28it/s]Measuring inference for batch_size=1:  46%|████▋     | 463/1000 [01:03<01:13,  7.28it/s]Measuring inference for batch_size=1:  46%|████▋     | 464/1000 [01:03<01:13,  7.28it/s]Measuring inference for batch_size=1:  46%|████▋     | 465/1000 [01:03<01:13,  7.28it/s]Measuring inference for batch_size=1:  47%|████▋     | 466/1000 [01:03<01:13,  7.28it/s]Measuring inference for batch_size=1:  47%|████▋     | 467/1000 [01:04<01:13,  7.28it/s]Measuring inference for batch_size=1:  47%|████▋     | 468/1000 [01:04<01:13,  7.29it/s]Measuring inference for batch_size=1:  47%|████▋     | 469/1000 [01:04<01:12,  7.29it/s]Measuring inference for batch_size=1:  47%|████▋     | 470/1000 [01:04<01:12,  7.29it/s]Measuring inference for batch_size=1:  47%|████▋     | 471/1000 [01:04<01:12,  7.29it/s]Measuring inference for batch_size=1:  47%|████▋     | 472/1000 [01:04<01:12,  7.29it/s]Measuring inference for batch_size=1:  47%|████▋     | 473/1000 [01:04<01:12,  7.28it/s]Measuring inference for batch_size=1:  47%|████▋     | 474/1000 [01:05<01:12,  7.27it/s]Measuring inference for batch_size=1:  48%|████▊     | 475/1000 [01:05<01:12,  7.28it/s]Measuring inference for batch_size=1:  48%|████▊     | 476/1000 [01:05<01:12,  7.27it/s]Measuring inference for batch_size=1:  48%|████▊     | 477/1000 [01:05<01:12,  7.25it/s]Measuring inference for batch_size=1:  48%|████▊     | 478/1000 [01:05<01:12,  7.24it/s]Measuring inference for batch_size=1:  48%|████▊     | 479/1000 [01:05<01:11,  7.24it/s]Measuring inference for batch_size=1:  48%|████▊     | 480/1000 [01:05<01:11,  7.25it/s]Measuring inference for batch_size=1:  48%|████▊     | 481/1000 [01:06<01:11,  7.26it/s]Measuring inference for batch_size=1:  48%|████▊     | 482/1000 [01:06<01:11,  7.27it/s]Measuring inference for batch_size=1:  48%|████▊     | 483/1000 [01:06<01:11,  7.27it/s]Measuring inference for batch_size=1:  48%|████▊     | 484/1000 [01:06<01:10,  7.28it/s]Measuring inference for batch_size=1:  48%|████▊     | 485/1000 [01:06<01:10,  7.28it/s]Measuring inference for batch_size=1:  49%|████▊     | 486/1000 [01:06<01:10,  7.29it/s]Measuring inference for batch_size=1:  49%|████▊     | 487/1000 [01:06<01:10,  7.28it/s]Measuring inference for batch_size=1:  49%|████▉     | 488/1000 [01:06<01:10,  7.28it/s]Measuring inference for batch_size=1:  49%|████▉     | 489/1000 [01:07<01:10,  7.28it/s]Measuring inference for batch_size=1:  49%|████▉     | 490/1000 [01:07<01:10,  7.28it/s]Measuring inference for batch_size=1:  49%|████▉     | 491/1000 [01:07<01:09,  7.29it/s]Measuring inference for batch_size=1:  49%|████▉     | 492/1000 [01:07<01:09,  7.29it/s]Measuring inference for batch_size=1:  49%|████▉     | 493/1000 [01:07<01:09,  7.29it/s]Measuring inference for batch_size=1:  49%|████▉     | 494/1000 [01:07<01:09,  7.29it/s]Measuring inference for batch_size=1:  50%|████▉     | 495/1000 [01:07<01:09,  7.29it/s]Measuring inference for batch_size=1:  50%|████▉     | 496/1000 [01:08<01:09,  7.29it/s]Measuring inference for batch_size=1:  50%|████▉     | 497/1000 [01:08<01:08,  7.29it/s]Measuring inference for batch_size=1:  50%|████▉     | 498/1000 [01:08<01:08,  7.29it/s]Measuring inference for batch_size=1:  50%|████▉     | 499/1000 [01:08<01:08,  7.29it/s]Measuring inference for batch_size=1:  50%|█████     | 500/1000 [01:08<01:08,  7.29it/s]Measuring inference for batch_size=1:  50%|█████     | 501/1000 [01:08<01:08,  7.29it/s]Measuring inference for batch_size=1:  50%|█████     | 502/1000 [01:08<01:08,  7.29it/s]Measuring inference for batch_size=1:  50%|█████     | 503/1000 [01:09<01:08,  7.29it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [01:09<01:08,  7.29it/s]Measuring inference for batch_size=1:  50%|█████     | 505/1000 [01:09<01:07,  7.29it/s]Measuring inference for batch_size=1:  51%|█████     | 506/1000 [01:09<01:07,  7.29it/s]Measuring inference for batch_size=1:  51%|█████     | 507/1000 [01:09<01:07,  7.29it/s]Measuring inference for batch_size=1:  51%|█████     | 508/1000 [01:09<01:07,  7.29it/s]Measuring inference for batch_size=1:  51%|█████     | 509/1000 [01:09<01:07,  7.29it/s]Measuring inference for batch_size=1:  51%|█████     | 510/1000 [01:10<01:07,  7.29it/s]Measuring inference for batch_size=1:  51%|█████     | 511/1000 [01:10<01:07,  7.29it/s]Measuring inference for batch_size=1:  51%|█████     | 512/1000 [01:10<01:06,  7.29it/s]Measuring inference for batch_size=1:  51%|█████▏    | 513/1000 [01:10<01:06,  7.29it/s]Measuring inference for batch_size=1:  51%|█████▏    | 514/1000 [01:10<01:06,  7.29it/s]Measuring inference for batch_size=1:  52%|█████▏    | 515/1000 [01:10<01:06,  7.29it/s]Measuring inference for batch_size=1:  52%|█████▏    | 516/1000 [01:10<01:06,  7.29it/s]Measuring inference for batch_size=1:  52%|█████▏    | 517/1000 [01:10<01:06,  7.29it/s]Measuring inference for batch_size=1:  52%|█████▏    | 518/1000 [01:11<01:06,  7.29it/s]Measuring inference for batch_size=1:  52%|█████▏    | 519/1000 [01:11<01:05,  7.29it/s]Measuring inference for batch_size=1:  52%|█████▏    | 520/1000 [01:11<01:05,  7.29it/s]Measuring inference for batch_size=1:  52%|█████▏    | 521/1000 [01:11<01:05,  7.29it/s]Measuring inference for batch_size=1:  52%|█████▏    | 522/1000 [01:11<01:05,  7.29it/s]Measuring inference for batch_size=1:  52%|█████▏    | 523/1000 [01:11<01:05,  7.29it/s]Measuring inference for batch_size=1:  52%|█████▏    | 524/1000 [01:11<01:05,  7.29it/s]Measuring inference for batch_size=1:  52%|█████▎    | 525/1000 [01:12<01:05,  7.29it/s]Measuring inference for batch_size=1:  53%|█████▎    | 526/1000 [01:12<01:05,  7.29it/s]Measuring inference for batch_size=1:  53%|█████▎    | 527/1000 [01:12<01:04,  7.30it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [01:12<01:04,  7.29it/s]Measuring inference for batch_size=1:  53%|█████▎    | 529/1000 [01:12<01:04,  7.29it/s]Measuring inference for batch_size=1:  53%|█████▎    | 530/1000 [01:12<01:04,  7.29it/s]Measuring inference for batch_size=1:  53%|█████▎    | 531/1000 [01:12<01:04,  7.28it/s]Measuring inference for batch_size=1:  53%|█████▎    | 532/1000 [01:13<01:04,  7.28it/s]Measuring inference for batch_size=1:  53%|█████▎    | 533/1000 [01:13<01:04,  7.28it/s]Measuring inference for batch_size=1:  53%|█████▎    | 534/1000 [01:13<01:03,  7.29it/s]Measuring inference for batch_size=1:  54%|█████▎    | 535/1000 [01:13<01:03,  7.29it/s]Measuring inference for batch_size=1:  54%|█████▎    | 536/1000 [01:13<01:03,  7.29it/s]Measuring inference for batch_size=1:  54%|█████▎    | 537/1000 [01:13<01:03,  7.29it/s]Measuring inference for batch_size=1:  54%|█████▍    | 538/1000 [01:13<01:03,  7.29it/s]Measuring inference for batch_size=1:  54%|█████▍    | 539/1000 [01:13<01:03,  7.29it/s]Measuring inference for batch_size=1:  54%|█████▍    | 540/1000 [01:14<01:03,  7.29it/s]Measuring inference for batch_size=1:  54%|█████▍    | 541/1000 [01:14<01:03,  7.28it/s]Measuring inference for batch_size=1:  54%|█████▍    | 542/1000 [01:14<01:02,  7.28it/s]Measuring inference for batch_size=1:  54%|█████▍    | 543/1000 [01:14<01:02,  7.28it/s]Measuring inference for batch_size=1:  54%|█████▍    | 544/1000 [01:14<01:02,  7.29it/s]Measuring inference for batch_size=1:  55%|█████▍    | 545/1000 [01:14<01:02,  7.29it/s]Measuring inference for batch_size=1:  55%|█████▍    | 546/1000 [01:14<01:02,  7.29it/s]Measuring inference for batch_size=1:  55%|█████▍    | 547/1000 [01:15<01:02,  7.29it/s]Measuring inference for batch_size=1:  55%|█████▍    | 548/1000 [01:15<01:02,  7.29it/s]Measuring inference for batch_size=1:  55%|█████▍    | 549/1000 [01:15<01:01,  7.28it/s]Measuring inference for batch_size=1:  55%|█████▌    | 550/1000 [01:15<01:01,  7.27it/s]Measuring inference for batch_size=1:  55%|█████▌    | 551/1000 [01:15<01:01,  7.28it/s]Measuring inference for batch_size=1:  55%|█████▌    | 552/1000 [01:15<01:01,  7.28it/s]Measuring inference for batch_size=1:  55%|█████▌    | 553/1000 [01:15<01:01,  7.28it/s]Measuring inference for batch_size=1:  55%|█████▌    | 554/1000 [01:16<01:01,  7.28it/s]Measuring inference for batch_size=1:  56%|█████▌    | 555/1000 [01:16<01:01,  7.27it/s]Measuring inference for batch_size=1:  56%|█████▌    | 556/1000 [01:16<01:01,  7.27it/s]Measuring inference for batch_size=1:  56%|█████▌    | 557/1000 [01:16<01:00,  7.28it/s]Measuring inference for batch_size=1:  56%|█████▌    | 558/1000 [01:16<01:00,  7.27it/s]Measuring inference for batch_size=1:  56%|█████▌    | 559/1000 [01:16<01:00,  7.28it/s]Measuring inference for batch_size=1:  56%|█████▌    | 560/1000 [01:16<01:00,  7.28it/s]Measuring inference for batch_size=1:  56%|█████▌    | 561/1000 [01:17<01:00,  7.28it/s]Measuring inference for batch_size=1:  56%|█████▌    | 562/1000 [01:17<01:00,  7.28it/s]Measuring inference for batch_size=1:  56%|█████▋    | 563/1000 [01:17<01:00,  7.28it/s]Measuring inference for batch_size=1:  56%|█████▋    | 564/1000 [01:17<00:59,  7.27it/s]Measuring inference for batch_size=1:  56%|█████▋    | 565/1000 [01:17<00:59,  7.28it/s]Measuring inference for batch_size=1:  57%|█████▋    | 566/1000 [01:17<00:59,  7.28it/s]Measuring inference for batch_size=1:  57%|█████▋    | 567/1000 [01:17<00:59,  7.28it/s]Measuring inference for batch_size=1:  57%|█████▋    | 568/1000 [01:17<00:59,  7.28it/s]Measuring inference for batch_size=1:  57%|█████▋    | 569/1000 [01:18<00:59,  7.27it/s]Measuring inference for batch_size=1:  57%|█████▋    | 570/1000 [01:18<00:59,  7.28it/s]Measuring inference for batch_size=1:  57%|█████▋    | 571/1000 [01:18<00:58,  7.28it/s]Measuring inference for batch_size=1:  57%|█████▋    | 572/1000 [01:18<00:58,  7.28it/s]Measuring inference for batch_size=1:  57%|█████▋    | 573/1000 [01:18<00:58,  7.28it/s]Measuring inference for batch_size=1:  57%|█████▋    | 574/1000 [01:18<00:58,  7.28it/s]Measuring inference for batch_size=1:  57%|█████▊    | 575/1000 [01:18<00:58,  7.28it/s]Measuring inference for batch_size=1:  58%|█████▊    | 576/1000 [01:19<00:58,  7.28it/s]Measuring inference for batch_size=1:  58%|█████▊    | 577/1000 [01:19<00:58,  7.28it/s]Measuring inference for batch_size=1:  58%|█████▊    | 578/1000 [01:19<00:57,  7.28it/s]Measuring inference for batch_size=1:  58%|█████▊    | 579/1000 [01:19<00:57,  7.28it/s]Measuring inference for batch_size=1:  58%|█████▊    | 580/1000 [01:19<00:57,  7.28it/s]Measuring inference for batch_size=1:  58%|█████▊    | 581/1000 [01:19<00:57,  7.28it/s]Measuring inference for batch_size=1:  58%|█████▊    | 582/1000 [01:19<00:57,  7.28it/s]Measuring inference for batch_size=1:  58%|█████▊    | 583/1000 [01:20<00:57,  7.29it/s]Measuring inference for batch_size=1:  58%|█████▊    | 584/1000 [01:20<00:57,  7.29it/s]Measuring inference for batch_size=1:  58%|█████▊    | 585/1000 [01:20<00:56,  7.29it/s]Measuring inference for batch_size=1:  59%|█████▊    | 586/1000 [01:20<00:56,  7.28it/s]Measuring inference for batch_size=1:  59%|█████▊    | 587/1000 [01:20<00:56,  7.28it/s]Measuring inference for batch_size=1:  59%|█████▉    | 588/1000 [01:20<00:56,  7.28it/s]Measuring inference for batch_size=1:  59%|█████▉    | 589/1000 [01:20<00:56,  7.28it/s]Measuring inference for batch_size=1:  59%|█████▉    | 590/1000 [01:21<00:56,  7.28it/s]Measuring inference for batch_size=1:  59%|█████▉    | 591/1000 [01:21<00:56,  7.28it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [01:21<00:55,  7.29it/s]Measuring inference for batch_size=1:  59%|█████▉    | 593/1000 [01:21<00:55,  7.29it/s]Measuring inference for batch_size=1:  59%|█████▉    | 594/1000 [01:21<00:55,  7.28it/s]Measuring inference for batch_size=1:  60%|█████▉    | 595/1000 [01:21<00:55,  7.28it/s]Measuring inference for batch_size=1:  60%|█████▉    | 596/1000 [01:21<00:55,  7.28it/s]Measuring inference for batch_size=1:  60%|█████▉    | 597/1000 [01:21<00:55,  7.28it/s]Measuring inference for batch_size=1:  60%|█████▉    | 598/1000 [01:22<00:55,  7.28it/s]Measuring inference for batch_size=1:  60%|█████▉    | 599/1000 [01:22<00:55,  7.28it/s]Measuring inference for batch_size=1:  60%|██████    | 600/1000 [01:22<00:54,  7.28it/s]Measuring inference for batch_size=1:  60%|██████    | 601/1000 [01:22<00:54,  7.29it/s]Measuring inference for batch_size=1:  60%|██████    | 602/1000 [01:22<00:54,  7.28it/s]Measuring inference for batch_size=1:  60%|██████    | 603/1000 [01:22<00:54,  7.28it/s]Measuring inference for batch_size=1:  60%|██████    | 604/1000 [01:22<00:54,  7.28it/s]Measuring inference for batch_size=1:  60%|██████    | 605/1000 [01:23<00:54,  7.27it/s]Measuring inference for batch_size=1:  61%|██████    | 606/1000 [01:23<00:54,  7.28it/s]Measuring inference for batch_size=1:  61%|██████    | 607/1000 [01:23<00:53,  7.28it/s]Measuring inference for batch_size=1:  61%|██████    | 608/1000 [01:23<00:53,  7.28it/s]Measuring inference for batch_size=1:  61%|██████    | 609/1000 [01:23<00:53,  7.29it/s]Measuring inference for batch_size=1:  61%|██████    | 610/1000 [01:23<00:53,  7.29it/s]Measuring inference for batch_size=1:  61%|██████    | 611/1000 [01:23<00:53,  7.29it/s]Measuring inference for batch_size=1:  61%|██████    | 612/1000 [01:24<00:53,  7.28it/s]Measuring inference for batch_size=1:  61%|██████▏   | 613/1000 [01:24<00:53,  7.29it/s]Measuring inference for batch_size=1:  61%|██████▏   | 614/1000 [01:24<00:52,  7.29it/s]Measuring inference for batch_size=1:  62%|██████▏   | 615/1000 [01:24<00:52,  7.28it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [01:24<00:52,  7.29it/s]Measuring inference for batch_size=1:  62%|██████▏   | 617/1000 [01:24<00:52,  7.29it/s]Measuring inference for batch_size=1:  62%|██████▏   | 618/1000 [01:24<00:52,  7.29it/s]Measuring inference for batch_size=1:  62%|██████▏   | 619/1000 [01:24<00:52,  7.28it/s]Measuring inference for batch_size=1:  62%|██████▏   | 620/1000 [01:25<00:52,  7.28it/s]Measuring inference for batch_size=1:  62%|██████▏   | 621/1000 [01:25<00:52,  7.28it/s]Measuring inference for batch_size=1:  62%|██████▏   | 622/1000 [01:25<00:51,  7.29it/s]Measuring inference for batch_size=1:  62%|██████▏   | 623/1000 [01:25<00:51,  7.28it/s]Measuring inference for batch_size=1:  62%|██████▏   | 624/1000 [01:25<00:51,  7.28it/s]Measuring inference for batch_size=1:  62%|██████▎   | 625/1000 [01:25<00:51,  7.28it/s]Measuring inference for batch_size=1:  63%|██████▎   | 626/1000 [01:25<00:51,  7.28it/s]Measuring inference for batch_size=1:  63%|██████▎   | 627/1000 [01:26<00:51,  7.28it/s]Measuring inference for batch_size=1:  63%|██████▎   | 628/1000 [01:26<00:51,  7.28it/s]Measuring inference for batch_size=1:  63%|██████▎   | 629/1000 [01:26<00:50,  7.29it/s]Measuring inference for batch_size=1:  63%|██████▎   | 630/1000 [01:26<00:50,  7.29it/s]Measuring inference for batch_size=1:  63%|██████▎   | 631/1000 [01:26<00:50,  7.28it/s]Measuring inference for batch_size=1:  63%|██████▎   | 632/1000 [01:26<00:50,  7.29it/s]Measuring inference for batch_size=1:  63%|██████▎   | 633/1000 [01:26<00:50,  7.29it/s]Measuring inference for batch_size=1:  63%|██████▎   | 634/1000 [01:27<00:50,  7.28it/s]Measuring inference for batch_size=1:  64%|██████▎   | 635/1000 [01:27<00:50,  7.28it/s]Measuring inference for batch_size=1:  64%|██████▎   | 636/1000 [01:27<00:49,  7.29it/s]Measuring inference for batch_size=1:  64%|██████▎   | 637/1000 [01:27<00:49,  7.29it/s]Measuring inference for batch_size=1:  64%|██████▍   | 638/1000 [01:27<00:49,  7.29it/s]Measuring inference for batch_size=1:  64%|██████▍   | 639/1000 [01:27<00:49,  7.29it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [01:27<00:49,  7.29it/s]Measuring inference for batch_size=1:  64%|██████▍   | 641/1000 [01:28<00:49,  7.29it/s]Measuring inference for batch_size=1:  64%|██████▍   | 642/1000 [01:28<00:49,  7.28it/s]Measuring inference for batch_size=1:  64%|██████▍   | 643/1000 [01:28<00:49,  7.28it/s]Measuring inference for batch_size=1:  64%|██████▍   | 644/1000 [01:28<00:48,  7.28it/s]Measuring inference for batch_size=1:  64%|██████▍   | 645/1000 [01:28<00:48,  7.27it/s]Measuring inference for batch_size=1:  65%|██████▍   | 646/1000 [01:28<00:48,  7.28it/s]Measuring inference for batch_size=1:  65%|██████▍   | 647/1000 [01:28<00:48,  7.29it/s]Measuring inference for batch_size=1:  65%|██████▍   | 648/1000 [01:28<00:48,  7.29it/s]Measuring inference for batch_size=1:  65%|██████▍   | 649/1000 [01:29<00:48,  7.30it/s]Measuring inference for batch_size=1:  65%|██████▌   | 650/1000 [01:29<00:47,  7.30it/s]Measuring inference for batch_size=1:  65%|██████▌   | 651/1000 [01:29<00:47,  7.30it/s]Measuring inference for batch_size=1:  65%|██████▌   | 652/1000 [01:29<00:47,  7.29it/s]Measuring inference for batch_size=1:  65%|██████▌   | 653/1000 [01:29<00:47,  7.29it/s]Measuring inference for batch_size=1:  65%|██████▌   | 654/1000 [01:29<00:47,  7.28it/s]Measuring inference for batch_size=1:  66%|██████▌   | 655/1000 [01:29<00:47,  7.28it/s]Measuring inference for batch_size=1:  66%|██████▌   | 656/1000 [01:30<00:47,  7.28it/s]Measuring inference for batch_size=1:  66%|██████▌   | 657/1000 [01:30<00:47,  7.28it/s]Measuring inference for batch_size=1:  66%|██████▌   | 658/1000 [01:30<00:46,  7.28it/s]Measuring inference for batch_size=1:  66%|██████▌   | 659/1000 [01:30<00:46,  7.28it/s]Measuring inference for batch_size=1:  66%|██████▌   | 660/1000 [01:30<00:46,  7.29it/s]Measuring inference for batch_size=1:  66%|██████▌   | 661/1000 [01:30<00:46,  7.28it/s]Measuring inference for batch_size=1:  66%|██████▌   | 662/1000 [01:30<00:46,  7.29it/s]Measuring inference for batch_size=1:  66%|██████▋   | 663/1000 [01:31<00:46,  7.28it/s]Measuring inference for batch_size=1:  66%|██████▋   | 664/1000 [01:31<00:46,  7.28it/s]Measuring inference for batch_size=1:  66%|██████▋   | 665/1000 [01:31<00:46,  7.28it/s]Measuring inference for batch_size=1:  67%|██████▋   | 666/1000 [01:31<00:45,  7.28it/s]Measuring inference for batch_size=1:  67%|██████▋   | 667/1000 [01:31<00:45,  7.28it/s]Measuring inference for batch_size=1:  67%|██████▋   | 668/1000 [01:31<00:45,  7.29it/s]Measuring inference for batch_size=1:  67%|██████▋   | 669/1000 [01:31<00:45,  7.28it/s]Measuring inference for batch_size=1:  67%|██████▋   | 670/1000 [01:31<00:45,  7.28it/s]Measuring inference for batch_size=1:  67%|██████▋   | 671/1000 [01:32<00:45,  7.28it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [01:32<00:45,  7.28it/s]Measuring inference for batch_size=1:  67%|██████▋   | 673/1000 [01:32<00:44,  7.29it/s]Measuring inference for batch_size=1:  67%|██████▋   | 674/1000 [01:32<00:44,  7.29it/s]Measuring inference for batch_size=1:  68%|██████▊   | 675/1000 [01:32<00:44,  7.29it/s]Measuring inference for batch_size=1:  68%|██████▊   | 676/1000 [01:32<00:44,  7.29it/s]Measuring inference for batch_size=1:  68%|██████▊   | 677/1000 [01:32<00:44,  7.29it/s]Measuring inference for batch_size=1:  68%|██████▊   | 678/1000 [01:33<00:44,  7.29it/s]Measuring inference for batch_size=1:  68%|██████▊   | 679/1000 [01:33<00:44,  7.29it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [01:33<00:43,  7.28it/s]Measuring inference for batch_size=1:  68%|██████▊   | 681/1000 [01:33<00:43,  7.29it/s]Measuring inference for batch_size=1:  68%|██████▊   | 682/1000 [01:33<00:43,  7.28it/s]Measuring inference for batch_size=1:  68%|██████▊   | 683/1000 [01:33<00:43,  7.28it/s]Measuring inference for batch_size=1:  68%|██████▊   | 684/1000 [01:33<00:43,  7.29it/s]Measuring inference for batch_size=1:  68%|██████▊   | 685/1000 [01:34<00:43,  7.29it/s]Measuring inference for batch_size=1:  69%|██████▊   | 686/1000 [01:34<00:43,  7.28it/s]Measuring inference for batch_size=1:  69%|██████▊   | 687/1000 [01:34<00:42,  7.28it/s]Measuring inference for batch_size=1:  69%|██████▉   | 688/1000 [01:34<00:42,  7.29it/s]Measuring inference for batch_size=1:  69%|██████▉   | 689/1000 [01:34<00:42,  7.28it/s]Measuring inference for batch_size=1:  69%|██████▉   | 690/1000 [01:34<00:42,  7.28it/s]Measuring inference for batch_size=1:  69%|██████▉   | 691/1000 [01:34<00:42,  7.28it/s]Measuring inference for batch_size=1:  69%|██████▉   | 692/1000 [01:35<00:42,  7.28it/s]Measuring inference for batch_size=1:  69%|██████▉   | 693/1000 [01:35<00:42,  7.28it/s]Measuring inference for batch_size=1:  69%|██████▉   | 694/1000 [01:35<00:42,  7.28it/s]Measuring inference for batch_size=1:  70%|██████▉   | 695/1000 [01:35<00:41,  7.29it/s]Measuring inference for batch_size=1:  70%|██████▉   | 696/1000 [01:35<00:41,  7.29it/s]Measuring inference for batch_size=1:  70%|██████▉   | 697/1000 [01:35<00:41,  7.29it/s]Measuring inference for batch_size=1:  70%|██████▉   | 698/1000 [01:35<00:41,  7.29it/s]Measuring inference for batch_size=1:  70%|██████▉   | 699/1000 [01:35<00:41,  7.28it/s]Measuring inference for batch_size=1:  70%|███████   | 700/1000 [01:36<00:41,  7.28it/s]Measuring inference for batch_size=1:  70%|███████   | 701/1000 [01:36<00:41,  7.28it/s]Measuring inference for batch_size=1:  70%|███████   | 702/1000 [01:36<00:40,  7.28it/s]Measuring inference for batch_size=1:  70%|███████   | 703/1000 [01:36<00:40,  7.28it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [01:36<00:40,  7.28it/s]Measuring inference for batch_size=1:  70%|███████   | 705/1000 [01:36<00:40,  7.28it/s]Measuring inference for batch_size=1:  71%|███████   | 706/1000 [01:36<00:40,  7.29it/s]Measuring inference for batch_size=1:  71%|███████   | 707/1000 [01:37<00:40,  7.28it/s]Measuring inference for batch_size=1:  71%|███████   | 708/1000 [01:37<00:40,  7.28it/s]Measuring inference for batch_size=1:  71%|███████   | 709/1000 [01:37<00:39,  7.28it/s]Measuring inference for batch_size=1:  71%|███████   | 710/1000 [01:37<00:39,  7.28it/s]Measuring inference for batch_size=1:  71%|███████   | 711/1000 [01:37<00:39,  7.29it/s]Measuring inference for batch_size=1:  71%|███████   | 712/1000 [01:37<00:39,  7.28it/s]Measuring inference for batch_size=1:  71%|███████▏  | 713/1000 [01:37<00:39,  7.28it/s]Measuring inference for batch_size=1:  71%|███████▏  | 714/1000 [01:38<00:39,  7.28it/s]Measuring inference for batch_size=1:  72%|███████▏  | 715/1000 [01:38<00:39,  7.28it/s]Measuring inference for batch_size=1:  72%|███████▏  | 716/1000 [01:38<00:39,  7.28it/s]Measuring inference for batch_size=1:  72%|███████▏  | 717/1000 [01:38<00:38,  7.28it/s]Measuring inference for batch_size=1:  72%|███████▏  | 718/1000 [01:38<00:38,  7.27it/s]Measuring inference for batch_size=1:  72%|███████▏  | 719/1000 [01:38<00:38,  7.27it/s]Measuring inference for batch_size=1:  72%|███████▏  | 720/1000 [01:38<00:38,  7.28it/s]Measuring inference for batch_size=1:  72%|███████▏  | 721/1000 [01:38<00:38,  7.27it/s]Measuring inference for batch_size=1:  72%|███████▏  | 722/1000 [01:39<00:38,  7.28it/s]Measuring inference for batch_size=1:  72%|███████▏  | 723/1000 [01:39<00:38,  7.28it/s]Measuring inference for batch_size=1:  72%|███████▏  | 724/1000 [01:39<00:37,  7.28it/s]Measuring inference for batch_size=1:  72%|███████▎  | 725/1000 [01:39<00:37,  7.28it/s]Measuring inference for batch_size=1:  73%|███████▎  | 726/1000 [01:39<00:37,  7.28it/s]Measuring inference for batch_size=1:  73%|███████▎  | 727/1000 [01:39<00:37,  7.28it/s]Measuring inference for batch_size=1:  73%|███████▎  | 728/1000 [01:39<00:37,  7.28it/s]Measuring inference for batch_size=1:  73%|███████▎  | 729/1000 [01:40<00:37,  7.28it/s]Measuring inference for batch_size=1:  73%|███████▎  | 730/1000 [01:40<00:37,  7.28it/s]Measuring inference for batch_size=1:  73%|███████▎  | 731/1000 [01:40<00:36,  7.28it/s]Measuring inference for batch_size=1:  73%|███████▎  | 732/1000 [01:40<00:36,  7.28it/s]Measuring inference for batch_size=1:  73%|███████▎  | 733/1000 [01:40<00:36,  7.28it/s]Measuring inference for batch_size=1:  73%|███████▎  | 734/1000 [01:40<00:36,  7.29it/s]Measuring inference for batch_size=1:  74%|███████▎  | 735/1000 [01:40<00:36,  7.29it/s]Measuring inference for batch_size=1:  74%|███████▎  | 736/1000 [01:41<00:36,  7.29it/s]Measuring inference for batch_size=1:  74%|███████▎  | 737/1000 [01:41<00:36,  7.30it/s]Measuring inference for batch_size=1:  74%|███████▍  | 738/1000 [01:41<00:35,  7.29it/s]Measuring inference for batch_size=1:  74%|███████▍  | 739/1000 [01:41<00:35,  7.29it/s]Measuring inference for batch_size=1:  74%|███████▍  | 740/1000 [01:41<00:35,  7.29it/s]Measuring inference for batch_size=1:  74%|███████▍  | 741/1000 [01:41<00:35,  7.29it/s]Measuring inference for batch_size=1:  74%|███████▍  | 742/1000 [01:41<00:35,  7.29it/s]Measuring inference for batch_size=1:  74%|███████▍  | 743/1000 [01:42<00:35,  7.28it/s]Measuring inference for batch_size=1:  74%|███████▍  | 744/1000 [01:42<00:35,  7.29it/s]Measuring inference for batch_size=1:  74%|███████▍  | 745/1000 [01:42<00:34,  7.29it/s]Measuring inference for batch_size=1:  75%|███████▍  | 746/1000 [01:42<00:34,  7.29it/s]Measuring inference for batch_size=1:  75%|███████▍  | 747/1000 [01:42<00:34,  7.29it/s]Measuring inference for batch_size=1:  75%|███████▍  | 748/1000 [01:42<00:34,  7.29it/s]Measuring inference for batch_size=1:  75%|███████▍  | 749/1000 [01:42<00:34,  7.28it/s]Measuring inference for batch_size=1:  75%|███████▌  | 750/1000 [01:42<00:34,  7.28it/s]Measuring inference for batch_size=1:  75%|███████▌  | 751/1000 [01:43<00:34,  7.29it/s]Measuring inference for batch_size=1:  75%|███████▌  | 752/1000 [01:43<00:34,  7.29it/s]Measuring inference for batch_size=1:  75%|███████▌  | 753/1000 [01:43<00:33,  7.28it/s]Measuring inference for batch_size=1:  75%|███████▌  | 754/1000 [01:43<00:33,  7.28it/s]Measuring inference for batch_size=1:  76%|███████▌  | 755/1000 [01:43<00:33,  7.29it/s]Measuring inference for batch_size=1:  76%|███████▌  | 756/1000 [01:43<00:33,  7.29it/s]Measuring inference for batch_size=1:  76%|███████▌  | 757/1000 [01:43<00:33,  7.27it/s]Measuring inference for batch_size=1:  76%|███████▌  | 758/1000 [01:44<00:33,  7.27it/s]Measuring inference for batch_size=1:  76%|███████▌  | 759/1000 [01:44<00:33,  7.27it/s]Measuring inference for batch_size=1:  76%|███████▌  | 760/1000 [01:44<00:32,  7.28it/s]Measuring inference for batch_size=1:  76%|███████▌  | 761/1000 [01:44<00:32,  7.28it/s]Measuring inference for batch_size=1:  76%|███████▌  | 762/1000 [01:44<00:32,  7.28it/s]Measuring inference for batch_size=1:  76%|███████▋  | 763/1000 [01:44<00:32,  7.28it/s]Measuring inference for batch_size=1:  76%|███████▋  | 764/1000 [01:44<00:32,  7.28it/s]Measuring inference for batch_size=1:  76%|███████▋  | 765/1000 [01:45<00:32,  7.29it/s]Measuring inference for batch_size=1:  77%|███████▋  | 766/1000 [01:45<00:32,  7.29it/s]Measuring inference for batch_size=1:  77%|███████▋  | 767/1000 [01:45<00:31,  7.29it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [01:45<00:31,  7.29it/s]Measuring inference for batch_size=1:  77%|███████▋  | 769/1000 [01:45<00:31,  7.29it/s]Measuring inference for batch_size=1:  77%|███████▋  | 770/1000 [01:45<00:31,  7.28it/s]Measuring inference for batch_size=1:  77%|███████▋  | 771/1000 [01:45<00:31,  7.28it/s]Measuring inference for batch_size=1:  77%|███████▋  | 772/1000 [01:45<00:31,  7.28it/s]Measuring inference for batch_size=1:  77%|███████▋  | 773/1000 [01:46<00:31,  7.28it/s]Measuring inference for batch_size=1:  77%|███████▋  | 774/1000 [01:46<00:31,  7.27it/s]Measuring inference for batch_size=1:  78%|███████▊  | 775/1000 [01:46<00:30,  7.28it/s]Measuring inference for batch_size=1:  78%|███████▊  | 776/1000 [01:46<00:30,  7.28it/s]Measuring inference for batch_size=1:  78%|███████▊  | 777/1000 [01:46<00:30,  7.28it/s]Measuring inference for batch_size=1:  78%|███████▊  | 778/1000 [01:46<00:30,  7.27it/s]Measuring inference for batch_size=1:  78%|███████▊  | 779/1000 [01:46<00:30,  7.28it/s]Measuring inference for batch_size=1:  78%|███████▊  | 780/1000 [01:47<00:30,  7.28it/s]Measuring inference for batch_size=1:  78%|███████▊  | 781/1000 [01:47<00:30,  7.29it/s]Measuring inference for batch_size=1:  78%|███████▊  | 782/1000 [01:47<00:29,  7.29it/s]Measuring inference for batch_size=1:  78%|███████▊  | 783/1000 [01:47<00:29,  7.29it/s]Measuring inference for batch_size=1:  78%|███████▊  | 784/1000 [01:47<00:29,  7.29it/s]Measuring inference for batch_size=1:  78%|███████▊  | 785/1000 [01:47<00:29,  7.29it/s]Measuring inference for batch_size=1:  79%|███████▊  | 786/1000 [01:47<00:29,  7.29it/s]Measuring inference for batch_size=1:  79%|███████▊  | 787/1000 [01:48<00:29,  7.29it/s]Measuring inference for batch_size=1:  79%|███████▉  | 788/1000 [01:48<00:29,  7.29it/s]Measuring inference for batch_size=1:  79%|███████▉  | 789/1000 [01:48<00:28,  7.29it/s]Measuring inference for batch_size=1:  79%|███████▉  | 790/1000 [01:48<00:28,  7.28it/s]Measuring inference for batch_size=1:  79%|███████▉  | 791/1000 [01:48<00:28,  7.28it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [01:48<00:28,  7.28it/s]Measuring inference for batch_size=1:  79%|███████▉  | 793/1000 [01:48<00:28,  7.28it/s]Measuring inference for batch_size=1:  79%|███████▉  | 794/1000 [01:49<00:28,  7.28it/s]Measuring inference for batch_size=1:  80%|███████▉  | 795/1000 [01:49<00:28,  7.28it/s]Measuring inference for batch_size=1:  80%|███████▉  | 796/1000 [01:49<00:28,  7.28it/s]Measuring inference for batch_size=1:  80%|███████▉  | 797/1000 [01:49<00:27,  7.28it/s]Measuring inference for batch_size=1:  80%|███████▉  | 798/1000 [01:49<00:27,  7.29it/s]Measuring inference for batch_size=1:  80%|███████▉  | 799/1000 [01:49<00:27,  7.28it/s]Measuring inference for batch_size=1:  80%|████████  | 800/1000 [01:49<00:27,  7.28it/s]Measuring inference for batch_size=1:  80%|████████  | 801/1000 [01:49<00:27,  7.28it/s]Measuring inference for batch_size=1:  80%|████████  | 802/1000 [01:50<00:27,  7.28it/s]Measuring inference for batch_size=1:  80%|████████  | 803/1000 [01:50<00:27,  7.28it/s]Measuring inference for batch_size=1:  80%|████████  | 804/1000 [01:50<00:26,  7.28it/s]Measuring inference for batch_size=1:  80%|████████  | 805/1000 [01:50<00:26,  7.27it/s]Measuring inference for batch_size=1:  81%|████████  | 806/1000 [01:50<00:26,  7.28it/s]Measuring inference for batch_size=1:  81%|████████  | 807/1000 [01:50<00:26,  7.27it/s]Measuring inference for batch_size=1:  81%|████████  | 808/1000 [01:50<00:26,  7.27it/s]Measuring inference for batch_size=1:  81%|████████  | 809/1000 [01:51<00:26,  7.28it/s]Measuring inference for batch_size=1:  81%|████████  | 810/1000 [01:51<00:26,  7.27it/s]Measuring inference for batch_size=1:  81%|████████  | 811/1000 [01:51<00:25,  7.27it/s]Measuring inference for batch_size=1:  81%|████████  | 812/1000 [01:51<00:25,  7.27it/s]Measuring inference for batch_size=1:  81%|████████▏ | 813/1000 [01:51<00:25,  7.27it/s]Measuring inference for batch_size=1:  81%|████████▏ | 814/1000 [01:51<00:25,  7.28it/s]Measuring inference for batch_size=1:  82%|████████▏ | 815/1000 [01:51<00:25,  7.28it/s]Measuring inference for batch_size=1:  82%|████████▏ | 816/1000 [01:52<00:25,  7.28it/s]Measuring inference for batch_size=1:  82%|████████▏ | 817/1000 [01:52<00:25,  7.29it/s]Measuring inference for batch_size=1:  82%|████████▏ | 818/1000 [01:52<00:24,  7.28it/s]Measuring inference for batch_size=1:  82%|████████▏ | 819/1000 [01:52<00:24,  7.28it/s]Measuring inference for batch_size=1:  82%|████████▏ | 820/1000 [01:52<00:24,  7.28it/s]Measuring inference for batch_size=1:  82%|████████▏ | 821/1000 [01:52<00:24,  7.28it/s]Measuring inference for batch_size=1:  82%|████████▏ | 822/1000 [01:52<00:24,  7.28it/s]Measuring inference for batch_size=1:  82%|████████▏ | 823/1000 [01:52<00:24,  7.28it/s]Measuring inference for batch_size=1:  82%|████████▏ | 824/1000 [01:53<00:24,  7.26it/s]Measuring inference for batch_size=1:  82%|████████▎ | 825/1000 [01:53<00:24,  7.27it/s]Measuring inference for batch_size=1:  83%|████████▎ | 826/1000 [01:53<00:23,  7.28it/s]Measuring inference for batch_size=1:  83%|████████▎ | 827/1000 [01:53<00:23,  7.28it/s]Measuring inference for batch_size=1:  83%|████████▎ | 828/1000 [01:53<00:23,  7.29it/s]Measuring inference for batch_size=1:  83%|████████▎ | 829/1000 [01:53<00:23,  7.29it/s]Measuring inference for batch_size=1:  83%|████████▎ | 830/1000 [01:53<00:23,  7.29it/s]Measuring inference for batch_size=1:  83%|████████▎ | 831/1000 [01:54<00:23,  7.30it/s]Measuring inference for batch_size=1:  83%|████████▎ | 832/1000 [01:54<00:23,  7.29it/s]Measuring inference for batch_size=1:  83%|████████▎ | 833/1000 [01:54<00:22,  7.30it/s]Measuring inference for batch_size=1:  83%|████████▎ | 834/1000 [01:54<00:22,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▎ | 835/1000 [01:54<00:22,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▎ | 836/1000 [01:54<00:22,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▎ | 837/1000 [01:54<00:22,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▍ | 838/1000 [01:55<00:22,  7.29it/s]Measuring inference for batch_size=1:  84%|████████▍ | 839/1000 [01:55<00:22,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [01:55<00:21,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▍ | 841/1000 [01:55<00:21,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▍ | 842/1000 [01:55<00:21,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▍ | 843/1000 [01:55<00:21,  7.31it/s]Measuring inference for batch_size=1:  84%|████████▍ | 844/1000 [01:55<00:21,  7.31it/s]Measuring inference for batch_size=1:  84%|████████▍ | 845/1000 [01:56<00:21,  7.30it/s]Measuring inference for batch_size=1:  85%|████████▍ | 846/1000 [01:56<00:21,  7.31it/s]Measuring inference for batch_size=1:  85%|████████▍ | 847/1000 [01:56<00:20,  7.31it/s]Measuring inference for batch_size=1:  85%|████████▍ | 848/1000 [01:56<00:20,  7.30it/s]Measuring inference for batch_size=1:  85%|████████▍ | 849/1000 [01:56<00:20,  7.30it/s]Measuring inference for batch_size=1:  85%|████████▌ | 850/1000 [01:56<00:20,  7.31it/s]Measuring inference for batch_size=1:  85%|████████▌ | 851/1000 [01:56<00:20,  7.31it/s]Measuring inference for batch_size=1:  85%|████████▌ | 852/1000 [01:56<00:20,  7.30it/s]Measuring inference for batch_size=1:  85%|████████▌ | 853/1000 [01:57<00:20,  7.30it/s]Measuring inference for batch_size=1:  85%|████████▌ | 854/1000 [01:57<00:20,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▌ | 855/1000 [01:57<00:19,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▌ | 856/1000 [01:57<00:19,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▌ | 857/1000 [01:57<00:19,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▌ | 858/1000 [01:57<00:19,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▌ | 859/1000 [01:57<00:19,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▌ | 860/1000 [01:58<00:19,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▌ | 861/1000 [01:58<00:19,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▌ | 862/1000 [01:58<00:18,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▋ | 863/1000 [01:58<00:18,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▋ | 864/1000 [01:58<00:18,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▋ | 865/1000 [01:58<00:18,  7.30it/s]Measuring inference for batch_size=1:  87%|████████▋ | 866/1000 [01:58<00:18,  7.30it/s]Measuring inference for batch_size=1:  87%|████████▋ | 867/1000 [01:59<00:18,  7.30it/s]Measuring inference for batch_size=1:  87%|████████▋ | 868/1000 [01:59<00:18,  7.30it/s]Measuring inference for batch_size=1:  87%|████████▋ | 869/1000 [01:59<00:17,  7.30it/s]Measuring inference for batch_size=1:  87%|████████▋ | 870/1000 [01:59<00:17,  7.30it/s]Measuring inference for batch_size=1:  87%|████████▋ | 871/1000 [01:59<00:17,  7.30it/s]Measuring inference for batch_size=1:  87%|████████▋ | 872/1000 [01:59<00:17,  7.30it/s]Measuring inference for batch_size=1:  87%|████████▋ | 873/1000 [01:59<00:17,  7.30it/s]Measuring inference for batch_size=1:  87%|████████▋ | 874/1000 [01:59<00:17,  7.30it/s]Measuring inference for batch_size=1:  88%|████████▊ | 875/1000 [02:00<00:17,  7.30it/s]Measuring inference for batch_size=1:  88%|████████▊ | 876/1000 [02:00<00:17,  7.29it/s]Measuring inference for batch_size=1:  88%|████████▊ | 877/1000 [02:00<00:16,  7.30it/s]Measuring inference for batch_size=1:  88%|████████▊ | 878/1000 [02:00<00:16,  7.30it/s]Measuring inference for batch_size=1:  88%|████████▊ | 879/1000 [02:00<00:16,  7.30it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [02:00<00:16,  7.30it/s]Measuring inference for batch_size=1:  88%|████████▊ | 881/1000 [02:00<00:16,  7.30it/s]Measuring inference for batch_size=1:  88%|████████▊ | 882/1000 [02:01<00:16,  7.30it/s]Measuring inference for batch_size=1:  88%|████████▊ | 883/1000 [02:01<00:16,  7.30it/s]Measuring inference for batch_size=1:  88%|████████▊ | 884/1000 [02:01<00:15,  7.29it/s]Measuring inference for batch_size=1:  88%|████████▊ | 885/1000 [02:01<00:15,  7.29it/s]Measuring inference for batch_size=1:  89%|████████▊ | 886/1000 [02:01<00:15,  7.29it/s]Measuring inference for batch_size=1:  89%|████████▊ | 887/1000 [02:01<00:15,  7.29it/s]Measuring inference for batch_size=1:  89%|████████▉ | 888/1000 [02:01<00:15,  7.29it/s]Measuring inference for batch_size=1:  89%|████████▉ | 889/1000 [02:02<00:15,  7.28it/s]Measuring inference for batch_size=1:  89%|████████▉ | 890/1000 [02:02<00:15,  7.28it/s]Measuring inference for batch_size=1:  89%|████████▉ | 891/1000 [02:02<00:14,  7.28it/s]Measuring inference for batch_size=1:  89%|████████▉ | 892/1000 [02:02<00:14,  7.27it/s]Measuring inference for batch_size=1:  89%|████████▉ | 893/1000 [02:02<00:14,  7.28it/s]Measuring inference for batch_size=1:  89%|████████▉ | 894/1000 [02:02<00:14,  7.28it/s]Measuring inference for batch_size=1:  90%|████████▉ | 895/1000 [02:02<00:14,  7.27it/s]Measuring inference for batch_size=1:  90%|████████▉ | 896/1000 [02:03<00:14,  7.28it/s]Measuring inference for batch_size=1:  90%|████████▉ | 897/1000 [02:03<00:14,  7.28it/s]Measuring inference for batch_size=1:  90%|████████▉ | 898/1000 [02:03<00:14,  7.28it/s]Measuring inference for batch_size=1:  90%|████████▉ | 899/1000 [02:03<00:13,  7.28it/s]Measuring inference for batch_size=1:  90%|█████████ | 900/1000 [02:03<00:13,  7.28it/s]Measuring inference for batch_size=1:  90%|█████████ | 901/1000 [02:03<00:13,  7.28it/s]Measuring inference for batch_size=1:  90%|█████████ | 902/1000 [02:03<00:13,  7.28it/s]Measuring inference for batch_size=1:  90%|█████████ | 903/1000 [02:03<00:13,  7.28it/s]Measuring inference for batch_size=1:  90%|█████████ | 904/1000 [02:04<00:13,  7.28it/s]Measuring inference for batch_size=1:  90%|█████████ | 905/1000 [02:04<00:13,  7.28it/s]Measuring inference for batch_size=1:  91%|█████████ | 906/1000 [02:04<00:12,  7.29it/s]Measuring inference for batch_size=1:  91%|█████████ | 907/1000 [02:04<00:12,  7.29it/s]Measuring inference for batch_size=1:  91%|█████████ | 908/1000 [02:04<00:12,  7.29it/s]Measuring inference for batch_size=1:  91%|█████████ | 909/1000 [02:04<00:12,  7.28it/s]Measuring inference for batch_size=1:  91%|█████████ | 910/1000 [02:04<00:12,  7.29it/s]Measuring inference for batch_size=1:  91%|█████████ | 911/1000 [02:05<00:12,  7.28it/s]Measuring inference for batch_size=1:  91%|█████████ | 912/1000 [02:05<00:12,  7.28it/s]Measuring inference for batch_size=1:  91%|█████████▏| 913/1000 [02:05<00:11,  7.28it/s]Measuring inference for batch_size=1:  91%|█████████▏| 914/1000 [02:05<00:11,  7.27it/s]Measuring inference for batch_size=1:  92%|█████████▏| 915/1000 [02:05<00:11,  7.28it/s]Measuring inference for batch_size=1:  92%|█████████▏| 916/1000 [02:05<00:11,  7.29it/s]Measuring inference for batch_size=1:  92%|█████████▏| 917/1000 [02:05<00:11,  7.29it/s]Measuring inference for batch_size=1:  92%|█████████▏| 918/1000 [02:06<00:11,  7.29it/s]Measuring inference for batch_size=1:  92%|█████████▏| 919/1000 [02:06<00:11,  7.29it/s]Measuring inference for batch_size=1:  92%|█████████▏| 920/1000 [02:06<00:10,  7.29it/s]Measuring inference for batch_size=1:  92%|█████████▏| 921/1000 [02:06<00:10,  7.29it/s]Measuring inference for batch_size=1:  92%|█████████▏| 922/1000 [02:06<00:10,  7.29it/s]Measuring inference for batch_size=1:  92%|█████████▏| 923/1000 [02:06<00:10,  7.29it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [02:06<00:10,  7.30it/s]Measuring inference for batch_size=1:  92%|█████████▎| 925/1000 [02:06<00:10,  7.30it/s]Measuring inference for batch_size=1:  93%|█████████▎| 926/1000 [02:07<00:10,  7.29it/s]Measuring inference for batch_size=1:  93%|█████████▎| 927/1000 [02:07<00:10,  7.30it/s]Measuring inference for batch_size=1:  93%|█████████▎| 928/1000 [02:07<00:09,  7.30it/s]Measuring inference for batch_size=1:  93%|█████████▎| 929/1000 [02:07<00:09,  7.30it/s]Measuring inference for batch_size=1:  93%|█████████▎| 930/1000 [02:07<00:09,  7.31it/s]Measuring inference for batch_size=1:  93%|█████████▎| 931/1000 [02:07<00:09,  7.30it/s]Measuring inference for batch_size=1:  93%|█████████▎| 932/1000 [02:07<00:09,  7.29it/s]Measuring inference for batch_size=1:  93%|█████████▎| 933/1000 [02:08<00:09,  7.29it/s]Measuring inference for batch_size=1:  93%|█████████▎| 934/1000 [02:08<00:09,  7.28it/s]Measuring inference for batch_size=1:  94%|█████████▎| 935/1000 [02:08<00:08,  7.28it/s]Measuring inference for batch_size=1:  94%|█████████▎| 936/1000 [02:08<00:08,  7.28it/s]Measuring inference for batch_size=1:  94%|█████████▎| 937/1000 [02:08<00:08,  7.28it/s]Measuring inference for batch_size=1:  94%|█████████▍| 938/1000 [02:08<00:08,  7.28it/s]Measuring inference for batch_size=1:  94%|█████████▍| 939/1000 [02:08<00:08,  7.28it/s]Measuring inference for batch_size=1:  94%|█████████▍| 940/1000 [02:09<00:08,  7.28it/s]Measuring inference for batch_size=1:  94%|█████████▍| 941/1000 [02:09<00:08,  7.28it/s]Measuring inference for batch_size=1:  94%|█████████▍| 942/1000 [02:09<00:07,  7.28it/s]Measuring inference for batch_size=1:  94%|█████████▍| 943/1000 [02:09<00:07,  7.28it/s]Measuring inference for batch_size=1:  94%|█████████▍| 944/1000 [02:09<00:07,  7.28it/s]Measuring inference for batch_size=1:  94%|█████████▍| 945/1000 [02:09<00:07,  7.28it/s]Measuring inference for batch_size=1:  95%|█████████▍| 946/1000 [02:09<00:07,  7.28it/s]Measuring inference for batch_size=1:  95%|█████████▍| 947/1000 [02:10<00:07,  7.28it/s]Measuring inference for batch_size=1:  95%|█████████▍| 948/1000 [02:10<00:07,  7.28it/s]Measuring inference for batch_size=1:  95%|█████████▍| 949/1000 [02:10<00:07,  7.27it/s]Measuring inference for batch_size=1:  95%|█████████▌| 950/1000 [02:10<00:06,  7.28it/s]Measuring inference for batch_size=1:  95%|█████████▌| 951/1000 [02:10<00:06,  7.28it/s]Measuring inference for batch_size=1:  95%|█████████▌| 952/1000 [02:10<00:06,  7.28it/s]Measuring inference for batch_size=1:  95%|█████████▌| 953/1000 [02:10<00:06,  7.28it/s]Measuring inference for batch_size=1:  95%|█████████▌| 954/1000 [02:10<00:06,  7.28it/s]Measuring inference for batch_size=1:  96%|█████████▌| 955/1000 [02:11<00:06,  7.27it/s]Measuring inference for batch_size=1:  96%|█████████▌| 956/1000 [02:11<00:06,  7.27it/s]Measuring inference for batch_size=1:  96%|█████████▌| 957/1000 [02:11<00:05,  7.28it/s]Measuring inference for batch_size=1:  96%|█████████▌| 958/1000 [02:11<00:05,  7.27it/s]Measuring inference for batch_size=1:  96%|█████████▌| 959/1000 [02:11<00:05,  7.28it/s]Measuring inference for batch_size=1:  96%|█████████▌| 960/1000 [02:11<00:05,  7.28it/s]Measuring inference for batch_size=1:  96%|█████████▌| 961/1000 [02:11<00:05,  7.28it/s]Measuring inference for batch_size=1:  96%|█████████▌| 962/1000 [02:12<00:05,  7.28it/s]Measuring inference for batch_size=1:  96%|█████████▋| 963/1000 [02:12<00:05,  7.28it/s]Measuring inference for batch_size=1:  96%|█████████▋| 964/1000 [02:12<00:04,  7.28it/s]Measuring inference for batch_size=1:  96%|█████████▋| 965/1000 [02:12<00:04,  7.27it/s]Measuring inference for batch_size=1:  97%|█████████▋| 966/1000 [02:12<00:04,  7.27it/s]Measuring inference for batch_size=1:  97%|█████████▋| 967/1000 [02:12<00:04,  7.28it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [02:12<00:04,  7.28it/s]Measuring inference for batch_size=1:  97%|█████████▋| 969/1000 [02:13<00:04,  7.28it/s]Measuring inference for batch_size=1:  97%|█████████▋| 970/1000 [02:13<00:04,  7.28it/s]Measuring inference for batch_size=1:  97%|█████████▋| 971/1000 [02:13<00:03,  7.27it/s]Measuring inference for batch_size=1:  97%|█████████▋| 972/1000 [02:13<00:03,  7.28it/s]Measuring inference for batch_size=1:  97%|█████████▋| 973/1000 [02:13<00:03,  7.27it/s]Measuring inference for batch_size=1:  97%|█████████▋| 974/1000 [02:13<00:03,  7.27it/s]Measuring inference for batch_size=1:  98%|█████████▊| 975/1000 [02:13<00:03,  7.27it/s]Measuring inference for batch_size=1:  98%|█████████▊| 976/1000 [02:13<00:03,  7.27it/s]Measuring inference for batch_size=1:  98%|█████████▊| 977/1000 [02:14<00:03,  7.27it/s]Measuring inference for batch_size=1:  98%|█████████▊| 978/1000 [02:14<00:03,  7.28it/s]Measuring inference for batch_size=1:  98%|█████████▊| 979/1000 [02:14<00:02,  7.27it/s]Measuring inference for batch_size=1:  98%|█████████▊| 980/1000 [02:14<00:02,  7.27it/s]Measuring inference for batch_size=1:  98%|█████████▊| 981/1000 [02:14<00:02,  7.27it/s]Measuring inference for batch_size=1:  98%|█████████▊| 982/1000 [02:14<00:02,  7.27it/s]Measuring inference for batch_size=1:  98%|█████████▊| 983/1000 [02:14<00:02,  7.27it/s]Measuring inference for batch_size=1:  98%|█████████▊| 984/1000 [02:15<00:02,  7.27it/s]Measuring inference for batch_size=1:  98%|█████████▊| 985/1000 [02:15<00:02,  7.27it/s]Measuring inference for batch_size=1:  99%|█████████▊| 986/1000 [02:15<00:01,  7.27it/s]Measuring inference for batch_size=1:  99%|█████████▊| 987/1000 [02:15<00:01,  7.27it/s]Measuring inference for batch_size=1:  99%|█████████▉| 988/1000 [02:15<00:01,  7.28it/s]Measuring inference for batch_size=1:  99%|█████████▉| 989/1000 [02:15<00:01,  7.28it/s]Measuring inference for batch_size=1:  99%|█████████▉| 990/1000 [02:15<00:01,  7.28it/s]Measuring inference for batch_size=1:  99%|█████████▉| 991/1000 [02:16<00:01,  7.28it/s]Measuring inference for batch_size=1:  99%|█████████▉| 992/1000 [02:16<00:01,  7.28it/s]Measuring inference for batch_size=1:  99%|█████████▉| 993/1000 [02:16<00:00,  7.28it/s]Measuring inference for batch_size=1:  99%|█████████▉| 994/1000 [02:16<00:00,  7.28it/s]Measuring inference for batch_size=1: 100%|█████████▉| 995/1000 [02:16<00:00,  7.28it/s]Measuring inference for batch_size=1: 100%|█████████▉| 996/1000 [02:16<00:00,  7.27it/s]Measuring inference for batch_size=1: 100%|█████████▉| 997/1000 [02:16<00:00,  7.28it/s]Measuring inference for batch_size=1: 100%|█████████▉| 998/1000 [02:17<00:00,  7.27it/s]Measuring inference for batch_size=1: 100%|█████████▉| 999/1000 [02:17<00:00,  7.28it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [02:17<00:00,  7.28it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [02:17<00:00,  7.28it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Measurement of allocated memory is only available on CUDA devices
Warming up with batch_size=512:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=512:   1%|          | 1/100 [00:00<00:18,  5.40it/s]Warming up with batch_size=512:   2%|▏         | 2/100 [00:00<00:17,  5.50it/s]Warming up with batch_size=512:   3%|▎         | 3/100 [00:00<00:17,  5.52it/s]Warming up with batch_size=512:   4%|▍         | 4/100 [00:00<00:17,  5.54it/s]Warming up with batch_size=512:   5%|▌         | 5/100 [00:00<00:17,  5.55it/s]Warming up with batch_size=512:   6%|▌         | 6/100 [00:01<00:16,  5.56it/s]Warming up with batch_size=512:   7%|▋         | 7/100 [00:01<00:16,  5.56it/s]Warming up with batch_size=512:   8%|▊         | 8/100 [00:01<00:16,  5.56it/s]Warming up with batch_size=512:   9%|▉         | 9/100 [00:01<00:16,  5.56it/s]Warming up with batch_size=512:  10%|█         | 10/100 [00:01<00:16,  5.57it/s]Warming up with batch_size=512:  11%|█         | 11/100 [00:01<00:15,  5.57it/s]Warming up with batch_size=512:  12%|█▏        | 12/100 [00:02<00:15,  5.57it/s]Warming up with batch_size=512:  13%|█▎        | 13/100 [00:02<00:15,  5.57it/s]Warming up with batch_size=512:  14%|█▍        | 14/100 [00:02<00:15,  5.58it/s]Warming up with batch_size=512:  15%|█▌        | 15/100 [00:02<00:15,  5.58it/s]Warming up with batch_size=512:  16%|█▌        | 16/100 [00:02<00:15,  5.58it/s]Warming up with batch_size=512:  17%|█▋        | 17/100 [00:03<00:14,  5.58it/s]Warming up with batch_size=512:  18%|█▊        | 18/100 [00:03<00:14,  5.58it/s]Warming up with batch_size=512:  19%|█▉        | 19/100 [00:03<00:14,  5.58it/s]Warming up with batch_size=512:  20%|██        | 20/100 [00:03<00:14,  5.58it/s]Warming up with batch_size=512:  21%|██        | 21/100 [00:03<00:14,  5.58it/s]Warming up with batch_size=512:  22%|██▏       | 22/100 [00:03<00:14,  5.57it/s]Warming up with batch_size=512:  23%|██▎       | 23/100 [00:04<00:13,  5.56it/s]Warming up with batch_size=512:  24%|██▍       | 24/100 [00:04<00:13,  5.56it/s]Warming up with batch_size=512:  25%|██▌       | 25/100 [00:04<00:13,  5.58it/s]Warming up with batch_size=512:  26%|██▌       | 26/100 [00:04<00:13,  5.57it/s]Warming up with batch_size=512:  27%|██▋       | 27/100 [00:04<00:13,  5.57it/s]Warming up with batch_size=512:  28%|██▊       | 28/100 [00:05<00:12,  5.57it/s]Warming up with batch_size=512:  29%|██▉       | 29/100 [00:05<00:12,  5.57it/s]Warming up with batch_size=512:  30%|███       | 30/100 [00:05<00:12,  5.58it/s]Warming up with batch_size=512:  31%|███       | 31/100 [00:05<00:12,  5.57it/s]Warming up with batch_size=512:  32%|███▏      | 32/100 [00:05<00:12,  5.57it/s]Warming up with batch_size=512:  33%|███▎      | 33/100 [00:05<00:12,  5.57it/s]Warming up with batch_size=512:  34%|███▍      | 34/100 [00:06<00:11,  5.58it/s]Warming up with batch_size=512:  35%|███▌      | 35/100 [00:06<00:11,  5.58it/s]Warming up with batch_size=512:  36%|███▌      | 36/100 [00:06<00:11,  5.58it/s]Warming up with batch_size=512:  37%|███▋      | 37/100 [00:06<00:11,  5.58it/s]Warming up with batch_size=512:  38%|███▊      | 38/100 [00:06<00:11,  5.58it/s]Warming up with batch_size=512:  39%|███▉      | 39/100 [00:07<00:10,  5.57it/s]Warming up with batch_size=512:  40%|████      | 40/100 [00:07<00:10,  5.57it/s]Warming up with batch_size=512:  41%|████      | 41/100 [00:07<00:10,  5.57it/s]Warming up with batch_size=512:  42%|████▏     | 42/100 [00:07<00:10,  5.57it/s]Warming up with batch_size=512:  43%|████▎     | 43/100 [00:07<00:10,  5.57it/s]Warming up with batch_size=512:  44%|████▍     | 44/100 [00:07<00:10,  5.58it/s]Warming up with batch_size=512:  45%|████▌     | 45/100 [00:08<00:09,  5.57it/s]Warming up with batch_size=512:  46%|████▌     | 46/100 [00:08<00:09,  5.57it/s]Warming up with batch_size=512:  47%|████▋     | 47/100 [00:08<00:09,  5.57it/s]Warming up with batch_size=512:  48%|████▊     | 48/100 [00:08<00:09,  5.57it/s]Warming up with batch_size=512:  49%|████▉     | 49/100 [00:08<00:09,  5.57it/s]Warming up with batch_size=512:  50%|█████     | 50/100 [00:08<00:08,  5.57it/s]Warming up with batch_size=512:  51%|█████     | 51/100 [00:09<00:08,  5.57it/s]Warming up with batch_size=512:  52%|█████▏    | 52/100 [00:09<00:08,  5.57it/s]Warming up with batch_size=512:  53%|█████▎    | 53/100 [00:09<00:08,  5.57it/s]Warming up with batch_size=512:  54%|█████▍    | 54/100 [00:09<00:08,  5.57it/s]Warming up with batch_size=512:  55%|█████▌    | 55/100 [00:09<00:08,  5.57it/s]Warming up with batch_size=512:  56%|█████▌    | 56/100 [00:10<00:07,  5.57it/s]Warming up with batch_size=512:  57%|█████▋    | 57/100 [00:10<00:07,  5.57it/s]Warming up with batch_size=512:  58%|█████▊    | 58/100 [00:10<00:07,  5.58it/s]Warming up with batch_size=512:  59%|█████▉    | 59/100 [00:10<00:07,  5.57it/s]Warming up with batch_size=512:  60%|██████    | 60/100 [00:10<00:07,  5.57it/s]Warming up with batch_size=512:  61%|██████    | 61/100 [00:10<00:06,  5.57it/s]Warming up with batch_size=512:  62%|██████▏   | 62/100 [00:11<00:06,  5.57it/s]Warming up with batch_size=512:  63%|██████▎   | 63/100 [00:11<00:06,  5.57it/s]Warming up with batch_size=512:  64%|██████▍   | 64/100 [00:11<00:06,  5.57it/s]Warming up with batch_size=512:  65%|██████▌   | 65/100 [00:11<00:06,  5.57it/s]Warming up with batch_size=512:  66%|██████▌   | 66/100 [00:11<00:06,  5.57it/s]Warming up with batch_size=512:  67%|██████▋   | 67/100 [00:12<00:05,  5.57it/s]Warming up with batch_size=512:  68%|██████▊   | 68/100 [00:12<00:05,  5.57it/s]Warming up with batch_size=512:  69%|██████▉   | 69/100 [00:12<00:05,  5.57it/s]Warming up with batch_size=512:  70%|███████   | 70/100 [00:12<00:05,  5.55it/s]Warming up with batch_size=512:  71%|███████   | 71/100 [00:12<00:05,  5.53it/s]Warming up with batch_size=512:  72%|███████▏  | 72/100 [00:12<00:05,  5.54it/s]Warming up with batch_size=512:  73%|███████▎  | 73/100 [00:13<00:04,  5.55it/s]Warming up with batch_size=512:  74%|███████▍  | 74/100 [00:13<00:04,  5.55it/s]Warming up with batch_size=512:  75%|███████▌  | 75/100 [00:13<00:04,  5.56it/s]Warming up with batch_size=512:  76%|███████▌  | 76/100 [00:13<00:04,  5.56it/s]Warming up with batch_size=512:  77%|███████▋  | 77/100 [00:13<00:04,  5.57it/s]Warming up with batch_size=512:  78%|███████▊  | 78/100 [00:14<00:03,  5.56it/s]Warming up with batch_size=512:  79%|███████▉  | 79/100 [00:14<00:03,  5.56it/s]Warming up with batch_size=512:  80%|████████  | 80/100 [00:14<00:03,  5.56it/s]Warming up with batch_size=512:  81%|████████  | 81/100 [00:14<00:03,  5.56it/s]Warming up with batch_size=512:  82%|████████▏ | 82/100 [00:14<00:03,  5.56it/s]Warming up with batch_size=512:  83%|████████▎ | 83/100 [00:14<00:03,  5.56it/s]Warming up with batch_size=512:  84%|████████▍ | 84/100 [00:15<00:02,  5.56it/s]Warming up with batch_size=512:  85%|████████▌ | 85/100 [00:15<00:02,  5.56it/s]Warming up with batch_size=512:  86%|████████▌ | 86/100 [00:15<00:02,  5.56it/s]Warming up with batch_size=512:  87%|████████▋ | 87/100 [00:15<00:02,  5.56it/s]Warming up with batch_size=512:  88%|████████▊ | 88/100 [00:15<00:02,  5.56it/s]Warming up with batch_size=512:  89%|████████▉ | 89/100 [00:15<00:01,  5.56it/s]Warming up with batch_size=512:  90%|█████████ | 90/100 [00:16<00:01,  5.56it/s]Warming up with batch_size=512:  91%|█████████ | 91/100 [00:16<00:01,  5.56it/s]Warming up with batch_size=512:  92%|█████████▏| 92/100 [00:16<00:01,  5.57it/s]Warming up with batch_size=512:  93%|█████████▎| 93/100 [00:16<00:01,  5.57it/s]Warming up with batch_size=512:  94%|█████████▍| 94/100 [00:16<00:01,  5.57it/s]Warming up with batch_size=512:  95%|█████████▌| 95/100 [00:17<00:00,  5.57it/s]Warming up with batch_size=512:  96%|█████████▌| 96/100 [00:17<00:00,  5.57it/s]Warming up with batch_size=512:  97%|█████████▋| 97/100 [00:17<00:00,  5.57it/s]Warming up with batch_size=512:  98%|█████████▊| 98/100 [00:17<00:00,  5.57it/s]Warming up with batch_size=512:  99%|█████████▉| 99/100 [00:17<00:00,  5.57it/s]Warming up with batch_size=512: 100%|██████████| 100/100 [00:17<00:00,  5.57it/s]Warming up with batch_size=512: 100%|██████████| 100/100 [00:17<00:00,  5.57it/s]
STAGE:2024-02-23 09:37:30 176263:176263 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:37:30 176263:176263 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:37:30 176263:176263 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=512:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=512:   0%|          | 1/1000 [00:00<02:59,  5.56it/s]Measuring inference for batch_size=512:   0%|          | 2/1000 [00:00<02:58,  5.58it/s]Measuring inference for batch_size=512:   0%|          | 3/1000 [00:00<02:58,  5.58it/s]Measuring inference for batch_size=512:   0%|          | 4/1000 [00:00<02:58,  5.59it/s]Measuring inference for batch_size=512:   0%|          | 5/1000 [00:00<02:58,  5.59it/s]Measuring inference for batch_size=512:   1%|          | 6/1000 [00:01<02:58,  5.58it/s]Measuring inference for batch_size=512:   1%|          | 7/1000 [00:01<02:57,  5.58it/s]Measuring inference for batch_size=512:   1%|          | 8/1000 [00:01<02:57,  5.58it/s]Measuring inference for batch_size=512:   1%|          | 9/1000 [00:01<02:57,  5.58it/s]Measuring inference for batch_size=512:   1%|          | 10/1000 [00:01<02:57,  5.58it/s]Measuring inference for batch_size=512:   1%|          | 11/1000 [00:01<02:57,  5.58it/s]Measuring inference for batch_size=512:   1%|          | 12/1000 [00:02<02:57,  5.58it/s]Measuring inference for batch_size=512:   1%|▏         | 13/1000 [00:02<02:56,  5.58it/s]Measuring inference for batch_size=512:   1%|▏         | 14/1000 [00:02<02:56,  5.58it/s]Measuring inference for batch_size=512:   2%|▏         | 15/1000 [00:02<02:57,  5.55it/s]Measuring inference for batch_size=512:   2%|▏         | 16/1000 [00:02<02:57,  5.53it/s]Measuring inference for batch_size=512:   2%|▏         | 17/1000 [00:03<02:57,  5.53it/s]Measuring inference for batch_size=512:   2%|▏         | 18/1000 [00:03<02:56,  5.55it/s]Measuring inference for batch_size=512:   2%|▏         | 19/1000 [00:03<02:56,  5.56it/s]Measuring inference for batch_size=512:   2%|▏         | 20/1000 [00:03<02:56,  5.56it/s]Measuring inference for batch_size=512:   2%|▏         | 21/1000 [00:03<02:55,  5.57it/s]Measuring inference for batch_size=512:   2%|▏         | 22/1000 [00:03<02:55,  5.57it/s]Measuring inference for batch_size=512:   2%|▏         | 23/1000 [00:04<02:55,  5.57it/s]Measuring inference for batch_size=512:   2%|▏         | 24/1000 [00:04<02:55,  5.58it/s]Measuring inference for batch_size=512:   2%|▎         | 25/1000 [00:04<02:54,  5.57it/s]Measuring inference for batch_size=512:   3%|▎         | 26/1000 [00:04<02:54,  5.57it/s]Measuring inference for batch_size=512:   3%|▎         | 27/1000 [00:04<02:54,  5.57it/s]Measuring inference for batch_size=512:   3%|▎         | 28/1000 [00:05<02:54,  5.58it/s]Measuring inference for batch_size=512:   3%|▎         | 29/1000 [00:05<02:53,  5.58it/s]Measuring inference for batch_size=512:   3%|▎         | 30/1000 [00:05<02:53,  5.58it/s]Measuring inference for batch_size=512:   3%|▎         | 31/1000 [00:05<02:53,  5.58it/s]Measuring inference for batch_size=512:   3%|▎         | 32/1000 [00:05<02:53,  5.58it/s]Measuring inference for batch_size=512:   3%|▎         | 33/1000 [00:05<02:53,  5.59it/s]Measuring inference for batch_size=512:   3%|▎         | 34/1000 [00:06<02:52,  5.59it/s]Measuring inference for batch_size=512:   4%|▎         | 35/1000 [00:06<02:52,  5.59it/s]Measuring inference for batch_size=512:   4%|▎         | 36/1000 [00:06<02:52,  5.59it/s]Measuring inference for batch_size=512:   4%|▎         | 37/1000 [00:06<02:52,  5.59it/s]Measuring inference for batch_size=512:   4%|▍         | 38/1000 [00:06<02:52,  5.59it/s]Measuring inference for batch_size=512:   4%|▍         | 39/1000 [00:06<02:52,  5.58it/s]Measuring inference for batch_size=512:   4%|▍         | 40/1000 [00:07<02:52,  5.58it/s]Measuring inference for batch_size=512:   4%|▍         | 41/1000 [00:07<02:51,  5.59it/s]Measuring inference for batch_size=512:   4%|▍         | 42/1000 [00:07<02:51,  5.59it/s]Measuring inference for batch_size=512:   4%|▍         | 43/1000 [00:07<02:51,  5.58it/s]Measuring inference for batch_size=512:   4%|▍         | 44/1000 [00:07<02:51,  5.59it/s]Measuring inference for batch_size=512:   4%|▍         | 45/1000 [00:08<02:50,  5.59it/s]Measuring inference for batch_size=512:   5%|▍         | 46/1000 [00:08<02:50,  5.59it/s]Measuring inference for batch_size=512:   5%|▍         | 47/1000 [00:08<02:50,  5.59it/s]Measuring inference for batch_size=512:   5%|▍         | 48/1000 [00:08<02:50,  5.58it/s]Measuring inference for batch_size=512:   5%|▍         | 49/1000 [00:08<02:50,  5.58it/s]Measuring inference for batch_size=512:   5%|▌         | 50/1000 [00:08<02:50,  5.58it/s]Measuring inference for batch_size=512:   5%|▌         | 51/1000 [00:09<02:50,  5.58it/s]Measuring inference for batch_size=512:   5%|▌         | 52/1000 [00:09<02:49,  5.58it/s]Measuring inference for batch_size=512:   5%|▌         | 53/1000 [00:09<02:49,  5.58it/s]Measuring inference for batch_size=512:   5%|▌         | 54/1000 [00:09<02:49,  5.58it/s]Measuring inference for batch_size=512:   6%|▌         | 55/1000 [00:09<02:49,  5.58it/s]Measuring inference for batch_size=512:   6%|▌         | 56/1000 [00:10<02:49,  5.58it/s]Measuring inference for batch_size=512:   6%|▌         | 57/1000 [00:10<02:48,  5.59it/s]Measuring inference for batch_size=512:   6%|▌         | 58/1000 [00:10<02:48,  5.58it/s]Measuring inference for batch_size=512:   6%|▌         | 59/1000 [00:10<02:48,  5.58it/s]Measuring inference for batch_size=512:   6%|▌         | 60/1000 [00:10<02:48,  5.58it/s]Measuring inference for batch_size=512:   6%|▌         | 61/1000 [00:10<02:48,  5.58it/s]Measuring inference for batch_size=512:   6%|▌         | 62/1000 [00:11<02:47,  5.59it/s]Measuring inference for batch_size=512:   6%|▋         | 63/1000 [00:11<02:47,  5.58it/s]Measuring inference for batch_size=512:   6%|▋         | 64/1000 [00:11<02:47,  5.58it/s]Measuring inference for batch_size=512:   6%|▋         | 65/1000 [00:11<02:47,  5.59it/s]Measuring inference for batch_size=512:   7%|▋         | 66/1000 [00:11<02:47,  5.59it/s]Measuring inference for batch_size=512:   7%|▋         | 67/1000 [00:12<02:47,  5.58it/s]Measuring inference for batch_size=512:   7%|▋         | 68/1000 [00:12<02:46,  5.58it/s]Measuring inference for batch_size=512:   7%|▋         | 69/1000 [00:12<02:46,  5.59it/s]Measuring inference for batch_size=512:   7%|▋         | 70/1000 [00:12<02:46,  5.59it/s]Measuring inference for batch_size=512:   7%|▋         | 71/1000 [00:12<02:46,  5.58it/s]Measuring inference for batch_size=512:   7%|▋         | 72/1000 [00:12<02:46,  5.57it/s]Measuring inference for batch_size=512:   7%|▋         | 73/1000 [00:13<02:46,  5.57it/s]Measuring inference for batch_size=512:   7%|▋         | 74/1000 [00:13<02:45,  5.58it/s]Measuring inference for batch_size=512:   8%|▊         | 75/1000 [00:13<02:45,  5.58it/s]Measuring inference for batch_size=512:   8%|▊         | 76/1000 [00:13<02:45,  5.57it/s]Measuring inference for batch_size=512:   8%|▊         | 77/1000 [00:13<02:45,  5.57it/s]Measuring inference for batch_size=512:   8%|▊         | 78/1000 [00:13<02:45,  5.58it/s]Measuring inference for batch_size=512:   8%|▊         | 79/1000 [00:14<02:44,  5.59it/s]Measuring inference for batch_size=512:   8%|▊         | 80/1000 [00:14<02:44,  5.58it/s]Measuring inference for batch_size=512:   8%|▊         | 81/1000 [00:14<02:44,  5.57it/s]Measuring inference for batch_size=512:   8%|▊         | 82/1000 [00:14<02:44,  5.57it/s]Measuring inference for batch_size=512:   8%|▊         | 83/1000 [00:14<02:44,  5.58it/s]Measuring inference for batch_size=512:   8%|▊         | 84/1000 [00:15<02:44,  5.58it/s]Measuring inference for batch_size=512:   8%|▊         | 85/1000 [00:15<02:43,  5.58it/s]Measuring inference for batch_size=512:   9%|▊         | 86/1000 [00:15<02:43,  5.58it/s]Measuring inference for batch_size=512:   9%|▊         | 87/1000 [00:15<02:43,  5.59it/s]Measuring inference for batch_size=512:   9%|▉         | 88/1000 [00:15<02:43,  5.58it/s]Measuring inference for batch_size=512:   9%|▉         | 89/1000 [00:15<02:43,  5.58it/s]Measuring inference for batch_size=512:   9%|▉         | 90/1000 [00:16<02:43,  5.58it/s]Measuring inference for batch_size=512:   9%|▉         | 91/1000 [00:16<02:43,  5.58it/s]Measuring inference for batch_size=512:   9%|▉         | 92/1000 [00:16<02:42,  5.58it/s]Measuring inference for batch_size=512:   9%|▉         | 93/1000 [00:16<02:42,  5.58it/s]Measuring inference for batch_size=512:   9%|▉         | 94/1000 [00:16<02:42,  5.59it/s]Measuring inference for batch_size=512:  10%|▉         | 95/1000 [00:17<02:42,  5.59it/s]Measuring inference for batch_size=512:  10%|▉         | 96/1000 [00:17<02:41,  5.59it/s]Measuring inference for batch_size=512:  10%|▉         | 97/1000 [00:17<02:41,  5.58it/s]Measuring inference for batch_size=512:  10%|▉         | 98/1000 [00:17<02:41,  5.58it/s]Measuring inference for batch_size=512:  10%|▉         | 99/1000 [00:17<02:41,  5.58it/s]Measuring inference for batch_size=512:  10%|█         | 100/1000 [00:17<02:41,  5.58it/s]Measuring inference for batch_size=512:  10%|█         | 101/1000 [00:18<02:40,  5.59it/s]Measuring inference for batch_size=512:  10%|█         | 102/1000 [00:18<02:40,  5.59it/s]Measuring inference for batch_size=512:  10%|█         | 103/1000 [00:18<02:40,  5.58it/s]Measuring inference for batch_size=512:  10%|█         | 104/1000 [00:18<02:40,  5.58it/s]Measuring inference for batch_size=512:  10%|█         | 105/1000 [00:18<02:40,  5.58it/s]Measuring inference for batch_size=512:  11%|█         | 106/1000 [00:18<02:40,  5.58it/s]Measuring inference for batch_size=512:  11%|█         | 107/1000 [00:19<02:39,  5.58it/s]Measuring inference for batch_size=512:  11%|█         | 108/1000 [00:19<02:39,  5.58it/s]Measuring inference for batch_size=512:  11%|█         | 109/1000 [00:19<02:39,  5.58it/s]Measuring inference for batch_size=512:  11%|█         | 110/1000 [00:19<02:39,  5.58it/s]Measuring inference for batch_size=512:  11%|█         | 111/1000 [00:19<02:39,  5.58it/s]Measuring inference for batch_size=512:  11%|█         | 112/1000 [00:20<02:39,  5.58it/s]Measuring inference for batch_size=512:  11%|█▏        | 113/1000 [00:20<02:38,  5.58it/s]Measuring inference for batch_size=512:  11%|█▏        | 114/1000 [00:20<02:38,  5.58it/s]Measuring inference for batch_size=512:  12%|█▏        | 115/1000 [00:20<02:38,  5.58it/s]Measuring inference for batch_size=512:  12%|█▏        | 116/1000 [00:20<02:38,  5.58it/s]Measuring inference for batch_size=512:  12%|█▏        | 117/1000 [00:20<02:38,  5.58it/s]Measuring inference for batch_size=512:  12%|█▏        | 118/1000 [00:21<02:38,  5.58it/s]Measuring inference for batch_size=512:  12%|█▏        | 119/1000 [00:21<02:37,  5.58it/s]Measuring inference for batch_size=512:  12%|█▏        | 120/1000 [00:21<02:37,  5.59it/s]Measuring inference for batch_size=512:  12%|█▏        | 121/1000 [00:21<02:37,  5.58it/s]Measuring inference for batch_size=512:  12%|█▏        | 122/1000 [00:21<02:37,  5.59it/s]Measuring inference for batch_size=512:  12%|█▏        | 123/1000 [00:22<02:36,  5.59it/s]Measuring inference for batch_size=512:  12%|█▏        | 124/1000 [00:22<02:36,  5.59it/s]Measuring inference for batch_size=512:  12%|█▎        | 125/1000 [00:22<02:36,  5.59it/s]Measuring inference for batch_size=512:  13%|█▎        | 126/1000 [00:22<02:36,  5.59it/s]Measuring inference for batch_size=512:  13%|█▎        | 127/1000 [00:22<02:36,  5.59it/s]Measuring inference for batch_size=512:  13%|█▎        | 128/1000 [00:22<02:36,  5.58it/s]Measuring inference for batch_size=512:  13%|█▎        | 129/1000 [00:23<02:35,  5.59it/s]Measuring inference for batch_size=512:  13%|█▎        | 130/1000 [00:23<02:35,  5.58it/s]Measuring inference for batch_size=512:  13%|█▎        | 131/1000 [00:23<02:35,  5.59it/s]Measuring inference for batch_size=512:  13%|█▎        | 132/1000 [00:23<02:35,  5.59it/s]Measuring inference for batch_size=512:  13%|█▎        | 133/1000 [00:23<02:35,  5.59it/s]Measuring inference for batch_size=512:  13%|█▎        | 134/1000 [00:24<02:35,  5.59it/s]Measuring inference for batch_size=512:  14%|█▎        | 135/1000 [00:24<02:34,  5.59it/s]Measuring inference for batch_size=512:  14%|█▎        | 136/1000 [00:24<02:34,  5.59it/s]Measuring inference for batch_size=512:  14%|█▎        | 137/1000 [00:24<02:34,  5.59it/s]Measuring inference for batch_size=512:  14%|█▍        | 138/1000 [00:24<02:34,  5.58it/s]Measuring inference for batch_size=512:  14%|█▍        | 139/1000 [00:24<02:34,  5.59it/s]Measuring inference for batch_size=512:  14%|█▍        | 140/1000 [00:25<02:33,  5.58it/s]Measuring inference for batch_size=512:  14%|█▍        | 141/1000 [00:25<02:33,  5.58it/s]Measuring inference for batch_size=512:  14%|█▍        | 142/1000 [00:25<02:33,  5.58it/s]Measuring inference for batch_size=512:  14%|█▍        | 143/1000 [00:25<02:33,  5.59it/s]Measuring inference for batch_size=512:  14%|█▍        | 144/1000 [00:25<02:33,  5.59it/s]Measuring inference for batch_size=512:  14%|█▍        | 145/1000 [00:25<02:33,  5.59it/s]Measuring inference for batch_size=512:  15%|█▍        | 146/1000 [00:26<02:32,  5.59it/s]Measuring inference for batch_size=512:  15%|█▍        | 147/1000 [00:26<02:32,  5.59it/s]Measuring inference for batch_size=512:  15%|█▍        | 148/1000 [00:26<02:32,  5.59it/s]Measuring inference for batch_size=512:  15%|█▍        | 149/1000 [00:26<02:32,  5.59it/s]Measuring inference for batch_size=512:  15%|█▌        | 150/1000 [00:26<02:32,  5.59it/s]Measuring inference for batch_size=512:  15%|█▌        | 151/1000 [00:27<02:31,  5.59it/s]Measuring inference for batch_size=512:  15%|█▌        | 152/1000 [00:27<02:31,  5.58it/s]Measuring inference for batch_size=512:  15%|█▌        | 153/1000 [00:27<02:31,  5.59it/s]Measuring inference for batch_size=512:  15%|█▌        | 154/1000 [00:27<02:31,  5.59it/s]Measuring inference for batch_size=512:  16%|█▌        | 155/1000 [00:27<02:31,  5.59it/s]Measuring inference for batch_size=512:  16%|█▌        | 156/1000 [00:27<02:31,  5.59it/s]Measuring inference for batch_size=512:  16%|█▌        | 157/1000 [00:28<02:30,  5.59it/s]Measuring inference for batch_size=512:  16%|█▌        | 158/1000 [00:28<02:30,  5.58it/s]Measuring inference for batch_size=512:  16%|█▌        | 159/1000 [00:28<02:30,  5.58it/s]Measuring inference for batch_size=512:  16%|█▌        | 160/1000 [00:28<02:30,  5.58it/s]Measuring inference for batch_size=512:  16%|█▌        | 161/1000 [00:28<02:30,  5.58it/s]Measuring inference for batch_size=512:  16%|█▌        | 162/1000 [00:29<02:30,  5.58it/s]Measuring inference for batch_size=512:  16%|█▋        | 163/1000 [00:29<02:29,  5.58it/s]Measuring inference for batch_size=512:  16%|█▋        | 164/1000 [00:29<02:29,  5.58it/s]Measuring inference for batch_size=512:  16%|█▋        | 165/1000 [00:29<02:29,  5.58it/s]Measuring inference for batch_size=512:  17%|█▋        | 166/1000 [00:29<02:29,  5.58it/s]Measuring inference for batch_size=512:  17%|█▋        | 167/1000 [00:29<02:29,  5.58it/s]Measuring inference for batch_size=512:  17%|█▋        | 168/1000 [00:30<02:28,  5.59it/s]Measuring inference for batch_size=512:  17%|█▋        | 169/1000 [00:30<02:28,  5.58it/s]Measuring inference for batch_size=512:  17%|█▋        | 170/1000 [00:30<02:28,  5.58it/s]Measuring inference for batch_size=512:  17%|█▋        | 171/1000 [00:30<02:28,  5.58it/s]Measuring inference for batch_size=512:  17%|█▋        | 172/1000 [00:30<02:28,  5.59it/s]Measuring inference for batch_size=512:  17%|█▋        | 173/1000 [00:30<02:28,  5.59it/s]Measuring inference for batch_size=512:  17%|█▋        | 174/1000 [00:31<02:27,  5.59it/s]Measuring inference for batch_size=512:  18%|█▊        | 175/1000 [00:31<02:27,  5.59it/s]Measuring inference for batch_size=512:  18%|█▊        | 176/1000 [00:31<02:27,  5.59it/s]Measuring inference for batch_size=512:  18%|█▊        | 177/1000 [00:31<02:27,  5.59it/s]Measuring inference for batch_size=512:  18%|█▊        | 178/1000 [00:31<02:27,  5.59it/s]Measuring inference for batch_size=512:  18%|█▊        | 179/1000 [00:32<02:26,  5.59it/s]Measuring inference for batch_size=512:  18%|█▊        | 180/1000 [00:32<02:26,  5.59it/s]Measuring inference for batch_size=512:  18%|█▊        | 181/1000 [00:32<02:26,  5.59it/s]Measuring inference for batch_size=512:  18%|█▊        | 182/1000 [00:32<02:26,  5.59it/s]Measuring inference for batch_size=512:  18%|█▊        | 183/1000 [00:32<02:26,  5.59it/s]Measuring inference for batch_size=512:  18%|█▊        | 184/1000 [00:32<02:26,  5.59it/s]Measuring inference for batch_size=512:  18%|█▊        | 185/1000 [00:33<02:25,  5.59it/s]Measuring inference for batch_size=512:  19%|█▊        | 186/1000 [00:33<02:25,  5.59it/s]Measuring inference for batch_size=512:  19%|█▊        | 187/1000 [00:33<02:25,  5.59it/s]Measuring inference for batch_size=512:  19%|█▉        | 188/1000 [00:33<02:25,  5.59it/s]Measuring inference for batch_size=512:  19%|█▉        | 189/1000 [00:33<02:25,  5.59it/s]Measuring inference for batch_size=512:  19%|█▉        | 190/1000 [00:34<02:24,  5.59it/s]Measuring inference for batch_size=512:  19%|█▉        | 191/1000 [00:34<02:24,  5.59it/s]Measuring inference for batch_size=512:  19%|█▉        | 192/1000 [00:34<02:24,  5.58it/s]Measuring inference for batch_size=512:  19%|█▉        | 193/1000 [00:34<02:24,  5.58it/s]Measuring inference for batch_size=512:  19%|█▉        | 194/1000 [00:34<02:24,  5.58it/s]Measuring inference for batch_size=512:  20%|█▉        | 195/1000 [00:34<02:24,  5.58it/s]Measuring inference for batch_size=512:  20%|█▉        | 196/1000 [00:35<02:23,  5.59it/s]Measuring inference for batch_size=512:  20%|█▉        | 197/1000 [00:35<02:23,  5.58it/s]Measuring inference for batch_size=512:  20%|█▉        | 198/1000 [00:35<02:23,  5.59it/s]Measuring inference for batch_size=512:  20%|█▉        | 199/1000 [00:35<02:23,  5.58it/s]Measuring inference for batch_size=512:  20%|██        | 200/1000 [00:35<02:23,  5.59it/s]Measuring inference for batch_size=512:  20%|██        | 201/1000 [00:36<02:23,  5.58it/s]Measuring inference for batch_size=512:  20%|██        | 202/1000 [00:36<02:23,  5.58it/s]Measuring inference for batch_size=512:  20%|██        | 203/1000 [00:36<02:22,  5.58it/s]Measuring inference for batch_size=512:  20%|██        | 204/1000 [00:36<02:22,  5.58it/s]Measuring inference for batch_size=512:  20%|██        | 205/1000 [00:36<02:22,  5.58it/s]Measuring inference for batch_size=512:  21%|██        | 206/1000 [00:36<02:22,  5.58it/s]Measuring inference for batch_size=512:  21%|██        | 207/1000 [00:37<02:22,  5.58it/s]Measuring inference for batch_size=512:  21%|██        | 208/1000 [00:37<02:21,  5.58it/s]Measuring inference for batch_size=512:  21%|██        | 209/1000 [00:37<02:21,  5.59it/s]Measuring inference for batch_size=512:  21%|██        | 210/1000 [00:37<02:21,  5.59it/s]Measuring inference for batch_size=512:  21%|██        | 211/1000 [00:37<02:21,  5.59it/s]Measuring inference for batch_size=512:  21%|██        | 212/1000 [00:37<02:21,  5.59it/s]Measuring inference for batch_size=512:  21%|██▏       | 213/1000 [00:38<02:20,  5.58it/s]Measuring inference for batch_size=512:  21%|██▏       | 214/1000 [00:38<02:20,  5.59it/s]Measuring inference for batch_size=512:  22%|██▏       | 215/1000 [00:38<02:20,  5.59it/s]Measuring inference for batch_size=512:  22%|██▏       | 216/1000 [00:38<02:20,  5.58it/s]Measuring inference for batch_size=512:  22%|██▏       | 217/1000 [00:38<02:20,  5.58it/s]Measuring inference for batch_size=512:  22%|██▏       | 218/1000 [00:39<02:20,  5.58it/s]Measuring inference for batch_size=512:  22%|██▏       | 219/1000 [00:39<02:19,  5.58it/s]Measuring inference for batch_size=512:  22%|██▏       | 220/1000 [00:39<02:19,  5.58it/s]Measuring inference for batch_size=512:  22%|██▏       | 221/1000 [00:39<02:19,  5.58it/s]Measuring inference for batch_size=512:  22%|██▏       | 222/1000 [00:39<02:19,  5.59it/s]Measuring inference for batch_size=512:  22%|██▏       | 223/1000 [00:39<02:19,  5.59it/s]Measuring inference for batch_size=512:  22%|██▏       | 224/1000 [00:40<02:18,  5.59it/s]Measuring inference for batch_size=512:  22%|██▎       | 225/1000 [00:40<02:18,  5.59it/s]Measuring inference for batch_size=512:  23%|██▎       | 226/1000 [00:40<02:18,  5.59it/s]Measuring inference for batch_size=512:  23%|██▎       | 227/1000 [00:40<02:18,  5.59it/s]Measuring inference for batch_size=512:  23%|██▎       | 228/1000 [00:40<02:18,  5.58it/s]Measuring inference for batch_size=512:  23%|██▎       | 229/1000 [00:41<02:18,  5.58it/s]Measuring inference for batch_size=512:  23%|██▎       | 230/1000 [00:41<02:17,  5.58it/s]Measuring inference for batch_size=512:  23%|██▎       | 231/1000 [00:41<02:17,  5.59it/s]Measuring inference for batch_size=512:  23%|██▎       | 232/1000 [00:41<02:17,  5.59it/s]Measuring inference for batch_size=512:  23%|██▎       | 233/1000 [00:41<02:17,  5.59it/s]Measuring inference for batch_size=512:  23%|██▎       | 234/1000 [00:41<02:17,  5.59it/s]Measuring inference for batch_size=512:  24%|██▎       | 235/1000 [00:42<02:17,  5.58it/s]Measuring inference for batch_size=512:  24%|██▎       | 236/1000 [00:42<02:16,  5.59it/s]Measuring inference for batch_size=512:  24%|██▎       | 237/1000 [00:42<02:16,  5.59it/s]Measuring inference for batch_size=512:  24%|██▍       | 238/1000 [00:42<02:16,  5.59it/s]Measuring inference for batch_size=512:  24%|██▍       | 239/1000 [00:42<02:16,  5.59it/s]Measuring inference for batch_size=512:  24%|██▍       | 240/1000 [00:42<02:15,  5.59it/s]Measuring inference for batch_size=512:  24%|██▍       | 241/1000 [00:43<02:15,  5.59it/s]Measuring inference for batch_size=512:  24%|██▍       | 242/1000 [00:43<02:15,  5.59it/s]Measuring inference for batch_size=512:  24%|██▍       | 243/1000 [00:43<02:15,  5.59it/s]Measuring inference for batch_size=512:  24%|██▍       | 244/1000 [00:43<02:15,  5.59it/s]Measuring inference for batch_size=512:  24%|██▍       | 245/1000 [00:43<02:15,  5.59it/s]Measuring inference for batch_size=512:  25%|██▍       | 246/1000 [00:44<02:14,  5.59it/s]Measuring inference for batch_size=512:  25%|██▍       | 247/1000 [00:44<02:14,  5.60it/s]Measuring inference for batch_size=512:  25%|██▍       | 248/1000 [00:44<02:14,  5.60it/s]Measuring inference for batch_size=512:  25%|██▍       | 249/1000 [00:44<02:14,  5.59it/s]Measuring inference for batch_size=512:  25%|██▌       | 250/1000 [00:44<02:14,  5.60it/s]Measuring inference for batch_size=512:  25%|██▌       | 251/1000 [00:44<02:13,  5.59it/s]Measuring inference for batch_size=512:  25%|██▌       | 252/1000 [00:45<02:13,  5.59it/s]Measuring inference for batch_size=512:  25%|██▌       | 253/1000 [00:45<02:13,  5.58it/s]Measuring inference for batch_size=512:  25%|██▌       | 254/1000 [00:45<02:13,  5.58it/s]Measuring inference for batch_size=512:  26%|██▌       | 255/1000 [00:45<02:13,  5.58it/s]Measuring inference for batch_size=512:  26%|██▌       | 256/1000 [00:45<02:13,  5.58it/s]Measuring inference for batch_size=512:  26%|██▌       | 257/1000 [00:46<02:13,  5.58it/s]Measuring inference for batch_size=512:  26%|██▌       | 258/1000 [00:46<02:12,  5.59it/s]Measuring inference for batch_size=512:  26%|██▌       | 259/1000 [00:46<02:12,  5.58it/s]Measuring inference for batch_size=512:  26%|██▌       | 260/1000 [00:46<02:12,  5.58it/s]Measuring inference for batch_size=512:  26%|██▌       | 261/1000 [00:46<02:12,  5.58it/s]Measuring inference for batch_size=512:  26%|██▌       | 262/1000 [00:46<02:12,  5.57it/s]Measuring inference for batch_size=512:  26%|██▋       | 263/1000 [00:47<02:12,  5.58it/s]Measuring inference for batch_size=512:  26%|██▋       | 264/1000 [00:47<02:11,  5.58it/s]Measuring inference for batch_size=512:  26%|██▋       | 265/1000 [00:47<02:11,  5.58it/s]Measuring inference for batch_size=512:  27%|██▋       | 266/1000 [00:47<02:11,  5.58it/s]Measuring inference for batch_size=512:  27%|██▋       | 267/1000 [00:47<02:11,  5.59it/s]Measuring inference for batch_size=512:  27%|██▋       | 268/1000 [00:48<02:10,  5.59it/s]Measuring inference for batch_size=512:  27%|██▋       | 269/1000 [00:48<02:10,  5.59it/s]Measuring inference for batch_size=512:  27%|██▋       | 270/1000 [00:48<02:10,  5.58it/s]Measuring inference for batch_size=512:  27%|██▋       | 271/1000 [00:48<02:10,  5.58it/s]Measuring inference for batch_size=512:  27%|██▋       | 272/1000 [00:48<02:10,  5.58it/s]Measuring inference for batch_size=512:  27%|██▋       | 273/1000 [00:48<02:10,  5.59it/s]Measuring inference for batch_size=512:  27%|██▋       | 274/1000 [00:49<02:09,  5.59it/s]Measuring inference for batch_size=512:  28%|██▊       | 275/1000 [00:49<02:09,  5.59it/s]Measuring inference for batch_size=512:  28%|██▊       | 276/1000 [00:49<02:09,  5.58it/s]Measuring inference for batch_size=512:  28%|██▊       | 277/1000 [00:49<02:09,  5.58it/s]Measuring inference for batch_size=512:  28%|██▊       | 278/1000 [00:49<02:09,  5.58it/s]Measuring inference for batch_size=512:  28%|██▊       | 279/1000 [00:49<02:09,  5.58it/s]Measuring inference for batch_size=512:  28%|██▊       | 280/1000 [00:50<02:08,  5.58it/s]Measuring inference for batch_size=512:  28%|██▊       | 281/1000 [00:50<02:08,  5.58it/s]Measuring inference for batch_size=512:  28%|██▊       | 282/1000 [00:50<02:08,  5.59it/s]Measuring inference for batch_size=512:  28%|██▊       | 283/1000 [00:50<02:08,  5.58it/s]Measuring inference for batch_size=512:  28%|██▊       | 284/1000 [00:50<02:08,  5.58it/s]Measuring inference for batch_size=512:  28%|██▊       | 285/1000 [00:51<02:08,  5.58it/s]Measuring inference for batch_size=512:  29%|██▊       | 286/1000 [00:51<02:07,  5.58it/s]Measuring inference for batch_size=512:  29%|██▊       | 287/1000 [00:51<02:07,  5.58it/s]Measuring inference for batch_size=512:  29%|██▉       | 288/1000 [00:51<02:07,  5.58it/s]Measuring inference for batch_size=512:  29%|██▉       | 289/1000 [00:51<02:07,  5.58it/s]Measuring inference for batch_size=512:  29%|██▉       | 290/1000 [00:51<02:07,  5.58it/s]Measuring inference for batch_size=512:  29%|██▉       | 291/1000 [00:52<02:06,  5.59it/s]Measuring inference for batch_size=512:  29%|██▉       | 292/1000 [00:52<02:06,  5.59it/s]Measuring inference for batch_size=512:  29%|██▉       | 293/1000 [00:52<02:06,  5.59it/s]Measuring inference for batch_size=512:  29%|██▉       | 294/1000 [00:52<02:06,  5.59it/s]Measuring inference for batch_size=512:  30%|██▉       | 295/1000 [00:52<02:06,  5.59it/s]Measuring inference for batch_size=512:  30%|██▉       | 296/1000 [00:53<02:06,  5.59it/s]Measuring inference for batch_size=512:  30%|██▉       | 297/1000 [00:53<02:05,  5.59it/s]Measuring inference for batch_size=512:  30%|██▉       | 298/1000 [00:53<02:05,  5.59it/s]Measuring inference for batch_size=512:  30%|██▉       | 299/1000 [00:53<02:05,  5.59it/s]Measuring inference for batch_size=512:  30%|███       | 300/1000 [00:53<02:05,  5.59it/s]Measuring inference for batch_size=512:  30%|███       | 301/1000 [00:53<02:05,  5.58it/s]Measuring inference for batch_size=512:  30%|███       | 302/1000 [00:54<02:05,  5.58it/s]Measuring inference for batch_size=512:  30%|███       | 303/1000 [00:54<02:04,  5.58it/s]Measuring inference for batch_size=512:  30%|███       | 304/1000 [00:54<02:04,  5.58it/s]Measuring inference for batch_size=512:  30%|███       | 305/1000 [00:54<02:04,  5.58it/s]Measuring inference for batch_size=512:  31%|███       | 306/1000 [00:54<02:04,  5.58it/s]Measuring inference for batch_size=512:  31%|███       | 307/1000 [00:54<02:03,  5.59it/s]Measuring inference for batch_size=512:  31%|███       | 308/1000 [00:55<02:03,  5.58it/s]Measuring inference for batch_size=512:  31%|███       | 309/1000 [00:55<02:03,  5.58it/s]Measuring inference for batch_size=512:  31%|███       | 310/1000 [00:55<02:03,  5.58it/s]Measuring inference for batch_size=512:  31%|███       | 311/1000 [00:55<02:03,  5.58it/s]Measuring inference for batch_size=512:  31%|███       | 312/1000 [00:55<02:03,  5.59it/s]Measuring inference for batch_size=512:  31%|███▏      | 313/1000 [00:56<02:02,  5.59it/s]Measuring inference for batch_size=512:  31%|███▏      | 314/1000 [00:56<02:02,  5.59it/s]Measuring inference for batch_size=512:  32%|███▏      | 315/1000 [00:56<02:02,  5.59it/s]Measuring inference for batch_size=512:  32%|███▏      | 316/1000 [00:56<02:02,  5.59it/s]Measuring inference for batch_size=512:  32%|███▏      | 317/1000 [00:56<02:02,  5.59it/s]Measuring inference for batch_size=512:  32%|███▏      | 318/1000 [00:56<02:02,  5.59it/s]Measuring inference for batch_size=512:  32%|███▏      | 319/1000 [00:57<02:01,  5.59it/s]Measuring inference for batch_size=512:  32%|███▏      | 320/1000 [00:57<02:01,  5.59it/s]Measuring inference for batch_size=512:  32%|███▏      | 321/1000 [00:57<02:01,  5.59it/s]Measuring inference for batch_size=512:  32%|███▏      | 322/1000 [00:57<02:01,  5.59it/s]Measuring inference for batch_size=512:  32%|███▏      | 323/1000 [00:57<02:01,  5.58it/s]Measuring inference for batch_size=512:  32%|███▏      | 324/1000 [00:58<02:01,  5.58it/s]Measuring inference for batch_size=512:  32%|███▎      | 325/1000 [00:58<02:01,  5.58it/s]Measuring inference for batch_size=512:  33%|███▎      | 326/1000 [00:58<02:00,  5.58it/s]Measuring inference for batch_size=512:  33%|███▎      | 327/1000 [00:58<02:00,  5.58it/s]Measuring inference for batch_size=512:  33%|███▎      | 328/1000 [00:58<02:00,  5.57it/s]Measuring inference for batch_size=512:  33%|███▎      | 329/1000 [00:58<02:00,  5.58it/s]Measuring inference for batch_size=512:  33%|███▎      | 330/1000 [00:59<02:00,  5.58it/s]Measuring inference for batch_size=512:  33%|███▎      | 331/1000 [00:59<02:00,  5.57it/s]Measuring inference for batch_size=512:  33%|███▎      | 332/1000 [00:59<01:59,  5.58it/s]Measuring inference for batch_size=512:  33%|███▎      | 333/1000 [00:59<01:59,  5.58it/s]Measuring inference for batch_size=512:  33%|███▎      | 334/1000 [00:59<01:59,  5.58it/s]Measuring inference for batch_size=512:  34%|███▎      | 335/1000 [01:00<01:59,  5.58it/s]Measuring inference for batch_size=512:  34%|███▎      | 336/1000 [01:00<01:59,  5.58it/s]Measuring inference for batch_size=512:  34%|███▎      | 337/1000 [01:00<01:58,  5.58it/s]Measuring inference for batch_size=512:  34%|███▍      | 338/1000 [01:00<01:58,  5.58it/s]Measuring inference for batch_size=512:  34%|███▍      | 339/1000 [01:00<01:58,  5.59it/s]Measuring inference for batch_size=512:  34%|███▍      | 340/1000 [01:00<01:58,  5.59it/s]Measuring inference for batch_size=512:  34%|███▍      | 341/1000 [01:01<01:57,  5.59it/s]Measuring inference for batch_size=512:  34%|███▍      | 342/1000 [01:01<01:57,  5.59it/s]Measuring inference for batch_size=512:  34%|███▍      | 343/1000 [01:01<01:57,  5.58it/s]Measuring inference for batch_size=512:  34%|███▍      | 344/1000 [01:01<01:57,  5.58it/s]Measuring inference for batch_size=512:  34%|███▍      | 345/1000 [01:01<01:57,  5.59it/s]Measuring inference for batch_size=512:  35%|███▍      | 346/1000 [01:01<01:57,  5.59it/s]Measuring inference for batch_size=512:  35%|███▍      | 347/1000 [01:02<01:56,  5.58it/s]Measuring inference for batch_size=512:  35%|███▍      | 348/1000 [01:02<01:56,  5.58it/s]Measuring inference for batch_size=512:  35%|███▍      | 349/1000 [01:02<01:56,  5.58it/s]Measuring inference for batch_size=512:  35%|███▌      | 350/1000 [01:02<01:56,  5.58it/s]Measuring inference for batch_size=512:  35%|███▌      | 351/1000 [01:02<01:56,  5.58it/s]Measuring inference for batch_size=512:  35%|███▌      | 352/1000 [01:03<01:56,  5.57it/s]Measuring inference for batch_size=512:  35%|███▌      | 353/1000 [01:03<01:55,  5.58it/s]Measuring inference for batch_size=512:  35%|███▌      | 354/1000 [01:03<01:55,  5.58it/s]Measuring inference for batch_size=512:  36%|███▌      | 355/1000 [01:03<01:55,  5.58it/s]Measuring inference for batch_size=512:  36%|███▌      | 356/1000 [01:03<01:55,  5.57it/s]Measuring inference for batch_size=512:  36%|███▌      | 357/1000 [01:03<01:55,  5.58it/s]Measuring inference for batch_size=512:  36%|███▌      | 358/1000 [01:04<01:55,  5.58it/s]Measuring inference for batch_size=512:  36%|███▌      | 359/1000 [01:04<01:54,  5.58it/s]Measuring inference for batch_size=512:  36%|███▌      | 360/1000 [01:04<01:54,  5.58it/s]Measuring inference for batch_size=512:  36%|███▌      | 361/1000 [01:04<01:54,  5.58it/s]Measuring inference for batch_size=512:  36%|███▌      | 362/1000 [01:04<01:54,  5.59it/s]Measuring inference for batch_size=512:  36%|███▋      | 363/1000 [01:05<01:53,  5.59it/s]Measuring inference for batch_size=512:  36%|███▋      | 364/1000 [01:05<01:53,  5.58it/s]Measuring inference for batch_size=512:  36%|███▋      | 365/1000 [01:05<01:53,  5.58it/s]Measuring inference for batch_size=512:  37%|███▋      | 366/1000 [01:05<01:53,  5.59it/s]Measuring inference for batch_size=512:  37%|███▋      | 367/1000 [01:05<01:53,  5.59it/s]Measuring inference for batch_size=512:  37%|███▋      | 368/1000 [01:05<01:53,  5.59it/s]Measuring inference for batch_size=512:  37%|███▋      | 369/1000 [01:06<01:52,  5.59it/s]Measuring inference for batch_size=512:  37%|███▋      | 370/1000 [01:06<01:52,  5.59it/s]Measuring inference for batch_size=512:  37%|███▋      | 371/1000 [01:06<01:52,  5.59it/s]Measuring inference for batch_size=512:  37%|███▋      | 372/1000 [01:06<01:52,  5.60it/s]Measuring inference for batch_size=512:  37%|███▋      | 373/1000 [01:06<01:52,  5.59it/s]Measuring inference for batch_size=512:  37%|███▋      | 374/1000 [01:06<01:52,  5.59it/s]Measuring inference for batch_size=512:  38%|███▊      | 375/1000 [01:07<01:51,  5.58it/s]Measuring inference for batch_size=512:  38%|███▊      | 376/1000 [01:07<01:51,  5.59it/s]Measuring inference for batch_size=512:  38%|███▊      | 377/1000 [01:07<01:51,  5.59it/s]Measuring inference for batch_size=512:  38%|███▊      | 378/1000 [01:07<01:51,  5.59it/s]Measuring inference for batch_size=512:  38%|███▊      | 379/1000 [01:07<01:51,  5.58it/s]Measuring inference for batch_size=512:  38%|███▊      | 380/1000 [01:08<01:51,  5.58it/s]Measuring inference for batch_size=512:  38%|███▊      | 381/1000 [01:08<01:50,  5.58it/s]Measuring inference for batch_size=512:  38%|███▊      | 382/1000 [01:08<01:50,  5.59it/s]Measuring inference for batch_size=512:  38%|███▊      | 383/1000 [01:08<01:50,  5.58it/s]Measuring inference for batch_size=512:  38%|███▊      | 384/1000 [01:08<01:50,  5.58it/s]Measuring inference for batch_size=512:  38%|███▊      | 385/1000 [01:08<01:50,  5.58it/s]Measuring inference for batch_size=512:  39%|███▊      | 386/1000 [01:09<01:49,  5.59it/s]Measuring inference for batch_size=512:  39%|███▊      | 387/1000 [01:09<01:49,  5.59it/s]Measuring inference for batch_size=512:  39%|███▉      | 388/1000 [01:09<01:49,  5.59it/s]Measuring inference for batch_size=512:  39%|███▉      | 389/1000 [01:09<01:49,  5.58it/s]Measuring inference for batch_size=512:  39%|███▉      | 390/1000 [01:09<01:49,  5.59it/s]Measuring inference for batch_size=512:  39%|███▉      | 391/1000 [01:10<01:49,  5.58it/s]Measuring inference for batch_size=512:  39%|███▉      | 392/1000 [01:10<01:48,  5.58it/s]Measuring inference for batch_size=512:  39%|███▉      | 393/1000 [01:10<01:48,  5.58it/s]Measuring inference for batch_size=512:  39%|███▉      | 394/1000 [01:10<01:48,  5.58it/s]Measuring inference for batch_size=512:  40%|███▉      | 395/1000 [01:10<01:48,  5.58it/s]Measuring inference for batch_size=512:  40%|███▉      | 396/1000 [01:10<01:48,  5.58it/s]Measuring inference for batch_size=512:  40%|███▉      | 397/1000 [01:11<01:48,  5.58it/s]Measuring inference for batch_size=512:  40%|███▉      | 398/1000 [01:11<01:47,  5.58it/s]Measuring inference for batch_size=512:  40%|███▉      | 399/1000 [01:11<01:47,  5.58it/s]Measuring inference for batch_size=512:  40%|████      | 400/1000 [01:11<01:47,  5.58it/s]Measuring inference for batch_size=512:  40%|████      | 401/1000 [01:11<01:47,  5.58it/s]Measuring inference for batch_size=512:  40%|████      | 402/1000 [01:12<01:47,  5.58it/s]Measuring inference for batch_size=512:  40%|████      | 403/1000 [01:12<01:47,  5.58it/s]Measuring inference for batch_size=512:  40%|████      | 404/1000 [01:12<01:46,  5.58it/s]Measuring inference for batch_size=512:  40%|████      | 405/1000 [01:12<01:46,  5.58it/s]Measuring inference for batch_size=512:  41%|████      | 406/1000 [01:12<01:46,  5.58it/s]Measuring inference for batch_size=512:  41%|████      | 407/1000 [01:12<01:46,  5.58it/s]Measuring inference for batch_size=512:  41%|████      | 408/1000 [01:13<01:46,  5.58it/s]Measuring inference for batch_size=512:  41%|████      | 409/1000 [01:13<01:45,  5.59it/s]Measuring inference for batch_size=512:  41%|████      | 410/1000 [01:13<01:45,  5.59it/s]Measuring inference for batch_size=512:  41%|████      | 411/1000 [01:13<01:45,  5.58it/s]Measuring inference for batch_size=512:  41%|████      | 412/1000 [01:13<01:45,  5.59it/s]Measuring inference for batch_size=512:  41%|████▏     | 413/1000 [01:13<01:45,  5.58it/s]Measuring inference for batch_size=512:  41%|████▏     | 414/1000 [01:14<01:44,  5.58it/s]Measuring inference for batch_size=512:  42%|████▏     | 415/1000 [01:14<01:44,  5.58it/s]Measuring inference for batch_size=512:  42%|████▏     | 416/1000 [01:14<01:44,  5.58it/s]Measuring inference for batch_size=512:  42%|████▏     | 417/1000 [01:14<01:44,  5.58it/s]Measuring inference for batch_size=512:  42%|████▏     | 418/1000 [01:14<01:44,  5.58it/s]Measuring inference for batch_size=512:  42%|████▏     | 419/1000 [01:15<01:44,  5.59it/s]Measuring inference for batch_size=512:  42%|████▏     | 420/1000 [01:15<01:43,  5.59it/s]Measuring inference for batch_size=512:  42%|████▏     | 421/1000 [01:15<01:43,  5.58it/s]Measuring inference for batch_size=512:  42%|████▏     | 422/1000 [01:15<01:43,  5.58it/s]Measuring inference for batch_size=512:  42%|████▏     | 423/1000 [01:15<01:43,  5.58it/s]Measuring inference for batch_size=512:  42%|████▏     | 424/1000 [01:15<01:43,  5.58it/s]Measuring inference for batch_size=512:  42%|████▎     | 425/1000 [01:16<01:42,  5.58it/s]Measuring inference for batch_size=512:  43%|████▎     | 426/1000 [01:16<01:42,  5.58it/s]Measuring inference for batch_size=512:  43%|████▎     | 427/1000 [01:16<01:42,  5.58it/s]Measuring inference for batch_size=512:  43%|████▎     | 428/1000 [01:16<01:42,  5.58it/s]Measuring inference for batch_size=512:  43%|████▎     | 429/1000 [01:16<01:42,  5.58it/s]Measuring inference for batch_size=512:  43%|████▎     | 430/1000 [01:17<01:42,  5.58it/s]Measuring inference for batch_size=512:  43%|████▎     | 431/1000 [01:17<01:41,  5.58it/s]Measuring inference for batch_size=512:  43%|████▎     | 432/1000 [01:17<01:41,  5.58it/s]Measuring inference for batch_size=512:  43%|████▎     | 433/1000 [01:17<01:41,  5.58it/s]Measuring inference for batch_size=512:  43%|████▎     | 434/1000 [01:17<01:41,  5.59it/s]Measuring inference for batch_size=512:  44%|████▎     | 435/1000 [01:17<01:41,  5.59it/s]Measuring inference for batch_size=512:  44%|████▎     | 436/1000 [01:18<01:41,  5.58it/s]Measuring inference for batch_size=512:  44%|████▎     | 437/1000 [01:18<01:40,  5.58it/s]Measuring inference for batch_size=512:  44%|████▍     | 438/1000 [01:18<01:40,  5.58it/s]Measuring inference for batch_size=512:  44%|████▍     | 439/1000 [01:18<01:40,  5.58it/s]Measuring inference for batch_size=512:  44%|████▍     | 440/1000 [01:18<01:40,  5.58it/s]Measuring inference for batch_size=512:  44%|████▍     | 441/1000 [01:18<01:40,  5.58it/s]Measuring inference for batch_size=512:  44%|████▍     | 442/1000 [01:19<01:39,  5.58it/s]Measuring inference for batch_size=512:  44%|████▍     | 443/1000 [01:19<01:39,  5.58it/s]Measuring inference for batch_size=512:  44%|████▍     | 444/1000 [01:19<01:39,  5.58it/s]Measuring inference for batch_size=512:  44%|████▍     | 445/1000 [01:19<01:39,  5.58it/s]Measuring inference for batch_size=512:  45%|████▍     | 446/1000 [01:19<01:39,  5.58it/s]Measuring inference for batch_size=512:  45%|████▍     | 447/1000 [01:20<01:39,  5.58it/s]Measuring inference for batch_size=512:  45%|████▍     | 448/1000 [01:20<01:38,  5.58it/s]Measuring inference for batch_size=512:  45%|████▍     | 449/1000 [01:20<01:38,  5.58it/s]Measuring inference for batch_size=512:  45%|████▌     | 450/1000 [01:20<01:38,  5.58it/s]Measuring inference for batch_size=512:  45%|████▌     | 451/1000 [01:20<01:38,  5.58it/s]Measuring inference for batch_size=512:  45%|████▌     | 452/1000 [01:20<01:38,  5.58it/s]Measuring inference for batch_size=512:  45%|████▌     | 453/1000 [01:21<01:38,  5.58it/s]Measuring inference for batch_size=512:  45%|████▌     | 454/1000 [01:21<01:37,  5.59it/s]Measuring inference for batch_size=512:  46%|████▌     | 455/1000 [01:21<01:37,  5.59it/s]Measuring inference for batch_size=512:  46%|████▌     | 456/1000 [01:21<01:37,  5.59it/s]Measuring inference for batch_size=512:  46%|████▌     | 457/1000 [01:21<01:37,  5.58it/s]Measuring inference for batch_size=512:  46%|████▌     | 458/1000 [01:22<01:37,  5.58it/s]Measuring inference for batch_size=512:  46%|████▌     | 459/1000 [01:22<01:36,  5.58it/s]Measuring inference for batch_size=512:  46%|████▌     | 460/1000 [01:22<01:36,  5.58it/s]Measuring inference for batch_size=512:  46%|████▌     | 461/1000 [01:22<01:36,  5.59it/s]Measuring inference for batch_size=512:  46%|████▌     | 462/1000 [01:22<01:36,  5.58it/s]Measuring inference for batch_size=512:  46%|████▋     | 463/1000 [01:22<01:36,  5.58it/s]Measuring inference for batch_size=512:  46%|████▋     | 464/1000 [01:23<01:36,  5.58it/s]Measuring inference for batch_size=512:  46%|████▋     | 465/1000 [01:23<01:35,  5.57it/s]Measuring inference for batch_size=512:  47%|████▋     | 466/1000 [01:23<01:35,  5.57it/s]Measuring inference for batch_size=512:  47%|████▋     | 467/1000 [01:23<01:35,  5.58it/s]Measuring inference for batch_size=512:  47%|████▋     | 468/1000 [01:23<01:35,  5.58it/s]Measuring inference for batch_size=512:  47%|████▋     | 469/1000 [01:24<01:35,  5.58it/s]Measuring inference for batch_size=512:  47%|████▋     | 470/1000 [01:24<01:34,  5.58it/s]Measuring inference for batch_size=512:  47%|████▋     | 471/1000 [01:24<01:34,  5.58it/s]Measuring inference for batch_size=512:  47%|████▋     | 472/1000 [01:24<01:34,  5.58it/s]Measuring inference for batch_size=512:  47%|████▋     | 473/1000 [01:24<01:34,  5.58it/s]Measuring inference for batch_size=512:  47%|████▋     | 474/1000 [01:24<01:34,  5.58it/s]Measuring inference for batch_size=512:  48%|████▊     | 475/1000 [01:25<01:34,  5.58it/s]Measuring inference for batch_size=512:  48%|████▊     | 476/1000 [01:25<01:33,  5.58it/s]Measuring inference for batch_size=512:  48%|████▊     | 477/1000 [01:25<01:33,  5.58it/s]Measuring inference for batch_size=512:  48%|████▊     | 478/1000 [01:25<01:33,  5.58it/s]Measuring inference for batch_size=512:  48%|████▊     | 479/1000 [01:25<01:33,  5.58it/s]Measuring inference for batch_size=512:  48%|████▊     | 480/1000 [01:25<01:33,  5.58it/s]Measuring inference for batch_size=512:  48%|████▊     | 481/1000 [01:26<01:32,  5.59it/s]Measuring inference for batch_size=512:  48%|████▊     | 482/1000 [01:26<01:32,  5.58it/s]Measuring inference for batch_size=512:  48%|████▊     | 483/1000 [01:26<01:32,  5.58it/s]Measuring inference for batch_size=512:  48%|████▊     | 484/1000 [01:26<01:32,  5.58it/s]Measuring inference for batch_size=512:  48%|████▊     | 485/1000 [01:26<01:32,  5.58it/s]Measuring inference for batch_size=512:  49%|████▊     | 486/1000 [01:27<01:32,  5.58it/s]Measuring inference for batch_size=512:  49%|████▊     | 487/1000 [01:27<01:31,  5.59it/s]Measuring inference for batch_size=512:  49%|████▉     | 488/1000 [01:27<01:31,  5.59it/s]Measuring inference for batch_size=512:  49%|████▉     | 489/1000 [01:27<01:31,  5.59it/s]Measuring inference for batch_size=512:  49%|████▉     | 490/1000 [01:27<01:31,  5.58it/s]Measuring inference for batch_size=512:  49%|████▉     | 491/1000 [01:27<01:31,  5.58it/s]Measuring inference for batch_size=512:  49%|████▉     | 492/1000 [01:28<01:30,  5.59it/s]Measuring inference for batch_size=512:  49%|████▉     | 493/1000 [01:28<01:30,  5.59it/s]Measuring inference for batch_size=512:  49%|████▉     | 494/1000 [01:28<01:30,  5.59it/s]Measuring inference for batch_size=512:  50%|████▉     | 495/1000 [01:28<01:30,  5.59it/s]Measuring inference for batch_size=512:  50%|████▉     | 496/1000 [01:28<01:30,  5.58it/s]Measuring inference for batch_size=512:  50%|████▉     | 497/1000 [01:29<01:30,  5.58it/s]Measuring inference for batch_size=512:  50%|████▉     | 498/1000 [01:29<01:30,  5.58it/s]Measuring inference for batch_size=512:  50%|████▉     | 499/1000 [01:29<01:29,  5.58it/s]Measuring inference for batch_size=512:  50%|█████     | 500/1000 [01:29<01:29,  5.58it/s]Measuring inference for batch_size=512:  50%|█████     | 501/1000 [01:29<01:29,  5.58it/s]Measuring inference for batch_size=512:  50%|█████     | 502/1000 [01:29<01:29,  5.58it/s]Measuring inference for batch_size=512:  50%|█████     | 503/1000 [01:30<01:28,  5.59it/s]Measuring inference for batch_size=512:  50%|█████     | 504/1000 [01:30<01:28,  5.58it/s]Measuring inference for batch_size=512:  50%|█████     | 505/1000 [01:30<01:28,  5.58it/s]Measuring inference for batch_size=512:  51%|█████     | 506/1000 [01:30<01:28,  5.58it/s]Measuring inference for batch_size=512:  51%|█████     | 507/1000 [01:30<01:28,  5.59it/s]Measuring inference for batch_size=512:  51%|█████     | 508/1000 [01:30<01:28,  5.58it/s]Measuring inference for batch_size=512:  51%|█████     | 509/1000 [01:31<01:28,  5.57it/s]Measuring inference for batch_size=512:  51%|█████     | 510/1000 [01:31<01:27,  5.57it/s]Measuring inference for batch_size=512:  51%|█████     | 511/1000 [01:31<01:27,  5.58it/s]Measuring inference for batch_size=512:  51%|█████     | 512/1000 [01:31<01:27,  5.58it/s]Measuring inference for batch_size=512:  51%|█████▏    | 513/1000 [01:31<01:27,  5.58it/s]Measuring inference for batch_size=512:  51%|█████▏    | 514/1000 [01:32<01:27,  5.58it/s]Measuring inference for batch_size=512:  52%|█████▏    | 515/1000 [01:32<01:26,  5.59it/s]Measuring inference for batch_size=512:  52%|█████▏    | 516/1000 [01:32<01:26,  5.59it/s]Measuring inference for batch_size=512:  52%|█████▏    | 517/1000 [01:32<01:26,  5.59it/s]Measuring inference for batch_size=512:  52%|█████▏    | 518/1000 [01:32<01:26,  5.59it/s]Measuring inference for batch_size=512:  52%|█████▏    | 519/1000 [01:32<01:26,  5.59it/s]Measuring inference for batch_size=512:  52%|█████▏    | 520/1000 [01:33<01:25,  5.59it/s]Measuring inference for batch_size=512:  52%|█████▏    | 521/1000 [01:33<01:25,  5.59it/s]Measuring inference for batch_size=512:  52%|█████▏    | 522/1000 [01:33<01:25,  5.58it/s]Measuring inference for batch_size=512:  52%|█████▏    | 523/1000 [01:33<01:25,  5.58it/s]Measuring inference for batch_size=512:  52%|█████▏    | 524/1000 [01:33<01:25,  5.58it/s]Measuring inference for batch_size=512:  52%|█████▎    | 525/1000 [01:34<01:25,  5.58it/s]Measuring inference for batch_size=512:  53%|█████▎    | 526/1000 [01:34<01:24,  5.58it/s]Measuring inference for batch_size=512:  53%|█████▎    | 527/1000 [01:34<01:24,  5.58it/s]Measuring inference for batch_size=512:  53%|█████▎    | 528/1000 [01:34<01:24,  5.58it/s]Measuring inference for batch_size=512:  53%|█████▎    | 529/1000 [01:34<01:24,  5.58it/s]Measuring inference for batch_size=512:  53%|█████▎    | 530/1000 [01:34<01:24,  5.59it/s]Measuring inference for batch_size=512:  53%|█████▎    | 531/1000 [01:35<01:23,  5.59it/s]Measuring inference for batch_size=512:  53%|█████▎    | 532/1000 [01:35<01:23,  5.58it/s]Measuring inference for batch_size=512:  53%|█████▎    | 533/1000 [01:35<01:23,  5.58it/s]Measuring inference for batch_size=512:  53%|█████▎    | 534/1000 [01:35<01:23,  5.58it/s]Measuring inference for batch_size=512:  54%|█████▎    | 535/1000 [01:35<01:23,  5.59it/s]Measuring inference for batch_size=512:  54%|█████▎    | 536/1000 [01:36<01:23,  5.59it/s]Measuring inference for batch_size=512:  54%|█████▎    | 537/1000 [01:36<01:23,  5.54it/s]Measuring inference for batch_size=512:  54%|█████▍    | 538/1000 [01:36<01:23,  5.55it/s]Measuring inference for batch_size=512:  54%|█████▍    | 539/1000 [01:36<01:22,  5.57it/s]Measuring inference for batch_size=512:  54%|█████▍    | 540/1000 [01:36<01:22,  5.58it/s]Measuring inference for batch_size=512:  54%|█████▍    | 541/1000 [01:36<01:22,  5.58it/s]Measuring inference for batch_size=512:  54%|█████▍    | 542/1000 [01:37<01:22,  5.58it/s]Measuring inference for batch_size=512:  54%|█████▍    | 543/1000 [01:37<01:21,  5.58it/s]Measuring inference for batch_size=512:  54%|█████▍    | 544/1000 [01:37<01:21,  5.58it/s]Measuring inference for batch_size=512:  55%|█████▍    | 545/1000 [01:37<01:21,  5.59it/s]Measuring inference for batch_size=512:  55%|█████▍    | 546/1000 [01:37<01:21,  5.58it/s]Measuring inference for batch_size=512:  55%|█████▍    | 547/1000 [01:37<01:21,  5.58it/s]Measuring inference for batch_size=512:  55%|█████▍    | 548/1000 [01:38<01:20,  5.59it/s]Measuring inference for batch_size=512:  55%|█████▍    | 549/1000 [01:38<01:20,  5.58it/s]Measuring inference for batch_size=512:  55%|█████▌    | 550/1000 [01:38<01:20,  5.58it/s]Measuring inference for batch_size=512:  55%|█████▌    | 551/1000 [01:38<01:20,  5.58it/s]Measuring inference for batch_size=512:  55%|█████▌    | 552/1000 [01:38<01:20,  5.58it/s]Measuring inference for batch_size=512:  55%|█████▌    | 553/1000 [01:39<01:20,  5.58it/s]Measuring inference for batch_size=512:  55%|█████▌    | 554/1000 [01:39<01:19,  5.58it/s]Measuring inference for batch_size=512:  56%|█████▌    | 555/1000 [01:39<01:19,  5.58it/s]Measuring inference for batch_size=512:  56%|█████▌    | 556/1000 [01:39<01:19,  5.58it/s]Measuring inference for batch_size=512:  56%|█████▌    | 557/1000 [01:39<01:19,  5.57it/s]Measuring inference for batch_size=512:  56%|█████▌    | 558/1000 [01:39<01:19,  5.58it/s]Measuring inference for batch_size=512:  56%|█████▌    | 559/1000 [01:40<01:18,  5.59it/s]Measuring inference for batch_size=512:  56%|█████▌    | 560/1000 [01:40<01:18,  5.58it/s]Measuring inference for batch_size=512:  56%|█████▌    | 561/1000 [01:40<01:18,  5.58it/s]Measuring inference for batch_size=512:  56%|█████▌    | 562/1000 [01:40<01:18,  5.58it/s]Measuring inference for batch_size=512:  56%|█████▋    | 563/1000 [01:40<01:18,  5.58it/s]Measuring inference for batch_size=512:  56%|█████▋    | 564/1000 [01:41<01:18,  5.59it/s]Measuring inference for batch_size=512:  56%|█████▋    | 565/1000 [01:41<01:17,  5.59it/s]Measuring inference for batch_size=512:  57%|█████▋    | 566/1000 [01:41<01:17,  5.59it/s]Measuring inference for batch_size=512:  57%|█████▋    | 567/1000 [01:41<01:17,  5.59it/s]Measuring inference for batch_size=512:  57%|█████▋    | 568/1000 [01:41<01:17,  5.60it/s]Measuring inference for batch_size=512:  57%|█████▋    | 569/1000 [01:41<01:16,  5.60it/s]Measuring inference for batch_size=512:  57%|█████▋    | 570/1000 [01:42<01:16,  5.60it/s]Measuring inference for batch_size=512:  57%|█████▋    | 571/1000 [01:42<01:16,  5.60it/s]Measuring inference for batch_size=512:  57%|█████▋    | 572/1000 [01:42<01:16,  5.61it/s]Measuring inference for batch_size=512:  57%|█████▋    | 573/1000 [01:42<01:16,  5.60it/s]Measuring inference for batch_size=512:  57%|█████▋    | 574/1000 [01:42<01:16,  5.60it/s]Measuring inference for batch_size=512:  57%|█████▊    | 575/1000 [01:42<01:15,  5.60it/s]Measuring inference for batch_size=512:  58%|█████▊    | 576/1000 [01:43<01:15,  5.60it/s]Measuring inference for batch_size=512:  58%|█████▊    | 577/1000 [01:43<01:15,  5.60it/s]Measuring inference for batch_size=512:  58%|█████▊    | 578/1000 [01:43<01:15,  5.59it/s]Measuring inference for batch_size=512:  58%|█████▊    | 579/1000 [01:43<01:15,  5.60it/s]Measuring inference for batch_size=512:  58%|█████▊    | 580/1000 [01:43<01:15,  5.60it/s]Measuring inference for batch_size=512:  58%|█████▊    | 581/1000 [01:44<01:14,  5.60it/s]Measuring inference for batch_size=512:  58%|█████▊    | 582/1000 [01:44<01:14,  5.60it/s]Measuring inference for batch_size=512:  58%|█████▊    | 583/1000 [01:44<01:14,  5.60it/s]Measuring inference for batch_size=512:  58%|█████▊    | 584/1000 [01:44<01:14,  5.60it/s]Measuring inference for batch_size=512:  58%|█████▊    | 585/1000 [01:44<01:14,  5.60it/s]Measuring inference for batch_size=512:  59%|█████▊    | 586/1000 [01:44<01:13,  5.60it/s]Measuring inference for batch_size=512:  59%|█████▊    | 587/1000 [01:45<01:13,  5.60it/s]Measuring inference for batch_size=512:  59%|█████▉    | 588/1000 [01:45<01:13,  5.59it/s]Measuring inference for batch_size=512:  59%|█████▉    | 589/1000 [01:45<01:13,  5.59it/s]Measuring inference for batch_size=512:  59%|█████▉    | 590/1000 [01:45<01:13,  5.60it/s]Measuring inference for batch_size=512:  59%|█████▉    | 591/1000 [01:45<01:13,  5.60it/s]Measuring inference for batch_size=512:  59%|█████▉    | 592/1000 [01:46<01:12,  5.60it/s]Measuring inference for batch_size=512:  59%|█████▉    | 593/1000 [01:46<01:12,  5.60it/s]Measuring inference for batch_size=512:  59%|█████▉    | 594/1000 [01:46<01:12,  5.60it/s]Measuring inference for batch_size=512:  60%|█████▉    | 595/1000 [01:46<01:12,  5.61it/s]Measuring inference for batch_size=512:  60%|█████▉    | 596/1000 [01:46<01:12,  5.60it/s]Measuring inference for batch_size=512:  60%|█████▉    | 597/1000 [01:46<01:11,  5.61it/s]Measuring inference for batch_size=512:  60%|█████▉    | 598/1000 [01:47<01:11,  5.61it/s]Measuring inference for batch_size=512:  60%|█████▉    | 599/1000 [01:47<01:11,  5.61it/s]Measuring inference for batch_size=512:  60%|██████    | 600/1000 [01:47<01:11,  5.60it/s]Measuring inference for batch_size=512:  60%|██████    | 601/1000 [01:47<01:11,  5.60it/s]Measuring inference for batch_size=512:  60%|██████    | 602/1000 [01:47<01:11,  5.60it/s]Measuring inference for batch_size=512:  60%|██████    | 603/1000 [01:47<01:10,  5.60it/s]Measuring inference for batch_size=512:  60%|██████    | 604/1000 [01:48<01:10,  5.60it/s]Measuring inference for batch_size=512:  60%|██████    | 605/1000 [01:48<01:10,  5.60it/s]Measuring inference for batch_size=512:  61%|██████    | 606/1000 [01:48<01:10,  5.60it/s]Measuring inference for batch_size=512:  61%|██████    | 607/1000 [01:48<01:10,  5.60it/s]Measuring inference for batch_size=512:  61%|██████    | 608/1000 [01:48<01:09,  5.60it/s]Measuring inference for batch_size=512:  61%|██████    | 609/1000 [01:49<01:09,  5.60it/s]Measuring inference for batch_size=512:  61%|██████    | 610/1000 [01:49<01:09,  5.60it/s]Measuring inference for batch_size=512:  61%|██████    | 611/1000 [01:49<01:09,  5.60it/s]Measuring inference for batch_size=512:  61%|██████    | 612/1000 [01:49<01:09,  5.59it/s]Measuring inference for batch_size=512:  61%|██████▏   | 613/1000 [01:49<01:09,  5.60it/s]Measuring inference for batch_size=512:  61%|██████▏   | 614/1000 [01:49<01:09,  5.59it/s]Measuring inference for batch_size=512:  62%|██████▏   | 615/1000 [01:50<01:08,  5.59it/s]Measuring inference for batch_size=512:  62%|██████▏   | 616/1000 [01:50<01:08,  5.59it/s]Measuring inference for batch_size=512:  62%|██████▏   | 617/1000 [01:50<01:08,  5.59it/s]Measuring inference for batch_size=512:  62%|██████▏   | 618/1000 [01:50<01:08,  5.60it/s]Measuring inference for batch_size=512:  62%|██████▏   | 619/1000 [01:50<01:08,  5.60it/s]Measuring inference for batch_size=512:  62%|██████▏   | 620/1000 [01:51<01:07,  5.60it/s]Measuring inference for batch_size=512:  62%|██████▏   | 621/1000 [01:51<01:07,  5.60it/s]Measuring inference for batch_size=512:  62%|██████▏   | 622/1000 [01:51<01:07,  5.60it/s]Measuring inference for batch_size=512:  62%|██████▏   | 623/1000 [01:51<01:07,  5.60it/s]Measuring inference for batch_size=512:  62%|██████▏   | 624/1000 [01:51<01:07,  5.60it/s]Measuring inference for batch_size=512:  62%|██████▎   | 625/1000 [01:51<01:06,  5.60it/s]Measuring inference for batch_size=512:  63%|██████▎   | 626/1000 [01:52<01:06,  5.60it/s]Measuring inference for batch_size=512:  63%|██████▎   | 627/1000 [01:52<01:06,  5.61it/s]Measuring inference for batch_size=512:  63%|██████▎   | 628/1000 [01:52<01:06,  5.60it/s]Measuring inference for batch_size=512:  63%|██████▎   | 629/1000 [01:52<01:06,  5.60it/s]Measuring inference for batch_size=512:  63%|██████▎   | 630/1000 [01:52<01:06,  5.59it/s]Measuring inference for batch_size=512:  63%|██████▎   | 631/1000 [01:52<01:05,  5.59it/s]Measuring inference for batch_size=512:  63%|██████▎   | 632/1000 [01:53<01:05,  5.59it/s]Measuring inference for batch_size=512:  63%|██████▎   | 633/1000 [01:53<01:05,  5.59it/s]Measuring inference for batch_size=512:  63%|██████▎   | 634/1000 [01:53<01:05,  5.60it/s]Measuring inference for batch_size=512:  64%|██████▎   | 635/1000 [01:53<01:05,  5.60it/s]Measuring inference for batch_size=512:  64%|██████▎   | 636/1000 [01:53<01:05,  5.60it/s]Measuring inference for batch_size=512:  64%|██████▎   | 637/1000 [01:54<01:04,  5.60it/s]Measuring inference for batch_size=512:  64%|██████▍   | 638/1000 [01:54<01:04,  5.60it/s]Measuring inference for batch_size=512:  64%|██████▍   | 639/1000 [01:54<01:04,  5.59it/s]Measuring inference for batch_size=512:  64%|██████▍   | 640/1000 [01:54<01:04,  5.59it/s]Measuring inference for batch_size=512:  64%|██████▍   | 641/1000 [01:54<01:04,  5.60it/s]Measuring inference for batch_size=512:  64%|██████▍   | 642/1000 [01:54<01:04,  5.59it/s]Measuring inference for batch_size=512:  64%|██████▍   | 643/1000 [01:55<01:03,  5.60it/s]Measuring inference for batch_size=512:  64%|██████▍   | 644/1000 [01:55<01:03,  5.60it/s]Measuring inference for batch_size=512:  64%|██████▍   | 645/1000 [01:55<01:03,  5.60it/s]Measuring inference for batch_size=512:  65%|██████▍   | 646/1000 [01:55<01:03,  5.60it/s]Measuring inference for batch_size=512:  65%|██████▍   | 647/1000 [01:55<01:03,  5.60it/s]Measuring inference for batch_size=512:  65%|██████▍   | 648/1000 [01:56<01:02,  5.60it/s]Measuring inference for batch_size=512:  65%|██████▍   | 649/1000 [01:56<01:02,  5.60it/s]Measuring inference for batch_size=512:  65%|██████▌   | 650/1000 [01:56<01:02,  5.60it/s]Measuring inference for batch_size=512:  65%|██████▌   | 651/1000 [01:56<01:02,  5.61it/s]Measuring inference for batch_size=512:  65%|██████▌   | 652/1000 [01:56<01:02,  5.60it/s]Measuring inference for batch_size=512:  65%|██████▌   | 653/1000 [01:56<01:01,  5.60it/s]Measuring inference for batch_size=512:  65%|██████▌   | 654/1000 [01:57<01:01,  5.60it/s]Measuring inference for batch_size=512:  66%|██████▌   | 655/1000 [01:57<01:01,  5.60it/s]Measuring inference for batch_size=512:  66%|██████▌   | 656/1000 [01:57<01:01,  5.60it/s]Measuring inference for batch_size=512:  66%|██████▌   | 657/1000 [01:57<01:01,  5.60it/s]Measuring inference for batch_size=512:  66%|██████▌   | 658/1000 [01:57<01:01,  5.60it/s]Measuring inference for batch_size=512:  66%|██████▌   | 659/1000 [01:57<01:00,  5.59it/s]Measuring inference for batch_size=512:  66%|██████▌   | 660/1000 [01:58<01:00,  5.59it/s]Measuring inference for batch_size=512:  66%|██████▌   | 661/1000 [01:58<01:00,  5.60it/s]Measuring inference for batch_size=512:  66%|██████▌   | 662/1000 [01:58<01:00,  5.60it/s]Measuring inference for batch_size=512:  66%|██████▋   | 663/1000 [01:58<01:00,  5.60it/s]Measuring inference for batch_size=512:  66%|██████▋   | 664/1000 [01:58<01:00,  5.59it/s]Measuring inference for batch_size=512:  66%|██████▋   | 665/1000 [01:59<00:59,  5.60it/s]Measuring inference for batch_size=512:  67%|██████▋   | 666/1000 [01:59<00:59,  5.60it/s]Measuring inference for batch_size=512:  67%|██████▋   | 667/1000 [01:59<00:59,  5.60it/s]Measuring inference for batch_size=512:  67%|██████▋   | 668/1000 [01:59<00:59,  5.61it/s]Measuring inference for batch_size=512:  67%|██████▋   | 669/1000 [01:59<00:59,  5.60it/s]Measuring inference for batch_size=512:  67%|██████▋   | 670/1000 [01:59<00:58,  5.61it/s]Measuring inference for batch_size=512:  67%|██████▋   | 671/1000 [02:00<00:58,  5.61it/s]Measuring inference for batch_size=512:  67%|██████▋   | 672/1000 [02:00<00:58,  5.60it/s]Measuring inference for batch_size=512:  67%|██████▋   | 673/1000 [02:00<00:58,  5.60it/s]Measuring inference for batch_size=512:  67%|██████▋   | 674/1000 [02:00<00:58,  5.60it/s]Measuring inference for batch_size=512:  68%|██████▊   | 675/1000 [02:00<00:58,  5.60it/s]Measuring inference for batch_size=512:  68%|██████▊   | 676/1000 [02:01<00:57,  5.60it/s]Measuring inference for batch_size=512:  68%|██████▊   | 677/1000 [02:01<00:57,  5.60it/s]Measuring inference for batch_size=512:  68%|██████▊   | 678/1000 [02:01<00:57,  5.60it/s]Measuring inference for batch_size=512:  68%|██████▊   | 679/1000 [02:01<00:57,  5.60it/s]Measuring inference for batch_size=512:  68%|██████▊   | 680/1000 [02:01<00:57,  5.60it/s]Measuring inference for batch_size=512:  68%|██████▊   | 681/1000 [02:01<00:56,  5.60it/s]Measuring inference for batch_size=512:  68%|██████▊   | 682/1000 [02:02<00:56,  5.60it/s]Measuring inference for batch_size=512:  68%|██████▊   | 683/1000 [02:02<00:56,  5.60it/s]Measuring inference for batch_size=512:  68%|██████▊   | 684/1000 [02:02<00:56,  5.60it/s]Measuring inference for batch_size=512:  68%|██████▊   | 685/1000 [02:02<00:56,  5.60it/s]Measuring inference for batch_size=512:  69%|██████▊   | 686/1000 [02:02<00:56,  5.60it/s]Measuring inference for batch_size=512:  69%|██████▊   | 687/1000 [02:02<00:55,  5.60it/s]Measuring inference for batch_size=512:  69%|██████▉   | 688/1000 [02:03<00:55,  5.60it/s]Measuring inference for batch_size=512:  69%|██████▉   | 689/1000 [02:03<00:55,  5.60it/s]Measuring inference for batch_size=512:  69%|██████▉   | 690/1000 [02:03<00:55,  5.61it/s]Measuring inference for batch_size=512:  69%|██████▉   | 691/1000 [02:03<00:55,  5.60it/s]Measuring inference for batch_size=512:  69%|██████▉   | 692/1000 [02:03<00:55,  5.60it/s]Measuring inference for batch_size=512:  69%|██████▉   | 693/1000 [02:04<00:54,  5.60it/s]Measuring inference for batch_size=512:  69%|██████▉   | 694/1000 [02:04<00:54,  5.60it/s]Measuring inference for batch_size=512:  70%|██████▉   | 695/1000 [02:04<00:54,  5.60it/s]Measuring inference for batch_size=512:  70%|██████▉   | 696/1000 [02:04<00:54,  5.60it/s]Measuring inference for batch_size=512:  70%|██████▉   | 697/1000 [02:04<00:54,  5.60it/s]Measuring inference for batch_size=512:  70%|██████▉   | 698/1000 [02:04<00:53,  5.60it/s]Measuring inference for batch_size=512:  70%|██████▉   | 699/1000 [02:05<00:53,  5.60it/s]Measuring inference for batch_size=512:  70%|███████   | 700/1000 [02:05<00:53,  5.60it/s]Measuring inference for batch_size=512:  70%|███████   | 701/1000 [02:05<00:53,  5.60it/s]Measuring inference for batch_size=512:  70%|███████   | 702/1000 [02:05<00:53,  5.60it/s]Measuring inference for batch_size=512:  70%|███████   | 703/1000 [02:05<00:53,  5.60it/s]Measuring inference for batch_size=512:  70%|███████   | 704/1000 [02:06<00:52,  5.60it/s]Measuring inference for batch_size=512:  70%|███████   | 705/1000 [02:06<00:52,  5.60it/s]Measuring inference for batch_size=512:  71%|███████   | 706/1000 [02:06<00:52,  5.60it/s]Measuring inference for batch_size=512:  71%|███████   | 707/1000 [02:06<00:52,  5.60it/s]Measuring inference for batch_size=512:  71%|███████   | 708/1000 [02:06<00:52,  5.60it/s]Measuring inference for batch_size=512:  71%|███████   | 709/1000 [02:06<00:51,  5.60it/s]Measuring inference for batch_size=512:  71%|███████   | 710/1000 [02:07<00:51,  5.60it/s]Measuring inference for batch_size=512:  71%|███████   | 711/1000 [02:07<00:51,  5.60it/s]Measuring inference for batch_size=512:  71%|███████   | 712/1000 [02:07<00:51,  5.59it/s]Measuring inference for batch_size=512:  71%|███████▏  | 713/1000 [02:07<00:51,  5.59it/s]Measuring inference for batch_size=512:  71%|███████▏  | 714/1000 [02:07<00:51,  5.59it/s]Measuring inference for batch_size=512:  72%|███████▏  | 715/1000 [02:07<00:50,  5.59it/s]Measuring inference for batch_size=512:  72%|███████▏  | 716/1000 [02:08<00:50,  5.60it/s]Measuring inference for batch_size=512:  72%|███████▏  | 717/1000 [02:08<00:50,  5.59it/s]Measuring inference for batch_size=512:  72%|███████▏  | 718/1000 [02:08<00:50,  5.59it/s]Measuring inference for batch_size=512:  72%|███████▏  | 719/1000 [02:08<00:50,  5.59it/s]Measuring inference for batch_size=512:  72%|███████▏  | 720/1000 [02:08<00:50,  5.60it/s]Measuring inference for batch_size=512:  72%|███████▏  | 721/1000 [02:09<00:49,  5.59it/s]Measuring inference for batch_size=512:  72%|███████▏  | 722/1000 [02:09<00:49,  5.59it/s]Measuring inference for batch_size=512:  72%|███████▏  | 723/1000 [02:09<00:49,  5.59it/s]Measuring inference for batch_size=512:  72%|███████▏  | 724/1000 [02:09<00:49,  5.60it/s]Measuring inference for batch_size=512:  72%|███████▎  | 725/1000 [02:09<00:49,  5.60it/s]Measuring inference for batch_size=512:  73%|███████▎  | 726/1000 [02:09<00:48,  5.60it/s]Measuring inference for batch_size=512:  73%|███████▎  | 727/1000 [02:10<00:48,  5.60it/s]Measuring inference for batch_size=512:  73%|███████▎  | 728/1000 [02:10<00:48,  5.60it/s]Measuring inference for batch_size=512:  73%|███████▎  | 729/1000 [02:10<00:48,  5.60it/s]Measuring inference for batch_size=512:  73%|███████▎  | 730/1000 [02:10<00:48,  5.59it/s]Measuring inference for batch_size=512:  73%|███████▎  | 731/1000 [02:10<00:48,  5.59it/s]Measuring inference for batch_size=512:  73%|███████▎  | 732/1000 [02:11<00:47,  5.59it/s]Measuring inference for batch_size=512:  73%|███████▎  | 733/1000 [02:11<00:47,  5.59it/s]Measuring inference for batch_size=512:  73%|███████▎  | 734/1000 [02:11<00:47,  5.59it/s]Measuring inference for batch_size=512:  74%|███████▎  | 735/1000 [02:11<00:47,  5.60it/s]Measuring inference for batch_size=512:  74%|███████▎  | 736/1000 [02:11<00:47,  5.60it/s]Measuring inference for batch_size=512:  74%|███████▎  | 737/1000 [02:11<00:46,  5.60it/s]Measuring inference for batch_size=512:  74%|███████▍  | 738/1000 [02:12<00:46,  5.60it/s]Measuring inference for batch_size=512:  74%|███████▍  | 739/1000 [02:12<00:46,  5.60it/s]Measuring inference for batch_size=512:  74%|███████▍  | 740/1000 [02:12<00:46,  5.60it/s]Measuring inference for batch_size=512:  74%|███████▍  | 741/1000 [02:12<00:46,  5.59it/s]Measuring inference for batch_size=512:  74%|███████▍  | 742/1000 [02:12<00:46,  5.56it/s]Measuring inference for batch_size=512:  74%|███████▍  | 743/1000 [02:13<00:46,  5.57it/s]Measuring inference for batch_size=512:  74%|███████▍  | 744/1000 [02:13<00:45,  5.57it/s]Measuring inference for batch_size=512:  74%|███████▍  | 745/1000 [02:13<00:45,  5.58it/s]Measuring inference for batch_size=512:  75%|███████▍  | 746/1000 [02:13<00:45,  5.58it/s]Measuring inference for batch_size=512:  75%|███████▍  | 747/1000 [02:13<00:45,  5.59it/s]Measuring inference for batch_size=512:  75%|███████▍  | 748/1000 [02:13<00:45,  5.59it/s]Measuring inference for batch_size=512:  75%|███████▍  | 749/1000 [02:14<00:44,  5.59it/s]Measuring inference for batch_size=512:  75%|███████▌  | 750/1000 [02:14<00:44,  5.59it/s]Measuring inference for batch_size=512:  75%|███████▌  | 751/1000 [02:14<00:44,  5.60it/s]Measuring inference for batch_size=512:  75%|███████▌  | 752/1000 [02:14<00:44,  5.59it/s]Measuring inference for batch_size=512:  75%|███████▌  | 753/1000 [02:14<00:44,  5.60it/s]Measuring inference for batch_size=512:  75%|███████▌  | 754/1000 [02:14<00:43,  5.60it/s]Measuring inference for batch_size=512:  76%|███████▌  | 755/1000 [02:15<00:43,  5.59it/s]Measuring inference for batch_size=512:  76%|███████▌  | 756/1000 [02:15<00:43,  5.59it/s]Measuring inference for batch_size=512:  76%|███████▌  | 757/1000 [02:15<00:43,  5.60it/s]Measuring inference for batch_size=512:  76%|███████▌  | 758/1000 [02:15<00:43,  5.60it/s]Measuring inference for batch_size=512:  76%|███████▌  | 759/1000 [02:15<00:43,  5.60it/s]Measuring inference for batch_size=512:  76%|███████▌  | 760/1000 [02:16<00:42,  5.60it/s]Measuring inference for batch_size=512:  76%|███████▌  | 761/1000 [02:16<00:42,  5.60it/s]Measuring inference for batch_size=512:  76%|███████▌  | 762/1000 [02:16<00:42,  5.60it/s]Measuring inference for batch_size=512:  76%|███████▋  | 763/1000 [02:16<00:42,  5.60it/s]Measuring inference for batch_size=512:  76%|███████▋  | 764/1000 [02:16<00:42,  5.60it/s]Measuring inference for batch_size=512:  76%|███████▋  | 765/1000 [02:16<00:41,  5.60it/s]Measuring inference for batch_size=512:  77%|███████▋  | 766/1000 [02:17<00:41,  5.60it/s]Measuring inference for batch_size=512:  77%|███████▋  | 767/1000 [02:17<00:41,  5.60it/s]Measuring inference for batch_size=512:  77%|███████▋  | 768/1000 [02:17<00:41,  5.59it/s]Measuring inference for batch_size=512:  77%|███████▋  | 769/1000 [02:17<00:41,  5.59it/s]Measuring inference for batch_size=512:  77%|███████▋  | 770/1000 [02:17<00:41,  5.60it/s]Measuring inference for batch_size=512:  77%|███████▋  | 771/1000 [02:18<00:40,  5.60it/s]Measuring inference for batch_size=512:  77%|███████▋  | 772/1000 [02:18<00:40,  5.60it/s]Measuring inference for batch_size=512:  77%|███████▋  | 773/1000 [02:18<00:40,  5.61it/s]Measuring inference for batch_size=512:  77%|███████▋  | 774/1000 [02:18<00:40,  5.61it/s]Measuring inference for batch_size=512:  78%|███████▊  | 775/1000 [02:18<00:40,  5.60it/s]Measuring inference for batch_size=512:  78%|███████▊  | 776/1000 [02:18<00:39,  5.61it/s]Measuring inference for batch_size=512:  78%|███████▊  | 777/1000 [02:19<00:39,  5.61it/s]Measuring inference for batch_size=512:  78%|███████▊  | 778/1000 [02:19<00:39,  5.61it/s]Measuring inference for batch_size=512:  78%|███████▊  | 779/1000 [02:19<00:39,  5.61it/s]Measuring inference for batch_size=512:  78%|███████▊  | 780/1000 [02:19<00:39,  5.60it/s]Measuring inference for batch_size=512:  78%|███████▊  | 781/1000 [02:19<00:39,  5.60it/s]Measuring inference for batch_size=512:  78%|███████▊  | 782/1000 [02:19<00:38,  5.60it/s]Measuring inference for batch_size=512:  78%|███████▊  | 783/1000 [02:20<00:38,  5.61it/s]Measuring inference for batch_size=512:  78%|███████▊  | 784/1000 [02:20<00:38,  5.60it/s]Measuring inference for batch_size=512:  78%|███████▊  | 785/1000 [02:20<00:38,  5.60it/s]Measuring inference for batch_size=512:  79%|███████▊  | 786/1000 [02:20<00:38,  5.60it/s]Measuring inference for batch_size=512:  79%|███████▊  | 787/1000 [02:20<00:38,  5.60it/s]Measuring inference for batch_size=512:  79%|███████▉  | 788/1000 [02:21<00:37,  5.60it/s]Measuring inference for batch_size=512:  79%|███████▉  | 789/1000 [02:21<00:37,  5.60it/s]Measuring inference for batch_size=512:  79%|███████▉  | 790/1000 [02:21<00:37,  5.60it/s]Measuring inference for batch_size=512:  79%|███████▉  | 791/1000 [02:21<00:37,  5.60it/s]Measuring inference for batch_size=512:  79%|███████▉  | 792/1000 [02:21<00:37,  5.60it/s]Measuring inference for batch_size=512:  79%|███████▉  | 793/1000 [02:21<00:36,  5.60it/s]Measuring inference for batch_size=512:  79%|███████▉  | 794/1000 [02:22<00:36,  5.60it/s]Measuring inference for batch_size=512:  80%|███████▉  | 795/1000 [02:22<00:36,  5.60it/s]Measuring inference for batch_size=512:  80%|███████▉  | 796/1000 [02:22<00:36,  5.60it/s]Measuring inference for batch_size=512:  80%|███████▉  | 797/1000 [02:22<00:36,  5.60it/s]Measuring inference for batch_size=512:  80%|███████▉  | 798/1000 [02:22<00:36,  5.60it/s]Measuring inference for batch_size=512:  80%|███████▉  | 799/1000 [02:23<00:35,  5.60it/s]Measuring inference for batch_size=512:  80%|████████  | 800/1000 [02:23<00:35,  5.60it/s]Measuring inference for batch_size=512:  80%|████████  | 801/1000 [02:23<00:35,  5.60it/s]Measuring inference for batch_size=512:  80%|████████  | 802/1000 [02:23<00:35,  5.60it/s]Measuring inference for batch_size=512:  80%|████████  | 803/1000 [02:23<00:35,  5.60it/s]Measuring inference for batch_size=512:  80%|████████  | 804/1000 [02:23<00:35,  5.59it/s]Measuring inference for batch_size=512:  80%|████████  | 805/1000 [02:24<00:34,  5.60it/s]Measuring inference for batch_size=512:  81%|████████  | 806/1000 [02:24<00:34,  5.59it/s]Measuring inference for batch_size=512:  81%|████████  | 807/1000 [02:24<00:34,  5.59it/s]Measuring inference for batch_size=512:  81%|████████  | 808/1000 [02:24<00:34,  5.59it/s]Measuring inference for batch_size=512:  81%|████████  | 809/1000 [02:24<00:34,  5.59it/s]Measuring inference for batch_size=512:  81%|████████  | 810/1000 [02:24<00:34,  5.59it/s]Measuring inference for batch_size=512:  81%|████████  | 811/1000 [02:25<00:33,  5.59it/s]Measuring inference for batch_size=512:  81%|████████  | 812/1000 [02:25<00:33,  5.59it/s]Measuring inference for batch_size=512:  81%|████████▏ | 813/1000 [02:25<00:33,  5.60it/s]Measuring inference for batch_size=512:  81%|████████▏ | 814/1000 [02:25<00:33,  5.60it/s]Measuring inference for batch_size=512:  82%|████████▏ | 815/1000 [02:25<00:33,  5.60it/s]Measuring inference for batch_size=512:  82%|████████▏ | 816/1000 [02:26<00:32,  5.60it/s]Measuring inference for batch_size=512:  82%|████████▏ | 817/1000 [02:26<00:32,  5.60it/s]Measuring inference for batch_size=512:  82%|████████▏ | 818/1000 [02:26<00:32,  5.61it/s]Measuring inference for batch_size=512:  82%|████████▏ | 819/1000 [02:26<00:32,  5.60it/s]Measuring inference for batch_size=512:  82%|████████▏ | 820/1000 [02:26<00:32,  5.60it/s]Measuring inference for batch_size=512:  82%|████████▏ | 821/1000 [02:26<00:31,  5.60it/s]Measuring inference for batch_size=512:  82%|████████▏ | 822/1000 [02:27<00:31,  5.60it/s]Measuring inference for batch_size=512:  82%|████████▏ | 823/1000 [02:27<00:31,  5.60it/s]Measuring inference for batch_size=512:  82%|████████▏ | 824/1000 [02:27<00:31,  5.60it/s]Measuring inference for batch_size=512:  82%|████████▎ | 825/1000 [02:27<00:31,  5.60it/s]Measuring inference for batch_size=512:  83%|████████▎ | 826/1000 [02:27<00:31,  5.60it/s]Measuring inference for batch_size=512:  83%|████████▎ | 827/1000 [02:28<00:30,  5.60it/s]Measuring inference for batch_size=512:  83%|████████▎ | 828/1000 [02:28<00:30,  5.60it/s]Measuring inference for batch_size=512:  83%|████████▎ | 829/1000 [02:28<00:30,  5.60it/s]Measuring inference for batch_size=512:  83%|████████▎ | 830/1000 [02:28<00:30,  5.60it/s]Measuring inference for batch_size=512:  83%|████████▎ | 831/1000 [02:28<00:30,  5.60it/s]Measuring inference for batch_size=512:  83%|████████▎ | 832/1000 [02:28<00:29,  5.60it/s]Measuring inference for batch_size=512:  83%|████████▎ | 833/1000 [02:29<00:29,  5.60it/s]Measuring inference for batch_size=512:  83%|████████▎ | 834/1000 [02:29<00:29,  5.60it/s]Measuring inference for batch_size=512:  84%|████████▎ | 835/1000 [02:29<00:29,  5.59it/s]Measuring inference for batch_size=512:  84%|████████▎ | 836/1000 [02:29<00:29,  5.60it/s]Measuring inference for batch_size=512:  84%|████████▎ | 837/1000 [02:29<00:29,  5.59it/s]Measuring inference for batch_size=512:  84%|████████▍ | 838/1000 [02:29<00:28,  5.60it/s]Measuring inference for batch_size=512:  84%|████████▍ | 839/1000 [02:30<00:28,  5.60it/s]Measuring inference for batch_size=512:  84%|████████▍ | 840/1000 [02:30<00:28,  5.60it/s]Measuring inference for batch_size=512:  84%|████████▍ | 841/1000 [02:30<00:28,  5.59it/s]Measuring inference for batch_size=512:  84%|████████▍ | 842/1000 [02:30<00:28,  5.60it/s]Measuring inference for batch_size=512:  84%|████████▍ | 843/1000 [02:30<00:28,  5.60it/s]Measuring inference for batch_size=512:  84%|████████▍ | 844/1000 [02:31<00:27,  5.60it/s]Measuring inference for batch_size=512:  84%|████████▍ | 845/1000 [02:31<00:27,  5.60it/s]Measuring inference for batch_size=512:  85%|████████▍ | 846/1000 [02:31<00:27,  5.60it/s]Measuring inference for batch_size=512:  85%|████████▍ | 847/1000 [02:31<00:27,  5.60it/s]Measuring inference for batch_size=512:  85%|████████▍ | 848/1000 [02:31<00:27,  5.60it/s]Measuring inference for batch_size=512:  85%|████████▍ | 849/1000 [02:31<00:26,  5.60it/s]Measuring inference for batch_size=512:  85%|████████▌ | 850/1000 [02:32<00:26,  5.60it/s]Measuring inference for batch_size=512:  85%|████████▌ | 851/1000 [02:32<00:26,  5.59it/s]Measuring inference for batch_size=512:  85%|████████▌ | 852/1000 [02:32<00:26,  5.60it/s]Measuring inference for batch_size=512:  85%|████████▌ | 853/1000 [02:32<00:26,  5.60it/s]Measuring inference for batch_size=512:  85%|████████▌ | 854/1000 [02:32<00:26,  5.60it/s]Measuring inference for batch_size=512:  86%|████████▌ | 855/1000 [02:33<00:25,  5.60it/s]Measuring inference for batch_size=512:  86%|████████▌ | 856/1000 [02:33<00:25,  5.60it/s]Measuring inference for batch_size=512:  86%|████████▌ | 857/1000 [02:33<00:25,  5.60it/s]Measuring inference for batch_size=512:  86%|████████▌ | 858/1000 [02:33<00:25,  5.60it/s]Measuring inference for batch_size=512:  86%|████████▌ | 859/1000 [02:33<00:25,  5.61it/s]Measuring inference for batch_size=512:  86%|████████▌ | 860/1000 [02:33<00:24,  5.60it/s]Measuring inference for batch_size=512:  86%|████████▌ | 861/1000 [02:34<00:24,  5.60it/s]Measuring inference for batch_size=512:  86%|████████▌ | 862/1000 [02:34<00:24,  5.61it/s]Measuring inference for batch_size=512:  86%|████████▋ | 863/1000 [02:34<00:24,  5.61it/s]Measuring inference for batch_size=512:  86%|████████▋ | 864/1000 [02:34<00:24,  5.61it/s]Measuring inference for batch_size=512:  86%|████████▋ | 865/1000 [02:34<00:24,  5.60it/s]Measuring inference for batch_size=512:  87%|████████▋ | 866/1000 [02:34<00:23,  5.60it/s]Measuring inference for batch_size=512:  87%|████████▋ | 867/1000 [02:35<00:23,  5.60it/s]Measuring inference for batch_size=512:  87%|████████▋ | 868/1000 [02:35<00:23,  5.60it/s]Measuring inference for batch_size=512:  87%|████████▋ | 869/1000 [02:35<00:23,  5.60it/s]Measuring inference for batch_size=512:  87%|████████▋ | 870/1000 [02:35<00:23,  5.60it/s]Measuring inference for batch_size=512:  87%|████████▋ | 871/1000 [02:35<00:23,  5.60it/s]Measuring inference for batch_size=512:  87%|████████▋ | 872/1000 [02:36<00:22,  5.60it/s]Measuring inference for batch_size=512:  87%|████████▋ | 873/1000 [02:36<00:22,  5.60it/s]Measuring inference for batch_size=512:  87%|████████▋ | 874/1000 [02:36<00:22,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 875/1000 [02:36<00:22,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 876/1000 [02:36<00:22,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 877/1000 [02:36<00:21,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 878/1000 [02:37<00:21,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 879/1000 [02:37<00:21,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 880/1000 [02:37<00:21,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 881/1000 [02:37<00:21,  5.61it/s]Measuring inference for batch_size=512:  88%|████████▊ | 882/1000 [02:37<00:21,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 883/1000 [02:38<00:20,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 884/1000 [02:38<00:20,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 885/1000 [02:38<00:20,  5.60it/s]Measuring inference for batch_size=512:  89%|████████▊ | 886/1000 [02:38<00:20,  5.60it/s]Measuring inference for batch_size=512:  89%|████████▊ | 887/1000 [02:38<00:20,  5.60it/s]Measuring inference for batch_size=512:  89%|████████▉ | 888/1000 [02:38<00:20,  5.60it/s]Measuring inference for batch_size=512:  89%|████████▉ | 889/1000 [02:39<00:19,  5.60it/s]Measuring inference for batch_size=512:  89%|████████▉ | 890/1000 [02:39<00:19,  5.60it/s]Measuring inference for batch_size=512:  89%|████████▉ | 891/1000 [02:39<00:19,  5.60it/s]Measuring inference for batch_size=512:  89%|████████▉ | 892/1000 [02:39<00:19,  5.60it/s]Measuring inference for batch_size=512:  89%|████████▉ | 893/1000 [02:39<00:19,  5.60it/s]Measuring inference for batch_size=512:  89%|████████▉ | 894/1000 [02:39<00:18,  5.60it/s]Measuring inference for batch_size=512:  90%|████████▉ | 895/1000 [02:40<00:18,  5.60it/s]Measuring inference for batch_size=512:  90%|████████▉ | 896/1000 [02:40<00:18,  5.60it/s]Measuring inference for batch_size=512:  90%|████████▉ | 897/1000 [02:40<00:18,  5.60it/s]Measuring inference for batch_size=512:  90%|████████▉ | 898/1000 [02:40<00:18,  5.60it/s]Measuring inference for batch_size=512:  90%|████████▉ | 899/1000 [02:40<00:18,  5.60it/s]Measuring inference for batch_size=512:  90%|█████████ | 900/1000 [02:41<00:17,  5.60it/s]Measuring inference for batch_size=512:  90%|█████████ | 901/1000 [02:41<00:17,  5.60it/s]Measuring inference for batch_size=512:  90%|█████████ | 902/1000 [02:41<00:17,  5.60it/s]Measuring inference for batch_size=512:  90%|█████████ | 903/1000 [02:41<00:17,  5.60it/s]Measuring inference for batch_size=512:  90%|█████████ | 904/1000 [02:41<00:17,  5.60it/s]Measuring inference for batch_size=512:  90%|█████████ | 905/1000 [02:41<00:16,  5.60it/s]Measuring inference for batch_size=512:  91%|█████████ | 906/1000 [02:42<00:16,  5.60it/s]Measuring inference for batch_size=512:  91%|█████████ | 907/1000 [02:42<00:16,  5.60it/s]Measuring inference for batch_size=512:  91%|█████████ | 908/1000 [02:42<00:16,  5.60it/s]Measuring inference for batch_size=512:  91%|█████████ | 909/1000 [02:42<00:16,  5.60it/s]Measuring inference for batch_size=512:  91%|█████████ | 910/1000 [02:42<00:16,  5.59it/s]Measuring inference for batch_size=512:  91%|█████████ | 911/1000 [02:43<00:15,  5.60it/s]Measuring inference for batch_size=512:  91%|█████████ | 912/1000 [02:43<00:15,  5.59it/s]Measuring inference for batch_size=512:  91%|█████████▏| 913/1000 [02:43<00:15,  5.60it/s]Measuring inference for batch_size=512:  91%|█████████▏| 914/1000 [02:43<00:15,  5.60it/s]Measuring inference for batch_size=512:  92%|█████████▏| 915/1000 [02:43<00:15,  5.59it/s]Measuring inference for batch_size=512:  92%|█████████▏| 916/1000 [02:43<00:15,  5.60it/s]Measuring inference for batch_size=512:  92%|█████████▏| 917/1000 [02:44<00:14,  5.60it/s]Measuring inference for batch_size=512:  92%|█████████▏| 918/1000 [02:44<00:14,  5.60it/s]Measuring inference for batch_size=512:  92%|█████████▏| 919/1000 [02:44<00:14,  5.59it/s]Measuring inference for batch_size=512:  92%|█████████▏| 920/1000 [02:44<00:14,  5.60it/s]Measuring inference for batch_size=512:  92%|█████████▏| 921/1000 [02:44<00:14,  5.60it/s]Measuring inference for batch_size=512:  92%|█████████▏| 922/1000 [02:44<00:13,  5.60it/s]Measuring inference for batch_size=512:  92%|█████████▏| 923/1000 [02:45<00:13,  5.60it/s]Measuring inference for batch_size=512:  92%|█████████▏| 924/1000 [02:45<00:13,  5.59it/s]Measuring inference for batch_size=512:  92%|█████████▎| 925/1000 [02:45<00:13,  5.59it/s]Measuring inference for batch_size=512:  93%|█████████▎| 926/1000 [02:45<00:13,  5.57it/s]Measuring inference for batch_size=512:  93%|█████████▎| 927/1000 [02:45<00:13,  5.58it/s]Measuring inference for batch_size=512:  93%|█████████▎| 928/1000 [02:46<00:12,  5.58it/s]Measuring inference for batch_size=512:  93%|█████████▎| 929/1000 [02:46<00:12,  5.59it/s]Measuring inference for batch_size=512:  93%|█████████▎| 930/1000 [02:46<00:12,  5.59it/s]Measuring inference for batch_size=512:  93%|█████████▎| 931/1000 [02:46<00:12,  5.60it/s]Measuring inference for batch_size=512:  93%|█████████▎| 932/1000 [02:46<00:12,  5.60it/s]Measuring inference for batch_size=512:  93%|█████████▎| 933/1000 [02:46<00:11,  5.60it/s]Measuring inference for batch_size=512:  93%|█████████▎| 934/1000 [02:47<00:11,  5.60it/s]Measuring inference for batch_size=512:  94%|█████████▎| 935/1000 [02:47<00:11,  5.60it/s]Measuring inference for batch_size=512:  94%|█████████▎| 936/1000 [02:47<00:11,  5.60it/s]Measuring inference for batch_size=512:  94%|█████████▎| 937/1000 [02:47<00:11,  5.60it/s]Measuring inference for batch_size=512:  94%|█████████▍| 938/1000 [02:47<00:11,  5.60it/s]Measuring inference for batch_size=512:  94%|█████████▍| 939/1000 [02:48<00:10,  5.60it/s]Measuring inference for batch_size=512:  94%|█████████▍| 940/1000 [02:48<00:10,  5.60it/s]Measuring inference for batch_size=512:  94%|█████████▍| 941/1000 [02:48<00:10,  5.60it/s]Measuring inference for batch_size=512:  94%|█████████▍| 942/1000 [02:48<00:10,  5.60it/s]Measuring inference for batch_size=512:  94%|█████████▍| 943/1000 [02:48<00:10,  5.61it/s]Measuring inference for batch_size=512:  94%|█████████▍| 944/1000 [02:48<00:09,  5.60it/s]Measuring inference for batch_size=512:  94%|█████████▍| 945/1000 [02:49<00:09,  5.60it/s]Measuring inference for batch_size=512:  95%|█████████▍| 946/1000 [02:49<00:09,  5.60it/s]Measuring inference for batch_size=512:  95%|█████████▍| 947/1000 [02:49<00:09,  5.60it/s]Measuring inference for batch_size=512:  95%|█████████▍| 948/1000 [02:49<00:09,  5.60it/s]Measuring inference for batch_size=512:  95%|█████████▍| 949/1000 [02:49<00:09,  5.60it/s]Measuring inference for batch_size=512:  95%|█████████▌| 950/1000 [02:49<00:08,  5.60it/s]Measuring inference for batch_size=512:  95%|█████████▌| 951/1000 [02:50<00:08,  5.60it/s]Measuring inference for batch_size=512:  95%|█████████▌| 952/1000 [02:50<00:08,  5.60it/s]Measuring inference for batch_size=512:  95%|█████████▌| 953/1000 [02:50<00:08,  5.60it/s]Measuring inference for batch_size=512:  95%|█████████▌| 954/1000 [02:50<00:08,  5.60it/s]Measuring inference for batch_size=512:  96%|█████████▌| 955/1000 [02:50<00:08,  5.60it/s]Measuring inference for batch_size=512:  96%|█████████▌| 956/1000 [02:51<00:07,  5.60it/s]Measuring inference for batch_size=512:  96%|█████████▌| 957/1000 [02:51<00:07,  5.60it/s]Measuring inference for batch_size=512:  96%|█████████▌| 958/1000 [02:51<00:07,  5.60it/s]Measuring inference for batch_size=512:  96%|█████████▌| 959/1000 [02:51<00:07,  5.60it/s]Measuring inference for batch_size=512:  96%|█████████▌| 960/1000 [02:51<00:07,  5.60it/s]Measuring inference for batch_size=512:  96%|█████████▌| 961/1000 [02:51<00:06,  5.60it/s]Measuring inference for batch_size=512:  96%|█████████▌| 962/1000 [02:52<00:06,  5.59it/s]Measuring inference for batch_size=512:  96%|█████████▋| 963/1000 [02:52<00:06,  5.59it/s]Measuring inference for batch_size=512:  96%|█████████▋| 964/1000 [02:52<00:06,  5.60it/s]Measuring inference for batch_size=512:  96%|█████████▋| 965/1000 [02:52<00:06,  5.60it/s]Measuring inference for batch_size=512:  97%|█████████▋| 966/1000 [02:52<00:06,  5.60it/s]Measuring inference for batch_size=512:  97%|█████████▋| 967/1000 [02:53<00:05,  5.60it/s]Measuring inference for batch_size=512:  97%|█████████▋| 968/1000 [02:53<00:05,  5.59it/s]Measuring inference for batch_size=512:  97%|█████████▋| 969/1000 [02:53<00:05,  5.60it/s]Measuring inference for batch_size=512:  97%|█████████▋| 970/1000 [02:53<00:05,  5.60it/s]Measuring inference for batch_size=512:  97%|█████████▋| 971/1000 [02:53<00:05,  5.60it/s]Measuring inference for batch_size=512:  97%|█████████▋| 972/1000 [02:53<00:05,  5.59it/s]Measuring inference for batch_size=512:  97%|█████████▋| 973/1000 [02:54<00:04,  5.60it/s]Measuring inference for batch_size=512:  97%|█████████▋| 974/1000 [02:54<00:04,  5.59it/s]Measuring inference for batch_size=512:  98%|█████████▊| 975/1000 [02:54<00:04,  5.59it/s]Measuring inference for batch_size=512:  98%|█████████▊| 976/1000 [02:54<00:04,  5.59it/s]Measuring inference for batch_size=512:  98%|█████████▊| 977/1000 [02:54<00:04,  5.59it/s]Measuring inference for batch_size=512:  98%|█████████▊| 978/1000 [02:54<00:03,  5.60it/s]Measuring inference for batch_size=512:  98%|█████████▊| 979/1000 [02:55<00:03,  5.59it/s]Measuring inference for batch_size=512:  98%|█████████▊| 980/1000 [02:55<00:03,  5.59it/s]Measuring inference for batch_size=512:  98%|█████████▊| 981/1000 [02:55<00:03,  5.59it/s]Measuring inference for batch_size=512:  98%|█████████▊| 982/1000 [02:55<00:03,  5.59it/s]Measuring inference for batch_size=512:  98%|█████████▊| 983/1000 [02:55<00:03,  5.59it/s]Measuring inference for batch_size=512:  98%|█████████▊| 984/1000 [02:56<00:02,  5.59it/s]Measuring inference for batch_size=512:  98%|█████████▊| 985/1000 [02:56<00:02,  5.60it/s]Measuring inference for batch_size=512:  99%|█████████▊| 986/1000 [02:56<00:02,  5.60it/s]Measuring inference for batch_size=512:  99%|█████████▊| 987/1000 [02:56<00:02,  5.60it/s]Measuring inference for batch_size=512:  99%|█████████▉| 988/1000 [02:56<00:02,  5.60it/s]Measuring inference for batch_size=512:  99%|█████████▉| 989/1000 [02:56<00:01,  5.60it/s]Measuring inference for batch_size=512:  99%|█████████▉| 990/1000 [02:57<00:01,  5.60it/s]Measuring inference for batch_size=512:  99%|█████████▉| 991/1000 [02:57<00:01,  5.60it/s]Measuring inference for batch_size=512:  99%|█████████▉| 992/1000 [02:57<00:01,  5.60it/s]Measuring inference for batch_size=512:  99%|█████████▉| 993/1000 [02:57<00:01,  5.61it/s]Measuring inference for batch_size=512:  99%|█████████▉| 994/1000 [02:57<00:01,  5.61it/s]Measuring inference for batch_size=512: 100%|█████████▉| 995/1000 [02:58<00:00,  5.60it/s]Measuring inference for batch_size=512: 100%|█████████▉| 996/1000 [02:58<00:00,  5.60it/s]Measuring inference for batch_size=512: 100%|█████████▉| 997/1000 [02:58<00:00,  5.60it/s]Measuring inference for batch_size=512: 100%|█████████▉| 998/1000 [02:58<00:00,  5.60it/s]Measuring inference for batch_size=512: 100%|█████████▉| 999/1000 [02:58<00:00,  5.60it/s]Measuring inference for batch_size=512: 100%|██████████| 1000/1000 [02:58<00:00,  5.60it/s]Measuring inference for batch_size=512: 100%|██████████| 1000/1000 [02:58<00:00,  5.59it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cpu
flops: 12310546408
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.22 GB
    total: 31.28 GB
    used: 3.61 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
params: 118515272
timing:
  batch_size_1:
    on_device_inference:
      human_readable:
        batch_latency: 137.157 ms +/- 255.143 us [136.429 ms, 138.403 ms]
        batches_per_second: 7.29 +/- 0.01 [7.23, 7.33]
      metrics:
        batches_per_second_max: 7.329841687593823
        batches_per_second_mean: 7.290964569845728
        batches_per_second_min: 7.225292548014394
        batches_per_second_std: 0.01355095574101499
        seconds_per_batch_max: 0.13840270042419434
        seconds_per_batch_mean: 0.13715653777122497
        seconds_per_batch_min: 0.1364285945892334
        seconds_per_batch_std: 0.0002551431062003908
  batch_size_512:
    on_device_inference:
      human_readable:
        batch_latency: 178.772 ms +/- 474.558 us [177.651 ms, 183.604 ms]
        batches_per_second: 5.59 +/- 0.01 [5.45, 5.63]
      metrics:
        batches_per_second_max: 5.629016456365535
        batches_per_second_mean: 5.593747193748177
        batches_per_second_min: 5.446497301621105
        batches_per_second_std: 0.01476138843197558
        seconds_per_batch_max: 0.18360424041748047
        seconds_per_batch_mean: 0.17877229166030884
        seconds_per_batch_min: 0.17765092849731445
        seconds_per_batch_std: 0.0004745582612156112


#####
baseline-baseline-py-id - Run 2
2024-02-23 09:46:28
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]
Warning: module SiLU is treated as a zero-op.
Warning: module Conv2dNormActivation is treated as a zero-op.
Warning: module StochasticDepth is treated as a zero-op.
Warning: module FusedMBConv is treated as a zero-op.
Warning: module Sigmoid is treated as a zero-op.
Warning: module SqueezeExcitation is treated as a zero-op.
Warning: module MBConv is treated as a zero-op.
Warning: module Dropout is treated as a zero-op.
Warning: module EfficientNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
EfficientNet(
  118.52 M, 100.000% Params, 12.31 GMac, 100.000% MACs, 
  (features): Sequential(
    117.23 M, 98.919% Params, 12.31 GMac, 99.989% MACs, 
    (0): Conv2dNormActivation(
      928, 0.001% Params, 11.64 MMac, 0.095% MACs, 
      (0): Conv2d(864, 0.001% Params, 10.84 MMac, 0.088% MACs, 3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, 0.000% Params, 802.82 KMac, 0.007% MACs, 32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
    )
    (1): Sequential(
      37.12 k, 0.031% Params, 465.63 MMac, 3.782% MACs, 
      (0): FusedMBConv(
        9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
        (block): Sequential(
          9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
          (0): Conv2dNormActivation(
            9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
            (0): Conv2d(9.22 k, 0.008% Params, 115.61 MMac, 0.939% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(64, 0.000% Params, 802.82 KMac, 0.007% MACs, 32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0, mode=row)
      )
      (1): FusedMBConv(
        9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
        (block): Sequential(
          9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
          (0): Conv2dNormActivation(
            9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
            (0): Conv2d(9.22 k, 0.008% Params, 115.61 MMac, 0.939% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(64, 0.000% Params, 802.82 KMac, 0.007% MACs, 32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.002531645569620253, mode=row)
      )
      (2): FusedMBConv(
        9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
        (block): Sequential(
          9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
          (0): Conv2dNormActivation(
            9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
            (0): Conv2d(9.22 k, 0.008% Params, 115.61 MMac, 0.939% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(64, 0.000% Params, 802.82 KMac, 0.007% MACs, 32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.005063291139240506, mode=row)
      )
      (3): FusedMBConv(
        9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
        (block): Sequential(
          9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
          (0): Conv2dNormActivation(
            9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
            (0): Conv2d(9.22 k, 0.008% Params, 115.61 MMac, 0.939% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(64, 0.000% Params, 802.82 KMac, 0.007% MACs, 32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.007594936708860761, mode=row)
      )
    )
    (2): Sequential(
      1.03 M, 0.871% Params, 3.24 GMac, 26.297% MACs, 
      (0): FusedMBConv(
        45.44 k, 0.038% Params, 142.5 MMac, 1.158% MACs, 
        (block): Sequential(
          45.44 k, 0.038% Params, 142.5 MMac, 1.158% MACs, 
          (0): Conv2dNormActivation(
            37.12 k, 0.031% Params, 116.41 MMac, 0.946% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 115.61 MMac, 0.939% MACs, 32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, 0.000% Params, 802.82 KMac, 0.007% MACs, 128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.32 k, 0.007% Params, 26.09 MMac, 0.212% MACs, 
            (0): Conv2d(8.19 k, 0.007% Params, 25.69 MMac, 0.209% MACs, 128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.010126582278481013, mode=row)
      )
      (1): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.012658227848101266, mode=row)
      )
      (2): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.015189873417721522, mode=row)
      )
      (3): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.017721518987341773, mode=row)
      )
      (4): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.020253164556962026, mode=row)
      )
      (5): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.02278481012658228, mode=row)
      )
      (6): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.02531645569620253, mode=row)
      )
    )
    (3): Sequential(
      2.39 M, 2.017% Params, 1.87 GMac, 15.223% MACs, 
      (0): FusedMBConv(
        172.74 k, 0.146% Params, 135.43 MMac, 1.100% MACs, 
        (block): Sequential(
          172.74 k, 0.146% Params, 135.43 MMac, 1.100% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 116.01 MMac, 0.942% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 115.61 MMac, 0.939% MACs, 64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 401.41 KMac, 0.003% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            24.77 k, 0.021% Params, 19.42 MMac, 0.158% MACs, 
            (0): Conv2d(24.58 k, 0.021% Params, 19.27 MMac, 0.157% MACs, 256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.027848101265822787, mode=row)
      )
      (1): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.030379746835443044, mode=row)
      )
      (2): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.03291139240506329, mode=row)
      )
      (3): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.035443037974683546, mode=row)
      )
      (4): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0379746835443038, mode=row)
      )
      (5): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.04050632911392405, mode=row)
      )
      (6): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.04303797468354431, mode=row)
      )
    )
    (4): Sequential(
      3.55 M, 2.998% Params, 585.49 MMac, 4.756% MACs, 
      (0): MBConv(
        134.81 k, 0.114% Params, 44.95 MMac, 0.365% MACs, 
        (block): Sequential(
          134.81 k, 0.114% Params, 44.95 MMac, 0.365% MACs, 
          (0): Conv2dNormActivation(
            37.63 k, 0.032% Params, 29.5 MMac, 0.240% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            4.22 k, 0.004% Params, 827.9 KMac, 0.007% MACs, 
            (0): Conv2d(3.46 k, 0.003% Params, 677.38 KMac, 0.006% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 150.53 KMac, 0.001% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            18.84 k, 0.016% Params, 94.1 KMac, 0.001% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 75.26 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(9.24 k, 0.008% Params, 9.24 KMac, 0.000% MACs, 384, 24, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(9.6 k, 0.008% Params, 9.6 KMac, 0.000% MACs, 24, 384, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            74.11 k, 0.063% Params, 14.53 MMac, 0.118% MACs, 
            (0): Conv2d(73.73 k, 0.062% Params, 14.45 MMac, 0.117% MACs, 384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.04556962025316456, mode=row)
      )
      (1): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.04810126582278482, mode=row)
      )
      (2): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.05063291139240506, mode=row)
      )
      (3): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.053164556962025315, mode=row)
      )
      (4): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.055696202531645575, mode=row)
      )
      (5): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.05822784810126583, mode=row)
      )
      (6): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06075949367088609, mode=row)
      )
      (7): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06329113924050633, mode=row)
      )
      (8): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06582278481012659, mode=row)
      )
      (9): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06835443037974684, mode=row)
      )
    )
    (5): Sequential(
      14.5 M, 12.236% Params, 2.29 GMac, 18.620% MACs, 
      (0): MBConv(
        606.45 k, 0.512% Params, 97.29 MMac, 0.790% MACs, 
        (block): Sequential(
          606.45 k, 0.512% Params, 97.29 MMac, 0.790% MACs, 
          (0): Conv2dNormActivation(
            223.49 k, 0.189% Params, 43.8 MMac, 0.356% MACs, 
            (0): Conv2d(221.18 k, 0.187% Params, 43.35 MMac, 0.352% MACs, 192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.3 k, 0.002% Params, 451.58 KMac, 0.004% MACs, 1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            12.67 k, 0.011% Params, 2.48 MMac, 0.020% MACs, 
            (0): Conv2d(10.37 k, 0.009% Params, 2.03 MMac, 0.017% MACs, 1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(2.3 k, 0.002% Params, 451.58 KMac, 0.004% MACs, 1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            111.79 k, 0.094% Params, 337.58 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 225.79 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(55.34 k, 0.047% Params, 55.34 KMac, 0.000% MACs, 1152, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(56.45 k, 0.048% Params, 56.45 KMac, 0.000% MACs, 48, 1152, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            258.5 k, 0.218% Params, 50.67 MMac, 0.412% MACs, 
            (0): Conv2d(258.05 k, 0.218% Params, 50.58 MMac, 0.411% MACs, 1152, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.07088607594936709, mode=row)
      )
      (1): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.07341772151898734, mode=row)
      )
      (2): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0759493670886076, mode=row)
      )
      (3): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.07848101265822785, mode=row)
      )
      (4): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0810126582278481, mode=row)
      )
      (5): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08354430379746836, mode=row)
      )
      (6): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08607594936708862, mode=row)
      )
      (7): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08860759493670886, mode=row)
      )
      (8): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.09113924050632911, mode=row)
      )
      (9): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.09367088607594937, mode=row)
      )
      (10): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.09620253164556963, mode=row)
      )
      (11): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.09873417721518989, mode=row)
      )
      (12): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.10126582278481013, mode=row)
      )
      (13): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.10379746835443039, mode=row)
      )
      (14): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.10632911392405063, mode=row)
      )
      (15): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.10886075949367088, mode=row)
      )
      (16): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11139240506329115, mode=row)
      )
      (17): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11392405063291139, mode=row)
      )
      (18): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11645569620253166, mode=row)
      )
    )
    (6): Sequential(
      54.87 M, 46.295% Params, 2.22 GMac, 18.003% MACs, 
      (0): MBConv(
        987.32 k, 0.833% Params, 85.8 MMac, 0.697% MACs, 
        (block): Sequential(
          987.32 k, 0.833% Params, 85.8 MMac, 0.697% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 724.42 KMac, 0.006% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 592.7 KMac, 0.005% MACs, 1344, 1344, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 131.71 KMac, 0.001% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 217.78 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 65.86 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            516.86 k, 0.436% Params, 25.33 MMac, 0.206% MACs, 
            (0): Conv2d(516.1 k, 0.435% Params, 25.29 MMac, 0.205% MACs, 1344, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11898734177215191, mode=row)
      )
      (1): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.12151898734177217, mode=row)
      )
      (2): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.12405063291139241, mode=row)
      )
      (3): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.12658227848101267, mode=row)
      )
      (4): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.12911392405063293, mode=row)
      )
      (5): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.13164556962025317, mode=row)
      )
      (6): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.13417721518987344, mode=row)
      )
      (7): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.13670886075949368, mode=row)
      )
      (8): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.13924050632911392, mode=row)
      )
      (9): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.14177215189873418, mode=row)
      )
      (10): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.14430379746835442, mode=row)
      )
      (11): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1468354430379747, mode=row)
      )
      (12): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.14936708860759496, mode=row)
      )
      (13): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1518987341772152, mode=row)
      )
      (14): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.15443037974683546, mode=row)
      )
      (15): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1569620253164557, mode=row)
      )
      (16): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.15949367088607597, mode=row)
      )
      (17): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1620253164556962, mode=row)
      )
      (18): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.16455696202531644, mode=row)
      )
      (19): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1670886075949367, mode=row)
      )
      (20): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.16962025316455698, mode=row)
      )
      (21): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.17215189873417724, mode=row)
      )
      (22): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.17468354430379748, mode=row)
      )
      (23): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.17721518987341772, mode=row)
      )
      (24): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.179746835443038, mode=row)
      )
    )
    (7): Sequential(
      40.03 M, 33.777% Params, 1.59 GMac, 12.886% MACs, 
      (0): MBConv(
        2.84 M, 2.392% Params, 117.69 MMac, 0.956% MACs, 
        (block): Sequential(
          2.84 M, 2.392% Params, 117.69 MMac, 0.956% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            1.48 M, 1.245% Params, 72.32 MMac, 0.587% MACs, 
            (0): Conv2d(1.47 M, 1.244% Params, 72.25 MMac, 0.587% MACs, 2304, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.18227848101265823, mode=row)
      )
      (1): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1848101265822785, mode=row)
      )
      (2): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.18734177215189873, mode=row)
      )
      (3): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.189873417721519, mode=row)
      )
      (4): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.19240506329113927, mode=row)
      )
      (5): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1949367088607595, mode=row)
      )
      (6): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.19746835443037977, mode=row)
      )
    )
    (8): Conv2dNormActivation(
      821.76 k, 0.693% Params, 40.27 MMac, 0.327% MACs, 
      (0): Conv2d(819.2 k, 0.691% Params, 40.14 MMac, 0.326% MACs, 640, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2.56 k, 0.002% Params, 125.44 KMac, 0.001% MACs, 1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 62.72 KMac, 0.001% MACs, output_size=1)
  (classifier): Sequential(
    1.28 M, 1.081% Params, 1.28 MMac, 0.010% MACs, 
    (0): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.4, inplace=True)
    (1): Linear(1.28 M, 1.081% Params, 1.28 MMac, 0.010% MACs, in_features=1280, out_features=1000, bias=True)
  )
)Measurement of allocated memory is only available on CUDA devices

Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:   1%|          | 1/100 [00:00<00:13,  7.36it/s]Warming up with batch_size=1:   2%|▏         | 2/100 [00:00<00:13,  7.33it/s]Warming up with batch_size=1:   3%|▎         | 3/100 [00:00<00:13,  7.33it/s]Warming up with batch_size=1:   4%|▍         | 4/100 [00:00<00:13,  7.34it/s]Warming up with batch_size=1:   5%|▌         | 5/100 [00:00<00:12,  7.34it/s]Warming up with batch_size=1:   6%|▌         | 6/100 [00:00<00:12,  7.32it/s]Warming up with batch_size=1:   7%|▋         | 7/100 [00:00<00:12,  7.33it/s]Warming up with batch_size=1:   8%|▊         | 8/100 [00:01<00:12,  7.32it/s]Warming up with batch_size=1:   9%|▉         | 9/100 [00:01<00:12,  7.32it/s]Warming up with batch_size=1:  10%|█         | 10/100 [00:01<00:12,  7.32it/s]Warming up with batch_size=1:  11%|█         | 11/100 [00:01<00:12,  7.32it/s]Warming up with batch_size=1:  12%|█▏        | 12/100 [00:01<00:12,  7.32it/s]Warming up with batch_size=1:  13%|█▎        | 13/100 [00:01<00:11,  7.32it/s]Warming up with batch_size=1:  14%|█▍        | 14/100 [00:01<00:11,  7.31it/s]Warming up with batch_size=1:  15%|█▌        | 15/100 [00:02<00:11,  7.31it/s]Warming up with batch_size=1:  16%|█▌        | 16/100 [00:02<00:11,  7.30it/s]Warming up with batch_size=1:  17%|█▋        | 17/100 [00:02<00:11,  7.30it/s]Warming up with batch_size=1:  18%|█▊        | 18/100 [00:02<00:11,  7.31it/s]Warming up with batch_size=1:  19%|█▉        | 19/100 [00:02<00:11,  7.29it/s]Warming up with batch_size=1:  20%|██        | 20/100 [00:02<00:10,  7.30it/s]Warming up with batch_size=1:  21%|██        | 21/100 [00:02<00:10,  7.31it/s]Warming up with batch_size=1:  22%|██▏       | 22/100 [00:03<00:10,  7.31it/s]Warming up with batch_size=1:  23%|██▎       | 23/100 [00:03<00:10,  7.31it/s]Warming up with batch_size=1:  24%|██▍       | 24/100 [00:03<00:10,  7.32it/s]Warming up with batch_size=1:  25%|██▌       | 25/100 [00:03<00:10,  7.32it/s]Warming up with batch_size=1:  26%|██▌       | 26/100 [00:03<00:10,  7.32it/s]Warming up with batch_size=1:  27%|██▋       | 27/100 [00:03<00:09,  7.33it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:03<00:09,  7.33it/s]Warming up with batch_size=1:  29%|██▉       | 29/100 [00:03<00:09,  7.33it/s]Warming up with batch_size=1:  30%|███       | 30/100 [00:04<00:09,  7.33it/s]Warming up with batch_size=1:  31%|███       | 31/100 [00:04<00:09,  7.32it/s]Warming up with batch_size=1:  32%|███▏      | 32/100 [00:04<00:09,  7.32it/s]Warming up with batch_size=1:  33%|███▎      | 33/100 [00:04<00:09,  7.32it/s]Warming up with batch_size=1:  34%|███▍      | 34/100 [00:04<00:09,  7.32it/s]Warming up with batch_size=1:  35%|███▌      | 35/100 [00:04<00:08,  7.32it/s]Warming up with batch_size=1:  36%|███▌      | 36/100 [00:04<00:08,  7.32it/s]Warming up with batch_size=1:  37%|███▋      | 37/100 [00:05<00:08,  7.32it/s]Warming up with batch_size=1:  38%|███▊      | 38/100 [00:05<00:08,  7.32it/s]Warming up with batch_size=1:  39%|███▉      | 39/100 [00:05<00:08,  7.33it/s]Warming up with batch_size=1:  40%|████      | 40/100 [00:05<00:08,  7.32it/s]Warming up with batch_size=1:  41%|████      | 41/100 [00:05<00:08,  7.32it/s]Warming up with batch_size=1:  42%|████▏     | 42/100 [00:05<00:07,  7.31it/s]Warming up with batch_size=1:  43%|████▎     | 43/100 [00:05<00:07,  7.31it/s]Warming up with batch_size=1:  44%|████▍     | 44/100 [00:06<00:07,  7.31it/s]Warming up with batch_size=1:  45%|████▌     | 45/100 [00:06<00:07,  7.32it/s]Warming up with batch_size=1:  46%|████▌     | 46/100 [00:06<00:07,  7.32it/s]Warming up with batch_size=1:  47%|████▋     | 47/100 [00:06<00:07,  7.32it/s]Warming up with batch_size=1:  48%|████▊     | 48/100 [00:06<00:07,  7.32it/s]Warming up with batch_size=1:  49%|████▉     | 49/100 [00:06<00:06,  7.32it/s]Warming up with batch_size=1:  50%|█████     | 50/100 [00:06<00:06,  7.33it/s]Warming up with batch_size=1:  51%|█████     | 51/100 [00:06<00:06,  7.33it/s]Warming up with batch_size=1:  52%|█████▏    | 52/100 [00:07<00:06,  7.33it/s]Warming up with batch_size=1:  53%|█████▎    | 53/100 [00:07<00:06,  7.33it/s]Warming up with batch_size=1:  54%|█████▍    | 54/100 [00:07<00:06,  7.33it/s]Warming up with batch_size=1:  55%|█████▌    | 55/100 [00:07<00:06,  7.33it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:07<00:06,  7.32it/s]Warming up with batch_size=1:  57%|█████▋    | 57/100 [00:07<00:05,  7.33it/s]Warming up with batch_size=1:  58%|█████▊    | 58/100 [00:07<00:05,  7.32it/s]Warming up with batch_size=1:  59%|█████▉    | 59/100 [00:08<00:05,  7.33it/s]Warming up with batch_size=1:  60%|██████    | 60/100 [00:08<00:05,  7.32it/s]Warming up with batch_size=1:  61%|██████    | 61/100 [00:08<00:05,  7.32it/s]Warming up with batch_size=1:  62%|██████▏   | 62/100 [00:08<00:05,  7.32it/s]Warming up with batch_size=1:  63%|██████▎   | 63/100 [00:08<00:05,  7.32it/s]Warming up with batch_size=1:  64%|██████▍   | 64/100 [00:08<00:04,  7.33it/s]Warming up with batch_size=1:  65%|██████▌   | 65/100 [00:08<00:04,  7.33it/s]Warming up with batch_size=1:  66%|██████▌   | 66/100 [00:09<00:04,  7.33it/s]Warming up with batch_size=1:  67%|██████▋   | 67/100 [00:09<00:04,  7.33it/s]Warming up with batch_size=1:  68%|██████▊   | 68/100 [00:09<00:04,  7.32it/s]Warming up with batch_size=1:  69%|██████▉   | 69/100 [00:09<00:04,  7.33it/s]Warming up with batch_size=1:  70%|███████   | 70/100 [00:09<00:04,  7.33it/s]Warming up with batch_size=1:  71%|███████   | 71/100 [00:09<00:03,  7.32it/s]Warming up with batch_size=1:  72%|███████▏  | 72/100 [00:09<00:03,  7.33it/s]Warming up with batch_size=1:  73%|███████▎  | 73/100 [00:09<00:03,  7.33it/s]Warming up with batch_size=1:  74%|███████▍  | 74/100 [00:10<00:03,  7.33it/s]Warming up with batch_size=1:  75%|███████▌  | 75/100 [00:10<00:03,  7.33it/s]Warming up with batch_size=1:  76%|███████▌  | 76/100 [00:10<00:03,  7.34it/s]Warming up with batch_size=1:  77%|███████▋  | 77/100 [00:10<00:03,  7.33it/s]Warming up with batch_size=1:  78%|███████▊  | 78/100 [00:10<00:03,  7.33it/s]Warming up with batch_size=1:  79%|███████▉  | 79/100 [00:10<00:02,  7.33it/s]Warming up with batch_size=1:  80%|████████  | 80/100 [00:10<00:02,  7.32it/s]Warming up with batch_size=1:  81%|████████  | 81/100 [00:11<00:02,  7.33it/s]Warming up with batch_size=1:  82%|████████▏ | 82/100 [00:11<00:02,  7.33it/s]Warming up with batch_size=1:  83%|████████▎ | 83/100 [00:11<00:02,  7.33it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:11<00:02,  7.33it/s]Warming up with batch_size=1:  85%|████████▌ | 85/100 [00:11<00:02,  7.33it/s]Warming up with batch_size=1:  86%|████████▌ | 86/100 [00:11<00:01,  7.33it/s]Warming up with batch_size=1:  87%|████████▋ | 87/100 [00:11<00:01,  7.33it/s]Warming up with batch_size=1:  88%|████████▊ | 88/100 [00:12<00:01,  7.33it/s]Warming up with batch_size=1:  89%|████████▉ | 89/100 [00:12<00:01,  7.33it/s]Warming up with batch_size=1:  90%|█████████ | 90/100 [00:12<00:01,  7.32it/s]Warming up with batch_size=1:  91%|█████████ | 91/100 [00:12<00:01,  7.32it/s]Warming up with batch_size=1:  92%|█████████▏| 92/100 [00:12<00:01,  7.32it/s]Warming up with batch_size=1:  93%|█████████▎| 93/100 [00:12<00:00,  7.32it/s]Warming up with batch_size=1:  94%|█████████▍| 94/100 [00:12<00:00,  7.33it/s]Warming up with batch_size=1:  95%|█████████▌| 95/100 [00:12<00:00,  7.33it/s]Warming up with batch_size=1:  96%|█████████▌| 96/100 [00:13<00:00,  7.33it/s]Warming up with batch_size=1:  97%|█████████▋| 97/100 [00:13<00:00,  7.33it/s]Warming up with batch_size=1:  98%|█████████▊| 98/100 [00:13<00:00,  7.33it/s]Warming up with batch_size=1:  99%|█████████▉| 99/100 [00:13<00:00,  7.32it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:13<00:00,  7.33it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:13<00:00,  7.32it/s]
STAGE:2024-02-23 09:40:51 176353:176353 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:40:51 176353:176353 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:40:51 176353:176353 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   0%|          | 1/1000 [00:00<02:17,  7.24it/s]Measuring inference for batch_size=1:   0%|          | 2/1000 [00:00<02:17,  7.28it/s]Measuring inference for batch_size=1:   0%|          | 3/1000 [00:00<02:16,  7.29it/s]Measuring inference for batch_size=1:   0%|          | 4/1000 [00:00<02:16,  7.28it/s]Measuring inference for batch_size=1:   0%|          | 5/1000 [00:00<02:16,  7.29it/s]Measuring inference for batch_size=1:   1%|          | 6/1000 [00:00<02:16,  7.29it/s]Measuring inference for batch_size=1:   1%|          | 7/1000 [00:00<02:16,  7.29it/s]Measuring inference for batch_size=1:   1%|          | 8/1000 [00:01<02:15,  7.30it/s]Measuring inference for batch_size=1:   1%|          | 9/1000 [00:01<02:15,  7.29it/s]Measuring inference for batch_size=1:   1%|          | 10/1000 [00:01<02:15,  7.30it/s]Measuring inference for batch_size=1:   1%|          | 11/1000 [00:01<02:15,  7.29it/s]Measuring inference for batch_size=1:   1%|          | 12/1000 [00:01<02:15,  7.29it/s]Measuring inference for batch_size=1:   1%|▏         | 13/1000 [00:01<02:15,  7.29it/s]Measuring inference for batch_size=1:   1%|▏         | 14/1000 [00:01<02:15,  7.29it/s]Measuring inference for batch_size=1:   2%|▏         | 15/1000 [00:02<02:15,  7.29it/s]Measuring inference for batch_size=1:   2%|▏         | 16/1000 [00:02<02:14,  7.29it/s]Measuring inference for batch_size=1:   2%|▏         | 17/1000 [00:02<02:14,  7.28it/s]Measuring inference for batch_size=1:   2%|▏         | 18/1000 [00:02<02:14,  7.29it/s]Measuring inference for batch_size=1:   2%|▏         | 19/1000 [00:02<02:14,  7.29it/s]Measuring inference for batch_size=1:   2%|▏         | 20/1000 [00:02<02:14,  7.28it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:02<02:14,  7.28it/s]Measuring inference for batch_size=1:   2%|▏         | 22/1000 [00:03<02:14,  7.28it/s]Measuring inference for batch_size=1:   2%|▏         | 23/1000 [00:03<02:14,  7.28it/s]Measuring inference for batch_size=1:   2%|▏         | 24/1000 [00:03<02:14,  7.28it/s]Measuring inference for batch_size=1:   2%|▎         | 25/1000 [00:03<02:13,  7.28it/s]Measuring inference for batch_size=1:   3%|▎         | 26/1000 [00:03<02:13,  7.29it/s]Measuring inference for batch_size=1:   3%|▎         | 27/1000 [00:03<02:13,  7.30it/s]Measuring inference for batch_size=1:   3%|▎         | 28/1000 [00:03<02:13,  7.29it/s]Measuring inference for batch_size=1:   3%|▎         | 29/1000 [00:03<02:13,  7.30it/s]Measuring inference for batch_size=1:   3%|▎         | 30/1000 [00:04<02:12,  7.29it/s]Measuring inference for batch_size=1:   3%|▎         | 31/1000 [00:04<02:12,  7.29it/s]Measuring inference for batch_size=1:   3%|▎         | 32/1000 [00:04<02:12,  7.29it/s]Measuring inference for batch_size=1:   3%|▎         | 33/1000 [00:04<02:12,  7.28it/s]Measuring inference for batch_size=1:   3%|▎         | 34/1000 [00:04<02:12,  7.28it/s]Measuring inference for batch_size=1:   4%|▎         | 35/1000 [00:04<02:12,  7.28it/s]Measuring inference for batch_size=1:   4%|▎         | 36/1000 [00:04<02:12,  7.29it/s]Measuring inference for batch_size=1:   4%|▎         | 37/1000 [00:05<02:12,  7.28it/s]Measuring inference for batch_size=1:   4%|▍         | 38/1000 [00:05<02:12,  7.29it/s]Measuring inference for batch_size=1:   4%|▍         | 39/1000 [00:05<02:11,  7.29it/s]Measuring inference for batch_size=1:   4%|▍         | 40/1000 [00:05<02:11,  7.29it/s]Measuring inference for batch_size=1:   4%|▍         | 41/1000 [00:05<02:11,  7.29it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:05<02:11,  7.29it/s]Measuring inference for batch_size=1:   4%|▍         | 43/1000 [00:05<02:11,  7.29it/s]Measuring inference for batch_size=1:   4%|▍         | 44/1000 [00:06<02:11,  7.28it/s]Measuring inference for batch_size=1:   4%|▍         | 45/1000 [00:06<02:11,  7.28it/s]Measuring inference for batch_size=1:   5%|▍         | 46/1000 [00:06<02:10,  7.28it/s]Measuring inference for batch_size=1:   5%|▍         | 47/1000 [00:06<02:10,  7.29it/s]Measuring inference for batch_size=1:   5%|▍         | 48/1000 [00:06<02:10,  7.29it/s]Measuring inference for batch_size=1:   5%|▍         | 49/1000 [00:06<02:10,  7.29it/s]Measuring inference for batch_size=1:   5%|▌         | 50/1000 [00:06<02:10,  7.28it/s]Measuring inference for batch_size=1:   5%|▌         | 51/1000 [00:06<02:10,  7.29it/s]Measuring inference for batch_size=1:   5%|▌         | 52/1000 [00:07<02:10,  7.29it/s]Measuring inference for batch_size=1:   5%|▌         | 53/1000 [00:07<02:09,  7.29it/s]Measuring inference for batch_size=1:   5%|▌         | 54/1000 [00:07<02:09,  7.29it/s]Measuring inference for batch_size=1:   6%|▌         | 55/1000 [00:07<02:09,  7.29it/s]Measuring inference for batch_size=1:   6%|▌         | 56/1000 [00:07<02:09,  7.29it/s]Measuring inference for batch_size=1:   6%|▌         | 57/1000 [00:07<02:09,  7.28it/s]Measuring inference for batch_size=1:   6%|▌         | 58/1000 [00:07<02:09,  7.28it/s]Measuring inference for batch_size=1:   6%|▌         | 59/1000 [00:08<02:09,  7.28it/s]Measuring inference for batch_size=1:   6%|▌         | 60/1000 [00:08<02:09,  7.26it/s]Measuring inference for batch_size=1:   6%|▌         | 61/1000 [00:08<02:09,  7.27it/s]Measuring inference for batch_size=1:   6%|▌         | 62/1000 [00:08<02:08,  7.28it/s]Measuring inference for batch_size=1:   6%|▋         | 63/1000 [00:08<02:08,  7.28it/s]Measuring inference for batch_size=1:   6%|▋         | 64/1000 [00:08<02:08,  7.29it/s]Measuring inference for batch_size=1:   6%|▋         | 65/1000 [00:08<02:08,  7.29it/s]Measuring inference for batch_size=1:   7%|▋         | 66/1000 [00:09<02:08,  7.28it/s]Measuring inference for batch_size=1:   7%|▋         | 67/1000 [00:09<02:07,  7.29it/s]Measuring inference for batch_size=1:   7%|▋         | 68/1000 [00:09<02:07,  7.29it/s]Measuring inference for batch_size=1:   7%|▋         | 69/1000 [00:09<02:07,  7.28it/s]Measuring inference for batch_size=1:   7%|▋         | 70/1000 [00:09<02:07,  7.28it/s]Measuring inference for batch_size=1:   7%|▋         | 71/1000 [00:09<02:07,  7.28it/s]Measuring inference for batch_size=1:   7%|▋         | 72/1000 [00:09<02:07,  7.28it/s]Measuring inference for batch_size=1:   7%|▋         | 73/1000 [00:10<02:07,  7.29it/s]Measuring inference for batch_size=1:   7%|▋         | 74/1000 [00:10<02:07,  7.29it/s]Measuring inference for batch_size=1:   8%|▊         | 75/1000 [00:10<02:06,  7.29it/s]Measuring inference for batch_size=1:   8%|▊         | 76/1000 [00:10<02:06,  7.29it/s]Measuring inference for batch_size=1:   8%|▊         | 77/1000 [00:10<02:07,  7.27it/s]Measuring inference for batch_size=1:   8%|▊         | 78/1000 [00:10<02:06,  7.26it/s]Measuring inference for batch_size=1:   8%|▊         | 79/1000 [00:10<02:06,  7.27it/s]Measuring inference for batch_size=1:   8%|▊         | 80/1000 [00:10<02:06,  7.27it/s]Measuring inference for batch_size=1:   8%|▊         | 81/1000 [00:11<02:06,  7.28it/s]Measuring inference for batch_size=1:   8%|▊         | 82/1000 [00:11<02:06,  7.28it/s]Measuring inference for batch_size=1:   8%|▊         | 83/1000 [00:11<02:05,  7.28it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:11<02:05,  7.28it/s]Measuring inference for batch_size=1:   8%|▊         | 85/1000 [00:11<02:05,  7.28it/s]Measuring inference for batch_size=1:   9%|▊         | 86/1000 [00:11<02:05,  7.29it/s]Measuring inference for batch_size=1:   9%|▊         | 87/1000 [00:11<02:05,  7.29it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:12<02:05,  7.29it/s]Measuring inference for batch_size=1:   9%|▉         | 89/1000 [00:12<02:04,  7.29it/s]Measuring inference for batch_size=1:   9%|▉         | 90/1000 [00:12<02:04,  7.29it/s]Measuring inference for batch_size=1:   9%|▉         | 91/1000 [00:12<02:04,  7.29it/s]Measuring inference for batch_size=1:   9%|▉         | 92/1000 [00:12<02:04,  7.29it/s]Measuring inference for batch_size=1:   9%|▉         | 93/1000 [00:12<02:04,  7.29it/s]Measuring inference for batch_size=1:   9%|▉         | 94/1000 [00:12<02:04,  7.30it/s]Measuring inference for batch_size=1:  10%|▉         | 95/1000 [00:13<02:03,  7.31it/s]Measuring inference for batch_size=1:  10%|▉         | 96/1000 [00:13<02:03,  7.31it/s]Measuring inference for batch_size=1:  10%|▉         | 97/1000 [00:13<02:03,  7.31it/s]Measuring inference for batch_size=1:  10%|▉         | 98/1000 [00:13<02:03,  7.31it/s]Measuring inference for batch_size=1:  10%|▉         | 99/1000 [00:13<02:03,  7.32it/s]Measuring inference for batch_size=1:  10%|█         | 100/1000 [00:13<02:03,  7.31it/s]Measuring inference for batch_size=1:  10%|█         | 101/1000 [00:13<02:02,  7.31it/s]Measuring inference for batch_size=1:  10%|█         | 102/1000 [00:13<02:02,  7.32it/s]Measuring inference for batch_size=1:  10%|█         | 103/1000 [00:14<02:02,  7.31it/s]Measuring inference for batch_size=1:  10%|█         | 104/1000 [00:14<02:02,  7.31it/s]Measuring inference for batch_size=1:  10%|█         | 105/1000 [00:14<02:02,  7.32it/s]Measuring inference for batch_size=1:  11%|█         | 106/1000 [00:14<02:02,  7.31it/s]Measuring inference for batch_size=1:  11%|█         | 107/1000 [00:14<02:02,  7.31it/s]Measuring inference for batch_size=1:  11%|█         | 108/1000 [00:14<02:02,  7.31it/s]Measuring inference for batch_size=1:  11%|█         | 109/1000 [00:14<02:01,  7.31it/s]Measuring inference for batch_size=1:  11%|█         | 110/1000 [00:15<02:01,  7.31it/s]Measuring inference for batch_size=1:  11%|█         | 111/1000 [00:15<02:01,  7.31it/s]Measuring inference for batch_size=1:  11%|█         | 112/1000 [00:15<02:01,  7.31it/s]Measuring inference for batch_size=1:  11%|█▏        | 113/1000 [00:15<02:01,  7.30it/s]Measuring inference for batch_size=1:  11%|█▏        | 114/1000 [00:15<02:01,  7.31it/s]Measuring inference for batch_size=1:  12%|█▏        | 115/1000 [00:15<02:01,  7.31it/s]Measuring inference for batch_size=1:  12%|█▏        | 116/1000 [00:15<02:00,  7.32it/s]Measuring inference for batch_size=1:  12%|█▏        | 117/1000 [00:16<02:00,  7.32it/s]Measuring inference for batch_size=1:  12%|█▏        | 118/1000 [00:16<02:00,  7.31it/s]Measuring inference for batch_size=1:  12%|█▏        | 119/1000 [00:16<02:00,  7.32it/s]Measuring inference for batch_size=1:  12%|█▏        | 120/1000 [00:16<02:00,  7.32it/s]Measuring inference for batch_size=1:  12%|█▏        | 121/1000 [00:16<02:00,  7.32it/s]Measuring inference for batch_size=1:  12%|█▏        | 122/1000 [00:16<02:00,  7.32it/s]Measuring inference for batch_size=1:  12%|█▏        | 123/1000 [00:16<01:59,  7.32it/s]Measuring inference for batch_size=1:  12%|█▏        | 124/1000 [00:17<01:59,  7.32it/s]Measuring inference for batch_size=1:  12%|█▎        | 125/1000 [00:17<01:59,  7.32it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:17<01:59,  7.32it/s]Measuring inference for batch_size=1:  13%|█▎        | 127/1000 [00:17<01:59,  7.32it/s]Measuring inference for batch_size=1:  13%|█▎        | 128/1000 [00:17<01:59,  7.32it/s]Measuring inference for batch_size=1:  13%|█▎        | 129/1000 [00:17<01:59,  7.32it/s]Measuring inference for batch_size=1:  13%|█▎        | 130/1000 [00:17<01:58,  7.32it/s]Measuring inference for batch_size=1:  13%|█▎        | 131/1000 [00:17<01:58,  7.31it/s]Measuring inference for batch_size=1:  13%|█▎        | 132/1000 [00:18<01:58,  7.31it/s]Measuring inference for batch_size=1:  13%|█▎        | 133/1000 [00:18<01:58,  7.31it/s]Measuring inference for batch_size=1:  13%|█▎        | 134/1000 [00:18<01:58,  7.31it/s]Measuring inference for batch_size=1:  14%|█▎        | 135/1000 [00:18<01:58,  7.31it/s]Measuring inference for batch_size=1:  14%|█▎        | 136/1000 [00:18<01:58,  7.31it/s]Measuring inference for batch_size=1:  14%|█▎        | 137/1000 [00:18<01:58,  7.31it/s]Measuring inference for batch_size=1:  14%|█▍        | 138/1000 [00:18<01:57,  7.31it/s]Measuring inference for batch_size=1:  14%|█▍        | 139/1000 [00:19<01:57,  7.31it/s]Measuring inference for batch_size=1:  14%|█▍        | 140/1000 [00:19<01:57,  7.31it/s]Measuring inference for batch_size=1:  14%|█▍        | 141/1000 [00:19<01:57,  7.31it/s]Measuring inference for batch_size=1:  14%|█▍        | 142/1000 [00:19<01:57,  7.31it/s]Measuring inference for batch_size=1:  14%|█▍        | 143/1000 [00:19<01:57,  7.31it/s]Measuring inference for batch_size=1:  14%|█▍        | 144/1000 [00:19<01:57,  7.31it/s]Measuring inference for batch_size=1:  14%|█▍        | 145/1000 [00:19<01:56,  7.31it/s]Measuring inference for batch_size=1:  15%|█▍        | 146/1000 [00:20<01:56,  7.31it/s]Measuring inference for batch_size=1:  15%|█▍        | 147/1000 [00:20<01:56,  7.31it/s]Measuring inference for batch_size=1:  15%|█▍        | 148/1000 [00:20<01:56,  7.31it/s]Measuring inference for batch_size=1:  15%|█▍        | 149/1000 [00:20<01:56,  7.31it/s]Measuring inference for batch_size=1:  15%|█▌        | 150/1000 [00:20<01:56,  7.31it/s]Measuring inference for batch_size=1:  15%|█▌        | 151/1000 [00:20<01:56,  7.31it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:20<01:56,  7.31it/s]Measuring inference for batch_size=1:  15%|█▌        | 153/1000 [00:20<01:55,  7.31it/s]Measuring inference for batch_size=1:  15%|█▌        | 154/1000 [00:21<01:55,  7.31it/s]Measuring inference for batch_size=1:  16%|█▌        | 155/1000 [00:21<01:55,  7.31it/s]Measuring inference for batch_size=1:  16%|█▌        | 156/1000 [00:21<01:55,  7.31it/s]Measuring inference for batch_size=1:  16%|█▌        | 157/1000 [00:21<01:55,  7.32it/s]Measuring inference for batch_size=1:  16%|█▌        | 158/1000 [00:21<01:55,  7.31it/s]Measuring inference for batch_size=1:  16%|█▌        | 159/1000 [00:21<01:54,  7.31it/s]Measuring inference for batch_size=1:  16%|█▌        | 160/1000 [00:21<01:54,  7.31it/s]Measuring inference for batch_size=1:  16%|█▌        | 161/1000 [00:22<01:54,  7.31it/s]Measuring inference for batch_size=1:  16%|█▌        | 162/1000 [00:22<01:54,  7.31it/s]Measuring inference for batch_size=1:  16%|█▋        | 163/1000 [00:22<01:54,  7.31it/s]Measuring inference for batch_size=1:  16%|█▋        | 164/1000 [00:22<01:54,  7.31it/s]Measuring inference for batch_size=1:  16%|█▋        | 165/1000 [00:22<01:54,  7.32it/s]Measuring inference for batch_size=1:  17%|█▋        | 166/1000 [00:22<01:54,  7.32it/s]Measuring inference for batch_size=1:  17%|█▋        | 167/1000 [00:22<01:53,  7.31it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:23<01:53,  7.31it/s]Measuring inference for batch_size=1:  17%|█▋        | 169/1000 [00:23<01:53,  7.31it/s]Measuring inference for batch_size=1:  17%|█▋        | 170/1000 [00:23<01:53,  7.31it/s]Measuring inference for batch_size=1:  17%|█▋        | 171/1000 [00:23<01:53,  7.31it/s]Measuring inference for batch_size=1:  17%|█▋        | 172/1000 [00:23<01:53,  7.31it/s]Measuring inference for batch_size=1:  17%|█▋        | 173/1000 [00:23<01:53,  7.31it/s]Measuring inference for batch_size=1:  17%|█▋        | 174/1000 [00:23<01:53,  7.31it/s]Measuring inference for batch_size=1:  18%|█▊        | 175/1000 [00:23<01:52,  7.31it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:24<01:52,  7.30it/s]Measuring inference for batch_size=1:  18%|█▊        | 177/1000 [00:24<01:52,  7.30it/s]Measuring inference for batch_size=1:  18%|█▊        | 178/1000 [00:24<01:52,  7.31it/s]Measuring inference for batch_size=1:  18%|█▊        | 179/1000 [00:24<01:52,  7.30it/s]Measuring inference for batch_size=1:  18%|█▊        | 180/1000 [00:24<01:52,  7.31it/s]Measuring inference for batch_size=1:  18%|█▊        | 181/1000 [00:24<01:52,  7.31it/s]Measuring inference for batch_size=1:  18%|█▊        | 182/1000 [00:24<01:51,  7.31it/s]Measuring inference for batch_size=1:  18%|█▊        | 183/1000 [00:25<01:51,  7.31it/s]Measuring inference for batch_size=1:  18%|█▊        | 184/1000 [00:25<01:51,  7.31it/s]Measuring inference for batch_size=1:  18%|█▊        | 185/1000 [00:25<01:51,  7.31it/s]Measuring inference for batch_size=1:  19%|█▊        | 186/1000 [00:25<01:51,  7.31it/s]Measuring inference for batch_size=1:  19%|█▊        | 187/1000 [00:25<01:51,  7.31it/s]Measuring inference for batch_size=1:  19%|█▉        | 188/1000 [00:25<01:51,  7.31it/s]Measuring inference for batch_size=1:  19%|█▉        | 189/1000 [00:25<01:50,  7.31it/s]Measuring inference for batch_size=1:  19%|█▉        | 190/1000 [00:26<01:50,  7.31it/s]Measuring inference for batch_size=1:  19%|█▉        | 191/1000 [00:26<01:50,  7.31it/s]Measuring inference for batch_size=1:  19%|█▉        | 192/1000 [00:26<01:50,  7.31it/s]Measuring inference for batch_size=1:  19%|█▉        | 193/1000 [00:26<01:50,  7.32it/s]Measuring inference for batch_size=1:  19%|█▉        | 194/1000 [00:26<01:50,  7.30it/s]Measuring inference for batch_size=1:  20%|█▉        | 195/1000 [00:26<01:50,  7.29it/s]Measuring inference for batch_size=1:  20%|█▉        | 196/1000 [00:26<01:50,  7.30it/s]Measuring inference for batch_size=1:  20%|█▉        | 197/1000 [00:26<01:50,  7.30it/s]Measuring inference for batch_size=1:  20%|█▉        | 198/1000 [00:27<01:49,  7.30it/s]Measuring inference for batch_size=1:  20%|█▉        | 199/1000 [00:27<01:49,  7.30it/s]Measuring inference for batch_size=1:  20%|██        | 200/1000 [00:27<01:49,  7.31it/s]Measuring inference for batch_size=1:  20%|██        | 201/1000 [00:27<01:49,  7.31it/s]Measuring inference for batch_size=1:  20%|██        | 202/1000 [00:27<01:49,  7.32it/s]Measuring inference for batch_size=1:  20%|██        | 203/1000 [00:27<01:48,  7.31it/s]Measuring inference for batch_size=1:  20%|██        | 204/1000 [00:27<01:48,  7.31it/s]Measuring inference for batch_size=1:  20%|██        | 205/1000 [00:28<01:48,  7.31it/s]Measuring inference for batch_size=1:  21%|██        | 206/1000 [00:28<01:48,  7.31it/s]Measuring inference for batch_size=1:  21%|██        | 207/1000 [00:28<01:48,  7.31it/s]Measuring inference for batch_size=1:  21%|██        | 208/1000 [00:28<01:48,  7.31it/s]Measuring inference for batch_size=1:  21%|██        | 209/1000 [00:28<01:48,  7.31it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:28<01:48,  7.31it/s]Measuring inference for batch_size=1:  21%|██        | 211/1000 [00:28<01:47,  7.31it/s]Measuring inference for batch_size=1:  21%|██        | 212/1000 [00:29<01:47,  7.31it/s]Measuring inference for batch_size=1:  21%|██▏       | 213/1000 [00:29<01:47,  7.31it/s]Measuring inference for batch_size=1:  21%|██▏       | 214/1000 [00:29<01:47,  7.30it/s]Measuring inference for batch_size=1:  22%|██▏       | 215/1000 [00:29<01:47,  7.31it/s]Measuring inference for batch_size=1:  22%|██▏       | 216/1000 [00:29<01:47,  7.31it/s]Measuring inference for batch_size=1:  22%|██▏       | 217/1000 [00:29<01:47,  7.31it/s]Measuring inference for batch_size=1:  22%|██▏       | 218/1000 [00:29<01:46,  7.31it/s]Measuring inference for batch_size=1:  22%|██▏       | 219/1000 [00:30<01:46,  7.31it/s]Measuring inference for batch_size=1:  22%|██▏       | 220/1000 [00:30<01:46,  7.31it/s]Measuring inference for batch_size=1:  22%|██▏       | 221/1000 [00:30<01:46,  7.31it/s]Measuring inference for batch_size=1:  22%|██▏       | 222/1000 [00:30<01:46,  7.30it/s]Measuring inference for batch_size=1:  22%|██▏       | 223/1000 [00:30<01:46,  7.30it/s]Measuring inference for batch_size=1:  22%|██▏       | 224/1000 [00:30<01:46,  7.30it/s]Measuring inference for batch_size=1:  22%|██▎       | 225/1000 [00:30<01:46,  7.31it/s]Measuring inference for batch_size=1:  23%|██▎       | 226/1000 [00:30<01:45,  7.31it/s]Measuring inference for batch_size=1:  23%|██▎       | 227/1000 [00:31<01:45,  7.31it/s]Measuring inference for batch_size=1:  23%|██▎       | 228/1000 [00:31<01:45,  7.30it/s]Measuring inference for batch_size=1:  23%|██▎       | 229/1000 [00:31<01:45,  7.31it/s]Measuring inference for batch_size=1:  23%|██▎       | 230/1000 [00:31<01:45,  7.31it/s]Measuring inference for batch_size=1:  23%|██▎       | 231/1000 [00:31<01:45,  7.30it/s]Measuring inference for batch_size=1:  23%|██▎       | 232/1000 [00:31<01:45,  7.31it/s]Measuring inference for batch_size=1:  23%|██▎       | 233/1000 [00:31<01:44,  7.31it/s]Measuring inference for batch_size=1:  23%|██▎       | 234/1000 [00:32<01:44,  7.30it/s]Measuring inference for batch_size=1:  24%|██▎       | 235/1000 [00:32<01:44,  7.30it/s]Measuring inference for batch_size=1:  24%|██▎       | 236/1000 [00:32<01:44,  7.31it/s]Measuring inference for batch_size=1:  24%|██▎       | 237/1000 [00:32<01:44,  7.31it/s]Measuring inference for batch_size=1:  24%|██▍       | 238/1000 [00:32<01:44,  7.31it/s]Measuring inference for batch_size=1:  24%|██▍       | 239/1000 [00:32<01:44,  7.31it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:32<01:43,  7.31it/s]Measuring inference for batch_size=1:  24%|██▍       | 241/1000 [00:33<01:43,  7.31it/s]Measuring inference for batch_size=1:  24%|██▍       | 242/1000 [00:33<01:43,  7.31it/s]Measuring inference for batch_size=1:  24%|██▍       | 243/1000 [00:33<01:43,  7.31it/s]Measuring inference for batch_size=1:  24%|██▍       | 244/1000 [00:33<01:43,  7.31it/s]Measuring inference for batch_size=1:  24%|██▍       | 245/1000 [00:33<01:43,  7.30it/s]Measuring inference for batch_size=1:  25%|██▍       | 246/1000 [00:33<01:43,  7.30it/s]Measuring inference for batch_size=1:  25%|██▍       | 247/1000 [00:33<01:43,  7.30it/s]Measuring inference for batch_size=1:  25%|██▍       | 248/1000 [00:33<01:42,  7.31it/s]Measuring inference for batch_size=1:  25%|██▍       | 249/1000 [00:34<01:42,  7.31it/s]Measuring inference for batch_size=1:  25%|██▌       | 250/1000 [00:34<01:42,  7.31it/s]Measuring inference for batch_size=1:  25%|██▌       | 251/1000 [00:34<01:42,  7.32it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:34<01:42,  7.31it/s]Measuring inference for batch_size=1:  25%|██▌       | 253/1000 [00:34<01:42,  7.31it/s]Measuring inference for batch_size=1:  25%|██▌       | 254/1000 [00:34<01:42,  7.30it/s]Measuring inference for batch_size=1:  26%|██▌       | 255/1000 [00:34<01:42,  7.30it/s]Measuring inference for batch_size=1:  26%|██▌       | 256/1000 [00:35<01:41,  7.31it/s]Measuring inference for batch_size=1:  26%|██▌       | 257/1000 [00:35<01:41,  7.30it/s]Measuring inference for batch_size=1:  26%|██▌       | 258/1000 [00:35<01:41,  7.31it/s]Measuring inference for batch_size=1:  26%|██▌       | 259/1000 [00:35<01:41,  7.31it/s]Measuring inference for batch_size=1:  26%|██▌       | 260/1000 [00:35<01:41,  7.32it/s]Measuring inference for batch_size=1:  26%|██▌       | 261/1000 [00:35<01:41,  7.31it/s]Measuring inference for batch_size=1:  26%|██▌       | 262/1000 [00:35<01:40,  7.32it/s]Measuring inference for batch_size=1:  26%|██▋       | 263/1000 [00:36<01:40,  7.31it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:36<01:40,  7.31it/s]Measuring inference for batch_size=1:  26%|██▋       | 265/1000 [00:36<01:40,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 266/1000 [00:36<01:40,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 267/1000 [00:36<01:40,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 268/1000 [00:36<01:40,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 269/1000 [00:36<01:40,  7.30it/s]Measuring inference for batch_size=1:  27%|██▋       | 270/1000 [00:36<01:39,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 271/1000 [00:37<01:39,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 272/1000 [00:37<01:39,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 273/1000 [00:37<01:39,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 274/1000 [00:37<01:39,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 275/1000 [00:37<01:39,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 276/1000 [00:37<01:39,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 277/1000 [00:37<01:38,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 278/1000 [00:38<01:38,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 279/1000 [00:38<01:38,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 280/1000 [00:38<01:38,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 281/1000 [00:38<01:38,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 282/1000 [00:38<01:38,  7.32it/s]Measuring inference for batch_size=1:  28%|██▊       | 283/1000 [00:38<01:37,  7.32it/s]Measuring inference for batch_size=1:  28%|██▊       | 284/1000 [00:38<01:37,  7.32it/s]Measuring inference for batch_size=1:  28%|██▊       | 285/1000 [00:39<01:37,  7.32it/s]Measuring inference for batch_size=1:  29%|██▊       | 286/1000 [00:39<01:37,  7.31it/s]Measuring inference for batch_size=1:  29%|██▊       | 287/1000 [00:39<01:37,  7.31it/s]Measuring inference for batch_size=1:  29%|██▉       | 288/1000 [00:39<01:37,  7.31it/s]Measuring inference for batch_size=1:  29%|██▉       | 289/1000 [00:39<01:37,  7.31it/s]Measuring inference for batch_size=1:  29%|██▉       | 290/1000 [00:39<01:37,  7.32it/s]Measuring inference for batch_size=1:  29%|██▉       | 291/1000 [00:39<01:36,  7.31it/s]Measuring inference for batch_size=1:  29%|██▉       | 292/1000 [00:39<01:36,  7.31it/s]Measuring inference for batch_size=1:  29%|██▉       | 293/1000 [00:40<01:36,  7.31it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:40<01:36,  7.31it/s]Measuring inference for batch_size=1:  30%|██▉       | 295/1000 [00:40<01:36,  7.32it/s]Measuring inference for batch_size=1:  30%|██▉       | 296/1000 [00:40<01:36,  7.32it/s]Measuring inference for batch_size=1:  30%|██▉       | 297/1000 [00:40<01:36,  7.31it/s]Measuring inference for batch_size=1:  30%|██▉       | 298/1000 [00:40<01:35,  7.32it/s]Measuring inference for batch_size=1:  30%|██▉       | 299/1000 [00:40<01:35,  7.32it/s]Measuring inference for batch_size=1:  30%|███       | 300/1000 [00:41<01:35,  7.31it/s]Measuring inference for batch_size=1:  30%|███       | 301/1000 [00:41<01:35,  7.31it/s]Measuring inference for batch_size=1:  30%|███       | 302/1000 [00:41<01:35,  7.31it/s]Measuring inference for batch_size=1:  30%|███       | 303/1000 [00:41<01:35,  7.31it/s]Measuring inference for batch_size=1:  30%|███       | 304/1000 [00:41<01:35,  7.32it/s]Measuring inference for batch_size=1:  30%|███       | 305/1000 [00:41<01:35,  7.31it/s]Measuring inference for batch_size=1:  31%|███       | 306/1000 [00:41<01:34,  7.31it/s]Measuring inference for batch_size=1:  31%|███       | 307/1000 [00:42<01:34,  7.31it/s]Measuring inference for batch_size=1:  31%|███       | 308/1000 [00:42<01:34,  7.31it/s]Measuring inference for batch_size=1:  31%|███       | 309/1000 [00:42<01:34,  7.31it/s]Measuring inference for batch_size=1:  31%|███       | 310/1000 [00:42<01:34,  7.32it/s]Measuring inference for batch_size=1:  31%|███       | 311/1000 [00:42<01:34,  7.31it/s]Measuring inference for batch_size=1:  31%|███       | 312/1000 [00:42<01:34,  7.31it/s]Measuring inference for batch_size=1:  31%|███▏      | 313/1000 [00:42<01:33,  7.31it/s]Measuring inference for batch_size=1:  31%|███▏      | 314/1000 [00:42<01:33,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 315/1000 [00:43<01:33,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 316/1000 [00:43<01:33,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 317/1000 [00:43<01:33,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 318/1000 [00:43<01:33,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 319/1000 [00:43<01:33,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 320/1000 [00:43<01:33,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 321/1000 [00:43<01:32,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 322/1000 [00:44<01:32,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 323/1000 [00:44<01:32,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 324/1000 [00:44<01:32,  7.31it/s]Measuring inference for batch_size=1:  32%|███▎      | 325/1000 [00:44<01:32,  7.31it/s]Measuring inference for batch_size=1:  33%|███▎      | 326/1000 [00:44<01:32,  7.31it/s]Measuring inference for batch_size=1:  33%|███▎      | 327/1000 [00:44<01:32,  7.31it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:44<01:31,  7.31it/s]Measuring inference for batch_size=1:  33%|███▎      | 329/1000 [00:45<01:31,  7.32it/s]Measuring inference for batch_size=1:  33%|███▎      | 330/1000 [00:45<01:31,  7.32it/s]Measuring inference for batch_size=1:  33%|███▎      | 331/1000 [00:45<01:31,  7.31it/s]Measuring inference for batch_size=1:  33%|███▎      | 332/1000 [00:45<01:31,  7.31it/s]Measuring inference for batch_size=1:  33%|███▎      | 333/1000 [00:45<01:31,  7.31it/s]Measuring inference for batch_size=1:  33%|███▎      | 334/1000 [00:45<01:31,  7.31it/s]Measuring inference for batch_size=1:  34%|███▎      | 335/1000 [00:45<01:30,  7.31it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:46<01:30,  7.31it/s]Measuring inference for batch_size=1:  34%|███▎      | 337/1000 [00:46<01:30,  7.31it/s]Measuring inference for batch_size=1:  34%|███▍      | 338/1000 [00:46<01:30,  7.31it/s]Measuring inference for batch_size=1:  34%|███▍      | 339/1000 [00:46<01:30,  7.31it/s]Measuring inference for batch_size=1:  34%|███▍      | 340/1000 [00:46<01:30,  7.32it/s]Measuring inference for batch_size=1:  34%|███▍      | 341/1000 [00:46<01:30,  7.31it/s]Measuring inference for batch_size=1:  34%|███▍      | 342/1000 [00:46<01:29,  7.32it/s]Measuring inference for batch_size=1:  34%|███▍      | 343/1000 [00:46<01:29,  7.31it/s]Measuring inference for batch_size=1:  34%|███▍      | 344/1000 [00:47<01:29,  7.31it/s]Measuring inference for batch_size=1:  34%|███▍      | 345/1000 [00:47<01:29,  7.31it/s]Measuring inference for batch_size=1:  35%|███▍      | 346/1000 [00:47<01:29,  7.31it/s]Measuring inference for batch_size=1:  35%|███▍      | 347/1000 [00:47<01:29,  7.31it/s]Measuring inference for batch_size=1:  35%|███▍      | 348/1000 [00:47<01:29,  7.31it/s]Measuring inference for batch_size=1:  35%|███▍      | 349/1000 [00:47<01:29,  7.31it/s]Measuring inference for batch_size=1:  35%|███▌      | 350/1000 [00:47<01:28,  7.31it/s]Measuring inference for batch_size=1:  35%|███▌      | 351/1000 [00:48<01:28,  7.31it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:48<01:28,  7.31it/s]Measuring inference for batch_size=1:  35%|███▌      | 353/1000 [00:48<01:28,  7.31it/s]Measuring inference for batch_size=1:  35%|███▌      | 354/1000 [00:48<01:28,  7.31it/s]Measuring inference for batch_size=1:  36%|███▌      | 355/1000 [00:48<01:28,  7.31it/s]Measuring inference for batch_size=1:  36%|███▌      | 356/1000 [00:48<01:28,  7.31it/s]Measuring inference for batch_size=1:  36%|███▌      | 357/1000 [00:48<01:27,  7.31it/s]Measuring inference for batch_size=1:  36%|███▌      | 358/1000 [00:49<01:27,  7.31it/s]Measuring inference for batch_size=1:  36%|███▌      | 359/1000 [00:49<01:27,  7.31it/s]Measuring inference for batch_size=1:  36%|███▌      | 360/1000 [00:49<01:27,  7.31it/s]Measuring inference for batch_size=1:  36%|███▌      | 361/1000 [00:49<01:27,  7.32it/s]Measuring inference for batch_size=1:  36%|███▌      | 362/1000 [00:49<01:27,  7.31it/s]Measuring inference for batch_size=1:  36%|███▋      | 363/1000 [00:49<01:27,  7.31it/s]Measuring inference for batch_size=1:  36%|███▋      | 364/1000 [00:49<01:26,  7.31it/s]Measuring inference for batch_size=1:  36%|███▋      | 365/1000 [00:49<01:26,  7.31it/s]Measuring inference for batch_size=1:  37%|███▋      | 366/1000 [00:50<01:26,  7.31it/s]Measuring inference for batch_size=1:  37%|███▋      | 367/1000 [00:50<01:26,  7.31it/s]Measuring inference for batch_size=1:  37%|███▋      | 368/1000 [00:50<01:26,  7.31it/s]Measuring inference for batch_size=1:  37%|███▋      | 369/1000 [00:50<01:26,  7.31it/s]Measuring inference for batch_size=1:  37%|███▋      | 370/1000 [00:50<01:26,  7.31it/s]Measuring inference for batch_size=1:  37%|███▋      | 371/1000 [00:50<01:26,  7.31it/s]Measuring inference for batch_size=1:  37%|███▋      | 372/1000 [00:50<01:25,  7.31it/s]Measuring inference for batch_size=1:  37%|███▋      | 373/1000 [00:51<01:25,  7.31it/s]Measuring inference for batch_size=1:  37%|███▋      | 374/1000 [00:51<01:25,  7.31it/s]Measuring inference for batch_size=1:  38%|███▊      | 375/1000 [00:51<01:25,  7.31it/s]Measuring inference for batch_size=1:  38%|███▊      | 376/1000 [00:51<01:25,  7.31it/s]Measuring inference for batch_size=1:  38%|███▊      | 377/1000 [00:51<01:25,  7.31it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:51<01:25,  7.31it/s]Measuring inference for batch_size=1:  38%|███▊      | 379/1000 [00:51<01:24,  7.31it/s]Measuring inference for batch_size=1:  38%|███▊      | 380/1000 [00:52<01:24,  7.31it/s]Measuring inference for batch_size=1:  38%|███▊      | 381/1000 [00:52<01:24,  7.31it/s]Measuring inference for batch_size=1:  38%|███▊      | 382/1000 [00:52<01:24,  7.31it/s]Measuring inference for batch_size=1:  38%|███▊      | 383/1000 [00:52<01:24,  7.31it/s]Measuring inference for batch_size=1:  38%|███▊      | 384/1000 [00:52<01:24,  7.32it/s]Measuring inference for batch_size=1:  38%|███▊      | 385/1000 [00:52<01:24,  7.31it/s]Measuring inference for batch_size=1:  39%|███▊      | 386/1000 [00:52<01:23,  7.31it/s]Measuring inference for batch_size=1:  39%|███▊      | 387/1000 [00:52<01:23,  7.31it/s]Measuring inference for batch_size=1:  39%|███▉      | 388/1000 [00:53<01:23,  7.31it/s]Measuring inference for batch_size=1:  39%|███▉      | 389/1000 [00:53<01:23,  7.32it/s]Measuring inference for batch_size=1:  39%|███▉      | 390/1000 [00:53<01:23,  7.32it/s]Measuring inference for batch_size=1:  39%|███▉      | 391/1000 [00:53<01:23,  7.32it/s]Measuring inference for batch_size=1:  39%|███▉      | 392/1000 [00:53<01:23,  7.32it/s]Measuring inference for batch_size=1:  39%|███▉      | 393/1000 [00:53<01:22,  7.31it/s]Measuring inference for batch_size=1:  39%|███▉      | 394/1000 [00:53<01:22,  7.31it/s]Measuring inference for batch_size=1:  40%|███▉      | 395/1000 [00:54<01:22,  7.31it/s]Measuring inference for batch_size=1:  40%|███▉      | 396/1000 [00:54<01:22,  7.32it/s]Measuring inference for batch_size=1:  40%|███▉      | 397/1000 [00:54<01:22,  7.32it/s]Measuring inference for batch_size=1:  40%|███▉      | 398/1000 [00:54<01:22,  7.31it/s]Measuring inference for batch_size=1:  40%|███▉      | 399/1000 [00:54<01:22,  7.31it/s]Measuring inference for batch_size=1:  40%|████      | 400/1000 [00:54<01:22,  7.31it/s]Measuring inference for batch_size=1:  40%|████      | 401/1000 [00:54<01:21,  7.31it/s]Measuring inference for batch_size=1:  40%|████      | 402/1000 [00:55<01:21,  7.31it/s]Measuring inference for batch_size=1:  40%|████      | 403/1000 [00:55<01:21,  7.31it/s]Measuring inference for batch_size=1:  40%|████      | 404/1000 [00:55<01:21,  7.31it/s]Measuring inference for batch_size=1:  40%|████      | 405/1000 [00:55<01:21,  7.31it/s]Measuring inference for batch_size=1:  41%|████      | 406/1000 [00:55<01:21,  7.31it/s]Measuring inference for batch_size=1:  41%|████      | 407/1000 [00:55<01:21,  7.30it/s]Measuring inference for batch_size=1:  41%|████      | 408/1000 [00:55<01:20,  7.31it/s]Measuring inference for batch_size=1:  41%|████      | 409/1000 [00:55<01:20,  7.31it/s]Measuring inference for batch_size=1:  41%|████      | 410/1000 [00:56<01:20,  7.31it/s]Measuring inference for batch_size=1:  41%|████      | 411/1000 [00:56<01:20,  7.31it/s]Measuring inference for batch_size=1:  41%|████      | 412/1000 [00:56<01:20,  7.31it/s]Measuring inference for batch_size=1:  41%|████▏     | 413/1000 [00:56<01:20,  7.31it/s]Measuring inference for batch_size=1:  41%|████▏     | 414/1000 [00:56<01:20,  7.31it/s]Measuring inference for batch_size=1:  42%|████▏     | 415/1000 [00:56<01:20,  7.31it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:56<01:19,  7.30it/s]Measuring inference for batch_size=1:  42%|████▏     | 417/1000 [00:57<01:19,  7.30it/s]Measuring inference for batch_size=1:  42%|████▏     | 418/1000 [00:57<01:19,  7.30it/s]Measuring inference for batch_size=1:  42%|████▏     | 419/1000 [00:57<01:19,  7.31it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:57<01:19,  7.31it/s]Measuring inference for batch_size=1:  42%|████▏     | 421/1000 [00:57<01:19,  7.31it/s]Measuring inference for batch_size=1:  42%|████▏     | 422/1000 [00:57<01:19,  7.31it/s]Measuring inference for batch_size=1:  42%|████▏     | 423/1000 [00:57<01:18,  7.32it/s]Measuring inference for batch_size=1:  42%|████▏     | 424/1000 [00:58<01:18,  7.32it/s]Measuring inference for batch_size=1:  42%|████▎     | 425/1000 [00:58<01:18,  7.31it/s]Measuring inference for batch_size=1:  43%|████▎     | 426/1000 [00:58<01:18,  7.31it/s]Measuring inference for batch_size=1:  43%|████▎     | 427/1000 [00:58<01:18,  7.32it/s]Measuring inference for batch_size=1:  43%|████▎     | 428/1000 [00:58<01:18,  7.32it/s]Measuring inference for batch_size=1:  43%|████▎     | 429/1000 [00:58<01:18,  7.31it/s]Measuring inference for batch_size=1:  43%|████▎     | 430/1000 [00:58<01:17,  7.31it/s]Measuring inference for batch_size=1:  43%|████▎     | 431/1000 [00:58<01:17,  7.31it/s]Measuring inference for batch_size=1:  43%|████▎     | 432/1000 [00:59<01:17,  7.31it/s]Measuring inference for batch_size=1:  43%|████▎     | 433/1000 [00:59<01:17,  7.31it/s]Measuring inference for batch_size=1:  43%|████▎     | 434/1000 [00:59<01:17,  7.31it/s]Measuring inference for batch_size=1:  44%|████▎     | 435/1000 [00:59<01:17,  7.31it/s]Measuring inference for batch_size=1:  44%|████▎     | 436/1000 [00:59<01:17,  7.31it/s]Measuring inference for batch_size=1:  44%|████▎     | 437/1000 [00:59<01:16,  7.32it/s]Measuring inference for batch_size=1:  44%|████▍     | 438/1000 [00:59<01:16,  7.32it/s]Measuring inference for batch_size=1:  44%|████▍     | 439/1000 [01:00<01:16,  7.32it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [01:00<01:16,  7.32it/s]Measuring inference for batch_size=1:  44%|████▍     | 441/1000 [01:00<01:16,  7.32it/s]Measuring inference for batch_size=1:  44%|████▍     | 442/1000 [01:00<01:16,  7.32it/s]Measuring inference for batch_size=1:  44%|████▍     | 443/1000 [01:00<01:16,  7.32it/s]Measuring inference for batch_size=1:  44%|████▍     | 444/1000 [01:00<01:15,  7.32it/s]Measuring inference for batch_size=1:  44%|████▍     | 445/1000 [01:00<01:15,  7.31it/s]Measuring inference for batch_size=1:  45%|████▍     | 446/1000 [01:01<01:15,  7.31it/s]Measuring inference for batch_size=1:  45%|████▍     | 447/1000 [01:01<01:15,  7.31it/s]Measuring inference for batch_size=1:  45%|████▍     | 448/1000 [01:01<01:15,  7.31it/s]Measuring inference for batch_size=1:  45%|████▍     | 449/1000 [01:01<01:15,  7.31it/s]Measuring inference for batch_size=1:  45%|████▌     | 450/1000 [01:01<01:15,  7.31it/s]Measuring inference for batch_size=1:  45%|████▌     | 451/1000 [01:01<01:15,  7.31it/s]Measuring inference for batch_size=1:  45%|████▌     | 452/1000 [01:01<01:14,  7.31it/s]Measuring inference for batch_size=1:  45%|████▌     | 453/1000 [01:02<01:14,  7.31it/s]Measuring inference for batch_size=1:  45%|████▌     | 454/1000 [01:02<01:14,  7.31it/s]Measuring inference for batch_size=1:  46%|████▌     | 455/1000 [01:02<01:14,  7.32it/s]Measuring inference for batch_size=1:  46%|████▌     | 456/1000 [01:02<01:14,  7.32it/s]Measuring inference for batch_size=1:  46%|████▌     | 457/1000 [01:02<01:14,  7.32it/s]Measuring inference for batch_size=1:  46%|████▌     | 458/1000 [01:02<01:14,  7.32it/s]Measuring inference for batch_size=1:  46%|████▌     | 459/1000 [01:02<01:13,  7.31it/s]Measuring inference for batch_size=1:  46%|████▌     | 460/1000 [01:02<01:13,  7.32it/s]Measuring inference for batch_size=1:  46%|████▌     | 461/1000 [01:03<01:13,  7.32it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [01:03<01:13,  7.31it/s]Measuring inference for batch_size=1:  46%|████▋     | 463/1000 [01:03<01:13,  7.30it/s]Measuring inference for batch_size=1:  46%|████▋     | 464/1000 [01:03<01:13,  7.30it/s]Measuring inference for batch_size=1:  46%|████▋     | 465/1000 [01:03<01:13,  7.31it/s]Measuring inference for batch_size=1:  47%|████▋     | 466/1000 [01:03<01:13,  7.31it/s]Measuring inference for batch_size=1:  47%|████▋     | 467/1000 [01:03<01:12,  7.31it/s]Measuring inference for batch_size=1:  47%|████▋     | 468/1000 [01:04<01:12,  7.31it/s]Measuring inference for batch_size=1:  47%|████▋     | 469/1000 [01:04<01:12,  7.31it/s]Measuring inference for batch_size=1:  47%|████▋     | 470/1000 [01:04<01:12,  7.31it/s]Measuring inference for batch_size=1:  47%|████▋     | 471/1000 [01:04<01:12,  7.31it/s]Measuring inference for batch_size=1:  47%|████▋     | 472/1000 [01:04<01:12,  7.31it/s]Measuring inference for batch_size=1:  47%|████▋     | 473/1000 [01:04<01:12,  7.31it/s]Measuring inference for batch_size=1:  47%|████▋     | 474/1000 [01:04<01:11,  7.31it/s]Measuring inference for batch_size=1:  48%|████▊     | 475/1000 [01:05<01:11,  7.31it/s]Measuring inference for batch_size=1:  48%|████▊     | 476/1000 [01:05<01:11,  7.32it/s]Measuring inference for batch_size=1:  48%|████▊     | 477/1000 [01:05<01:11,  7.32it/s]Measuring inference for batch_size=1:  48%|████▊     | 478/1000 [01:05<01:11,  7.31it/s]Measuring inference for batch_size=1:  48%|████▊     | 479/1000 [01:05<01:11,  7.32it/s]Measuring inference for batch_size=1:  48%|████▊     | 480/1000 [01:05<01:11,  7.32it/s]Measuring inference for batch_size=1:  48%|████▊     | 481/1000 [01:05<01:10,  7.31it/s]Measuring inference for batch_size=1:  48%|████▊     | 482/1000 [01:05<01:10,  7.31it/s]Measuring inference for batch_size=1:  48%|████▊     | 483/1000 [01:06<01:10,  7.31it/s]Measuring inference for batch_size=1:  48%|████▊     | 484/1000 [01:06<01:10,  7.31it/s]Measuring inference for batch_size=1:  48%|████▊     | 485/1000 [01:06<01:10,  7.31it/s]Measuring inference for batch_size=1:  49%|████▊     | 486/1000 [01:06<01:10,  7.32it/s]Measuring inference for batch_size=1:  49%|████▊     | 487/1000 [01:06<01:10,  7.31it/s]Measuring inference for batch_size=1:  49%|████▉     | 488/1000 [01:06<01:10,  7.31it/s]Measuring inference for batch_size=1:  49%|████▉     | 489/1000 [01:06<01:09,  7.32it/s]Measuring inference for batch_size=1:  49%|████▉     | 490/1000 [01:07<01:09,  7.32it/s]Measuring inference for batch_size=1:  49%|████▉     | 491/1000 [01:07<01:09,  7.32it/s]Measuring inference for batch_size=1:  49%|████▉     | 492/1000 [01:07<01:09,  7.32it/s]Measuring inference for batch_size=1:  49%|████▉     | 493/1000 [01:07<01:09,  7.31it/s]Measuring inference for batch_size=1:  49%|████▉     | 494/1000 [01:07<01:09,  7.31it/s]Measuring inference for batch_size=1:  50%|████▉     | 495/1000 [01:07<01:09,  7.31it/s]Measuring inference for batch_size=1:  50%|████▉     | 496/1000 [01:07<01:08,  7.31it/s]Measuring inference for batch_size=1:  50%|████▉     | 497/1000 [01:08<01:08,  7.30it/s]Measuring inference for batch_size=1:  50%|████▉     | 498/1000 [01:08<01:08,  7.31it/s]Measuring inference for batch_size=1:  50%|████▉     | 499/1000 [01:08<01:08,  7.31it/s]Measuring inference for batch_size=1:  50%|█████     | 500/1000 [01:08<01:08,  7.31it/s]Measuring inference for batch_size=1:  50%|█████     | 501/1000 [01:08<01:08,  7.31it/s]Measuring inference for batch_size=1:  50%|█████     | 502/1000 [01:08<01:08,  7.31it/s]Measuring inference for batch_size=1:  50%|█████     | 503/1000 [01:08<01:07,  7.31it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [01:08<01:07,  7.31it/s]Measuring inference for batch_size=1:  50%|█████     | 505/1000 [01:09<01:07,  7.32it/s]Measuring inference for batch_size=1:  51%|█████     | 506/1000 [01:09<01:07,  7.31it/s]Measuring inference for batch_size=1:  51%|█████     | 507/1000 [01:09<01:07,  7.32it/s]Measuring inference for batch_size=1:  51%|█████     | 508/1000 [01:09<01:07,  7.32it/s]Measuring inference for batch_size=1:  51%|█████     | 509/1000 [01:09<01:07,  7.31it/s]Measuring inference for batch_size=1:  51%|█████     | 510/1000 [01:09<01:07,  7.31it/s]Measuring inference for batch_size=1:  51%|█████     | 511/1000 [01:09<01:06,  7.31it/s]Measuring inference for batch_size=1:  51%|█████     | 512/1000 [01:10<01:06,  7.31it/s]Measuring inference for batch_size=1:  51%|█████▏    | 513/1000 [01:10<01:06,  7.31it/s]Measuring inference for batch_size=1:  51%|█████▏    | 514/1000 [01:10<01:06,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 515/1000 [01:10<01:06,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 516/1000 [01:10<01:06,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 517/1000 [01:10<01:06,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 518/1000 [01:10<01:05,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 519/1000 [01:11<01:05,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 520/1000 [01:11<01:05,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 521/1000 [01:11<01:05,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 522/1000 [01:11<01:05,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 523/1000 [01:11<01:05,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 524/1000 [01:11<01:05,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▎    | 525/1000 [01:11<01:05,  7.31it/s]Measuring inference for batch_size=1:  53%|█████▎    | 526/1000 [01:11<01:04,  7.31it/s]Measuring inference for batch_size=1:  53%|█████▎    | 527/1000 [01:12<01:04,  7.31it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [01:12<01:04,  7.31it/s]Measuring inference for batch_size=1:  53%|█████▎    | 529/1000 [01:12<01:04,  7.31it/s]Measuring inference for batch_size=1:  53%|█████▎    | 530/1000 [01:12<01:04,  7.31it/s]Measuring inference for batch_size=1:  53%|█████▎    | 531/1000 [01:12<01:04,  7.31it/s]Measuring inference for batch_size=1:  53%|█████▎    | 532/1000 [01:12<01:04,  7.31it/s]Measuring inference for batch_size=1:  53%|█████▎    | 533/1000 [01:12<01:03,  7.31it/s]Measuring inference for batch_size=1:  53%|█████▎    | 534/1000 [01:13<01:03,  7.31it/s]Measuring inference for batch_size=1:  54%|█████▎    | 535/1000 [01:13<01:03,  7.31it/s]Measuring inference for batch_size=1:  54%|█████▎    | 536/1000 [01:13<01:03,  7.31it/s]Measuring inference for batch_size=1:  54%|█████▎    | 537/1000 [01:13<01:03,  7.31it/s]Measuring inference for batch_size=1:  54%|█████▍    | 538/1000 [01:13<01:03,  7.31it/s]Measuring inference for batch_size=1:  54%|█████▍    | 539/1000 [01:13<01:03,  7.31it/s]Measuring inference for batch_size=1:  54%|█████▍    | 540/1000 [01:13<01:02,  7.31it/s]Measuring inference for batch_size=1:  54%|█████▍    | 541/1000 [01:14<01:02,  7.31it/s]Measuring inference for batch_size=1:  54%|█████▍    | 542/1000 [01:14<01:02,  7.32it/s]Measuring inference for batch_size=1:  54%|█████▍    | 543/1000 [01:14<01:02,  7.32it/s]Measuring inference for batch_size=1:  54%|█████▍    | 544/1000 [01:14<01:02,  7.32it/s]Measuring inference for batch_size=1:  55%|█████▍    | 545/1000 [01:14<01:02,  7.32it/s]Measuring inference for batch_size=1:  55%|█████▍    | 546/1000 [01:14<01:02,  7.32it/s]Measuring inference for batch_size=1:  55%|█████▍    | 547/1000 [01:14<01:01,  7.31it/s]Measuring inference for batch_size=1:  55%|█████▍    | 548/1000 [01:14<01:01,  7.32it/s]Measuring inference for batch_size=1:  55%|█████▍    | 549/1000 [01:15<01:01,  7.31it/s]Measuring inference for batch_size=1:  55%|█████▌    | 550/1000 [01:15<01:01,  7.31it/s]Measuring inference for batch_size=1:  55%|█████▌    | 551/1000 [01:15<01:01,  7.31it/s]Measuring inference for batch_size=1:  55%|█████▌    | 552/1000 [01:15<01:01,  7.31it/s]Measuring inference for batch_size=1:  55%|█████▌    | 553/1000 [01:15<01:01,  7.31it/s]Measuring inference for batch_size=1:  55%|█████▌    | 554/1000 [01:15<01:00,  7.31it/s]Measuring inference for batch_size=1:  56%|█████▌    | 555/1000 [01:15<01:00,  7.31it/s]Measuring inference for batch_size=1:  56%|█████▌    | 556/1000 [01:16<01:00,  7.31it/s]Measuring inference for batch_size=1:  56%|█████▌    | 557/1000 [01:16<01:00,  7.31it/s]Measuring inference for batch_size=1:  56%|█████▌    | 558/1000 [01:16<01:00,  7.31it/s]Measuring inference for batch_size=1:  56%|█████▌    | 559/1000 [01:16<01:00,  7.31it/s]Measuring inference for batch_size=1:  56%|█████▌    | 560/1000 [01:16<01:00,  7.31it/s]Measuring inference for batch_size=1:  56%|█████▌    | 561/1000 [01:16<01:00,  7.32it/s]Measuring inference for batch_size=1:  56%|█████▌    | 562/1000 [01:16<00:59,  7.31it/s]Measuring inference for batch_size=1:  56%|█████▋    | 563/1000 [01:17<00:59,  7.31it/s]Measuring inference for batch_size=1:  56%|█████▋    | 564/1000 [01:17<00:59,  7.31it/s]Measuring inference for batch_size=1:  56%|█████▋    | 565/1000 [01:17<00:59,  7.31it/s]Measuring inference for batch_size=1:  57%|█████▋    | 566/1000 [01:17<00:59,  7.31it/s]Measuring inference for batch_size=1:  57%|█████▋    | 567/1000 [01:17<00:59,  7.31it/s]Measuring inference for batch_size=1:  57%|█████▋    | 568/1000 [01:17<00:59,  7.31it/s]Measuring inference for batch_size=1:  57%|█████▋    | 569/1000 [01:17<00:58,  7.31it/s]Measuring inference for batch_size=1:  57%|█████▋    | 570/1000 [01:18<00:58,  7.31it/s]Measuring inference for batch_size=1:  57%|█████▋    | 571/1000 [01:18<00:58,  7.31it/s]Measuring inference for batch_size=1:  57%|█████▋    | 572/1000 [01:18<00:58,  7.31it/s]Measuring inference for batch_size=1:  57%|█████▋    | 573/1000 [01:18<00:58,  7.31it/s]Measuring inference for batch_size=1:  57%|█████▋    | 574/1000 [01:18<00:58,  7.31it/s]Measuring inference for batch_size=1:  57%|█████▊    | 575/1000 [01:18<00:58,  7.31it/s]Measuring inference for batch_size=1:  58%|█████▊    | 576/1000 [01:18<00:58,  7.31it/s]Measuring inference for batch_size=1:  58%|█████▊    | 577/1000 [01:18<00:57,  7.31it/s]Measuring inference for batch_size=1:  58%|█████▊    | 578/1000 [01:19<00:57,  7.31it/s]Measuring inference for batch_size=1:  58%|█████▊    | 579/1000 [01:19<00:57,  7.31it/s]Measuring inference for batch_size=1:  58%|█████▊    | 580/1000 [01:19<00:57,  7.31it/s]Measuring inference for batch_size=1:  58%|█████▊    | 581/1000 [01:19<00:57,  7.30it/s]Measuring inference for batch_size=1:  58%|█████▊    | 582/1000 [01:19<00:57,  7.30it/s]Measuring inference for batch_size=1:  58%|█████▊    | 583/1000 [01:19<00:57,  7.30it/s]Measuring inference for batch_size=1:  58%|█████▊    | 584/1000 [01:19<00:56,  7.31it/s]Measuring inference for batch_size=1:  58%|█████▊    | 585/1000 [01:20<00:56,  7.31it/s]Measuring inference for batch_size=1:  59%|█████▊    | 586/1000 [01:20<00:56,  7.31it/s]Measuring inference for batch_size=1:  59%|█████▊    | 587/1000 [01:20<00:56,  7.30it/s]Measuring inference for batch_size=1:  59%|█████▉    | 588/1000 [01:20<00:56,  7.30it/s]Measuring inference for batch_size=1:  59%|█████▉    | 589/1000 [01:20<00:56,  7.30it/s]Measuring inference for batch_size=1:  59%|█████▉    | 590/1000 [01:20<00:56,  7.31it/s]Measuring inference for batch_size=1:  59%|█████▉    | 591/1000 [01:20<00:56,  7.29it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [01:21<00:55,  7.30it/s]Measuring inference for batch_size=1:  59%|█████▉    | 593/1000 [01:21<00:55,  7.30it/s]Measuring inference for batch_size=1:  59%|█████▉    | 594/1000 [01:21<00:55,  7.30it/s]Measuring inference for batch_size=1:  60%|█████▉    | 595/1000 [01:21<00:55,  7.30it/s]Measuring inference for batch_size=1:  60%|█████▉    | 596/1000 [01:21<00:55,  7.31it/s]Measuring inference for batch_size=1:  60%|█████▉    | 597/1000 [01:21<00:55,  7.31it/s]Measuring inference for batch_size=1:  60%|█████▉    | 598/1000 [01:21<00:54,  7.31it/s]Measuring inference for batch_size=1:  60%|█████▉    | 599/1000 [01:21<00:54,  7.30it/s]Measuring inference for batch_size=1:  60%|██████    | 600/1000 [01:22<00:54,  7.31it/s]Measuring inference for batch_size=1:  60%|██████    | 601/1000 [01:22<00:54,  7.31it/s]Measuring inference for batch_size=1:  60%|██████    | 602/1000 [01:22<00:54,  7.31it/s]Measuring inference for batch_size=1:  60%|██████    | 603/1000 [01:22<00:54,  7.32it/s]Measuring inference for batch_size=1:  60%|██████    | 604/1000 [01:22<00:54,  7.32it/s]Measuring inference for batch_size=1:  60%|██████    | 605/1000 [01:22<00:53,  7.32it/s]Measuring inference for batch_size=1:  61%|██████    | 606/1000 [01:22<00:53,  7.31it/s]Measuring inference for batch_size=1:  61%|██████    | 607/1000 [01:23<00:53,  7.31it/s]Measuring inference for batch_size=1:  61%|██████    | 608/1000 [01:23<00:53,  7.31it/s]Measuring inference for batch_size=1:  61%|██████    | 609/1000 [01:23<00:53,  7.31it/s]Measuring inference for batch_size=1:  61%|██████    | 610/1000 [01:23<00:53,  7.31it/s]Measuring inference for batch_size=1:  61%|██████    | 611/1000 [01:23<00:53,  7.31it/s]Measuring inference for batch_size=1:  61%|██████    | 612/1000 [01:23<00:53,  7.31it/s]Measuring inference for batch_size=1:  61%|██████▏   | 613/1000 [01:23<00:52,  7.31it/s]Measuring inference for batch_size=1:  61%|██████▏   | 614/1000 [01:24<00:52,  7.31it/s]Measuring inference for batch_size=1:  62%|██████▏   | 615/1000 [01:24<00:52,  7.31it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [01:24<00:52,  7.31it/s]Measuring inference for batch_size=1:  62%|██████▏   | 617/1000 [01:24<00:52,  7.31it/s]Measuring inference for batch_size=1:  62%|██████▏   | 618/1000 [01:24<00:52,  7.30it/s]Measuring inference for batch_size=1:  62%|██████▏   | 619/1000 [01:24<00:52,  7.31it/s]Measuring inference for batch_size=1:  62%|██████▏   | 620/1000 [01:24<00:52,  7.31it/s]Measuring inference for batch_size=1:  62%|██████▏   | 621/1000 [01:24<00:51,  7.31it/s]Measuring inference for batch_size=1:  62%|██████▏   | 622/1000 [01:25<00:51,  7.31it/s]Measuring inference for batch_size=1:  62%|██████▏   | 623/1000 [01:25<00:51,  7.31it/s]Measuring inference for batch_size=1:  62%|██████▏   | 624/1000 [01:25<00:51,  7.31it/s]Measuring inference for batch_size=1:  62%|██████▎   | 625/1000 [01:25<00:51,  7.31it/s]Measuring inference for batch_size=1:  63%|██████▎   | 626/1000 [01:25<00:51,  7.31it/s]Measuring inference for batch_size=1:  63%|██████▎   | 627/1000 [01:25<00:51,  7.31it/s]Measuring inference for batch_size=1:  63%|██████▎   | 628/1000 [01:25<00:50,  7.31it/s]Measuring inference for batch_size=1:  63%|██████▎   | 629/1000 [01:26<00:50,  7.31it/s]Measuring inference for batch_size=1:  63%|██████▎   | 630/1000 [01:26<00:50,  7.29it/s]Measuring inference for batch_size=1:  63%|██████▎   | 631/1000 [01:26<00:50,  7.29it/s]Measuring inference for batch_size=1:  63%|██████▎   | 632/1000 [01:26<00:50,  7.30it/s]Measuring inference for batch_size=1:  63%|██████▎   | 633/1000 [01:26<00:50,  7.31it/s]Measuring inference for batch_size=1:  63%|██████▎   | 634/1000 [01:26<00:50,  7.31it/s]Measuring inference for batch_size=1:  64%|██████▎   | 635/1000 [01:26<00:49,  7.31it/s]Measuring inference for batch_size=1:  64%|██████▎   | 636/1000 [01:27<00:49,  7.31it/s]Measuring inference for batch_size=1:  64%|██████▎   | 637/1000 [01:27<00:49,  7.31it/s]Measuring inference for batch_size=1:  64%|██████▍   | 638/1000 [01:27<00:49,  7.32it/s]Measuring inference for batch_size=1:  64%|██████▍   | 639/1000 [01:27<00:49,  7.32it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [01:27<00:49,  7.32it/s]Measuring inference for batch_size=1:  64%|██████▍   | 641/1000 [01:27<00:49,  7.32it/s]Measuring inference for batch_size=1:  64%|██████▍   | 642/1000 [01:27<00:48,  7.32it/s]Measuring inference for batch_size=1:  64%|██████▍   | 643/1000 [01:27<00:48,  7.32it/s]Measuring inference for batch_size=1:  64%|██████▍   | 644/1000 [01:28<00:48,  7.32it/s]Measuring inference for batch_size=1:  64%|██████▍   | 645/1000 [01:28<00:48,  7.32it/s]Measuring inference for batch_size=1:  65%|██████▍   | 646/1000 [01:28<00:48,  7.31it/s]Measuring inference for batch_size=1:  65%|██████▍   | 647/1000 [01:28<00:48,  7.29it/s]Measuring inference for batch_size=1:  65%|██████▍   | 648/1000 [01:28<00:48,  7.30it/s]Measuring inference for batch_size=1:  65%|██████▍   | 649/1000 [01:28<00:48,  7.30it/s]Measuring inference for batch_size=1:  65%|██████▌   | 650/1000 [01:28<00:47,  7.31it/s]Measuring inference for batch_size=1:  65%|██████▌   | 651/1000 [01:29<00:47,  7.31it/s]Measuring inference for batch_size=1:  65%|██████▌   | 652/1000 [01:29<00:47,  7.31it/s]Measuring inference for batch_size=1:  65%|██████▌   | 653/1000 [01:29<00:47,  7.31it/s]Measuring inference for batch_size=1:  65%|██████▌   | 654/1000 [01:29<00:47,  7.31it/s]Measuring inference for batch_size=1:  66%|██████▌   | 655/1000 [01:29<00:47,  7.30it/s]Measuring inference for batch_size=1:  66%|██████▌   | 656/1000 [01:29<00:47,  7.31it/s]Measuring inference for batch_size=1:  66%|██████▌   | 657/1000 [01:29<00:46,  7.31it/s]Measuring inference for batch_size=1:  66%|██████▌   | 658/1000 [01:30<00:46,  7.31it/s]Measuring inference for batch_size=1:  66%|██████▌   | 659/1000 [01:30<00:46,  7.31it/s]Measuring inference for batch_size=1:  66%|██████▌   | 660/1000 [01:30<00:46,  7.31it/s]Measuring inference for batch_size=1:  66%|██████▌   | 661/1000 [01:30<00:46,  7.31it/s]Measuring inference for batch_size=1:  66%|██████▌   | 662/1000 [01:30<00:46,  7.31it/s]Measuring inference for batch_size=1:  66%|██████▋   | 663/1000 [01:30<00:46,  7.31it/s]Measuring inference for batch_size=1:  66%|██████▋   | 664/1000 [01:30<00:46,  7.30it/s]Measuring inference for batch_size=1:  66%|██████▋   | 665/1000 [01:31<00:45,  7.31it/s]Measuring inference for batch_size=1:  67%|██████▋   | 666/1000 [01:31<00:45,  7.31it/s]Measuring inference for batch_size=1:  67%|██████▋   | 667/1000 [01:31<00:45,  7.31it/s]Measuring inference for batch_size=1:  67%|██████▋   | 668/1000 [01:31<00:45,  7.32it/s]Measuring inference for batch_size=1:  67%|██████▋   | 669/1000 [01:31<00:45,  7.32it/s]Measuring inference for batch_size=1:  67%|██████▋   | 670/1000 [01:31<00:45,  7.32it/s]Measuring inference for batch_size=1:  67%|██████▋   | 671/1000 [01:31<00:44,  7.31it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [01:31<00:44,  7.31it/s]Measuring inference for batch_size=1:  67%|██████▋   | 673/1000 [01:32<00:44,  7.31it/s]Measuring inference for batch_size=1:  67%|██████▋   | 674/1000 [01:32<00:44,  7.31it/s]Measuring inference for batch_size=1:  68%|██████▊   | 675/1000 [01:32<00:44,  7.31it/s]Measuring inference for batch_size=1:  68%|██████▊   | 676/1000 [01:32<00:44,  7.31it/s]Measuring inference for batch_size=1:  68%|██████▊   | 677/1000 [01:32<00:44,  7.31it/s]Measuring inference for batch_size=1:  68%|██████▊   | 678/1000 [01:32<00:44,  7.31it/s]Measuring inference for batch_size=1:  68%|██████▊   | 679/1000 [01:32<00:43,  7.32it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [01:33<00:43,  7.31it/s]Measuring inference for batch_size=1:  68%|██████▊   | 681/1000 [01:33<00:43,  7.31it/s]Measuring inference for batch_size=1:  68%|██████▊   | 682/1000 [01:33<00:43,  7.31it/s]Measuring inference for batch_size=1:  68%|██████▊   | 683/1000 [01:33<00:43,  7.31it/s]Measuring inference for batch_size=1:  68%|██████▊   | 684/1000 [01:33<00:43,  7.31it/s]Measuring inference for batch_size=1:  68%|██████▊   | 685/1000 [01:33<00:43,  7.31it/s]Measuring inference for batch_size=1:  69%|██████▊   | 686/1000 [01:33<00:42,  7.31it/s]Measuring inference for batch_size=1:  69%|██████▊   | 687/1000 [01:34<00:42,  7.31it/s]Measuring inference for batch_size=1:  69%|██████▉   | 688/1000 [01:34<00:42,  7.31it/s]Measuring inference for batch_size=1:  69%|██████▉   | 689/1000 [01:34<00:42,  7.31it/s]Measuring inference for batch_size=1:  69%|██████▉   | 690/1000 [01:34<00:42,  7.30it/s]Measuring inference for batch_size=1:  69%|██████▉   | 691/1000 [01:34<00:42,  7.31it/s]Measuring inference for batch_size=1:  69%|██████▉   | 692/1000 [01:34<00:42,  7.30it/s]Measuring inference for batch_size=1:  69%|██████▉   | 693/1000 [01:34<00:42,  7.30it/s]Measuring inference for batch_size=1:  69%|██████▉   | 694/1000 [01:34<00:41,  7.30it/s]Measuring inference for batch_size=1:  70%|██████▉   | 695/1000 [01:35<00:41,  7.30it/s]Measuring inference for batch_size=1:  70%|██████▉   | 696/1000 [01:35<00:41,  7.31it/s]Measuring inference for batch_size=1:  70%|██████▉   | 697/1000 [01:35<00:41,  7.31it/s]Measuring inference for batch_size=1:  70%|██████▉   | 698/1000 [01:35<00:41,  7.31it/s]Measuring inference for batch_size=1:  70%|██████▉   | 699/1000 [01:35<00:41,  7.31it/s]Measuring inference for batch_size=1:  70%|███████   | 700/1000 [01:35<00:41,  7.31it/s]Measuring inference for batch_size=1:  70%|███████   | 701/1000 [01:35<00:40,  7.31it/s]Measuring inference for batch_size=1:  70%|███████   | 702/1000 [01:36<00:40,  7.31it/s]Measuring inference for batch_size=1:  70%|███████   | 703/1000 [01:36<00:40,  7.32it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [01:36<00:40,  7.31it/s]Measuring inference for batch_size=1:  70%|███████   | 705/1000 [01:36<00:40,  7.31it/s]Measuring inference for batch_size=1:  71%|███████   | 706/1000 [01:36<00:40,  7.31it/s]Measuring inference for batch_size=1:  71%|███████   | 707/1000 [01:36<00:40,  7.31it/s]Measuring inference for batch_size=1:  71%|███████   | 708/1000 [01:36<00:39,  7.31it/s]Measuring inference for batch_size=1:  71%|███████   | 709/1000 [01:37<00:39,  7.31it/s]Measuring inference for batch_size=1:  71%|███████   | 710/1000 [01:37<00:39,  7.32it/s]Measuring inference for batch_size=1:  71%|███████   | 711/1000 [01:37<00:39,  7.32it/s]Measuring inference for batch_size=1:  71%|███████   | 712/1000 [01:37<00:39,  7.32it/s]Measuring inference for batch_size=1:  71%|███████▏  | 713/1000 [01:37<00:39,  7.32it/s]Measuring inference for batch_size=1:  71%|███████▏  | 714/1000 [01:37<00:39,  7.31it/s]Measuring inference for batch_size=1:  72%|███████▏  | 715/1000 [01:37<00:38,  7.31it/s]Measuring inference for batch_size=1:  72%|███████▏  | 716/1000 [01:37<00:38,  7.31it/s]Measuring inference for batch_size=1:  72%|███████▏  | 717/1000 [01:38<00:38,  7.32it/s]Measuring inference for batch_size=1:  72%|███████▏  | 718/1000 [01:38<00:38,  7.32it/s]Measuring inference for batch_size=1:  72%|███████▏  | 719/1000 [01:38<00:38,  7.32it/s]Measuring inference for batch_size=1:  72%|███████▏  | 720/1000 [01:38<00:38,  7.32it/s]Measuring inference for batch_size=1:  72%|███████▏  | 721/1000 [01:38<00:38,  7.32it/s]Measuring inference for batch_size=1:  72%|███████▏  | 722/1000 [01:38<00:37,  7.32it/s]Measuring inference for batch_size=1:  72%|███████▏  | 723/1000 [01:38<00:37,  7.32it/s]Measuring inference for batch_size=1:  72%|███████▏  | 724/1000 [01:39<00:37,  7.31it/s]Measuring inference for batch_size=1:  72%|███████▎  | 725/1000 [01:39<00:37,  7.31it/s]Measuring inference for batch_size=1:  73%|███████▎  | 726/1000 [01:39<00:37,  7.31it/s]Measuring inference for batch_size=1:  73%|███████▎  | 727/1000 [01:39<00:37,  7.31it/s]Measuring inference for batch_size=1:  73%|███████▎  | 728/1000 [01:39<00:37,  7.32it/s]Measuring inference for batch_size=1:  73%|███████▎  | 729/1000 [01:39<00:37,  7.31it/s]Measuring inference for batch_size=1:  73%|███████▎  | 730/1000 [01:39<00:36,  7.31it/s]Measuring inference for batch_size=1:  73%|███████▎  | 731/1000 [01:40<00:36,  7.31it/s]Measuring inference for batch_size=1:  73%|███████▎  | 732/1000 [01:40<00:36,  7.32it/s]Measuring inference for batch_size=1:  73%|███████▎  | 733/1000 [01:40<00:36,  7.31it/s]Measuring inference for batch_size=1:  73%|███████▎  | 734/1000 [01:40<00:36,  7.32it/s]Measuring inference for batch_size=1:  74%|███████▎  | 735/1000 [01:40<00:36,  7.32it/s]Measuring inference for batch_size=1:  74%|███████▎  | 736/1000 [01:40<00:36,  7.32it/s]Measuring inference for batch_size=1:  74%|███████▎  | 737/1000 [01:40<00:35,  7.31it/s]Measuring inference for batch_size=1:  74%|███████▍  | 738/1000 [01:40<00:35,  7.32it/s]Measuring inference for batch_size=1:  74%|███████▍  | 739/1000 [01:41<00:35,  7.32it/s]Measuring inference for batch_size=1:  74%|███████▍  | 740/1000 [01:41<00:35,  7.32it/s]Measuring inference for batch_size=1:  74%|███████▍  | 741/1000 [01:41<00:35,  7.32it/s]Measuring inference for batch_size=1:  74%|███████▍  | 742/1000 [01:41<00:35,  7.32it/s]Measuring inference for batch_size=1:  74%|███████▍  | 743/1000 [01:41<00:35,  7.32it/s]Measuring inference for batch_size=1:  74%|███████▍  | 744/1000 [01:41<00:34,  7.32it/s]Measuring inference for batch_size=1:  74%|███████▍  | 745/1000 [01:41<00:34,  7.32it/s]Measuring inference for batch_size=1:  75%|███████▍  | 746/1000 [01:42<00:34,  7.32it/s]Measuring inference for batch_size=1:  75%|███████▍  | 747/1000 [01:42<00:34,  7.32it/s]Measuring inference for batch_size=1:  75%|███████▍  | 748/1000 [01:42<00:34,  7.32it/s]Measuring inference for batch_size=1:  75%|███████▍  | 749/1000 [01:42<00:34,  7.32it/s]Measuring inference for batch_size=1:  75%|███████▌  | 750/1000 [01:42<00:34,  7.32it/s]Measuring inference for batch_size=1:  75%|███████▌  | 751/1000 [01:42<00:34,  7.31it/s]Measuring inference for batch_size=1:  75%|███████▌  | 752/1000 [01:42<00:33,  7.31it/s]Measuring inference for batch_size=1:  75%|███████▌  | 753/1000 [01:43<00:33,  7.32it/s]Measuring inference for batch_size=1:  75%|███████▌  | 754/1000 [01:43<00:33,  7.32it/s]Measuring inference for batch_size=1:  76%|███████▌  | 755/1000 [01:43<00:33,  7.31it/s]Measuring inference for batch_size=1:  76%|███████▌  | 756/1000 [01:43<00:33,  7.31it/s]Measuring inference for batch_size=1:  76%|███████▌  | 757/1000 [01:43<00:33,  7.31it/s]Measuring inference for batch_size=1:  76%|███████▌  | 758/1000 [01:43<00:33,  7.32it/s]Measuring inference for batch_size=1:  76%|███████▌  | 759/1000 [01:43<00:32,  7.32it/s]Measuring inference for batch_size=1:  76%|███████▌  | 760/1000 [01:43<00:32,  7.32it/s]Measuring inference for batch_size=1:  76%|███████▌  | 761/1000 [01:44<00:32,  7.31it/s]Measuring inference for batch_size=1:  76%|███████▌  | 762/1000 [01:44<00:32,  7.31it/s]Measuring inference for batch_size=1:  76%|███████▋  | 763/1000 [01:44<00:32,  7.31it/s]Measuring inference for batch_size=1:  76%|███████▋  | 764/1000 [01:44<00:32,  7.32it/s]Measuring inference for batch_size=1:  76%|███████▋  | 765/1000 [01:44<00:32,  7.31it/s]Measuring inference for batch_size=1:  77%|███████▋  | 766/1000 [01:44<00:32,  7.31it/s]Measuring inference for batch_size=1:  77%|███████▋  | 767/1000 [01:44<00:31,  7.31it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [01:45<00:31,  7.31it/s]Measuring inference for batch_size=1:  77%|███████▋  | 769/1000 [01:45<00:31,  7.31it/s]Measuring inference for batch_size=1:  77%|███████▋  | 770/1000 [01:45<00:31,  7.31it/s]Measuring inference for batch_size=1:  77%|███████▋  | 771/1000 [01:45<00:31,  7.31it/s]Measuring inference for batch_size=1:  77%|███████▋  | 772/1000 [01:45<00:31,  7.31it/s]Measuring inference for batch_size=1:  77%|███████▋  | 773/1000 [01:45<00:31,  7.31it/s]Measuring inference for batch_size=1:  77%|███████▋  | 774/1000 [01:45<00:30,  7.32it/s]Measuring inference for batch_size=1:  78%|███████▊  | 775/1000 [01:46<00:30,  7.30it/s]Measuring inference for batch_size=1:  78%|███████▊  | 776/1000 [01:46<00:30,  7.30it/s]Measuring inference for batch_size=1:  78%|███████▊  | 777/1000 [01:46<00:30,  7.30it/s]Measuring inference for batch_size=1:  78%|███████▊  | 778/1000 [01:46<00:30,  7.31it/s]Measuring inference for batch_size=1:  78%|███████▊  | 779/1000 [01:46<00:30,  7.31it/s]Measuring inference for batch_size=1:  78%|███████▊  | 780/1000 [01:46<00:30,  7.31it/s]Measuring inference for batch_size=1:  78%|███████▊  | 781/1000 [01:46<00:29,  7.31it/s]Measuring inference for batch_size=1:  78%|███████▊  | 782/1000 [01:47<00:29,  7.31it/s]Measuring inference for batch_size=1:  78%|███████▊  | 783/1000 [01:47<00:29,  7.31it/s]Measuring inference for batch_size=1:  78%|███████▊  | 784/1000 [01:47<00:29,  7.31it/s]Measuring inference for batch_size=1:  78%|███████▊  | 785/1000 [01:47<00:29,  7.32it/s]Measuring inference for batch_size=1:  79%|███████▊  | 786/1000 [01:47<00:29,  7.31it/s]Measuring inference for batch_size=1:  79%|███████▊  | 787/1000 [01:47<00:29,  7.31it/s]Measuring inference for batch_size=1:  79%|███████▉  | 788/1000 [01:47<00:28,  7.31it/s]Measuring inference for batch_size=1:  79%|███████▉  | 789/1000 [01:47<00:28,  7.31it/s]Measuring inference for batch_size=1:  79%|███████▉  | 790/1000 [01:48<00:28,  7.31it/s]Measuring inference for batch_size=1:  79%|███████▉  | 791/1000 [01:48<00:28,  7.31it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [01:48<00:28,  7.30it/s]Measuring inference for batch_size=1:  79%|███████▉  | 793/1000 [01:48<00:28,  7.31it/s]Measuring inference for batch_size=1:  79%|███████▉  | 794/1000 [01:48<00:28,  7.30it/s]Measuring inference for batch_size=1:  80%|███████▉  | 795/1000 [01:48<00:28,  7.30it/s]Measuring inference for batch_size=1:  80%|███████▉  | 796/1000 [01:48<00:27,  7.31it/s]Measuring inference for batch_size=1:  80%|███████▉  | 797/1000 [01:49<00:27,  7.31it/s]Measuring inference for batch_size=1:  80%|███████▉  | 798/1000 [01:49<00:27,  7.31it/s]Measuring inference for batch_size=1:  80%|███████▉  | 799/1000 [01:49<00:27,  7.32it/s]Measuring inference for batch_size=1:  80%|████████  | 800/1000 [01:49<00:27,  7.31it/s]Measuring inference for batch_size=1:  80%|████████  | 801/1000 [01:49<00:27,  7.31it/s]Measuring inference for batch_size=1:  80%|████████  | 802/1000 [01:49<00:27,  7.31it/s]Measuring inference for batch_size=1:  80%|████████  | 803/1000 [01:49<00:26,  7.31it/s]Measuring inference for batch_size=1:  80%|████████  | 804/1000 [01:50<00:26,  7.31it/s]Measuring inference for batch_size=1:  80%|████████  | 805/1000 [01:50<00:26,  7.31it/s]Measuring inference for batch_size=1:  81%|████████  | 806/1000 [01:50<00:26,  7.30it/s]Measuring inference for batch_size=1:  81%|████████  | 807/1000 [01:50<00:26,  7.30it/s]Measuring inference for batch_size=1:  81%|████████  | 808/1000 [01:50<00:26,  7.30it/s]Measuring inference for batch_size=1:  81%|████████  | 809/1000 [01:50<00:26,  7.30it/s]Measuring inference for batch_size=1:  81%|████████  | 810/1000 [01:50<00:26,  7.30it/s]Measuring inference for batch_size=1:  81%|████████  | 811/1000 [01:50<00:25,  7.31it/s]Measuring inference for batch_size=1:  81%|████████  | 812/1000 [01:51<00:25,  7.31it/s]Measuring inference for batch_size=1:  81%|████████▏ | 813/1000 [01:51<00:25,  7.30it/s]Measuring inference for batch_size=1:  81%|████████▏ | 814/1000 [01:51<00:25,  7.30it/s]Measuring inference for batch_size=1:  82%|████████▏ | 815/1000 [01:51<00:25,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▏ | 816/1000 [01:51<00:25,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▏ | 817/1000 [01:51<00:25,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▏ | 818/1000 [01:51<00:24,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▏ | 819/1000 [01:52<00:24,  7.30it/s]Measuring inference for batch_size=1:  82%|████████▏ | 820/1000 [01:52<00:24,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▏ | 821/1000 [01:52<00:24,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▏ | 822/1000 [01:52<00:24,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▏ | 823/1000 [01:52<00:24,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▏ | 824/1000 [01:52<00:24,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▎ | 825/1000 [01:52<00:23,  7.31it/s]Measuring inference for batch_size=1:  83%|████████▎ | 826/1000 [01:53<00:23,  7.31it/s]Measuring inference for batch_size=1:  83%|████████▎ | 827/1000 [01:53<00:23,  7.31it/s]Measuring inference for batch_size=1:  83%|████████▎ | 828/1000 [01:53<00:23,  7.31it/s]Measuring inference for batch_size=1:  83%|████████▎ | 829/1000 [01:53<00:23,  7.31it/s]Measuring inference for batch_size=1:  83%|████████▎ | 830/1000 [01:53<00:23,  7.31it/s]Measuring inference for batch_size=1:  83%|████████▎ | 831/1000 [01:53<00:23,  7.31it/s]Measuring inference for batch_size=1:  83%|████████▎ | 832/1000 [01:53<00:22,  7.31it/s]Measuring inference for batch_size=1:  83%|████████▎ | 833/1000 [01:53<00:22,  7.31it/s]Measuring inference for batch_size=1:  83%|████████▎ | 834/1000 [01:54<00:22,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▎ | 835/1000 [01:54<00:22,  7.31it/s]Measuring inference for batch_size=1:  84%|████████▎ | 836/1000 [01:54<00:22,  7.31it/s]Measuring inference for batch_size=1:  84%|████████▎ | 837/1000 [01:54<00:22,  7.31it/s]Measuring inference for batch_size=1:  84%|████████▍ | 838/1000 [01:54<00:22,  7.31it/s]Measuring inference for batch_size=1:  84%|████████▍ | 839/1000 [01:54<00:22,  7.31it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [01:54<00:21,  7.31it/s]Measuring inference for batch_size=1:  84%|████████▍ | 841/1000 [01:55<00:21,  7.31it/s]Measuring inference for batch_size=1:  84%|████████▍ | 842/1000 [01:55<00:21,  7.31it/s]Measuring inference for batch_size=1:  84%|████████▍ | 843/1000 [01:55<00:21,  7.31it/s]Measuring inference for batch_size=1:  84%|████████▍ | 844/1000 [01:55<00:21,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▍ | 845/1000 [01:55<00:21,  7.30it/s]Measuring inference for batch_size=1:  85%|████████▍ | 846/1000 [01:55<00:21,  7.30it/s]Measuring inference for batch_size=1:  85%|████████▍ | 847/1000 [01:55<00:20,  7.31it/s]Measuring inference for batch_size=1:  85%|████████▍ | 848/1000 [01:56<00:20,  7.31it/s]Measuring inference for batch_size=1:  85%|████████▍ | 849/1000 [01:56<00:20,  7.30it/s]Measuring inference for batch_size=1:  85%|████████▌ | 850/1000 [01:56<00:20,  7.30it/s]Measuring inference for batch_size=1:  85%|████████▌ | 851/1000 [01:56<00:20,  7.30it/s]Measuring inference for batch_size=1:  85%|████████▌ | 852/1000 [01:56<00:20,  7.31it/s]Measuring inference for batch_size=1:  85%|████████▌ | 853/1000 [01:56<00:20,  7.31it/s]Measuring inference for batch_size=1:  85%|████████▌ | 854/1000 [01:56<00:19,  7.31it/s]Measuring inference for batch_size=1:  86%|████████▌ | 855/1000 [01:56<00:19,  7.31it/s]Measuring inference for batch_size=1:  86%|████████▌ | 856/1000 [01:57<00:19,  7.31it/s]Measuring inference for batch_size=1:  86%|████████▌ | 857/1000 [01:57<00:19,  7.31it/s]Measuring inference for batch_size=1:  86%|████████▌ | 858/1000 [01:57<00:19,  7.31it/s]Measuring inference for batch_size=1:  86%|████████▌ | 859/1000 [01:57<00:19,  7.32it/s]Measuring inference for batch_size=1:  86%|████████▌ | 860/1000 [01:57<00:19,  7.32it/s]Measuring inference for batch_size=1:  86%|████████▌ | 861/1000 [01:57<00:19,  7.31it/s]Measuring inference for batch_size=1:  86%|████████▌ | 862/1000 [01:57<00:18,  7.31it/s]Measuring inference for batch_size=1:  86%|████████▋ | 863/1000 [01:58<00:18,  7.31it/s]Measuring inference for batch_size=1:  86%|████████▋ | 864/1000 [01:58<00:18,  7.32it/s]Measuring inference for batch_size=1:  86%|████████▋ | 865/1000 [01:58<00:18,  7.32it/s]Measuring inference for batch_size=1:  87%|████████▋ | 866/1000 [01:58<00:18,  7.32it/s]Measuring inference for batch_size=1:  87%|████████▋ | 867/1000 [01:58<00:18,  7.32it/s]Measuring inference for batch_size=1:  87%|████████▋ | 868/1000 [01:58<00:18,  7.32it/s]Measuring inference for batch_size=1:  87%|████████▋ | 869/1000 [01:58<00:17,  7.32it/s]Measuring inference for batch_size=1:  87%|████████▋ | 870/1000 [01:59<00:17,  7.32it/s]Measuring inference for batch_size=1:  87%|████████▋ | 871/1000 [01:59<00:17,  7.31it/s]Measuring inference for batch_size=1:  87%|████████▋ | 872/1000 [01:59<00:17,  7.31it/s]Measuring inference for batch_size=1:  87%|████████▋ | 873/1000 [01:59<00:17,  7.31it/s]Measuring inference for batch_size=1:  87%|████████▋ | 874/1000 [01:59<00:17,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 875/1000 [01:59<00:17,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 876/1000 [01:59<00:16,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 877/1000 [02:00<00:16,  7.32it/s]Measuring inference for batch_size=1:  88%|████████▊ | 878/1000 [02:00<00:16,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 879/1000 [02:00<00:16,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [02:00<00:16,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 881/1000 [02:00<00:16,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 882/1000 [02:00<00:16,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 883/1000 [02:00<00:16,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 884/1000 [02:00<00:15,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 885/1000 [02:01<00:15,  7.31it/s]Measuring inference for batch_size=1:  89%|████████▊ | 886/1000 [02:01<00:15,  7.30it/s]Measuring inference for batch_size=1:  89%|████████▊ | 887/1000 [02:01<00:15,  7.30it/s]Measuring inference for batch_size=1:  89%|████████▉ | 888/1000 [02:01<00:15,  7.30it/s]Measuring inference for batch_size=1:  89%|████████▉ | 889/1000 [02:01<00:15,  7.30it/s]Measuring inference for batch_size=1:  89%|████████▉ | 890/1000 [02:01<00:15,  7.30it/s]Measuring inference for batch_size=1:  89%|████████▉ | 891/1000 [02:01<00:14,  7.30it/s]Measuring inference for batch_size=1:  89%|████████▉ | 892/1000 [02:02<00:14,  7.31it/s]Measuring inference for batch_size=1:  89%|████████▉ | 893/1000 [02:02<00:14,  7.31it/s]Measuring inference for batch_size=1:  89%|████████▉ | 894/1000 [02:02<00:14,  7.31it/s]Measuring inference for batch_size=1:  90%|████████▉ | 895/1000 [02:02<00:14,  7.31it/s]Measuring inference for batch_size=1:  90%|████████▉ | 896/1000 [02:02<00:14,  7.31it/s]Measuring inference for batch_size=1:  90%|████████▉ | 897/1000 [02:02<00:14,  7.31it/s]Measuring inference for batch_size=1:  90%|████████▉ | 898/1000 [02:02<00:13,  7.31it/s]Measuring inference for batch_size=1:  90%|████████▉ | 899/1000 [02:03<00:13,  7.31it/s]Measuring inference for batch_size=1:  90%|█████████ | 900/1000 [02:03<00:13,  7.31it/s]Measuring inference for batch_size=1:  90%|█████████ | 901/1000 [02:03<00:13,  7.30it/s]Measuring inference for batch_size=1:  90%|█████████ | 902/1000 [02:03<00:13,  7.30it/s]Measuring inference for batch_size=1:  90%|█████████ | 903/1000 [02:03<00:13,  7.30it/s]Measuring inference for batch_size=1:  90%|█████████ | 904/1000 [02:03<00:13,  7.30it/s]Measuring inference for batch_size=1:  90%|█████████ | 905/1000 [02:03<00:13,  7.31it/s]Measuring inference for batch_size=1:  91%|█████████ | 906/1000 [02:03<00:12,  7.30it/s]Measuring inference for batch_size=1:  91%|█████████ | 907/1000 [02:04<00:12,  7.30it/s]Measuring inference for batch_size=1:  91%|█████████ | 908/1000 [02:04<00:12,  7.30it/s]Measuring inference for batch_size=1:  91%|█████████ | 909/1000 [02:04<00:12,  7.30it/s]Measuring inference for batch_size=1:  91%|█████████ | 910/1000 [02:04<00:12,  7.30it/s]Measuring inference for batch_size=1:  91%|█████████ | 911/1000 [02:04<00:12,  7.30it/s]Measuring inference for batch_size=1:  91%|█████████ | 912/1000 [02:04<00:12,  7.31it/s]Measuring inference for batch_size=1:  91%|█████████▏| 913/1000 [02:04<00:11,  7.31it/s]Measuring inference for batch_size=1:  91%|█████████▏| 914/1000 [02:05<00:11,  7.31it/s]Measuring inference for batch_size=1:  92%|█████████▏| 915/1000 [02:05<00:11,  7.31it/s]Measuring inference for batch_size=1:  92%|█████████▏| 916/1000 [02:05<00:11,  7.31it/s]Measuring inference for batch_size=1:  92%|█████████▏| 917/1000 [02:05<00:11,  7.30it/s]Measuring inference for batch_size=1:  92%|█████████▏| 918/1000 [02:05<00:11,  7.30it/s]Measuring inference for batch_size=1:  92%|█████████▏| 919/1000 [02:05<00:11,  7.30it/s]Measuring inference for batch_size=1:  92%|█████████▏| 920/1000 [02:05<00:10,  7.30it/s]Measuring inference for batch_size=1:  92%|█████████▏| 921/1000 [02:06<00:10,  7.30it/s]Measuring inference for batch_size=1:  92%|█████████▏| 922/1000 [02:06<00:10,  7.30it/s]Measuring inference for batch_size=1:  92%|█████████▏| 923/1000 [02:06<00:10,  7.31it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [02:06<00:10,  7.31it/s]Measuring inference for batch_size=1:  92%|█████████▎| 925/1000 [02:06<00:10,  7.31it/s]Measuring inference for batch_size=1:  93%|█████████▎| 926/1000 [02:06<00:10,  7.31it/s]Measuring inference for batch_size=1:  93%|█████████▎| 927/1000 [02:06<00:09,  7.31it/s]Measuring inference for batch_size=1:  93%|█████████▎| 928/1000 [02:06<00:09,  7.31it/s]Measuring inference for batch_size=1:  93%|█████████▎| 929/1000 [02:07<00:09,  7.31it/s]Measuring inference for batch_size=1:  93%|█████████▎| 930/1000 [02:07<00:09,  7.31it/s]Measuring inference for batch_size=1:  93%|█████████▎| 931/1000 [02:07<00:09,  7.31it/s]Measuring inference for batch_size=1:  93%|█████████▎| 932/1000 [02:07<00:09,  7.31it/s]Measuring inference for batch_size=1:  93%|█████████▎| 933/1000 [02:07<00:09,  7.31it/s]Measuring inference for batch_size=1:  93%|█████████▎| 934/1000 [02:07<00:09,  7.31it/s]Measuring inference for batch_size=1:  94%|█████████▎| 935/1000 [02:07<00:08,  7.30it/s]Measuring inference for batch_size=1:  94%|█████████▎| 936/1000 [02:08<00:08,  7.30it/s]Measuring inference for batch_size=1:  94%|█████████▎| 937/1000 [02:08<00:08,  7.30it/s]Measuring inference for batch_size=1:  94%|█████████▍| 938/1000 [02:08<00:08,  7.31it/s]Measuring inference for batch_size=1:  94%|█████████▍| 939/1000 [02:08<00:08,  7.30it/s]Measuring inference for batch_size=1:  94%|█████████▍| 940/1000 [02:08<00:08,  7.31it/s]Measuring inference for batch_size=1:  94%|█████████▍| 941/1000 [02:08<00:08,  7.30it/s]Measuring inference for batch_size=1:  94%|█████████▍| 942/1000 [02:08<00:07,  7.31it/s]Measuring inference for batch_size=1:  94%|█████████▍| 943/1000 [02:09<00:07,  7.31it/s]Measuring inference for batch_size=1:  94%|█████████▍| 944/1000 [02:09<00:07,  7.31it/s]Measuring inference for batch_size=1:  94%|█████████▍| 945/1000 [02:09<00:07,  7.30it/s]Measuring inference for batch_size=1:  95%|█████████▍| 946/1000 [02:09<00:07,  7.30it/s]Measuring inference for batch_size=1:  95%|█████████▍| 947/1000 [02:09<00:07,  7.31it/s]Measuring inference for batch_size=1:  95%|█████████▍| 948/1000 [02:09<00:07,  7.30it/s]Measuring inference for batch_size=1:  95%|█████████▍| 949/1000 [02:09<00:06,  7.30it/s]Measuring inference for batch_size=1:  95%|█████████▌| 950/1000 [02:09<00:06,  7.31it/s]Measuring inference for batch_size=1:  95%|█████████▌| 951/1000 [02:10<00:06,  7.31it/s]Measuring inference for batch_size=1:  95%|█████████▌| 952/1000 [02:10<00:06,  7.31it/s]Measuring inference for batch_size=1:  95%|█████████▌| 953/1000 [02:10<00:06,  7.31it/s]Measuring inference for batch_size=1:  95%|█████████▌| 954/1000 [02:10<00:06,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▌| 955/1000 [02:10<00:06,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▌| 956/1000 [02:10<00:06,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▌| 957/1000 [02:10<00:05,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▌| 958/1000 [02:11<00:05,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▌| 959/1000 [02:11<00:05,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▌| 960/1000 [02:11<00:05,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▌| 961/1000 [02:11<00:05,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▌| 962/1000 [02:11<00:05,  7.31it/s]Measuring inference for batch_size=1:  96%|█████████▋| 963/1000 [02:11<00:05,  7.31it/s]Measuring inference for batch_size=1:  96%|█████████▋| 964/1000 [02:11<00:04,  7.31it/s]Measuring inference for batch_size=1:  96%|█████████▋| 965/1000 [02:12<00:04,  7.30it/s]Measuring inference for batch_size=1:  97%|█████████▋| 966/1000 [02:12<00:04,  7.30it/s]Measuring inference for batch_size=1:  97%|█████████▋| 967/1000 [02:12<00:04,  7.30it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [02:12<00:04,  7.30it/s]Measuring inference for batch_size=1:  97%|█████████▋| 969/1000 [02:12<00:04,  7.31it/s]Measuring inference for batch_size=1:  97%|█████████▋| 970/1000 [02:12<00:04,  7.29it/s]Measuring inference for batch_size=1:  97%|█████████▋| 971/1000 [02:12<00:03,  7.30it/s]Measuring inference for batch_size=1:  97%|█████████▋| 972/1000 [02:13<00:03,  7.30it/s]Measuring inference for batch_size=1:  97%|█████████▋| 973/1000 [02:13<00:03,  7.30it/s]Measuring inference for batch_size=1:  97%|█████████▋| 974/1000 [02:13<00:03,  7.30it/s]Measuring inference for batch_size=1:  98%|█████████▊| 975/1000 [02:13<00:03,  7.30it/s]Measuring inference for batch_size=1:  98%|█████████▊| 976/1000 [02:13<00:03,  7.29it/s]Measuring inference for batch_size=1:  98%|█████████▊| 977/1000 [02:13<00:03,  7.30it/s]Measuring inference for batch_size=1:  98%|█████████▊| 978/1000 [02:13<00:03,  7.30it/s]Measuring inference for batch_size=1:  98%|█████████▊| 979/1000 [02:13<00:02,  7.30it/s]Measuring inference for batch_size=1:  98%|█████████▊| 980/1000 [02:14<00:02,  7.31it/s]Measuring inference for batch_size=1:  98%|█████████▊| 981/1000 [02:14<00:02,  7.30it/s]Measuring inference for batch_size=1:  98%|█████████▊| 982/1000 [02:14<00:02,  7.31it/s]Measuring inference for batch_size=1:  98%|█████████▊| 983/1000 [02:14<00:02,  7.31it/s]Measuring inference for batch_size=1:  98%|█████████▊| 984/1000 [02:14<00:02,  7.31it/s]Measuring inference for batch_size=1:  98%|█████████▊| 985/1000 [02:14<00:02,  7.31it/s]Measuring inference for batch_size=1:  99%|█████████▊| 986/1000 [02:14<00:01,  7.31it/s]Measuring inference for batch_size=1:  99%|█████████▊| 987/1000 [02:15<00:01,  7.30it/s]Measuring inference for batch_size=1:  99%|█████████▉| 988/1000 [02:15<00:01,  7.31it/s]Measuring inference for batch_size=1:  99%|█████████▉| 989/1000 [02:15<00:01,  7.31it/s]Measuring inference for batch_size=1:  99%|█████████▉| 990/1000 [02:15<00:01,  7.31it/s]Measuring inference for batch_size=1:  99%|█████████▉| 991/1000 [02:15<00:01,  7.32it/s]Measuring inference for batch_size=1:  99%|█████████▉| 992/1000 [02:15<00:01,  7.31it/s]Measuring inference for batch_size=1:  99%|█████████▉| 993/1000 [02:15<00:00,  7.31it/s]Measuring inference for batch_size=1:  99%|█████████▉| 994/1000 [02:16<00:00,  7.31it/s]Measuring inference for batch_size=1: 100%|█████████▉| 995/1000 [02:16<00:00,  7.31it/s]Measuring inference for batch_size=1: 100%|█████████▉| 996/1000 [02:16<00:00,  7.31it/s]Measuring inference for batch_size=1: 100%|█████████▉| 997/1000 [02:16<00:00,  7.31it/s]Measuring inference for batch_size=1: 100%|█████████▉| 998/1000 [02:16<00:00,  7.31it/s]Measuring inference for batch_size=1: 100%|█████████▉| 999/1000 [02:16<00:00,  7.31it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [02:16<00:00,  7.30it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [02:16<00:00,  7.31it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Measurement of allocated memory is only available on CUDA devices
Warming up with batch_size=512:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=512:   1%|          | 1/100 [00:00<00:18,  5.41it/s]Warming up with batch_size=512:   2%|▏         | 2/100 [00:00<00:17,  5.52it/s]Warming up with batch_size=512:   3%|▎         | 3/100 [00:00<00:17,  5.57it/s]Warming up with batch_size=512:   4%|▍         | 4/100 [00:00<00:17,  5.59it/s]Warming up with batch_size=512:   5%|▌         | 5/100 [00:00<00:16,  5.60it/s]Warming up with batch_size=512:   6%|▌         | 6/100 [00:01<00:16,  5.60it/s]Warming up with batch_size=512:   7%|▋         | 7/100 [00:01<00:16,  5.60it/s]Warming up with batch_size=512:   8%|▊         | 8/100 [00:01<00:16,  5.61it/s]Warming up with batch_size=512:   9%|▉         | 9/100 [00:01<00:16,  5.60it/s]Warming up with batch_size=512:  10%|█         | 10/100 [00:01<00:16,  5.60it/s]Warming up with batch_size=512:  11%|█         | 11/100 [00:01<00:15,  5.61it/s]Warming up with batch_size=512:  12%|█▏        | 12/100 [00:02<00:15,  5.60it/s]Warming up with batch_size=512:  13%|█▎        | 13/100 [00:02<00:15,  5.60it/s]Warming up with batch_size=512:  14%|█▍        | 14/100 [00:02<00:15,  5.61it/s]Warming up with batch_size=512:  15%|█▌        | 15/100 [00:02<00:15,  5.62it/s]Warming up with batch_size=512:  16%|█▌        | 16/100 [00:02<00:14,  5.62it/s]Warming up with batch_size=512:  17%|█▋        | 17/100 [00:03<00:14,  5.61it/s]Warming up with batch_size=512:  18%|█▊        | 18/100 [00:03<00:14,  5.61it/s]Warming up with batch_size=512:  19%|█▉        | 19/100 [00:03<00:14,  5.61it/s]Warming up with batch_size=512:  20%|██        | 20/100 [00:03<00:14,  5.61it/s]Warming up with batch_size=512:  21%|██        | 21/100 [00:03<00:14,  5.61it/s]Warming up with batch_size=512:  22%|██▏       | 22/100 [00:03<00:13,  5.61it/s]Warming up with batch_size=512:  23%|██▎       | 23/100 [00:04<00:13,  5.61it/s]Warming up with batch_size=512:  24%|██▍       | 24/100 [00:04<00:13,  5.62it/s]Warming up with batch_size=512:  25%|██▌       | 25/100 [00:04<00:13,  5.62it/s]Warming up with batch_size=512:  26%|██▌       | 26/100 [00:04<00:13,  5.61it/s]Warming up with batch_size=512:  27%|██▋       | 27/100 [00:04<00:13,  5.61it/s]Warming up with batch_size=512:  28%|██▊       | 28/100 [00:04<00:12,  5.61it/s]Warming up with batch_size=512:  29%|██▉       | 29/100 [00:05<00:12,  5.61it/s]Warming up with batch_size=512:  30%|███       | 30/100 [00:05<00:12,  5.61it/s]Warming up with batch_size=512:  31%|███       | 31/100 [00:05<00:12,  5.61it/s]Warming up with batch_size=512:  32%|███▏      | 32/100 [00:05<00:12,  5.61it/s]Warming up with batch_size=512:  33%|███▎      | 33/100 [00:05<00:11,  5.61it/s]Warming up with batch_size=512:  34%|███▍      | 34/100 [00:06<00:11,  5.61it/s]Warming up with batch_size=512:  35%|███▌      | 35/100 [00:06<00:11,  5.61it/s]Warming up with batch_size=512:  36%|███▌      | 36/100 [00:06<00:11,  5.62it/s]Warming up with batch_size=512:  37%|███▋      | 37/100 [00:06<00:11,  5.62it/s]Warming up with batch_size=512:  38%|███▊      | 38/100 [00:06<00:11,  5.61it/s]Warming up with batch_size=512:  39%|███▉      | 39/100 [00:06<00:10,  5.61it/s]Warming up with batch_size=512:  40%|████      | 40/100 [00:07<00:10,  5.61it/s]Warming up with batch_size=512:  41%|████      | 41/100 [00:07<00:10,  5.61it/s]Warming up with batch_size=512:  42%|████▏     | 42/100 [00:07<00:10,  5.61it/s]Warming up with batch_size=512:  43%|████▎     | 43/100 [00:07<00:10,  5.62it/s]Warming up with batch_size=512:  44%|████▍     | 44/100 [00:07<00:09,  5.61it/s]Warming up with batch_size=512:  45%|████▌     | 45/100 [00:08<00:09,  5.62it/s]Warming up with batch_size=512:  46%|████▌     | 46/100 [00:08<00:09,  5.62it/s]Warming up with batch_size=512:  47%|████▋     | 47/100 [00:08<00:09,  5.62it/s]Warming up with batch_size=512:  48%|████▊     | 48/100 [00:08<00:09,  5.62it/s]Warming up with batch_size=512:  49%|████▉     | 49/100 [00:08<00:09,  5.61it/s]Warming up with batch_size=512:  50%|█████     | 50/100 [00:08<00:08,  5.61it/s]Warming up with batch_size=512:  51%|█████     | 51/100 [00:09<00:08,  5.61it/s]Warming up with batch_size=512:  52%|█████▏    | 52/100 [00:09<00:08,  5.61it/s]Warming up with batch_size=512:  53%|█████▎    | 53/100 [00:09<00:08,  5.61it/s]Warming up with batch_size=512:  54%|█████▍    | 54/100 [00:09<00:08,  5.61it/s]Warming up with batch_size=512:  55%|█████▌    | 55/100 [00:09<00:08,  5.61it/s]Warming up with batch_size=512:  56%|█████▌    | 56/100 [00:09<00:07,  5.61it/s]Warming up with batch_size=512:  57%|█████▋    | 57/100 [00:10<00:07,  5.61it/s]Warming up with batch_size=512:  58%|█████▊    | 58/100 [00:10<00:07,  5.61it/s]Warming up with batch_size=512:  59%|█████▉    | 59/100 [00:10<00:07,  5.62it/s]Warming up with batch_size=512:  60%|██████    | 60/100 [00:10<00:07,  5.62it/s]Warming up with batch_size=512:  61%|██████    | 61/100 [00:10<00:06,  5.62it/s]Warming up with batch_size=512:  62%|██████▏   | 62/100 [00:11<00:06,  5.62it/s]Warming up with batch_size=512:  63%|██████▎   | 63/100 [00:11<00:06,  5.62it/s]Warming up with batch_size=512:  64%|██████▍   | 64/100 [00:11<00:06,  5.62it/s]Warming up with batch_size=512:  65%|██████▌   | 65/100 [00:11<00:06,  5.61it/s]Warming up with batch_size=512:  66%|██████▌   | 66/100 [00:11<00:06,  5.62it/s]Warming up with batch_size=512:  67%|██████▋   | 67/100 [00:11<00:05,  5.61it/s]Warming up with batch_size=512:  68%|██████▊   | 68/100 [00:12<00:05,  5.61it/s]Warming up with batch_size=512:  69%|██████▉   | 69/100 [00:12<00:05,  5.61it/s]Warming up with batch_size=512:  70%|███████   | 70/100 [00:12<00:05,  5.62it/s]Warming up with batch_size=512:  71%|███████   | 71/100 [00:12<00:05,  5.61it/s]Warming up with batch_size=512:  72%|███████▏  | 72/100 [00:12<00:04,  5.61it/s]Warming up with batch_size=512:  73%|███████▎  | 73/100 [00:13<00:04,  5.62it/s]Warming up with batch_size=512:  74%|███████▍  | 74/100 [00:13<00:04,  5.61it/s]Warming up with batch_size=512:  75%|███████▌  | 75/100 [00:13<00:04,  5.61it/s]Warming up with batch_size=512:  76%|███████▌  | 76/100 [00:13<00:04,  5.60it/s]Warming up with batch_size=512:  77%|███████▋  | 77/100 [00:13<00:04,  5.60it/s]Warming up with batch_size=512:  78%|███████▊  | 78/100 [00:13<00:03,  5.61it/s]Warming up with batch_size=512:  79%|███████▉  | 79/100 [00:14<00:03,  5.61it/s]Warming up with batch_size=512:  80%|████████  | 80/100 [00:14<00:03,  5.61it/s]Warming up with batch_size=512:  81%|████████  | 81/100 [00:14<00:03,  5.61it/s]Warming up with batch_size=512:  82%|████████▏ | 82/100 [00:14<00:03,  5.61it/s]Warming up with batch_size=512:  83%|████████▎ | 83/100 [00:14<00:03,  5.62it/s]Warming up with batch_size=512:  84%|████████▍ | 84/100 [00:14<00:02,  5.61it/s]Warming up with batch_size=512:  85%|████████▌ | 85/100 [00:15<00:02,  5.61it/s]Warming up with batch_size=512:  86%|████████▌ | 86/100 [00:15<00:02,  5.61it/s]Warming up with batch_size=512:  87%|████████▋ | 87/100 [00:15<00:02,  5.61it/s]Warming up with batch_size=512:  88%|████████▊ | 88/100 [00:15<00:02,  5.61it/s]Warming up with batch_size=512:  89%|████████▉ | 89/100 [00:15<00:01,  5.61it/s]Warming up with batch_size=512:  90%|█████████ | 90/100 [00:16<00:01,  5.61it/s]Warming up with batch_size=512:  91%|█████████ | 91/100 [00:16<00:01,  5.61it/s]Warming up with batch_size=512:  92%|█████████▏| 92/100 [00:16<00:01,  5.61it/s]Warming up with batch_size=512:  93%|█████████▎| 93/100 [00:16<00:01,  5.61it/s]Warming up with batch_size=512:  94%|█████████▍| 94/100 [00:16<00:01,  5.62it/s]Warming up with batch_size=512:  95%|█████████▌| 95/100 [00:16<00:00,  5.62it/s]Warming up with batch_size=512:  96%|█████████▌| 96/100 [00:17<00:00,  5.62it/s]Warming up with batch_size=512:  97%|█████████▋| 97/100 [00:17<00:00,  5.62it/s]Warming up with batch_size=512:  98%|█████████▊| 98/100 [00:17<00:00,  5.62it/s]Warming up with batch_size=512:  99%|█████████▉| 99/100 [00:17<00:00,  5.62it/s]Warming up with batch_size=512: 100%|██████████| 100/100 [00:17<00:00,  5.62it/s]Warming up with batch_size=512: 100%|██████████| 100/100 [00:17<00:00,  5.61it/s]
STAGE:2024-02-23 09:43:26 176353:176353 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:43:27 176353:176353 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:43:27 176353:176353 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=512:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=512:   0%|          | 1/1000 [00:00<02:58,  5.60it/s]Measuring inference for batch_size=512:   0%|          | 2/1000 [00:00<02:58,  5.61it/s]Measuring inference for batch_size=512:   0%|          | 3/1000 [00:00<02:57,  5.60it/s]Measuring inference for batch_size=512:   0%|          | 4/1000 [00:00<02:57,  5.61it/s]Measuring inference for batch_size=512:   0%|          | 5/1000 [00:00<02:57,  5.61it/s]Measuring inference for batch_size=512:   1%|          | 6/1000 [00:01<02:57,  5.61it/s]Measuring inference for batch_size=512:   1%|          | 7/1000 [00:01<02:56,  5.61it/s]Measuring inference for batch_size=512:   1%|          | 8/1000 [00:01<02:56,  5.61it/s]Measuring inference for batch_size=512:   1%|          | 9/1000 [00:01<02:56,  5.60it/s]Measuring inference for batch_size=512:   1%|          | 10/1000 [00:01<02:56,  5.60it/s]Measuring inference for batch_size=512:   1%|          | 11/1000 [00:01<02:56,  5.61it/s]Measuring inference for batch_size=512:   1%|          | 12/1000 [00:02<02:56,  5.61it/s]Measuring inference for batch_size=512:   1%|▏         | 13/1000 [00:02<02:55,  5.61it/s]Measuring inference for batch_size=512:   1%|▏         | 14/1000 [00:02<02:55,  5.61it/s]Measuring inference for batch_size=512:   2%|▏         | 15/1000 [00:02<02:55,  5.61it/s]Measuring inference for batch_size=512:   2%|▏         | 16/1000 [00:02<02:55,  5.61it/s]Measuring inference for batch_size=512:   2%|▏         | 17/1000 [00:03<02:55,  5.61it/s]Measuring inference for batch_size=512:   2%|▏         | 18/1000 [00:03<02:55,  5.61it/s]Measuring inference for batch_size=512:   2%|▏         | 19/1000 [00:03<02:55,  5.60it/s]Measuring inference for batch_size=512:   2%|▏         | 20/1000 [00:03<02:54,  5.61it/s]Measuring inference for batch_size=512:   2%|▏         | 21/1000 [00:03<02:54,  5.60it/s]Measuring inference for batch_size=512:   2%|▏         | 22/1000 [00:03<02:54,  5.60it/s]Measuring inference for batch_size=512:   2%|▏         | 23/1000 [00:04<02:54,  5.60it/s]Measuring inference for batch_size=512:   2%|▏         | 24/1000 [00:04<02:54,  5.60it/s]Measuring inference for batch_size=512:   2%|▎         | 25/1000 [00:04<02:53,  5.60it/s]Measuring inference for batch_size=512:   3%|▎         | 26/1000 [00:04<02:53,  5.60it/s]Measuring inference for batch_size=512:   3%|▎         | 27/1000 [00:04<02:53,  5.61it/s]Measuring inference for batch_size=512:   3%|▎         | 28/1000 [00:04<02:53,  5.60it/s]Measuring inference for batch_size=512:   3%|▎         | 29/1000 [00:05<02:53,  5.61it/s]Measuring inference for batch_size=512:   3%|▎         | 30/1000 [00:05<02:52,  5.61it/s]Measuring inference for batch_size=512:   3%|▎         | 31/1000 [00:05<02:52,  5.61it/s]Measuring inference for batch_size=512:   3%|▎         | 32/1000 [00:05<02:52,  5.61it/s]Measuring inference for batch_size=512:   3%|▎         | 33/1000 [00:05<02:52,  5.61it/s]Measuring inference for batch_size=512:   3%|▎         | 34/1000 [00:06<02:52,  5.61it/s]Measuring inference for batch_size=512:   4%|▎         | 35/1000 [00:06<02:51,  5.61it/s]Measuring inference for batch_size=512:   4%|▎         | 36/1000 [00:06<02:51,  5.61it/s]Measuring inference for batch_size=512:   4%|▎         | 37/1000 [00:06<02:51,  5.61it/s]Measuring inference for batch_size=512:   4%|▍         | 38/1000 [00:06<02:51,  5.61it/s]Measuring inference for batch_size=512:   4%|▍         | 39/1000 [00:06<02:51,  5.60it/s]Measuring inference for batch_size=512:   4%|▍         | 40/1000 [00:07<02:51,  5.60it/s]Measuring inference for batch_size=512:   4%|▍         | 41/1000 [00:07<02:51,  5.60it/s]Measuring inference for batch_size=512:   4%|▍         | 42/1000 [00:07<02:50,  5.61it/s]Measuring inference for batch_size=512:   4%|▍         | 43/1000 [00:07<02:50,  5.61it/s]Measuring inference for batch_size=512:   4%|▍         | 44/1000 [00:07<02:50,  5.61it/s]Measuring inference for batch_size=512:   4%|▍         | 45/1000 [00:08<02:50,  5.61it/s]Measuring inference for batch_size=512:   5%|▍         | 46/1000 [00:08<02:50,  5.61it/s]Measuring inference for batch_size=512:   5%|▍         | 47/1000 [00:08<02:49,  5.61it/s]Measuring inference for batch_size=512:   5%|▍         | 48/1000 [00:08<02:49,  5.61it/s]Measuring inference for batch_size=512:   5%|▍         | 49/1000 [00:08<02:49,  5.62it/s]Measuring inference for batch_size=512:   5%|▌         | 50/1000 [00:08<02:49,  5.61it/s]Measuring inference for batch_size=512:   5%|▌         | 51/1000 [00:09<02:49,  5.61it/s]Measuring inference for batch_size=512:   5%|▌         | 52/1000 [00:09<02:49,  5.60it/s]Measuring inference for batch_size=512:   5%|▌         | 53/1000 [00:09<02:49,  5.60it/s]Measuring inference for batch_size=512:   5%|▌         | 54/1000 [00:09<02:48,  5.61it/s]Measuring inference for batch_size=512:   6%|▌         | 55/1000 [00:09<02:48,  5.61it/s]Measuring inference for batch_size=512:   6%|▌         | 56/1000 [00:09<02:48,  5.61it/s]Measuring inference for batch_size=512:   6%|▌         | 57/1000 [00:10<02:48,  5.61it/s]Measuring inference for batch_size=512:   6%|▌         | 58/1000 [00:10<02:47,  5.61it/s]Measuring inference for batch_size=512:   6%|▌         | 59/1000 [00:10<02:47,  5.61it/s]Measuring inference for batch_size=512:   6%|▌         | 60/1000 [00:10<02:47,  5.61it/s]Measuring inference for batch_size=512:   6%|▌         | 61/1000 [00:10<02:47,  5.61it/s]Measuring inference for batch_size=512:   6%|▌         | 62/1000 [00:11<02:47,  5.61it/s]Measuring inference for batch_size=512:   6%|▋         | 63/1000 [00:11<02:46,  5.61it/s]Measuring inference for batch_size=512:   6%|▋         | 64/1000 [00:11<02:46,  5.61it/s]Measuring inference for batch_size=512:   6%|▋         | 65/1000 [00:11<02:46,  5.61it/s]Measuring inference for batch_size=512:   7%|▋         | 66/1000 [00:11<02:46,  5.61it/s]Measuring inference for batch_size=512:   7%|▋         | 67/1000 [00:11<02:46,  5.61it/s]Measuring inference for batch_size=512:   7%|▋         | 68/1000 [00:12<02:46,  5.61it/s]Measuring inference for batch_size=512:   7%|▋         | 69/1000 [00:12<02:45,  5.61it/s]Measuring inference for batch_size=512:   7%|▋         | 70/1000 [00:12<02:45,  5.61it/s]Measuring inference for batch_size=512:   7%|▋         | 71/1000 [00:12<02:45,  5.60it/s]Measuring inference for batch_size=512:   7%|▋         | 72/1000 [00:12<02:45,  5.60it/s]Measuring inference for batch_size=512:   7%|▋         | 73/1000 [00:13<02:45,  5.60it/s]Measuring inference for batch_size=512:   7%|▋         | 74/1000 [00:13<02:45,  5.58it/s]Measuring inference for batch_size=512:   8%|▊         | 75/1000 [00:13<02:45,  5.59it/s]Measuring inference for batch_size=512:   8%|▊         | 76/1000 [00:13<02:45,  5.60it/s]Measuring inference for batch_size=512:   8%|▊         | 77/1000 [00:13<02:44,  5.60it/s]Measuring inference for batch_size=512:   8%|▊         | 78/1000 [00:13<02:44,  5.61it/s]Measuring inference for batch_size=512:   8%|▊         | 79/1000 [00:14<02:44,  5.61it/s]Measuring inference for batch_size=512:   8%|▊         | 80/1000 [00:14<02:44,  5.60it/s]Measuring inference for batch_size=512:   8%|▊         | 81/1000 [00:14<02:43,  5.61it/s]Measuring inference for batch_size=512:   8%|▊         | 82/1000 [00:14<02:43,  5.61it/s]Measuring inference for batch_size=512:   8%|▊         | 83/1000 [00:14<02:43,  5.61it/s]Measuring inference for batch_size=512:   8%|▊         | 84/1000 [00:14<02:43,  5.61it/s]Measuring inference for batch_size=512:   8%|▊         | 85/1000 [00:15<02:43,  5.60it/s]Measuring inference for batch_size=512:   9%|▊         | 86/1000 [00:15<02:43,  5.61it/s]Measuring inference for batch_size=512:   9%|▊         | 87/1000 [00:15<02:42,  5.61it/s]Measuring inference for batch_size=512:   9%|▉         | 88/1000 [00:15<02:42,  5.61it/s]Measuring inference for batch_size=512:   9%|▉         | 89/1000 [00:15<02:42,  5.61it/s]Measuring inference for batch_size=512:   9%|▉         | 90/1000 [00:16<02:42,  5.61it/s]Measuring inference for batch_size=512:   9%|▉         | 91/1000 [00:16<02:42,  5.61it/s]Measuring inference for batch_size=512:   9%|▉         | 92/1000 [00:16<02:41,  5.61it/s]Measuring inference for batch_size=512:   9%|▉         | 93/1000 [00:16<02:41,  5.61it/s]Measuring inference for batch_size=512:   9%|▉         | 94/1000 [00:16<02:41,  5.61it/s]Measuring inference for batch_size=512:  10%|▉         | 95/1000 [00:16<02:41,  5.61it/s]Measuring inference for batch_size=512:  10%|▉         | 96/1000 [00:17<02:41,  5.61it/s]Measuring inference for batch_size=512:  10%|▉         | 97/1000 [00:17<02:40,  5.61it/s]Measuring inference for batch_size=512:  10%|▉         | 98/1000 [00:17<02:40,  5.61it/s]Measuring inference for batch_size=512:  10%|▉         | 99/1000 [00:17<02:40,  5.61it/s]Measuring inference for batch_size=512:  10%|█         | 100/1000 [00:17<02:40,  5.61it/s]Measuring inference for batch_size=512:  10%|█         | 101/1000 [00:18<02:40,  5.61it/s]Measuring inference for batch_size=512:  10%|█         | 102/1000 [00:18<02:40,  5.61it/s]Measuring inference for batch_size=512:  10%|█         | 103/1000 [00:18<02:39,  5.61it/s]Measuring inference for batch_size=512:  10%|█         | 104/1000 [00:18<02:39,  5.61it/s]Measuring inference for batch_size=512:  10%|█         | 105/1000 [00:18<02:39,  5.61it/s]Measuring inference for batch_size=512:  11%|█         | 106/1000 [00:18<02:39,  5.61it/s]Measuring inference for batch_size=512:  11%|█         | 107/1000 [00:19<02:39,  5.60it/s]Measuring inference for batch_size=512:  11%|█         | 108/1000 [00:19<02:39,  5.60it/s]Measuring inference for batch_size=512:  11%|█         | 109/1000 [00:19<02:39,  5.60it/s]Measuring inference for batch_size=512:  11%|█         | 110/1000 [00:19<02:38,  5.60it/s]Measuring inference for batch_size=512:  11%|█         | 111/1000 [00:19<02:38,  5.60it/s]Measuring inference for batch_size=512:  11%|█         | 112/1000 [00:19<02:38,  5.61it/s]Measuring inference for batch_size=512:  11%|█▏        | 113/1000 [00:20<02:38,  5.61it/s]Measuring inference for batch_size=512:  11%|█▏        | 114/1000 [00:20<02:37,  5.61it/s]Measuring inference for batch_size=512:  12%|█▏        | 115/1000 [00:20<02:37,  5.61it/s]Measuring inference for batch_size=512:  12%|█▏        | 116/1000 [00:20<02:37,  5.61it/s]Measuring inference for batch_size=512:  12%|█▏        | 117/1000 [00:20<02:37,  5.61it/s]Measuring inference for batch_size=512:  12%|█▏        | 118/1000 [00:21<02:37,  5.61it/s]Measuring inference for batch_size=512:  12%|█▏        | 119/1000 [00:21<02:37,  5.61it/s]Measuring inference for batch_size=512:  12%|█▏        | 120/1000 [00:21<02:37,  5.60it/s]Measuring inference for batch_size=512:  12%|█▏        | 121/1000 [00:21<02:36,  5.61it/s]Measuring inference for batch_size=512:  12%|█▏        | 122/1000 [00:21<02:36,  5.60it/s]Measuring inference for batch_size=512:  12%|█▏        | 123/1000 [00:21<02:36,  5.61it/s]Measuring inference for batch_size=512:  12%|█▏        | 124/1000 [00:22<02:36,  5.61it/s]Measuring inference for batch_size=512:  12%|█▎        | 125/1000 [00:22<02:35,  5.61it/s]Measuring inference for batch_size=512:  13%|█▎        | 126/1000 [00:22<02:35,  5.61it/s]Measuring inference for batch_size=512:  13%|█▎        | 127/1000 [00:22<02:35,  5.61it/s]Measuring inference for batch_size=512:  13%|█▎        | 128/1000 [00:22<02:35,  5.61it/s]Measuring inference for batch_size=512:  13%|█▎        | 129/1000 [00:23<02:35,  5.61it/s]Measuring inference for batch_size=512:  13%|█▎        | 130/1000 [00:23<02:34,  5.62it/s]Measuring inference for batch_size=512:  13%|█▎        | 131/1000 [00:23<02:34,  5.61it/s]Measuring inference for batch_size=512:  13%|█▎        | 132/1000 [00:23<02:34,  5.61it/s]Measuring inference for batch_size=512:  13%|█▎        | 133/1000 [00:23<02:34,  5.61it/s]Measuring inference for batch_size=512:  13%|█▎        | 134/1000 [00:23<02:34,  5.61it/s]Measuring inference for batch_size=512:  14%|█▎        | 135/1000 [00:24<02:34,  5.61it/s]Measuring inference for batch_size=512:  14%|█▎        | 136/1000 [00:24<02:33,  5.62it/s]Measuring inference for batch_size=512:  14%|█▎        | 137/1000 [00:24<02:33,  5.62it/s]Measuring inference for batch_size=512:  14%|█▍        | 138/1000 [00:24<02:33,  5.62it/s]Measuring inference for batch_size=512:  14%|█▍        | 139/1000 [00:24<02:33,  5.62it/s]Measuring inference for batch_size=512:  14%|█▍        | 140/1000 [00:24<02:33,  5.62it/s]Measuring inference for batch_size=512:  14%|█▍        | 141/1000 [00:25<02:32,  5.62it/s]Measuring inference for batch_size=512:  14%|█▍        | 142/1000 [00:25<02:32,  5.62it/s]Measuring inference for batch_size=512:  14%|█▍        | 143/1000 [00:25<02:32,  5.62it/s]Measuring inference for batch_size=512:  14%|█▍        | 144/1000 [00:25<02:32,  5.62it/s]Measuring inference for batch_size=512:  14%|█▍        | 145/1000 [00:25<02:32,  5.62it/s]Measuring inference for batch_size=512:  15%|█▍        | 146/1000 [00:26<02:32,  5.62it/s]Measuring inference for batch_size=512:  15%|█▍        | 147/1000 [00:26<02:32,  5.61it/s]Measuring inference for batch_size=512:  15%|█▍        | 148/1000 [00:26<02:31,  5.61it/s]Measuring inference for batch_size=512:  15%|█▍        | 149/1000 [00:26<02:31,  5.61it/s]Measuring inference for batch_size=512:  15%|█▌        | 150/1000 [00:26<02:31,  5.61it/s]Measuring inference for batch_size=512:  15%|█▌        | 151/1000 [00:26<02:31,  5.61it/s]Measuring inference for batch_size=512:  15%|█▌        | 152/1000 [00:27<02:30,  5.62it/s]Measuring inference for batch_size=512:  15%|█▌        | 153/1000 [00:27<02:30,  5.62it/s]Measuring inference for batch_size=512:  15%|█▌        | 154/1000 [00:27<02:30,  5.62it/s]Measuring inference for batch_size=512:  16%|█▌        | 155/1000 [00:27<02:30,  5.61it/s]Measuring inference for batch_size=512:  16%|█▌        | 156/1000 [00:27<02:30,  5.61it/s]Measuring inference for batch_size=512:  16%|█▌        | 157/1000 [00:27<02:30,  5.61it/s]Measuring inference for batch_size=512:  16%|█▌        | 158/1000 [00:28<02:30,  5.61it/s]Measuring inference for batch_size=512:  16%|█▌        | 159/1000 [00:28<02:29,  5.61it/s]Measuring inference for batch_size=512:  16%|█▌        | 160/1000 [00:28<02:29,  5.61it/s]Measuring inference for batch_size=512:  16%|█▌        | 161/1000 [00:28<02:29,  5.61it/s]Measuring inference for batch_size=512:  16%|█▌        | 162/1000 [00:28<02:29,  5.61it/s]Measuring inference for batch_size=512:  16%|█▋        | 163/1000 [00:29<02:29,  5.61it/s]Measuring inference for batch_size=512:  16%|█▋        | 164/1000 [00:29<02:28,  5.61it/s]Measuring inference for batch_size=512:  16%|█▋        | 165/1000 [00:29<02:28,  5.61it/s]Measuring inference for batch_size=512:  17%|█▋        | 166/1000 [00:29<02:28,  5.61it/s]Measuring inference for batch_size=512:  17%|█▋        | 167/1000 [00:29<02:28,  5.61it/s]Measuring inference for batch_size=512:  17%|█▋        | 168/1000 [00:29<02:28,  5.61it/s]Measuring inference for batch_size=512:  17%|█▋        | 169/1000 [00:30<02:28,  5.61it/s]Measuring inference for batch_size=512:  17%|█▋        | 170/1000 [00:30<02:27,  5.61it/s]Measuring inference for batch_size=512:  17%|█▋        | 171/1000 [00:30<02:27,  5.61it/s]Measuring inference for batch_size=512:  17%|█▋        | 172/1000 [00:30<02:27,  5.62it/s]Measuring inference for batch_size=512:  17%|█▋        | 173/1000 [00:30<02:27,  5.61it/s]Measuring inference for batch_size=512:  17%|█▋        | 174/1000 [00:31<02:27,  5.62it/s]Measuring inference for batch_size=512:  18%|█▊        | 175/1000 [00:31<02:26,  5.62it/s]Measuring inference for batch_size=512:  18%|█▊        | 176/1000 [00:31<02:26,  5.61it/s]Measuring inference for batch_size=512:  18%|█▊        | 177/1000 [00:31<02:26,  5.61it/s]Measuring inference for batch_size=512:  18%|█▊        | 178/1000 [00:31<02:26,  5.61it/s]Measuring inference for batch_size=512:  18%|█▊        | 179/1000 [00:31<02:26,  5.61it/s]Measuring inference for batch_size=512:  18%|█▊        | 180/1000 [00:32<02:26,  5.60it/s]Measuring inference for batch_size=512:  18%|█▊        | 181/1000 [00:32<02:26,  5.59it/s]Measuring inference for batch_size=512:  18%|█▊        | 182/1000 [00:32<02:26,  5.60it/s]Measuring inference for batch_size=512:  18%|█▊        | 183/1000 [00:32<02:25,  5.60it/s]Measuring inference for batch_size=512:  18%|█▊        | 184/1000 [00:32<02:25,  5.61it/s]Measuring inference for batch_size=512:  18%|█▊        | 185/1000 [00:32<02:25,  5.61it/s]Measuring inference for batch_size=512:  19%|█▊        | 186/1000 [00:33<02:25,  5.61it/s]Measuring inference for batch_size=512:  19%|█▊        | 187/1000 [00:33<02:24,  5.61it/s]Measuring inference for batch_size=512:  19%|█▉        | 188/1000 [00:33<02:24,  5.61it/s]Measuring inference for batch_size=512:  19%|█▉        | 189/1000 [00:33<02:24,  5.61it/s]Measuring inference for batch_size=512:  19%|█▉        | 190/1000 [00:33<02:24,  5.60it/s]Measuring inference for batch_size=512:  19%|█▉        | 191/1000 [00:34<02:24,  5.61it/s]Measuring inference for batch_size=512:  19%|█▉        | 192/1000 [00:34<02:24,  5.61it/s]Measuring inference for batch_size=512:  19%|█▉        | 193/1000 [00:34<02:23,  5.61it/s]Measuring inference for batch_size=512:  19%|█▉        | 194/1000 [00:34<02:23,  5.61it/s]Measuring inference for batch_size=512:  20%|█▉        | 195/1000 [00:34<02:23,  5.61it/s]Measuring inference for batch_size=512:  20%|█▉        | 196/1000 [00:34<02:23,  5.61it/s]Measuring inference for batch_size=512:  20%|█▉        | 197/1000 [00:35<02:23,  5.61it/s]Measuring inference for batch_size=512:  20%|█▉        | 198/1000 [00:35<02:23,  5.61it/s]Measuring inference for batch_size=512:  20%|█▉        | 199/1000 [00:35<02:22,  5.61it/s]Measuring inference for batch_size=512:  20%|██        | 200/1000 [00:35<02:22,  5.61it/s]Measuring inference for batch_size=512:  20%|██        | 201/1000 [00:35<02:22,  5.60it/s]Measuring inference for batch_size=512:  20%|██        | 202/1000 [00:36<02:22,  5.61it/s]Measuring inference for batch_size=512:  20%|██        | 203/1000 [00:36<02:22,  5.61it/s]Measuring inference for batch_size=512:  20%|██        | 204/1000 [00:36<02:21,  5.61it/s]Measuring inference for batch_size=512:  20%|██        | 205/1000 [00:36<02:21,  5.61it/s]Measuring inference for batch_size=512:  21%|██        | 206/1000 [00:36<02:21,  5.61it/s]Measuring inference for batch_size=512:  21%|██        | 207/1000 [00:36<02:21,  5.61it/s]Measuring inference for batch_size=512:  21%|██        | 208/1000 [00:37<02:21,  5.61it/s]Measuring inference for batch_size=512:  21%|██        | 209/1000 [00:37<02:21,  5.61it/s]Measuring inference for batch_size=512:  21%|██        | 210/1000 [00:37<02:20,  5.61it/s]Measuring inference for batch_size=512:  21%|██        | 211/1000 [00:37<02:20,  5.61it/s]Measuring inference for batch_size=512:  21%|██        | 212/1000 [00:37<02:20,  5.61it/s]Measuring inference for batch_size=512:  21%|██▏       | 213/1000 [00:37<02:20,  5.61it/s]Measuring inference for batch_size=512:  21%|██▏       | 214/1000 [00:38<02:20,  5.61it/s]Measuring inference for batch_size=512:  22%|██▏       | 215/1000 [00:38<02:19,  5.61it/s]Measuring inference for batch_size=512:  22%|██▏       | 216/1000 [00:38<02:19,  5.61it/s]Measuring inference for batch_size=512:  22%|██▏       | 217/1000 [00:38<02:19,  5.60it/s]Measuring inference for batch_size=512:  22%|██▏       | 218/1000 [00:38<02:19,  5.61it/s]Measuring inference for batch_size=512:  22%|██▏       | 219/1000 [00:39<02:19,  5.61it/s]Measuring inference for batch_size=512:  22%|██▏       | 220/1000 [00:39<02:18,  5.61it/s]Measuring inference for batch_size=512:  22%|██▏       | 221/1000 [00:39<02:18,  5.61it/s]Measuring inference for batch_size=512:  22%|██▏       | 222/1000 [00:39<02:18,  5.61it/s]Measuring inference for batch_size=512:  22%|██▏       | 223/1000 [00:39<02:18,  5.61it/s]Measuring inference for batch_size=512:  22%|██▏       | 224/1000 [00:39<02:18,  5.61it/s]Measuring inference for batch_size=512:  22%|██▎       | 225/1000 [00:40<02:18,  5.61it/s]Measuring inference for batch_size=512:  23%|██▎       | 226/1000 [00:40<02:17,  5.61it/s]Measuring inference for batch_size=512:  23%|██▎       | 227/1000 [00:40<02:17,  5.61it/s]Measuring inference for batch_size=512:  23%|██▎       | 228/1000 [00:40<02:17,  5.61it/s]Measuring inference for batch_size=512:  23%|██▎       | 229/1000 [00:40<02:17,  5.61it/s]Measuring inference for batch_size=512:  23%|██▎       | 230/1000 [00:41<02:17,  5.61it/s]Measuring inference for batch_size=512:  23%|██▎       | 231/1000 [00:41<02:17,  5.61it/s]Measuring inference for batch_size=512:  23%|██▎       | 232/1000 [00:41<02:16,  5.61it/s]Measuring inference for batch_size=512:  23%|██▎       | 233/1000 [00:41<02:16,  5.61it/s]Measuring inference for batch_size=512:  23%|██▎       | 234/1000 [00:41<02:16,  5.62it/s]Measuring inference for batch_size=512:  24%|██▎       | 235/1000 [00:41<02:16,  5.62it/s]Measuring inference for batch_size=512:  24%|██▎       | 236/1000 [00:42<02:16,  5.62it/s]Measuring inference for batch_size=512:  24%|██▎       | 237/1000 [00:42<02:15,  5.62it/s]Measuring inference for batch_size=512:  24%|██▍       | 238/1000 [00:42<02:15,  5.62it/s]Measuring inference for batch_size=512:  24%|██▍       | 239/1000 [00:42<02:15,  5.62it/s]Measuring inference for batch_size=512:  24%|██▍       | 240/1000 [00:42<02:15,  5.62it/s]Measuring inference for batch_size=512:  24%|██▍       | 241/1000 [00:42<02:15,  5.62it/s]Measuring inference for batch_size=512:  24%|██▍       | 242/1000 [00:43<02:14,  5.62it/s]Measuring inference for batch_size=512:  24%|██▍       | 243/1000 [00:43<02:14,  5.62it/s]Measuring inference for batch_size=512:  24%|██▍       | 244/1000 [00:43<02:14,  5.62it/s]Measuring inference for batch_size=512:  24%|██▍       | 245/1000 [00:43<02:14,  5.62it/s]Measuring inference for batch_size=512:  25%|██▍       | 246/1000 [00:43<02:14,  5.61it/s]Measuring inference for batch_size=512:  25%|██▍       | 247/1000 [00:44<02:14,  5.62it/s]Measuring inference for batch_size=512:  25%|██▍       | 248/1000 [00:44<02:13,  5.62it/s]Measuring inference for batch_size=512:  25%|██▍       | 249/1000 [00:44<02:13,  5.61it/s]Measuring inference for batch_size=512:  25%|██▌       | 250/1000 [00:44<02:13,  5.61it/s]Measuring inference for batch_size=512:  25%|██▌       | 251/1000 [00:44<02:13,  5.61it/s]Measuring inference for batch_size=512:  25%|██▌       | 252/1000 [00:44<02:13,  5.61it/s]Measuring inference for batch_size=512:  25%|██▌       | 253/1000 [00:45<02:13,  5.61it/s]Measuring inference for batch_size=512:  25%|██▌       | 254/1000 [00:45<02:12,  5.61it/s]Measuring inference for batch_size=512:  26%|██▌       | 255/1000 [00:45<02:12,  5.61it/s]Measuring inference for batch_size=512:  26%|██▌       | 256/1000 [00:45<02:12,  5.62it/s]Measuring inference for batch_size=512:  26%|██▌       | 257/1000 [00:45<02:12,  5.62it/s]Measuring inference for batch_size=512:  26%|██▌       | 258/1000 [00:45<02:12,  5.62it/s]Measuring inference for batch_size=512:  26%|██▌       | 259/1000 [00:46<02:12,  5.61it/s]Measuring inference for batch_size=512:  26%|██▌       | 260/1000 [00:46<02:11,  5.61it/s]Measuring inference for batch_size=512:  26%|██▌       | 261/1000 [00:46<02:11,  5.61it/s]Measuring inference for batch_size=512:  26%|██▌       | 262/1000 [00:46<02:11,  5.62it/s]Measuring inference for batch_size=512:  26%|██▋       | 263/1000 [00:46<02:11,  5.61it/s]Measuring inference for batch_size=512:  26%|██▋       | 264/1000 [00:47<02:11,  5.61it/s]Measuring inference for batch_size=512:  26%|██▋       | 265/1000 [00:47<02:10,  5.61it/s]Measuring inference for batch_size=512:  27%|██▋       | 266/1000 [00:47<02:10,  5.61it/s]Measuring inference for batch_size=512:  27%|██▋       | 267/1000 [00:47<02:10,  5.61it/s]Measuring inference for batch_size=512:  27%|██▋       | 268/1000 [00:47<02:10,  5.61it/s]Measuring inference for batch_size=512:  27%|██▋       | 269/1000 [00:47<02:10,  5.61it/s]Measuring inference for batch_size=512:  27%|██▋       | 270/1000 [00:48<02:10,  5.61it/s]Measuring inference for batch_size=512:  27%|██▋       | 271/1000 [00:48<02:09,  5.62it/s]Measuring inference for batch_size=512:  27%|██▋       | 272/1000 [00:48<02:09,  5.62it/s]Measuring inference for batch_size=512:  27%|██▋       | 273/1000 [00:48<02:09,  5.62it/s]Measuring inference for batch_size=512:  27%|██▋       | 274/1000 [00:48<02:09,  5.61it/s]Measuring inference for batch_size=512:  28%|██▊       | 275/1000 [00:49<02:09,  5.62it/s]Measuring inference for batch_size=512:  28%|██▊       | 276/1000 [00:49<02:08,  5.61it/s]Measuring inference for batch_size=512:  28%|██▊       | 277/1000 [00:49<02:08,  5.61it/s]Measuring inference for batch_size=512:  28%|██▊       | 278/1000 [00:49<02:08,  5.61it/s]Measuring inference for batch_size=512:  28%|██▊       | 279/1000 [00:49<02:08,  5.61it/s]Measuring inference for batch_size=512:  28%|██▊       | 280/1000 [00:49<02:08,  5.61it/s]Measuring inference for batch_size=512:  28%|██▊       | 281/1000 [00:50<02:08,  5.60it/s]Measuring inference for batch_size=512:  28%|██▊       | 282/1000 [00:50<02:08,  5.61it/s]Measuring inference for batch_size=512:  28%|██▊       | 283/1000 [00:50<02:07,  5.61it/s]Measuring inference for batch_size=512:  28%|██▊       | 284/1000 [00:50<02:07,  5.61it/s]Measuring inference for batch_size=512:  28%|██▊       | 285/1000 [00:50<02:07,  5.61it/s]Measuring inference for batch_size=512:  29%|██▊       | 286/1000 [00:50<02:07,  5.61it/s]Measuring inference for batch_size=512:  29%|██▊       | 287/1000 [00:51<02:07,  5.61it/s]Measuring inference for batch_size=512:  29%|██▉       | 288/1000 [00:51<02:06,  5.61it/s]Measuring inference for batch_size=512:  29%|██▉       | 289/1000 [00:51<02:06,  5.61it/s]Measuring inference for batch_size=512:  29%|██▉       | 290/1000 [00:51<02:06,  5.62it/s]Measuring inference for batch_size=512:  29%|██▉       | 291/1000 [00:51<02:06,  5.62it/s]Measuring inference for batch_size=512:  29%|██▉       | 292/1000 [00:52<02:06,  5.62it/s]Measuring inference for batch_size=512:  29%|██▉       | 293/1000 [00:52<02:05,  5.61it/s]Measuring inference for batch_size=512:  29%|██▉       | 294/1000 [00:52<02:05,  5.61it/s]Measuring inference for batch_size=512:  30%|██▉       | 295/1000 [00:52<02:05,  5.61it/s]Measuring inference for batch_size=512:  30%|██▉       | 296/1000 [00:52<02:05,  5.61it/s]Measuring inference for batch_size=512:  30%|██▉       | 297/1000 [00:52<02:05,  5.61it/s]Measuring inference for batch_size=512:  30%|██▉       | 298/1000 [00:53<02:05,  5.61it/s]Measuring inference for batch_size=512:  30%|██▉       | 299/1000 [00:53<02:05,  5.60it/s]Measuring inference for batch_size=512:  30%|███       | 300/1000 [00:53<02:04,  5.61it/s]Measuring inference for batch_size=512:  30%|███       | 301/1000 [00:53<02:04,  5.61it/s]Measuring inference for batch_size=512:  30%|███       | 302/1000 [00:53<02:04,  5.61it/s]Measuring inference for batch_size=512:  30%|███       | 303/1000 [00:54<02:04,  5.61it/s]Measuring inference for batch_size=512:  30%|███       | 304/1000 [00:54<02:04,  5.61it/s]Measuring inference for batch_size=512:  30%|███       | 305/1000 [00:54<02:03,  5.61it/s]Measuring inference for batch_size=512:  31%|███       | 306/1000 [00:54<02:03,  5.61it/s]Measuring inference for batch_size=512:  31%|███       | 307/1000 [00:54<02:03,  5.61it/s]Measuring inference for batch_size=512:  31%|███       | 308/1000 [00:54<02:03,  5.61it/s]Measuring inference for batch_size=512:  31%|███       | 309/1000 [00:55<02:03,  5.61it/s]Measuring inference for batch_size=512:  31%|███       | 310/1000 [00:55<02:02,  5.61it/s]Measuring inference for batch_size=512:  31%|███       | 311/1000 [00:55<02:02,  5.61it/s]Measuring inference for batch_size=512:  31%|███       | 312/1000 [00:55<02:02,  5.62it/s]Measuring inference for batch_size=512:  31%|███▏      | 313/1000 [00:55<02:02,  5.62it/s]Measuring inference for batch_size=512:  31%|███▏      | 314/1000 [00:55<02:02,  5.61it/s]Measuring inference for batch_size=512:  32%|███▏      | 315/1000 [00:56<02:01,  5.61it/s]Measuring inference for batch_size=512:  32%|███▏      | 316/1000 [00:56<02:01,  5.61it/s]Measuring inference for batch_size=512:  32%|███▏      | 317/1000 [00:56<02:01,  5.61it/s]Measuring inference for batch_size=512:  32%|███▏      | 318/1000 [00:56<02:01,  5.61it/s]Measuring inference for batch_size=512:  32%|███▏      | 319/1000 [00:56<02:01,  5.61it/s]Measuring inference for batch_size=512:  32%|███▏      | 320/1000 [00:57<02:01,  5.61it/s]Measuring inference for batch_size=512:  32%|███▏      | 321/1000 [00:57<02:00,  5.61it/s]Measuring inference for batch_size=512:  32%|███▏      | 322/1000 [00:57<02:00,  5.61it/s]Measuring inference for batch_size=512:  32%|███▏      | 323/1000 [00:57<02:00,  5.61it/s]Measuring inference for batch_size=512:  32%|███▏      | 324/1000 [00:57<02:00,  5.62it/s]Measuring inference for batch_size=512:  32%|███▎      | 325/1000 [00:57<02:00,  5.61it/s]Measuring inference for batch_size=512:  33%|███▎      | 326/1000 [00:58<02:00,  5.61it/s]Measuring inference for batch_size=512:  33%|███▎      | 327/1000 [00:58<01:59,  5.61it/s]Measuring inference for batch_size=512:  33%|███▎      | 328/1000 [00:58<01:59,  5.62it/s]Measuring inference for batch_size=512:  33%|███▎      | 329/1000 [00:58<01:59,  5.61it/s]Measuring inference for batch_size=512:  33%|███▎      | 330/1000 [00:58<01:59,  5.62it/s]Measuring inference for batch_size=512:  33%|███▎      | 331/1000 [00:58<01:59,  5.62it/s]Measuring inference for batch_size=512:  33%|███▎      | 332/1000 [00:59<01:58,  5.62it/s]Measuring inference for batch_size=512:  33%|███▎      | 333/1000 [00:59<01:58,  5.62it/s]Measuring inference for batch_size=512:  33%|███▎      | 334/1000 [00:59<01:58,  5.62it/s]Measuring inference for batch_size=512:  34%|███▎      | 335/1000 [00:59<01:58,  5.62it/s]Measuring inference for batch_size=512:  34%|███▎      | 336/1000 [00:59<01:58,  5.61it/s]Measuring inference for batch_size=512:  34%|███▎      | 337/1000 [01:00<01:58,  5.61it/s]Measuring inference for batch_size=512:  34%|███▍      | 338/1000 [01:00<01:57,  5.62it/s]Measuring inference for batch_size=512:  34%|███▍      | 339/1000 [01:00<01:57,  5.62it/s]Measuring inference for batch_size=512:  34%|███▍      | 340/1000 [01:00<01:57,  5.62it/s]Measuring inference for batch_size=512:  34%|███▍      | 341/1000 [01:00<01:57,  5.62it/s]Measuring inference for batch_size=512:  34%|███▍      | 342/1000 [01:00<01:57,  5.61it/s]Measuring inference for batch_size=512:  34%|███▍      | 343/1000 [01:01<01:57,  5.62it/s]Measuring inference for batch_size=512:  34%|███▍      | 344/1000 [01:01<01:56,  5.62it/s]Measuring inference for batch_size=512:  34%|███▍      | 345/1000 [01:01<01:56,  5.61it/s]Measuring inference for batch_size=512:  35%|███▍      | 346/1000 [01:01<01:56,  5.61it/s]Measuring inference for batch_size=512:  35%|███▍      | 347/1000 [01:01<01:56,  5.61it/s]Measuring inference for batch_size=512:  35%|███▍      | 348/1000 [01:02<01:56,  5.60it/s]Measuring inference for batch_size=512:  35%|███▍      | 349/1000 [01:02<01:56,  5.61it/s]Measuring inference for batch_size=512:  35%|███▌      | 350/1000 [01:02<01:55,  5.61it/s]Measuring inference for batch_size=512:  35%|███▌      | 351/1000 [01:02<01:55,  5.61it/s]Measuring inference for batch_size=512:  35%|███▌      | 352/1000 [01:02<01:55,  5.61it/s]Measuring inference for batch_size=512:  35%|███▌      | 353/1000 [01:02<01:55,  5.61it/s]Measuring inference for batch_size=512:  35%|███▌      | 354/1000 [01:03<01:55,  5.61it/s]Measuring inference for batch_size=512:  36%|███▌      | 355/1000 [01:03<01:54,  5.61it/s]Measuring inference for batch_size=512:  36%|███▌      | 356/1000 [01:03<01:54,  5.61it/s]Measuring inference for batch_size=512:  36%|███▌      | 357/1000 [01:03<01:54,  5.61it/s]Measuring inference for batch_size=512:  36%|███▌      | 358/1000 [01:03<01:54,  5.61it/s]Measuring inference for batch_size=512:  36%|███▌      | 359/1000 [01:03<01:54,  5.61it/s]Measuring inference for batch_size=512:  36%|███▌      | 360/1000 [01:04<01:54,  5.61it/s]Measuring inference for batch_size=512:  36%|███▌      | 361/1000 [01:04<01:53,  5.62it/s]Measuring inference for batch_size=512:  36%|███▌      | 362/1000 [01:04<01:53,  5.62it/s]Measuring inference for batch_size=512:  36%|███▋      | 363/1000 [01:04<01:53,  5.62it/s]Measuring inference for batch_size=512:  36%|███▋      | 364/1000 [01:04<01:53,  5.61it/s]Measuring inference for batch_size=512:  36%|███▋      | 365/1000 [01:05<01:53,  5.61it/s]Measuring inference for batch_size=512:  37%|███▋      | 366/1000 [01:05<01:52,  5.61it/s]Measuring inference for batch_size=512:  37%|███▋      | 367/1000 [01:05<01:52,  5.61it/s]Measuring inference for batch_size=512:  37%|███▋      | 368/1000 [01:05<01:52,  5.61it/s]Measuring inference for batch_size=512:  37%|███▋      | 369/1000 [01:05<01:52,  5.61it/s]Measuring inference for batch_size=512:  37%|███▋      | 370/1000 [01:05<01:52,  5.61it/s]Measuring inference for batch_size=512:  37%|███▋      | 371/1000 [01:06<01:52,  5.60it/s]Measuring inference for batch_size=512:  37%|███▋      | 372/1000 [01:06<01:52,  5.60it/s]Measuring inference for batch_size=512:  37%|███▋      | 373/1000 [01:06<01:51,  5.60it/s]Measuring inference for batch_size=512:  37%|███▋      | 374/1000 [01:06<01:51,  5.60it/s]Measuring inference for batch_size=512:  38%|███▊      | 375/1000 [01:06<01:51,  5.60it/s]Measuring inference for batch_size=512:  38%|███▊      | 376/1000 [01:07<01:51,  5.60it/s]Measuring inference for batch_size=512:  38%|███▊      | 377/1000 [01:07<01:51,  5.61it/s]Measuring inference for batch_size=512:  38%|███▊      | 378/1000 [01:07<01:50,  5.61it/s]Measuring inference for batch_size=512:  38%|███▊      | 379/1000 [01:07<01:50,  5.61it/s]Measuring inference for batch_size=512:  38%|███▊      | 380/1000 [01:07<01:50,  5.61it/s]Measuring inference for batch_size=512:  38%|███▊      | 381/1000 [01:07<01:50,  5.61it/s]Measuring inference for batch_size=512:  38%|███▊      | 382/1000 [01:08<01:50,  5.61it/s]Measuring inference for batch_size=512:  38%|███▊      | 383/1000 [01:08<01:49,  5.61it/s]Measuring inference for batch_size=512:  38%|███▊      | 384/1000 [01:08<01:49,  5.61it/s]Measuring inference for batch_size=512:  38%|███▊      | 385/1000 [01:08<01:49,  5.61it/s]Measuring inference for batch_size=512:  39%|███▊      | 386/1000 [01:08<01:49,  5.61it/s]Measuring inference for batch_size=512:  39%|███▊      | 387/1000 [01:08<01:49,  5.61it/s]Measuring inference for batch_size=512:  39%|███▉      | 388/1000 [01:09<01:49,  5.61it/s]Measuring inference for batch_size=512:  39%|███▉      | 389/1000 [01:09<01:49,  5.60it/s]Measuring inference for batch_size=512:  39%|███▉      | 390/1000 [01:09<01:48,  5.61it/s]Measuring inference for batch_size=512:  39%|███▉      | 391/1000 [01:09<01:48,  5.61it/s]Measuring inference for batch_size=512:  39%|███▉      | 392/1000 [01:09<01:48,  5.60it/s]Measuring inference for batch_size=512:  39%|███▉      | 393/1000 [01:10<01:48,  5.60it/s]Measuring inference for batch_size=512:  39%|███▉      | 394/1000 [01:10<01:48,  5.60it/s]Measuring inference for batch_size=512:  40%|███▉      | 395/1000 [01:10<01:47,  5.60it/s]Measuring inference for batch_size=512:  40%|███▉      | 396/1000 [01:10<01:47,  5.60it/s]Measuring inference for batch_size=512:  40%|███▉      | 397/1000 [01:10<01:47,  5.60it/s]Measuring inference for batch_size=512:  40%|███▉      | 398/1000 [01:10<01:47,  5.60it/s]Measuring inference for batch_size=512:  40%|███▉      | 399/1000 [01:11<01:47,  5.60it/s]Measuring inference for batch_size=512:  40%|████      | 400/1000 [01:11<01:47,  5.61it/s]Measuring inference for batch_size=512:  40%|████      | 401/1000 [01:11<01:46,  5.60it/s]Measuring inference for batch_size=512:  40%|████      | 402/1000 [01:11<01:46,  5.61it/s]Measuring inference for batch_size=512:  40%|████      | 403/1000 [01:11<01:46,  5.61it/s]Measuring inference for batch_size=512:  40%|████      | 404/1000 [01:12<01:46,  5.61it/s]Measuring inference for batch_size=512:  40%|████      | 405/1000 [01:12<01:46,  5.61it/s]Measuring inference for batch_size=512:  41%|████      | 406/1000 [01:12<01:45,  5.61it/s]Measuring inference for batch_size=512:  41%|████      | 407/1000 [01:12<01:45,  5.61it/s]Measuring inference for batch_size=512:  41%|████      | 408/1000 [01:12<01:45,  5.61it/s]Measuring inference for batch_size=512:  41%|████      | 409/1000 [01:12<01:45,  5.61it/s]Measuring inference for batch_size=512:  41%|████      | 410/1000 [01:13<01:45,  5.60it/s]Measuring inference for batch_size=512:  41%|████      | 411/1000 [01:13<01:45,  5.61it/s]Measuring inference for batch_size=512:  41%|████      | 412/1000 [01:13<01:44,  5.61it/s]Measuring inference for batch_size=512:  41%|████▏     | 413/1000 [01:13<01:44,  5.61it/s]Measuring inference for batch_size=512:  41%|████▏     | 414/1000 [01:13<01:44,  5.61it/s]Measuring inference for batch_size=512:  42%|████▏     | 415/1000 [01:13<01:44,  5.60it/s]Measuring inference for batch_size=512:  42%|████▏     | 416/1000 [01:14<01:44,  5.60it/s]Measuring inference for batch_size=512:  42%|████▏     | 417/1000 [01:14<01:43,  5.61it/s]Measuring inference for batch_size=512:  42%|████▏     | 418/1000 [01:14<01:43,  5.61it/s]Measuring inference for batch_size=512:  42%|████▏     | 419/1000 [01:14<01:43,  5.61it/s]Measuring inference for batch_size=512:  42%|████▏     | 420/1000 [01:14<01:43,  5.61it/s]Measuring inference for batch_size=512:  42%|████▏     | 421/1000 [01:15<01:43,  5.61it/s]Measuring inference for batch_size=512:  42%|████▏     | 422/1000 [01:15<01:42,  5.61it/s]Measuring inference for batch_size=512:  42%|████▏     | 423/1000 [01:15<01:42,  5.61it/s]Measuring inference for batch_size=512:  42%|████▏     | 424/1000 [01:15<01:42,  5.61it/s]Measuring inference for batch_size=512:  42%|████▎     | 425/1000 [01:15<01:42,  5.61it/s]Measuring inference for batch_size=512:  43%|████▎     | 426/1000 [01:15<01:42,  5.61it/s]Measuring inference for batch_size=512:  43%|████▎     | 427/1000 [01:16<01:42,  5.60it/s]Measuring inference for batch_size=512:  43%|████▎     | 428/1000 [01:16<01:42,  5.60it/s]Measuring inference for batch_size=512:  43%|████▎     | 429/1000 [01:16<01:41,  5.61it/s]Measuring inference for batch_size=512:  43%|████▎     | 430/1000 [01:16<01:41,  5.60it/s]Measuring inference for batch_size=512:  43%|████▎     | 431/1000 [01:16<01:41,  5.60it/s]Measuring inference for batch_size=512:  43%|████▎     | 432/1000 [01:17<01:41,  5.60it/s]Measuring inference for batch_size=512:  43%|████▎     | 433/1000 [01:17<01:41,  5.61it/s]Measuring inference for batch_size=512:  43%|████▎     | 434/1000 [01:17<01:40,  5.61it/s]Measuring inference for batch_size=512:  44%|████▎     | 435/1000 [01:17<01:40,  5.60it/s]Measuring inference for batch_size=512:  44%|████▎     | 436/1000 [01:17<01:40,  5.61it/s]Measuring inference for batch_size=512:  44%|████▎     | 437/1000 [01:17<01:40,  5.61it/s]Measuring inference for batch_size=512:  44%|████▍     | 438/1000 [01:18<01:40,  5.61it/s]Measuring inference for batch_size=512:  44%|████▍     | 439/1000 [01:18<01:40,  5.61it/s]Measuring inference for batch_size=512:  44%|████▍     | 440/1000 [01:18<01:39,  5.61it/s]Measuring inference for batch_size=512:  44%|████▍     | 441/1000 [01:18<01:39,  5.61it/s]Measuring inference for batch_size=512:  44%|████▍     | 442/1000 [01:18<01:39,  5.61it/s]Measuring inference for batch_size=512:  44%|████▍     | 443/1000 [01:18<01:39,  5.61it/s]Measuring inference for batch_size=512:  44%|████▍     | 444/1000 [01:19<01:39,  5.61it/s]Measuring inference for batch_size=512:  44%|████▍     | 445/1000 [01:19<01:38,  5.61it/s]Measuring inference for batch_size=512:  45%|████▍     | 446/1000 [01:19<01:38,  5.61it/s]Measuring inference for batch_size=512:  45%|████▍     | 447/1000 [01:19<01:38,  5.61it/s]Measuring inference for batch_size=512:  45%|████▍     | 448/1000 [01:19<01:38,  5.61it/s]Measuring inference for batch_size=512:  45%|████▍     | 449/1000 [01:20<01:38,  5.61it/s]Measuring inference for batch_size=512:  45%|████▌     | 450/1000 [01:20<01:37,  5.61it/s]Measuring inference for batch_size=512:  45%|████▌     | 451/1000 [01:20<01:37,  5.61it/s]Measuring inference for batch_size=512:  45%|████▌     | 452/1000 [01:20<01:37,  5.61it/s]Measuring inference for batch_size=512:  45%|████▌     | 453/1000 [01:20<01:37,  5.61it/s]Measuring inference for batch_size=512:  45%|████▌     | 454/1000 [01:20<01:37,  5.61it/s]Measuring inference for batch_size=512:  46%|████▌     | 455/1000 [01:21<01:37,  5.61it/s]Measuring inference for batch_size=512:  46%|████▌     | 456/1000 [01:21<01:37,  5.60it/s]Measuring inference for batch_size=512:  46%|████▌     | 457/1000 [01:21<01:37,  5.60it/s]Measuring inference for batch_size=512:  46%|████▌     | 458/1000 [01:21<01:36,  5.59it/s]Measuring inference for batch_size=512:  46%|████▌     | 459/1000 [01:21<01:36,  5.59it/s]Measuring inference for batch_size=512:  46%|████▌     | 460/1000 [01:22<01:36,  5.58it/s]Measuring inference for batch_size=512:  46%|████▌     | 461/1000 [01:22<01:36,  5.59it/s]Measuring inference for batch_size=512:  46%|████▌     | 462/1000 [01:22<01:36,  5.60it/s]Measuring inference for batch_size=512:  46%|████▋     | 463/1000 [01:22<01:35,  5.60it/s]Measuring inference for batch_size=512:  46%|████▋     | 464/1000 [01:22<01:35,  5.60it/s]Measuring inference for batch_size=512:  46%|████▋     | 465/1000 [01:22<01:35,  5.61it/s]Measuring inference for batch_size=512:  47%|████▋     | 466/1000 [01:23<01:35,  5.60it/s]Measuring inference for batch_size=512:  47%|████▋     | 467/1000 [01:23<01:35,  5.61it/s]Measuring inference for batch_size=512:  47%|████▋     | 468/1000 [01:23<01:34,  5.61it/s]Measuring inference for batch_size=512:  47%|████▋     | 469/1000 [01:23<01:34,  5.61it/s]Measuring inference for batch_size=512:  47%|████▋     | 470/1000 [01:23<01:34,  5.61it/s]Measuring inference for batch_size=512:  47%|████▋     | 471/1000 [01:23<01:34,  5.61it/s]Measuring inference for batch_size=512:  47%|████▋     | 472/1000 [01:24<01:34,  5.60it/s]Measuring inference for batch_size=512:  47%|████▋     | 473/1000 [01:24<01:34,  5.60it/s]Measuring inference for batch_size=512:  47%|████▋     | 474/1000 [01:24<01:33,  5.60it/s]Measuring inference for batch_size=512:  48%|████▊     | 475/1000 [01:24<01:33,  5.60it/s]Measuring inference for batch_size=512:  48%|████▊     | 476/1000 [01:24<01:33,  5.60it/s]Measuring inference for batch_size=512:  48%|████▊     | 477/1000 [01:25<01:33,  5.60it/s]Measuring inference for batch_size=512:  48%|████▊     | 478/1000 [01:25<01:33,  5.60it/s]Measuring inference for batch_size=512:  48%|████▊     | 479/1000 [01:25<01:33,  5.60it/s]Measuring inference for batch_size=512:  48%|████▊     | 480/1000 [01:25<01:32,  5.60it/s]Measuring inference for batch_size=512:  48%|████▊     | 481/1000 [01:25<01:32,  5.60it/s]Measuring inference for batch_size=512:  48%|████▊     | 482/1000 [01:25<01:32,  5.61it/s]Measuring inference for batch_size=512:  48%|████▊     | 483/1000 [01:26<01:32,  5.61it/s]Measuring inference for batch_size=512:  48%|████▊     | 484/1000 [01:26<01:31,  5.61it/s]Measuring inference for batch_size=512:  48%|████▊     | 485/1000 [01:26<01:31,  5.62it/s]Measuring inference for batch_size=512:  49%|████▊     | 486/1000 [01:26<01:31,  5.62it/s]Measuring inference for batch_size=512:  49%|████▊     | 487/1000 [01:26<01:31,  5.62it/s]Measuring inference for batch_size=512:  49%|████▉     | 488/1000 [01:26<01:31,  5.62it/s]Measuring inference for batch_size=512:  49%|████▉     | 489/1000 [01:27<01:31,  5.61it/s]Measuring inference for batch_size=512:  49%|████▉     | 490/1000 [01:27<01:30,  5.61it/s]Measuring inference for batch_size=512:  49%|████▉     | 491/1000 [01:27<01:30,  5.61it/s]Measuring inference for batch_size=512:  49%|████▉     | 492/1000 [01:27<01:30,  5.61it/s]Measuring inference for batch_size=512:  49%|████▉     | 493/1000 [01:27<01:30,  5.61it/s]Measuring inference for batch_size=512:  49%|████▉     | 494/1000 [01:28<01:30,  5.62it/s]Measuring inference for batch_size=512:  50%|████▉     | 495/1000 [01:28<01:29,  5.61it/s]Measuring inference for batch_size=512:  50%|████▉     | 496/1000 [01:28<01:29,  5.61it/s]Measuring inference for batch_size=512:  50%|████▉     | 497/1000 [01:28<01:29,  5.61it/s]Measuring inference for batch_size=512:  50%|████▉     | 498/1000 [01:28<01:29,  5.61it/s]Measuring inference for batch_size=512:  50%|████▉     | 499/1000 [01:28<01:29,  5.61it/s]Measuring inference for batch_size=512:  50%|█████     | 500/1000 [01:29<01:29,  5.61it/s]Measuring inference for batch_size=512:  50%|█████     | 501/1000 [01:29<01:28,  5.61it/s]Measuring inference for batch_size=512:  50%|█████     | 502/1000 [01:29<01:28,  5.61it/s]Measuring inference for batch_size=512:  50%|█████     | 503/1000 [01:29<01:28,  5.61it/s]Measuring inference for batch_size=512:  50%|█████     | 504/1000 [01:29<01:28,  5.61it/s]Measuring inference for batch_size=512:  50%|█████     | 505/1000 [01:30<01:28,  5.61it/s]Measuring inference for batch_size=512:  51%|█████     | 506/1000 [01:30<01:27,  5.62it/s]Measuring inference for batch_size=512:  51%|█████     | 507/1000 [01:30<01:27,  5.61it/s]Measuring inference for batch_size=512:  51%|█████     | 508/1000 [01:30<01:27,  5.61it/s]Measuring inference for batch_size=512:  51%|█████     | 509/1000 [01:30<01:27,  5.61it/s]Measuring inference for batch_size=512:  51%|█████     | 510/1000 [01:30<01:27,  5.61it/s]Measuring inference for batch_size=512:  51%|█████     | 511/1000 [01:31<01:27,  5.61it/s]Measuring inference for batch_size=512:  51%|█████     | 512/1000 [01:31<01:26,  5.61it/s]Measuring inference for batch_size=512:  51%|█████▏    | 513/1000 [01:31<01:26,  5.61it/s]Measuring inference for batch_size=512:  51%|█████▏    | 514/1000 [01:31<01:26,  5.61it/s]Measuring inference for batch_size=512:  52%|█████▏    | 515/1000 [01:31<01:26,  5.61it/s]Measuring inference for batch_size=512:  52%|█████▏    | 516/1000 [01:31<01:26,  5.61it/s]Measuring inference for batch_size=512:  52%|█████▏    | 517/1000 [01:32<01:26,  5.61it/s]Measuring inference for batch_size=512:  52%|█████▏    | 518/1000 [01:32<01:25,  5.61it/s]Measuring inference for batch_size=512:  52%|█████▏    | 519/1000 [01:32<01:25,  5.60it/s]Measuring inference for batch_size=512:  52%|█████▏    | 520/1000 [01:32<01:25,  5.61it/s]Measuring inference for batch_size=512:  52%|█████▏    | 521/1000 [01:32<01:25,  5.61it/s]Measuring inference for batch_size=512:  52%|█████▏    | 522/1000 [01:33<01:25,  5.61it/s]Measuring inference for batch_size=512:  52%|█████▏    | 523/1000 [01:33<01:25,  5.61it/s]Measuring inference for batch_size=512:  52%|█████▏    | 524/1000 [01:33<01:24,  5.61it/s]Measuring inference for batch_size=512:  52%|█████▎    | 525/1000 [01:33<01:24,  5.61it/s]Measuring inference for batch_size=512:  53%|█████▎    | 526/1000 [01:33<01:24,  5.62it/s]Measuring inference for batch_size=512:  53%|█████▎    | 527/1000 [01:33<01:24,  5.61it/s]Measuring inference for batch_size=512:  53%|█████▎    | 528/1000 [01:34<01:24,  5.61it/s]Measuring inference for batch_size=512:  53%|█████▎    | 529/1000 [01:34<01:23,  5.61it/s]Measuring inference for batch_size=512:  53%|█████▎    | 530/1000 [01:34<01:23,  5.61it/s]Measuring inference for batch_size=512:  53%|█████▎    | 531/1000 [01:34<01:23,  5.61it/s]Measuring inference for batch_size=512:  53%|█████▎    | 532/1000 [01:34<01:23,  5.60it/s]Measuring inference for batch_size=512:  53%|█████▎    | 533/1000 [01:35<01:23,  5.61it/s]Measuring inference for batch_size=512:  53%|█████▎    | 534/1000 [01:35<01:23,  5.61it/s]Measuring inference for batch_size=512:  54%|█████▎    | 535/1000 [01:35<01:22,  5.61it/s]Measuring inference for batch_size=512:  54%|█████▎    | 536/1000 [01:35<01:22,  5.61it/s]Measuring inference for batch_size=512:  54%|█████▎    | 537/1000 [01:35<01:22,  5.61it/s]Measuring inference for batch_size=512:  54%|█████▍    | 538/1000 [01:35<01:22,  5.61it/s]Measuring inference for batch_size=512:  54%|█████▍    | 539/1000 [01:36<01:22,  5.61it/s]Measuring inference for batch_size=512:  54%|█████▍    | 540/1000 [01:36<01:21,  5.61it/s]Measuring inference for batch_size=512:  54%|█████▍    | 541/1000 [01:36<01:21,  5.61it/s]Measuring inference for batch_size=512:  54%|█████▍    | 542/1000 [01:36<01:21,  5.61it/s]Measuring inference for batch_size=512:  54%|█████▍    | 543/1000 [01:36<01:21,  5.61it/s]Measuring inference for batch_size=512:  54%|█████▍    | 544/1000 [01:36<01:21,  5.61it/s]Measuring inference for batch_size=512:  55%|█████▍    | 545/1000 [01:37<01:21,  5.61it/s]Measuring inference for batch_size=512:  55%|█████▍    | 546/1000 [01:37<01:20,  5.61it/s]Measuring inference for batch_size=512:  55%|█████▍    | 547/1000 [01:37<01:20,  5.61it/s]Measuring inference for batch_size=512:  55%|█████▍    | 548/1000 [01:37<01:20,  5.62it/s]Measuring inference for batch_size=512:  55%|█████▍    | 549/1000 [01:37<01:20,  5.62it/s]Measuring inference for batch_size=512:  55%|█████▌    | 550/1000 [01:38<01:20,  5.62it/s]Measuring inference for batch_size=512:  55%|█████▌    | 551/1000 [01:38<01:19,  5.62it/s]Measuring inference for batch_size=512:  55%|█████▌    | 552/1000 [01:38<01:19,  5.62it/s]Measuring inference for batch_size=512:  55%|█████▌    | 553/1000 [01:38<01:19,  5.62it/s]Measuring inference for batch_size=512:  55%|█████▌    | 554/1000 [01:38<01:19,  5.61it/s]Measuring inference for batch_size=512:  56%|█████▌    | 555/1000 [01:38<01:19,  5.61it/s]Measuring inference for batch_size=512:  56%|█████▌    | 556/1000 [01:39<01:19,  5.61it/s]Measuring inference for batch_size=512:  56%|█████▌    | 557/1000 [01:39<01:18,  5.62it/s]Measuring inference for batch_size=512:  56%|█████▌    | 558/1000 [01:39<01:18,  5.61it/s]Measuring inference for batch_size=512:  56%|█████▌    | 559/1000 [01:39<01:18,  5.61it/s]Measuring inference for batch_size=512:  56%|█████▌    | 560/1000 [01:39<01:18,  5.61it/s]Measuring inference for batch_size=512:  56%|█████▌    | 561/1000 [01:40<01:18,  5.61it/s]Measuring inference for batch_size=512:  56%|█████▌    | 562/1000 [01:40<01:18,  5.61it/s]Measuring inference for batch_size=512:  56%|█████▋    | 563/1000 [01:40<01:17,  5.61it/s]Measuring inference for batch_size=512:  56%|█████▋    | 564/1000 [01:40<01:17,  5.61it/s]Measuring inference for batch_size=512:  56%|█████▋    | 565/1000 [01:40<01:17,  5.61it/s]Measuring inference for batch_size=512:  57%|█████▋    | 566/1000 [01:40<01:17,  5.61it/s]Measuring inference for batch_size=512:  57%|█████▋    | 567/1000 [01:41<01:17,  5.61it/s]Measuring inference for batch_size=512:  57%|█████▋    | 568/1000 [01:41<01:16,  5.61it/s]Measuring inference for batch_size=512:  57%|█████▋    | 569/1000 [01:41<01:16,  5.61it/s]Measuring inference for batch_size=512:  57%|█████▋    | 570/1000 [01:41<01:16,  5.61it/s]Measuring inference for batch_size=512:  57%|█████▋    | 571/1000 [01:41<01:16,  5.61it/s]Measuring inference for batch_size=512:  57%|█████▋    | 572/1000 [01:41<01:16,  5.61it/s]Measuring inference for batch_size=512:  57%|█████▋    | 573/1000 [01:42<01:16,  5.61it/s]Measuring inference for batch_size=512:  57%|█████▋    | 574/1000 [01:42<01:15,  5.61it/s]Measuring inference for batch_size=512:  57%|█████▊    | 575/1000 [01:42<01:15,  5.61it/s]Measuring inference for batch_size=512:  58%|█████▊    | 576/1000 [01:42<01:15,  5.61it/s]Measuring inference for batch_size=512:  58%|█████▊    | 577/1000 [01:42<01:15,  5.61it/s]Measuring inference for batch_size=512:  58%|█████▊    | 578/1000 [01:43<01:15,  5.61it/s]Measuring inference for batch_size=512:  58%|█████▊    | 579/1000 [01:43<01:14,  5.61it/s]Measuring inference for batch_size=512:  58%|█████▊    | 580/1000 [01:43<01:14,  5.61it/s]Measuring inference for batch_size=512:  58%|█████▊    | 581/1000 [01:43<01:14,  5.61it/s]Measuring inference for batch_size=512:  58%|█████▊    | 582/1000 [01:43<01:14,  5.61it/s]Measuring inference for batch_size=512:  58%|█████▊    | 583/1000 [01:43<01:14,  5.61it/s]Measuring inference for batch_size=512:  58%|█████▊    | 584/1000 [01:44<01:14,  5.61it/s]Measuring inference for batch_size=512:  58%|█████▊    | 585/1000 [01:44<01:14,  5.61it/s]Measuring inference for batch_size=512:  59%|█████▊    | 586/1000 [01:44<01:13,  5.60it/s]Measuring inference for batch_size=512:  59%|█████▊    | 587/1000 [01:44<01:13,  5.60it/s]Measuring inference for batch_size=512:  59%|█████▉    | 588/1000 [01:44<01:13,  5.61it/s]Measuring inference for batch_size=512:  59%|█████▉    | 589/1000 [01:45<01:13,  5.61it/s]Measuring inference for batch_size=512:  59%|█████▉    | 590/1000 [01:45<01:13,  5.60it/s]Measuring inference for batch_size=512:  59%|█████▉    | 591/1000 [01:45<01:12,  5.61it/s]Measuring inference for batch_size=512:  59%|█████▉    | 592/1000 [01:45<01:12,  5.60it/s]Measuring inference for batch_size=512:  59%|█████▉    | 593/1000 [01:45<01:12,  5.61it/s]Measuring inference for batch_size=512:  59%|█████▉    | 594/1000 [01:45<01:12,  5.61it/s]Measuring inference for batch_size=512:  60%|█████▉    | 595/1000 [01:46<01:12,  5.61it/s]Measuring inference for batch_size=512:  60%|█████▉    | 596/1000 [01:46<01:12,  5.61it/s]Measuring inference for batch_size=512:  60%|█████▉    | 597/1000 [01:46<01:11,  5.61it/s]Measuring inference for batch_size=512:  60%|█████▉    | 598/1000 [01:46<01:11,  5.61it/s]Measuring inference for batch_size=512:  60%|█████▉    | 599/1000 [01:46<01:11,  5.61it/s]Measuring inference for batch_size=512:  60%|██████    | 600/1000 [01:46<01:11,  5.61it/s]Measuring inference for batch_size=512:  60%|██████    | 601/1000 [01:47<01:11,  5.61it/s]Measuring inference for batch_size=512:  60%|██████    | 602/1000 [01:47<01:10,  5.61it/s]Measuring inference for batch_size=512:  60%|██████    | 603/1000 [01:47<01:10,  5.61it/s]Measuring inference for batch_size=512:  60%|██████    | 604/1000 [01:47<01:10,  5.61it/s]Measuring inference for batch_size=512:  60%|██████    | 605/1000 [01:47<01:10,  5.61it/s]Measuring inference for batch_size=512:  61%|██████    | 606/1000 [01:48<01:10,  5.61it/s]Measuring inference for batch_size=512:  61%|██████    | 607/1000 [01:48<01:10,  5.61it/s]Measuring inference for batch_size=512:  61%|██████    | 608/1000 [01:48<01:09,  5.61it/s]Measuring inference for batch_size=512:  61%|██████    | 609/1000 [01:48<01:09,  5.61it/s]Measuring inference for batch_size=512:  61%|██████    | 610/1000 [01:48<01:09,  5.61it/s]Measuring inference for batch_size=512:  61%|██████    | 611/1000 [01:48<01:09,  5.61it/s]Measuring inference for batch_size=512:  61%|██████    | 612/1000 [01:49<01:09,  5.61it/s]Measuring inference for batch_size=512:  61%|██████▏   | 613/1000 [01:49<01:08,  5.61it/s]Measuring inference for batch_size=512:  61%|██████▏   | 614/1000 [01:49<01:08,  5.62it/s]Measuring inference for batch_size=512:  62%|██████▏   | 615/1000 [01:49<01:08,  5.61it/s]Measuring inference for batch_size=512:  62%|██████▏   | 616/1000 [01:49<01:08,  5.61it/s]Measuring inference for batch_size=512:  62%|██████▏   | 617/1000 [01:49<01:08,  5.61it/s]Measuring inference for batch_size=512:  62%|██████▏   | 618/1000 [01:50<01:08,  5.62it/s]Measuring inference for batch_size=512:  62%|██████▏   | 619/1000 [01:50<01:07,  5.61it/s]Measuring inference for batch_size=512:  62%|██████▏   | 620/1000 [01:50<01:07,  5.61it/s]Measuring inference for batch_size=512:  62%|██████▏   | 621/1000 [01:50<01:07,  5.62it/s]Measuring inference for batch_size=512:  62%|██████▏   | 622/1000 [01:50<01:07,  5.61it/s]Measuring inference for batch_size=512:  62%|██████▏   | 623/1000 [01:51<01:07,  5.61it/s]Measuring inference for batch_size=512:  62%|██████▏   | 624/1000 [01:51<01:06,  5.61it/s]Measuring inference for batch_size=512:  62%|██████▎   | 625/1000 [01:51<01:06,  5.62it/s]Measuring inference for batch_size=512:  63%|██████▎   | 626/1000 [01:51<01:06,  5.62it/s]Measuring inference for batch_size=512:  63%|██████▎   | 627/1000 [01:51<01:06,  5.61it/s]Measuring inference for batch_size=512:  63%|██████▎   | 628/1000 [01:51<01:06,  5.62it/s]Measuring inference for batch_size=512:  63%|██████▎   | 629/1000 [01:52<01:06,  5.61it/s]Measuring inference for batch_size=512:  63%|██████▎   | 630/1000 [01:52<01:05,  5.61it/s]Measuring inference for batch_size=512:  63%|██████▎   | 631/1000 [01:52<01:05,  5.61it/s]Measuring inference for batch_size=512:  63%|██████▎   | 632/1000 [01:52<01:05,  5.61it/s]Measuring inference for batch_size=512:  63%|██████▎   | 633/1000 [01:52<01:05,  5.60it/s]Measuring inference for batch_size=512:  63%|██████▎   | 634/1000 [01:53<01:05,  5.60it/s]Measuring inference for batch_size=512:  64%|██████▎   | 635/1000 [01:53<01:05,  5.60it/s]Measuring inference for batch_size=512:  64%|██████▎   | 636/1000 [01:53<01:04,  5.61it/s]Measuring inference for batch_size=512:  64%|██████▎   | 637/1000 [01:53<01:04,  5.61it/s]Measuring inference for batch_size=512:  64%|██████▍   | 638/1000 [01:53<01:04,  5.61it/s]Measuring inference for batch_size=512:  64%|██████▍   | 639/1000 [01:53<01:04,  5.61it/s]Measuring inference for batch_size=512:  64%|██████▍   | 640/1000 [01:54<01:04,  5.61it/s]Measuring inference for batch_size=512:  64%|██████▍   | 641/1000 [01:54<01:04,  5.61it/s]Measuring inference for batch_size=512:  64%|██████▍   | 642/1000 [01:54<01:03,  5.61it/s]Measuring inference for batch_size=512:  64%|██████▍   | 643/1000 [01:54<01:03,  5.61it/s]Measuring inference for batch_size=512:  64%|██████▍   | 644/1000 [01:54<01:03,  5.61it/s]Measuring inference for batch_size=512:  64%|██████▍   | 645/1000 [01:54<01:03,  5.61it/s]Measuring inference for batch_size=512:  65%|██████▍   | 646/1000 [01:55<01:03,  5.61it/s]Measuring inference for batch_size=512:  65%|██████▍   | 647/1000 [01:55<01:02,  5.61it/s]Measuring inference for batch_size=512:  65%|██████▍   | 648/1000 [01:55<01:02,  5.61it/s]Measuring inference for batch_size=512:  65%|██████▍   | 649/1000 [01:55<01:02,  5.60it/s]Measuring inference for batch_size=512:  65%|██████▌   | 650/1000 [01:55<01:02,  5.60it/s]Measuring inference for batch_size=512:  65%|██████▌   | 651/1000 [01:56<01:02,  5.60it/s]Measuring inference for batch_size=512:  65%|██████▌   | 652/1000 [01:56<01:02,  5.60it/s]Measuring inference for batch_size=512:  65%|██████▌   | 653/1000 [01:56<01:01,  5.60it/s]Measuring inference for batch_size=512:  65%|██████▌   | 654/1000 [01:56<01:01,  5.61it/s]Measuring inference for batch_size=512:  66%|██████▌   | 655/1000 [01:56<01:01,  5.61it/s]Measuring inference for batch_size=512:  66%|██████▌   | 656/1000 [01:56<01:01,  5.61it/s]Measuring inference for batch_size=512:  66%|██████▌   | 657/1000 [01:57<01:01,  5.61it/s]Measuring inference for batch_size=512:  66%|██████▌   | 658/1000 [01:57<01:00,  5.61it/s]Measuring inference for batch_size=512:  66%|██████▌   | 659/1000 [01:57<01:00,  5.61it/s]Measuring inference for batch_size=512:  66%|██████▌   | 660/1000 [01:57<01:00,  5.61it/s]Measuring inference for batch_size=512:  66%|██████▌   | 661/1000 [01:57<01:00,  5.61it/s]Measuring inference for batch_size=512:  66%|██████▌   | 662/1000 [01:58<01:00,  5.61it/s]Measuring inference for batch_size=512:  66%|██████▋   | 663/1000 [01:58<01:00,  5.61it/s]Measuring inference for batch_size=512:  66%|██████▋   | 664/1000 [01:58<00:59,  5.61it/s]Measuring inference for batch_size=512:  66%|██████▋   | 665/1000 [01:58<00:59,  5.60it/s]Measuring inference for batch_size=512:  67%|██████▋   | 666/1000 [01:58<00:59,  5.61it/s]Measuring inference for batch_size=512:  67%|██████▋   | 667/1000 [01:58<00:59,  5.60it/s]Measuring inference for batch_size=512:  67%|██████▋   | 668/1000 [01:59<00:59,  5.60it/s]Measuring inference for batch_size=512:  67%|██████▋   | 669/1000 [01:59<00:59,  5.60it/s]Measuring inference for batch_size=512:  67%|██████▋   | 670/1000 [01:59<00:58,  5.60it/s]Measuring inference for batch_size=512:  67%|██████▋   | 671/1000 [01:59<00:58,  5.61it/s]Measuring inference for batch_size=512:  67%|██████▋   | 672/1000 [01:59<00:58,  5.61it/s]Measuring inference for batch_size=512:  67%|██████▋   | 673/1000 [01:59<00:58,  5.61it/s]Measuring inference for batch_size=512:  67%|██████▋   | 674/1000 [02:00<00:58,  5.61it/s]Measuring inference for batch_size=512:  68%|██████▊   | 675/1000 [02:00<00:58,  5.60it/s]Measuring inference for batch_size=512:  68%|██████▊   | 676/1000 [02:00<00:57,  5.60it/s]Measuring inference for batch_size=512:  68%|██████▊   | 677/1000 [02:00<00:57,  5.61it/s]Measuring inference for batch_size=512:  68%|██████▊   | 678/1000 [02:00<00:57,  5.61it/s]Measuring inference for batch_size=512:  68%|██████▊   | 679/1000 [02:01<00:57,  5.61it/s]Measuring inference for batch_size=512:  68%|██████▊   | 680/1000 [02:01<00:57,  5.61it/s]Measuring inference for batch_size=512:  68%|██████▊   | 681/1000 [02:01<00:56,  5.61it/s]Measuring inference for batch_size=512:  68%|██████▊   | 682/1000 [02:01<00:56,  5.61it/s]Measuring inference for batch_size=512:  68%|██████▊   | 683/1000 [02:01<00:56,  5.61it/s]Measuring inference for batch_size=512:  68%|██████▊   | 684/1000 [02:01<00:56,  5.61it/s]Measuring inference for batch_size=512:  68%|██████▊   | 685/1000 [02:02<00:56,  5.61it/s]Measuring inference for batch_size=512:  69%|██████▊   | 686/1000 [02:02<00:55,  5.61it/s]Measuring inference for batch_size=512:  69%|██████▊   | 687/1000 [02:02<00:55,  5.61it/s]Measuring inference for batch_size=512:  69%|██████▉   | 688/1000 [02:02<00:55,  5.61it/s]Measuring inference for batch_size=512:  69%|██████▉   | 689/1000 [02:02<00:55,  5.61it/s]Measuring inference for batch_size=512:  69%|██████▉   | 690/1000 [02:03<00:55,  5.61it/s]Measuring inference for batch_size=512:  69%|██████▉   | 691/1000 [02:03<00:55,  5.61it/s]Measuring inference for batch_size=512:  69%|██████▉   | 692/1000 [02:03<00:54,  5.61it/s]Measuring inference for batch_size=512:  69%|██████▉   | 693/1000 [02:03<00:54,  5.61it/s]Measuring inference for batch_size=512:  69%|██████▉   | 694/1000 [02:03<00:54,  5.61it/s]Measuring inference for batch_size=512:  70%|██████▉   | 695/1000 [02:03<00:54,  5.60it/s]Measuring inference for batch_size=512:  70%|██████▉   | 696/1000 [02:04<00:54,  5.60it/s]Measuring inference for batch_size=512:  70%|██████▉   | 697/1000 [02:04<00:54,  5.61it/s]Measuring inference for batch_size=512:  70%|██████▉   | 698/1000 [02:04<00:53,  5.60it/s]Measuring inference for batch_size=512:  70%|██████▉   | 699/1000 [02:04<00:53,  5.60it/s]Measuring inference for batch_size=512:  70%|███████   | 700/1000 [02:04<00:53,  5.60it/s]Measuring inference for batch_size=512:  70%|███████   | 701/1000 [02:04<00:53,  5.60it/s]Measuring inference for batch_size=512:  70%|███████   | 702/1000 [02:05<00:53,  5.60it/s]Measuring inference for batch_size=512:  70%|███████   | 703/1000 [02:05<00:53,  5.60it/s]Measuring inference for batch_size=512:  70%|███████   | 704/1000 [02:05<00:52,  5.61it/s]Measuring inference for batch_size=512:  70%|███████   | 705/1000 [02:05<00:52,  5.61it/s]Measuring inference for batch_size=512:  71%|███████   | 706/1000 [02:05<00:52,  5.61it/s]Measuring inference for batch_size=512:  71%|███████   | 707/1000 [02:06<00:52,  5.61it/s]Measuring inference for batch_size=512:  71%|███████   | 708/1000 [02:06<00:52,  5.61it/s]Measuring inference for batch_size=512:  71%|███████   | 709/1000 [02:06<00:51,  5.61it/s]Measuring inference for batch_size=512:  71%|███████   | 710/1000 [02:06<00:51,  5.60it/s]Measuring inference for batch_size=512:  71%|███████   | 711/1000 [02:06<00:51,  5.61it/s]Measuring inference for batch_size=512:  71%|███████   | 712/1000 [02:06<00:51,  5.61it/s]Measuring inference for batch_size=512:  71%|███████▏  | 713/1000 [02:07<00:51,  5.61it/s]Measuring inference for batch_size=512:  71%|███████▏  | 714/1000 [02:07<00:50,  5.61it/s]Measuring inference for batch_size=512:  72%|███████▏  | 715/1000 [02:07<00:50,  5.61it/s]Measuring inference for batch_size=512:  72%|███████▏  | 716/1000 [02:07<00:50,  5.61it/s]Measuring inference for batch_size=512:  72%|███████▏  | 717/1000 [02:07<00:50,  5.61it/s]Measuring inference for batch_size=512:  72%|███████▏  | 718/1000 [02:08<00:50,  5.61it/s]Measuring inference for batch_size=512:  72%|███████▏  | 719/1000 [02:08<00:50,  5.61it/s]Measuring inference for batch_size=512:  72%|███████▏  | 720/1000 [02:08<00:49,  5.60it/s]Measuring inference for batch_size=512:  72%|███████▏  | 721/1000 [02:08<00:49,  5.60it/s]Measuring inference for batch_size=512:  72%|███████▏  | 722/1000 [02:08<00:49,  5.60it/s]Measuring inference for batch_size=512:  72%|███████▏  | 723/1000 [02:08<00:49,  5.60it/s]Measuring inference for batch_size=512:  72%|███████▏  | 724/1000 [02:09<00:49,  5.60it/s]Measuring inference for batch_size=512:  72%|███████▎  | 725/1000 [02:09<00:49,  5.60it/s]Measuring inference for batch_size=512:  73%|███████▎  | 726/1000 [02:09<00:48,  5.60it/s]Measuring inference for batch_size=512:  73%|███████▎  | 727/1000 [02:09<00:48,  5.60it/s]Measuring inference for batch_size=512:  73%|███████▎  | 728/1000 [02:09<00:48,  5.61it/s]Measuring inference for batch_size=512:  73%|███████▎  | 729/1000 [02:09<00:48,  5.61it/s]Measuring inference for batch_size=512:  73%|███████▎  | 730/1000 [02:10<00:48,  5.61it/s]Measuring inference for batch_size=512:  73%|███████▎  | 731/1000 [02:10<00:47,  5.61it/s]Measuring inference for batch_size=512:  73%|███████▎  | 732/1000 [02:10<00:47,  5.61it/s]Measuring inference for batch_size=512:  73%|███████▎  | 733/1000 [02:10<00:47,  5.60it/s]Measuring inference for batch_size=512:  73%|███████▎  | 734/1000 [02:10<00:47,  5.61it/s]Measuring inference for batch_size=512:  74%|███████▎  | 735/1000 [02:11<00:47,  5.61it/s]Measuring inference for batch_size=512:  74%|███████▎  | 736/1000 [02:11<00:47,  5.61it/s]Measuring inference for batch_size=512:  74%|███████▎  | 737/1000 [02:11<00:46,  5.61it/s]Measuring inference for batch_size=512:  74%|███████▍  | 738/1000 [02:11<00:46,  5.61it/s]Measuring inference for batch_size=512:  74%|███████▍  | 739/1000 [02:11<00:46,  5.61it/s]Measuring inference for batch_size=512:  74%|███████▍  | 740/1000 [02:11<00:46,  5.61it/s]Measuring inference for batch_size=512:  74%|███████▍  | 741/1000 [02:12<00:46,  5.61it/s]Measuring inference for batch_size=512:  74%|███████▍  | 742/1000 [02:12<00:46,  5.60it/s]Measuring inference for batch_size=512:  74%|███████▍  | 743/1000 [02:12<00:45,  5.60it/s]Measuring inference for batch_size=512:  74%|███████▍  | 744/1000 [02:12<00:45,  5.61it/s]Measuring inference for batch_size=512:  74%|███████▍  | 745/1000 [02:12<00:45,  5.60it/s]Measuring inference for batch_size=512:  75%|███████▍  | 746/1000 [02:12<00:45,  5.60it/s]Measuring inference for batch_size=512:  75%|███████▍  | 747/1000 [02:13<00:45,  5.60it/s]Measuring inference for batch_size=512:  75%|███████▍  | 748/1000 [02:13<00:44,  5.61it/s]Measuring inference for batch_size=512:  75%|███████▍  | 749/1000 [02:13<00:44,  5.61it/s]Measuring inference for batch_size=512:  75%|███████▌  | 750/1000 [02:13<00:44,  5.61it/s]Measuring inference for batch_size=512:  75%|███████▌  | 751/1000 [02:13<00:44,  5.61it/s]Measuring inference for batch_size=512:  75%|███████▌  | 752/1000 [02:14<00:44,  5.61it/s]Measuring inference for batch_size=512:  75%|███████▌  | 753/1000 [02:14<00:44,  5.61it/s]Measuring inference for batch_size=512:  75%|███████▌  | 754/1000 [02:14<00:43,  5.61it/s]Measuring inference for batch_size=512:  76%|███████▌  | 755/1000 [02:14<00:43,  5.60it/s]Measuring inference for batch_size=512:  76%|███████▌  | 756/1000 [02:14<00:43,  5.61it/s]Measuring inference for batch_size=512:  76%|███████▌  | 757/1000 [02:14<00:43,  5.61it/s]Measuring inference for batch_size=512:  76%|███████▌  | 758/1000 [02:15<00:43,  5.61it/s]Measuring inference for batch_size=512:  76%|███████▌  | 759/1000 [02:15<00:42,  5.61it/s]Measuring inference for batch_size=512:  76%|███████▌  | 760/1000 [02:15<00:42,  5.61it/s]Measuring inference for batch_size=512:  76%|███████▌  | 761/1000 [02:15<00:42,  5.59it/s]Measuring inference for batch_size=512:  76%|███████▌  | 762/1000 [02:15<00:42,  5.59it/s]Measuring inference for batch_size=512:  76%|███████▋  | 763/1000 [02:16<00:42,  5.60it/s]Measuring inference for batch_size=512:  76%|███████▋  | 764/1000 [02:16<00:42,  5.59it/s]Measuring inference for batch_size=512:  76%|███████▋  | 765/1000 [02:16<00:41,  5.60it/s]Measuring inference for batch_size=512:  77%|███████▋  | 766/1000 [02:16<00:41,  5.60it/s]Measuring inference for batch_size=512:  77%|███████▋  | 767/1000 [02:16<00:41,  5.61it/s]Measuring inference for batch_size=512:  77%|███████▋  | 768/1000 [02:16<00:41,  5.60it/s]Measuring inference for batch_size=512:  77%|███████▋  | 769/1000 [02:17<00:41,  5.61it/s]Measuring inference for batch_size=512:  77%|███████▋  | 770/1000 [02:17<00:41,  5.61it/s]Measuring inference for batch_size=512:  77%|███████▋  | 771/1000 [02:17<00:40,  5.60it/s]Measuring inference for batch_size=512:  77%|███████▋  | 772/1000 [02:17<00:40,  5.60it/s]Measuring inference for batch_size=512:  77%|███████▋  | 773/1000 [02:17<00:40,  5.60it/s]Measuring inference for batch_size=512:  77%|███████▋  | 774/1000 [02:17<00:40,  5.60it/s]Measuring inference for batch_size=512:  78%|███████▊  | 775/1000 [02:18<00:40,  5.60it/s]Measuring inference for batch_size=512:  78%|███████▊  | 776/1000 [02:18<00:39,  5.60it/s]Measuring inference for batch_size=512:  78%|███████▊  | 777/1000 [02:18<00:39,  5.60it/s]Measuring inference for batch_size=512:  78%|███████▊  | 778/1000 [02:18<00:39,  5.60it/s]Measuring inference for batch_size=512:  78%|███████▊  | 779/1000 [02:18<00:39,  5.60it/s]Measuring inference for batch_size=512:  78%|███████▊  | 780/1000 [02:19<00:39,  5.60it/s]Measuring inference for batch_size=512:  78%|███████▊  | 781/1000 [02:19<00:39,  5.60it/s]Measuring inference for batch_size=512:  78%|███████▊  | 782/1000 [02:19<00:38,  5.61it/s]Measuring inference for batch_size=512:  78%|███████▊  | 783/1000 [02:19<00:38,  5.61it/s]Measuring inference for batch_size=512:  78%|███████▊  | 784/1000 [02:19<00:38,  5.60it/s]Measuring inference for batch_size=512:  78%|███████▊  | 785/1000 [02:19<00:38,  5.60it/s]Measuring inference for batch_size=512:  79%|███████▊  | 786/1000 [02:20<00:38,  5.60it/s]Measuring inference for batch_size=512:  79%|███████▊  | 787/1000 [02:20<00:38,  5.59it/s]Measuring inference for batch_size=512:  79%|███████▉  | 788/1000 [02:20<00:37,  5.60it/s]Measuring inference for batch_size=512:  79%|███████▉  | 789/1000 [02:20<00:37,  5.60it/s]Measuring inference for batch_size=512:  79%|███████▉  | 790/1000 [02:20<00:37,  5.59it/s]Measuring inference for batch_size=512:  79%|███████▉  | 791/1000 [02:21<00:37,  5.60it/s]Measuring inference for batch_size=512:  79%|███████▉  | 792/1000 [02:21<00:37,  5.60it/s]Measuring inference for batch_size=512:  79%|███████▉  | 793/1000 [02:21<00:36,  5.59it/s]Measuring inference for batch_size=512:  79%|███████▉  | 794/1000 [02:21<00:36,  5.60it/s]Measuring inference for batch_size=512:  80%|███████▉  | 795/1000 [02:21<00:36,  5.61it/s]Measuring inference for batch_size=512:  80%|███████▉  | 796/1000 [02:21<00:36,  5.61it/s]Measuring inference for batch_size=512:  80%|███████▉  | 797/1000 [02:22<00:36,  5.61it/s]Measuring inference for batch_size=512:  80%|███████▉  | 798/1000 [02:22<00:36,  5.61it/s]Measuring inference for batch_size=512:  80%|███████▉  | 799/1000 [02:22<00:35,  5.61it/s]Measuring inference for batch_size=512:  80%|████████  | 800/1000 [02:22<00:35,  5.61it/s]Measuring inference for batch_size=512:  80%|████████  | 801/1000 [02:22<00:35,  5.61it/s]Measuring inference for batch_size=512:  80%|████████  | 802/1000 [02:22<00:35,  5.60it/s]Measuring inference for batch_size=512:  80%|████████  | 803/1000 [02:23<00:35,  5.60it/s]Measuring inference for batch_size=512:  80%|████████  | 804/1000 [02:23<00:34,  5.61it/s]Measuring inference for batch_size=512:  80%|████████  | 805/1000 [02:23<00:34,  5.61it/s]Measuring inference for batch_size=512:  81%|████████  | 806/1000 [02:23<00:34,  5.61it/s]Measuring inference for batch_size=512:  81%|████████  | 807/1000 [02:23<00:34,  5.61it/s]Measuring inference for batch_size=512:  81%|████████  | 808/1000 [02:24<00:34,  5.61it/s]Measuring inference for batch_size=512:  81%|████████  | 809/1000 [02:24<00:34,  5.61it/s]Measuring inference for batch_size=512:  81%|████████  | 810/1000 [02:24<00:33,  5.61it/s]Measuring inference for batch_size=512:  81%|████████  | 811/1000 [02:24<00:33,  5.61it/s]Measuring inference for batch_size=512:  81%|████████  | 812/1000 [02:24<00:33,  5.62it/s]Measuring inference for batch_size=512:  81%|████████▏ | 813/1000 [02:24<00:33,  5.61it/s]Measuring inference for batch_size=512:  81%|████████▏ | 814/1000 [02:25<00:33,  5.60it/s]Measuring inference for batch_size=512:  82%|████████▏ | 815/1000 [02:25<00:32,  5.61it/s]Measuring inference for batch_size=512:  82%|████████▏ | 816/1000 [02:25<00:32,  5.60it/s]Measuring inference for batch_size=512:  82%|████████▏ | 817/1000 [02:25<00:32,  5.60it/s]Measuring inference for batch_size=512:  82%|████████▏ | 818/1000 [02:25<00:32,  5.61it/s]Measuring inference for batch_size=512:  82%|████████▏ | 819/1000 [02:26<00:32,  5.61it/s]Measuring inference for batch_size=512:  82%|████████▏ | 820/1000 [02:26<00:32,  5.61it/s]Measuring inference for batch_size=512:  82%|████████▏ | 821/1000 [02:26<00:31,  5.61it/s]Measuring inference for batch_size=512:  82%|████████▏ | 822/1000 [02:26<00:31,  5.62it/s]Measuring inference for batch_size=512:  82%|████████▏ | 823/1000 [02:26<00:31,  5.61it/s]Measuring inference for batch_size=512:  82%|████████▏ | 824/1000 [02:26<00:31,  5.61it/s]Measuring inference for batch_size=512:  82%|████████▎ | 825/1000 [02:27<00:31,  5.61it/s]Measuring inference for batch_size=512:  83%|████████▎ | 826/1000 [02:27<00:30,  5.61it/s]Measuring inference for batch_size=512:  83%|████████▎ | 827/1000 [02:27<00:30,  5.61it/s]Measuring inference for batch_size=512:  83%|████████▎ | 828/1000 [02:27<00:30,  5.62it/s]Measuring inference for batch_size=512:  83%|████████▎ | 829/1000 [02:27<00:30,  5.62it/s]Measuring inference for batch_size=512:  83%|████████▎ | 830/1000 [02:27<00:30,  5.61it/s]Measuring inference for batch_size=512:  83%|████████▎ | 831/1000 [02:28<00:30,  5.62it/s]Measuring inference for batch_size=512:  83%|████████▎ | 832/1000 [02:28<00:29,  5.62it/s]Measuring inference for batch_size=512:  83%|████████▎ | 833/1000 [02:28<00:29,  5.62it/s]Measuring inference for batch_size=512:  83%|████████▎ | 834/1000 [02:28<00:29,  5.62it/s]Measuring inference for batch_size=512:  84%|████████▎ | 835/1000 [02:28<00:29,  5.62it/s]Measuring inference for batch_size=512:  84%|████████▎ | 836/1000 [02:29<00:29,  5.61it/s]Measuring inference for batch_size=512:  84%|████████▎ | 837/1000 [02:29<00:29,  5.60it/s]Measuring inference for batch_size=512:  84%|████████▍ | 838/1000 [02:29<00:28,  5.61it/s]Measuring inference for batch_size=512:  84%|████████▍ | 839/1000 [02:29<00:28,  5.61it/s]Measuring inference for batch_size=512:  84%|████████▍ | 840/1000 [02:29<00:28,  5.61it/s]Measuring inference for batch_size=512:  84%|████████▍ | 841/1000 [02:29<00:28,  5.62it/s]Measuring inference for batch_size=512:  84%|████████▍ | 842/1000 [02:30<00:28,  5.61it/s]Measuring inference for batch_size=512:  84%|████████▍ | 843/1000 [02:30<00:27,  5.61it/s]Measuring inference for batch_size=512:  84%|████████▍ | 844/1000 [02:30<00:27,  5.61it/s]Measuring inference for batch_size=512:  84%|████████▍ | 845/1000 [02:30<00:27,  5.61it/s]Measuring inference for batch_size=512:  85%|████████▍ | 846/1000 [02:30<00:27,  5.61it/s]Measuring inference for batch_size=512:  85%|████████▍ | 847/1000 [02:31<00:27,  5.61it/s]Measuring inference for batch_size=512:  85%|████████▍ | 848/1000 [02:31<00:27,  5.61it/s]Measuring inference for batch_size=512:  85%|████████▍ | 849/1000 [02:31<00:26,  5.61it/s]Measuring inference for batch_size=512:  85%|████████▌ | 850/1000 [02:31<00:26,  5.61it/s]Measuring inference for batch_size=512:  85%|████████▌ | 851/1000 [02:31<00:26,  5.60it/s]Measuring inference for batch_size=512:  85%|████████▌ | 852/1000 [02:31<00:26,  5.60it/s]Measuring inference for batch_size=512:  85%|████████▌ | 853/1000 [02:32<00:26,  5.60it/s]Measuring inference for batch_size=512:  85%|████████▌ | 854/1000 [02:32<00:26,  5.61it/s]Measuring inference for batch_size=512:  86%|████████▌ | 855/1000 [02:32<00:25,  5.60it/s]Measuring inference for batch_size=512:  86%|████████▌ | 856/1000 [02:32<00:25,  5.60it/s]Measuring inference for batch_size=512:  86%|████████▌ | 857/1000 [02:32<00:25,  5.59it/s]Measuring inference for batch_size=512:  86%|████████▌ | 858/1000 [02:32<00:25,  5.60it/s]Measuring inference for batch_size=512:  86%|████████▌ | 859/1000 [02:33<00:25,  5.59it/s]Measuring inference for batch_size=512:  86%|████████▌ | 860/1000 [02:33<00:25,  5.60it/s]Measuring inference for batch_size=512:  86%|████████▌ | 861/1000 [02:33<00:24,  5.60it/s]Measuring inference for batch_size=512:  86%|████████▌ | 862/1000 [02:33<00:24,  5.60it/s]Measuring inference for batch_size=512:  86%|████████▋ | 863/1000 [02:33<00:24,  5.61it/s]Measuring inference for batch_size=512:  86%|████████▋ | 864/1000 [02:34<00:24,  5.61it/s]Measuring inference for batch_size=512:  86%|████████▋ | 865/1000 [02:34<00:24,  5.61it/s]Measuring inference for batch_size=512:  87%|████████▋ | 866/1000 [02:34<00:23,  5.61it/s]Measuring inference for batch_size=512:  87%|████████▋ | 867/1000 [02:34<00:23,  5.61it/s]Measuring inference for batch_size=512:  87%|████████▋ | 868/1000 [02:34<00:23,  5.61it/s]Measuring inference for batch_size=512:  87%|████████▋ | 869/1000 [02:34<00:23,  5.61it/s]Measuring inference for batch_size=512:  87%|████████▋ | 870/1000 [02:35<00:23,  5.61it/s]Measuring inference for batch_size=512:  87%|████████▋ | 871/1000 [02:35<00:23,  5.60it/s]Measuring inference for batch_size=512:  87%|████████▋ | 872/1000 [02:35<00:22,  5.60it/s]Measuring inference for batch_size=512:  87%|████████▋ | 873/1000 [02:35<00:22,  5.60it/s]Measuring inference for batch_size=512:  87%|████████▋ | 874/1000 [02:35<00:22,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 875/1000 [02:36<00:22,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 876/1000 [02:36<00:22,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 877/1000 [02:36<00:21,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 878/1000 [02:36<00:21,  5.61it/s]Measuring inference for batch_size=512:  88%|████████▊ | 879/1000 [02:36<00:21,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 880/1000 [02:36<00:21,  5.59it/s]Measuring inference for batch_size=512:  88%|████████▊ | 881/1000 [02:37<00:21,  5.59it/s]Measuring inference for batch_size=512:  88%|████████▊ | 882/1000 [02:37<00:21,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 883/1000 [02:37<00:20,  5.61it/s]Measuring inference for batch_size=512:  88%|████████▊ | 884/1000 [02:37<00:20,  5.60it/s]Measuring inference for batch_size=512:  88%|████████▊ | 885/1000 [02:37<00:20,  5.61it/s]Measuring inference for batch_size=512:  89%|████████▊ | 886/1000 [02:37<00:20,  5.61it/s]Measuring inference for batch_size=512:  89%|████████▊ | 887/1000 [02:38<00:20,  5.62it/s]Measuring inference for batch_size=512:  89%|████████▉ | 888/1000 [02:38<00:19,  5.62it/s]Measuring inference for batch_size=512:  89%|████████▉ | 889/1000 [02:38<00:19,  5.61it/s]Measuring inference for batch_size=512:  89%|████████▉ | 890/1000 [02:38<00:19,  5.61it/s]Measuring inference for batch_size=512:  89%|████████▉ | 891/1000 [02:38<00:19,  5.60it/s]Measuring inference for batch_size=512:  89%|████████▉ | 892/1000 [02:39<00:19,  5.60it/s]Measuring inference for batch_size=512:  89%|████████▉ | 893/1000 [02:39<00:19,  5.61it/s]Measuring inference for batch_size=512:  89%|████████▉ | 894/1000 [02:39<00:18,  5.61it/s]Measuring inference for batch_size=512:  90%|████████▉ | 895/1000 [02:39<00:18,  5.61it/s]Measuring inference for batch_size=512:  90%|████████▉ | 896/1000 [02:39<00:18,  5.61it/s]Measuring inference for batch_size=512:  90%|████████▉ | 897/1000 [02:39<00:18,  5.62it/s]Measuring inference for batch_size=512:  90%|████████▉ | 898/1000 [02:40<00:18,  5.61it/s]Measuring inference for batch_size=512:  90%|████████▉ | 899/1000 [02:40<00:17,  5.62it/s]Measuring inference for batch_size=512:  90%|█████████ | 900/1000 [02:40<00:17,  5.62it/s]Measuring inference for batch_size=512:  90%|█████████ | 901/1000 [02:40<00:17,  5.62it/s]Measuring inference for batch_size=512:  90%|█████████ | 902/1000 [02:40<00:17,  5.62it/s]Measuring inference for batch_size=512:  90%|█████████ | 903/1000 [02:41<00:17,  5.62it/s]Measuring inference for batch_size=512:  90%|█████████ | 904/1000 [02:41<00:17,  5.62it/s]Measuring inference for batch_size=512:  90%|█████████ | 905/1000 [02:41<00:16,  5.62it/s]Measuring inference for batch_size=512:  91%|█████████ | 906/1000 [02:41<00:16,  5.61it/s]Measuring inference for batch_size=512:  91%|█████████ | 907/1000 [02:41<00:16,  5.60it/s]Measuring inference for batch_size=512:  91%|█████████ | 908/1000 [02:41<00:16,  5.60it/s]Measuring inference for batch_size=512:  91%|█████████ | 909/1000 [02:42<00:16,  5.60it/s]Measuring inference for batch_size=512:  91%|█████████ | 910/1000 [02:42<00:16,  5.61it/s]Measuring inference for batch_size=512:  91%|█████████ | 911/1000 [02:42<00:15,  5.60it/s]Measuring inference for batch_size=512:  91%|█████████ | 912/1000 [02:42<00:15,  5.60it/s]Measuring inference for batch_size=512:  91%|█████████▏| 913/1000 [02:42<00:15,  5.60it/s]Measuring inference for batch_size=512:  91%|█████████▏| 914/1000 [02:42<00:15,  5.60it/s]Measuring inference for batch_size=512:  92%|█████████▏| 915/1000 [02:43<00:15,  5.60it/s]Measuring inference for batch_size=512:  92%|█████████▏| 916/1000 [02:43<00:14,  5.60it/s]Measuring inference for batch_size=512:  92%|█████████▏| 917/1000 [02:43<00:14,  5.60it/s]Measuring inference for batch_size=512:  92%|█████████▏| 918/1000 [02:43<00:14,  5.60it/s]Measuring inference for batch_size=512:  92%|█████████▏| 919/1000 [02:43<00:14,  5.60it/s]Measuring inference for batch_size=512:  92%|█████████▏| 920/1000 [02:44<00:14,  5.61it/s]Measuring inference for batch_size=512:  92%|█████████▏| 921/1000 [02:44<00:14,  5.61it/s]Measuring inference for batch_size=512:  92%|█████████▏| 922/1000 [02:44<00:13,  5.61it/s]Measuring inference for batch_size=512:  92%|█████████▏| 923/1000 [02:44<00:13,  5.61it/s]Measuring inference for batch_size=512:  92%|█████████▏| 924/1000 [02:44<00:13,  5.62it/s]Measuring inference for batch_size=512:  92%|█████████▎| 925/1000 [02:44<00:13,  5.62it/s]Measuring inference for batch_size=512:  93%|█████████▎| 926/1000 [02:45<00:13,  5.62it/s]Measuring inference for batch_size=512:  93%|█████████▎| 927/1000 [02:45<00:12,  5.62it/s]Measuring inference for batch_size=512:  93%|█████████▎| 928/1000 [02:45<00:12,  5.61it/s]Measuring inference for batch_size=512:  93%|█████████▎| 929/1000 [02:45<00:12,  5.61it/s]Measuring inference for batch_size=512:  93%|█████████▎| 930/1000 [02:45<00:12,  5.61it/s]Measuring inference for batch_size=512:  93%|█████████▎| 931/1000 [02:45<00:12,  5.61it/s]Measuring inference for batch_size=512:  93%|█████████▎| 932/1000 [02:46<00:12,  5.61it/s]Measuring inference for batch_size=512:  93%|█████████▎| 933/1000 [02:46<00:11,  5.62it/s]Measuring inference for batch_size=512:  93%|█████████▎| 934/1000 [02:46<00:11,  5.62it/s]Measuring inference for batch_size=512:  94%|█████████▎| 935/1000 [02:46<00:11,  5.62it/s]Measuring inference for batch_size=512:  94%|█████████▎| 936/1000 [02:46<00:11,  5.62it/s]Measuring inference for batch_size=512:  94%|█████████▎| 937/1000 [02:47<00:11,  5.61it/s]Measuring inference for batch_size=512:  94%|█████████▍| 938/1000 [02:47<00:11,  5.61it/s]Measuring inference for batch_size=512:  94%|█████████▍| 939/1000 [02:47<00:10,  5.61it/s]Measuring inference for batch_size=512:  94%|█████████▍| 940/1000 [02:47<00:10,  5.61it/s]Measuring inference for batch_size=512:  94%|█████████▍| 941/1000 [02:47<00:10,  5.61it/s]Measuring inference for batch_size=512:  94%|█████████▍| 942/1000 [02:47<00:10,  5.61it/s]Measuring inference for batch_size=512:  94%|█████████▍| 943/1000 [02:48<00:10,  5.61it/s]Measuring inference for batch_size=512:  94%|█████████▍| 944/1000 [02:48<00:09,  5.61it/s]Measuring inference for batch_size=512:  94%|█████████▍| 945/1000 [02:48<00:09,  5.61it/s]Measuring inference for batch_size=512:  95%|█████████▍| 946/1000 [02:48<00:09,  5.61it/s]Measuring inference for batch_size=512:  95%|█████████▍| 947/1000 [02:48<00:09,  5.60it/s]Measuring inference for batch_size=512:  95%|█████████▍| 948/1000 [02:49<00:09,  5.61it/s]Measuring inference for batch_size=512:  95%|█████████▍| 949/1000 [02:49<00:09,  5.61it/s]Measuring inference for batch_size=512:  95%|█████████▌| 950/1000 [02:49<00:08,  5.60it/s]Measuring inference for batch_size=512:  95%|█████████▌| 951/1000 [02:49<00:08,  5.60it/s]Measuring inference for batch_size=512:  95%|█████████▌| 952/1000 [02:49<00:08,  5.60it/s]Measuring inference for batch_size=512:  95%|█████████▌| 953/1000 [02:49<00:08,  5.60it/s]Measuring inference for batch_size=512:  95%|█████████▌| 954/1000 [02:50<00:08,  5.61it/s]Measuring inference for batch_size=512:  96%|█████████▌| 955/1000 [02:50<00:08,  5.61it/s]Measuring inference for batch_size=512:  96%|█████████▌| 956/1000 [02:50<00:07,  5.61it/s]Measuring inference for batch_size=512:  96%|█████████▌| 957/1000 [02:50<00:07,  5.61it/s]Measuring inference for batch_size=512:  96%|█████████▌| 958/1000 [02:50<00:07,  5.61it/s]Measuring inference for batch_size=512:  96%|█████████▌| 959/1000 [02:50<00:07,  5.61it/s]Measuring inference for batch_size=512:  96%|█████████▌| 960/1000 [02:51<00:07,  5.61it/s]Measuring inference for batch_size=512:  96%|█████████▌| 961/1000 [02:51<00:06,  5.61it/s]Measuring inference for batch_size=512:  96%|█████████▌| 962/1000 [02:51<00:06,  5.61it/s]Measuring inference for batch_size=512:  96%|█████████▋| 963/1000 [02:51<00:06,  5.61it/s]Measuring inference for batch_size=512:  96%|█████████▋| 964/1000 [02:51<00:06,  5.61it/s]Measuring inference for batch_size=512:  96%|█████████▋| 965/1000 [02:52<00:06,  5.61it/s]Measuring inference for batch_size=512:  97%|█████████▋| 966/1000 [02:52<00:06,  5.61it/s]Measuring inference for batch_size=512:  97%|█████████▋| 967/1000 [02:52<00:05,  5.61it/s]Measuring inference for batch_size=512:  97%|█████████▋| 968/1000 [02:52<00:05,  5.60it/s]Measuring inference for batch_size=512:  97%|█████████▋| 969/1000 [02:52<00:05,  5.60it/s]Measuring inference for batch_size=512:  97%|█████████▋| 970/1000 [02:52<00:05,  5.60it/s]Measuring inference for batch_size=512:  97%|█████████▋| 971/1000 [02:53<00:05,  5.60it/s]Measuring inference for batch_size=512:  97%|█████████▋| 972/1000 [02:53<00:04,  5.60it/s]Measuring inference for batch_size=512:  97%|█████████▋| 973/1000 [02:53<00:04,  5.60it/s]Measuring inference for batch_size=512:  97%|█████████▋| 974/1000 [02:53<00:04,  5.60it/s]Measuring inference for batch_size=512:  98%|█████████▊| 975/1000 [02:53<00:04,  5.60it/s]Measuring inference for batch_size=512:  98%|█████████▊| 976/1000 [02:54<00:04,  5.60it/s]Measuring inference for batch_size=512:  98%|█████████▊| 977/1000 [02:54<00:04,  5.60it/s]Measuring inference for batch_size=512:  98%|█████████▊| 978/1000 [02:54<00:03,  5.60it/s]Measuring inference for batch_size=512:  98%|█████████▊| 979/1000 [02:54<00:03,  5.60it/s]Measuring inference for batch_size=512:  98%|█████████▊| 980/1000 [02:54<00:03,  5.60it/s]Measuring inference for batch_size=512:  98%|█████████▊| 981/1000 [02:54<00:03,  5.60it/s]Measuring inference for batch_size=512:  98%|█████████▊| 982/1000 [02:55<00:03,  5.60it/s]Measuring inference for batch_size=512:  98%|█████████▊| 983/1000 [02:55<00:03,  5.61it/s]Measuring inference for batch_size=512:  98%|█████████▊| 984/1000 [02:55<00:02,  5.60it/s]Measuring inference for batch_size=512:  98%|█████████▊| 985/1000 [02:55<00:02,  5.60it/s]Measuring inference for batch_size=512:  99%|█████████▊| 986/1000 [02:55<00:02,  5.60it/s]Measuring inference for batch_size=512:  99%|█████████▊| 987/1000 [02:55<00:02,  5.60it/s]Measuring inference for batch_size=512:  99%|█████████▉| 988/1000 [02:56<00:02,  5.60it/s]Measuring inference for batch_size=512:  99%|█████████▉| 989/1000 [02:56<00:01,  5.60it/s]Measuring inference for batch_size=512:  99%|█████████▉| 990/1000 [02:56<00:01,  5.60it/s]Measuring inference for batch_size=512:  99%|█████████▉| 991/1000 [02:56<00:01,  5.60it/s]Measuring inference for batch_size=512:  99%|█████████▉| 992/1000 [02:56<00:01,  5.60it/s]Measuring inference for batch_size=512:  99%|█████████▉| 993/1000 [02:57<00:01,  5.61it/s]Measuring inference for batch_size=512:  99%|█████████▉| 994/1000 [02:57<00:01,  5.61it/s]Measuring inference for batch_size=512: 100%|█████████▉| 995/1000 [02:57<00:00,  5.61it/s]Measuring inference for batch_size=512: 100%|█████████▉| 996/1000 [02:57<00:00,  5.61it/s]Measuring inference for batch_size=512: 100%|█████████▉| 997/1000 [02:57<00:00,  5.60it/s]Measuring inference for batch_size=512: 100%|█████████▉| 998/1000 [02:57<00:00,  5.61it/s]Measuring inference for batch_size=512: 100%|█████████▉| 999/1000 [02:58<00:00,  5.61it/s]Measuring inference for batch_size=512: 100%|██████████| 1000/1000 [02:58<00:00,  5.60it/s]Measuring inference for batch_size=512: 100%|██████████| 1000/1000 [02:58<00:00,  5.61it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cpu
flops: 12310546408
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.21 GB
    total: 31.28 GB
    used: 3.61 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
params: 118515272
timing:
  batch_size_1:
    on_device_inference:
      human_readable:
        batch_latency: 136.713 ms +/- 255.541 us [136.234 ms, 138.504 ms]
        batches_per_second: 7.31 +/- 0.01 [7.22, 7.34]
      metrics:
        batches_per_second_max: 7.340321942716737
        batches_per_second_mean: 7.314592480614995
        batches_per_second_min: 7.2200314670026815
        batches_per_second_std: 0.013625015619030312
        seconds_per_batch_max: 0.1385035514831543
        seconds_per_batch_mean: 0.13671349215507508
        seconds_per_batch_min: 0.13623380661010742
        seconds_per_batch_std: 0.0002555410769323556
  batch_size_512:
    on_device_inference:
      human_readable:
        batch_latency: 178.168 ms +/- 349.483 us [177.210 ms, 180.538 ms]
        batches_per_second: 5.61 +/- 0.01 [5.54, 5.64]
      metrics:
        batches_per_second_max: 5.6430118637272395
        batches_per_second_mean: 5.612702194674701
        batches_per_second_min: 5.538994654214297
        batches_per_second_std: 0.010988376199649574
        seconds_per_batch_max: 0.18053817749023438
        seconds_per_batch_mean: 0.1781679849624634
        seconds_per_batch_min: 0.17721033096313477
        seconds_per_batch_std: 0.0003494833056702305


#####
baseline-baseline-py-id - Run 3
2024-02-23 09:52:21
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.89it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.89it/s]
Warning: module SiLU is treated as a zero-op.
Warning: module Conv2dNormActivation is treated as a zero-op.
Warning: module StochasticDepth is treated as a zero-op.
Warning: module FusedMBConv is treated as a zero-op.
Warning: module Sigmoid is treated as a zero-op.
Warning: module SqueezeExcitation is treated as a zero-op.
Warning: module MBConv is treated as a zero-op.
Warning: module Dropout is treated as a zero-op.
Warning: module EfficientNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
EfficientNet(
  118.52 M, 100.000% Params, 12.31 GMac, 100.000% MACs, 
  (features): Sequential(
    117.23 M, 98.919% Params, 12.31 GMac, 99.989% MACs, 
    (0): Conv2dNormActivation(
      928, 0.001% Params, 11.64 MMac, 0.095% MACs, 
      (0): Conv2d(864, 0.001% Params, 10.84 MMac, 0.088% MACs, 3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, 0.000% Params, 802.82 KMac, 0.007% MACs, 32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
    )
    (1): Sequential(
      37.12 k, 0.031% Params, 465.63 MMac, 3.782% MACs, 
      (0): FusedMBConv(
        9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
        (block): Sequential(
          9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
          (0): Conv2dNormActivation(
            9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
            (0): Conv2d(9.22 k, 0.008% Params, 115.61 MMac, 0.939% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(64, 0.000% Params, 802.82 KMac, 0.007% MACs, 32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0, mode=row)
      )
      (1): FusedMBConv(
        9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
        (block): Sequential(
          9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
          (0): Conv2dNormActivation(
            9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
            (0): Conv2d(9.22 k, 0.008% Params, 115.61 MMac, 0.939% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(64, 0.000% Params, 802.82 KMac, 0.007% MACs, 32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.002531645569620253, mode=row)
      )
      (2): FusedMBConv(
        9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
        (block): Sequential(
          9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
          (0): Conv2dNormActivation(
            9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
            (0): Conv2d(9.22 k, 0.008% Params, 115.61 MMac, 0.939% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(64, 0.000% Params, 802.82 KMac, 0.007% MACs, 32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.005063291139240506, mode=row)
      )
      (3): FusedMBConv(
        9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
        (block): Sequential(
          9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
          (0): Conv2dNormActivation(
            9.28 k, 0.008% Params, 116.41 MMac, 0.946% MACs, 
            (0): Conv2d(9.22 k, 0.008% Params, 115.61 MMac, 0.939% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(64, 0.000% Params, 802.82 KMac, 0.007% MACs, 32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.007594936708860761, mode=row)
      )
    )
    (2): Sequential(
      1.03 M, 0.871% Params, 3.24 GMac, 26.297% MACs, 
      (0): FusedMBConv(
        45.44 k, 0.038% Params, 142.5 MMac, 1.158% MACs, 
        (block): Sequential(
          45.44 k, 0.038% Params, 142.5 MMac, 1.158% MACs, 
          (0): Conv2dNormActivation(
            37.12 k, 0.031% Params, 116.41 MMac, 0.946% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 115.61 MMac, 0.939% MACs, 32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, 0.000% Params, 802.82 KMac, 0.007% MACs, 128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.32 k, 0.007% Params, 26.09 MMac, 0.212% MACs, 
            (0): Conv2d(8.19 k, 0.007% Params, 25.69 MMac, 0.209% MACs, 128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.010126582278481013, mode=row)
      )
      (1): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.012658227848101266, mode=row)
      )
      (2): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.015189873417721522, mode=row)
      )
      (3): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.017721518987341773, mode=row)
      )
      (4): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.020253164556962026, mode=row)
      )
      (5): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.02278481012658228, mode=row)
      )
      (6): FusedMBConv(
        164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
        (block): Sequential(
          164.48 k, 0.139% Params, 515.81 MMac, 4.190% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 464.03 MMac, 3.769% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 462.42 MMac, 3.756% MACs, 64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.013% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            16.51 k, 0.014% Params, 51.78 MMac, 0.421% MACs, 
            (0): Conv2d(16.38 k, 0.014% Params, 51.38 MMac, 0.417% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.02531645569620253, mode=row)
      )
    )
    (3): Sequential(
      2.39 M, 2.017% Params, 1.87 GMac, 15.223% MACs, 
      (0): FusedMBConv(
        172.74 k, 0.146% Params, 135.43 MMac, 1.100% MACs, 
        (block): Sequential(
          172.74 k, 0.146% Params, 135.43 MMac, 1.100% MACs, 
          (0): Conv2dNormActivation(
            147.97 k, 0.125% Params, 116.01 MMac, 0.942% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 115.61 MMac, 0.939% MACs, 64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, 0.000% Params, 401.41 KMac, 0.003% MACs, 256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            24.77 k, 0.021% Params, 19.42 MMac, 0.158% MACs, 
            (0): Conv2d(24.58 k, 0.021% Params, 19.27 MMac, 0.157% MACs, 256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.027848101265822787, mode=row)
      )
      (1): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.030379746835443044, mode=row)
      )
      (2): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.03291139240506329, mode=row)
      )
      (3): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.035443037974683546, mode=row)
      )
      (4): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0379746835443038, mode=row)
      )
      (5): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.04050632911392405, mode=row)
      )
      (6): FusedMBConv(
        369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
        (block): Sequential(
          369.6 k, 0.312% Params, 289.77 MMac, 2.354% MACs, 
          (0): Conv2dNormActivation(
            332.54 k, 0.281% Params, 260.71 MMac, 2.118% MACs, 
            (0): Conv2d(331.78 k, 0.280% Params, 260.11 MMac, 2.113% MACs, 96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            37.06 k, 0.031% Params, 29.05 MMac, 0.236% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, 0.000% Params, 150.53 KMac, 0.001% MACs, 96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.04303797468354431, mode=row)
      )
    )
    (4): Sequential(
      3.55 M, 2.998% Params, 585.49 MMac, 4.756% MACs, 
      (0): MBConv(
        134.81 k, 0.114% Params, 44.95 MMac, 0.365% MACs, 
        (block): Sequential(
          134.81 k, 0.114% Params, 44.95 MMac, 0.365% MACs, 
          (0): Conv2dNormActivation(
            37.63 k, 0.032% Params, 29.5 MMac, 0.240% MACs, 
            (0): Conv2d(36.86 k, 0.031% Params, 28.9 MMac, 0.235% MACs, 96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 602.11 KMac, 0.005% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            4.22 k, 0.004% Params, 827.9 KMac, 0.007% MACs, 
            (0): Conv2d(3.46 k, 0.003% Params, 677.38 KMac, 0.006% MACs, 384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 150.53 KMac, 0.001% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            18.84 k, 0.016% Params, 94.1 KMac, 0.001% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 75.26 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(9.24 k, 0.008% Params, 9.24 KMac, 0.000% MACs, 384, 24, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(9.6 k, 0.008% Params, 9.6 KMac, 0.000% MACs, 24, 384, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            74.11 k, 0.063% Params, 14.53 MMac, 0.118% MACs, 
            (0): Conv2d(73.73 k, 0.062% Params, 14.45 MMac, 0.117% MACs, 384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.04556962025316456, mode=row)
      )
      (1): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.04810126582278482, mode=row)
      )
      (2): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.05063291139240506, mode=row)
      )
      (3): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.053164556962025315, mode=row)
      )
      (4): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.055696202531645575, mode=row)
      )
      (5): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.05822784810126583, mode=row)
      )
      (6): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06075949367088609, mode=row)
      )
      (7): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06329113924050633, mode=row)
      )
      (8): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06582278481012659, mode=row)
      )
      (9): MBConv(
        379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
        (block): Sequential(
          379.82 k, 0.320% Params, 60.06 MMac, 0.488% MACs, 
          (0): Conv2dNormActivation(
            148.99 k, 0.126% Params, 29.2 MMac, 0.237% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            8.45 k, 0.007% Params, 1.66 MMac, 0.013% MACs, 
            (0): Conv2d(6.91 k, 0.006% Params, 1.35 MMac, 0.011% MACs, 768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
            (1): BatchNorm2d(1.54 k, 0.001% Params, 301.06 KMac, 0.002% MACs, 768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            74.54 k, 0.063% Params, 225.07 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 150.53 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(36.91 k, 0.031% Params, 36.91 KMac, 0.000% MACs, 768, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(37.63 k, 0.032% Params, 37.63 KMac, 0.000% MACs, 48, 768, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            147.84 k, 0.125% Params, 28.98 MMac, 0.235% MACs, 
            (0): Conv2d(147.46 k, 0.124% Params, 28.9 MMac, 0.235% MACs, 768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, 0.000% Params, 75.26 KMac, 0.001% MACs, 192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06835443037974684, mode=row)
      )
    )
    (5): Sequential(
      14.5 M, 12.236% Params, 2.29 GMac, 18.620% MACs, 
      (0): MBConv(
        606.45 k, 0.512% Params, 97.29 MMac, 0.790% MACs, 
        (block): Sequential(
          606.45 k, 0.512% Params, 97.29 MMac, 0.790% MACs, 
          (0): Conv2dNormActivation(
            223.49 k, 0.189% Params, 43.8 MMac, 0.356% MACs, 
            (0): Conv2d(221.18 k, 0.187% Params, 43.35 MMac, 0.352% MACs, 192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.3 k, 0.002% Params, 451.58 KMac, 0.004% MACs, 1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            12.67 k, 0.011% Params, 2.48 MMac, 0.020% MACs, 
            (0): Conv2d(10.37 k, 0.009% Params, 2.03 MMac, 0.017% MACs, 1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(2.3 k, 0.002% Params, 451.58 KMac, 0.004% MACs, 1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            111.79 k, 0.094% Params, 337.58 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 225.79 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(55.34 k, 0.047% Params, 55.34 KMac, 0.000% MACs, 1152, 48, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(56.45 k, 0.048% Params, 56.45 KMac, 0.000% MACs, 48, 1152, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            258.5 k, 0.218% Params, 50.67 MMac, 0.412% MACs, 
            (0): Conv2d(258.05 k, 0.218% Params, 50.58 MMac, 0.411% MACs, 1152, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.07088607594936709, mode=row)
      )
      (1): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.07341772151898734, mode=row)
      )
      (2): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0759493670886076, mode=row)
      )
      (3): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.07848101265822785, mode=row)
      )
      (4): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0810126582278481, mode=row)
      )
      (5): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08354430379746836, mode=row)
      )
      (6): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08607594936708862, mode=row)
      )
      (7): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08860759493670886, mode=row)
      )
      (8): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.09113924050632911, mode=row)
      )
      (9): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.09367088607594937, mode=row)
      )
      (10): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.09620253164556963, mode=row)
      )
      (11): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.09873417721518989, mode=row)
      )
      (12): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.10126582278481013, mode=row)
      )
      (13): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.10379746835443039, mode=row)
      )
      (14): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.10632911392405063, mode=row)
      )
      (15): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.10886075949367088, mode=row)
      )
      (16): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11139240506329115, mode=row)
      )
      (17): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11392405063291139, mode=row)
      )
      (18): MBConv(
        771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
        (block): Sequential(
          771.96 k, 0.651% Params, 121.94 MMac, 0.991% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 2.9 MMac, 0.024% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 2.37 MMac, 0.019% MACs, 1344, 1344, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 415.35 KMac, 0.003% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 263.42 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            301.5 k, 0.254% Params, 59.09 MMac, 0.480% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(448, 0.000% Params, 87.81 KMac, 0.001% MACs, 224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11645569620253166, mode=row)
      )
    )
    (6): Sequential(
      54.87 M, 46.295% Params, 2.22 GMac, 18.003% MACs, 
      (0): MBConv(
        987.32 k, 0.833% Params, 85.8 MMac, 0.697% MACs, 
        (block): Sequential(
          987.32 k, 0.833% Params, 85.8 MMac, 0.697% MACs, 
          (0): Conv2dNormActivation(
            303.74 k, 0.256% Params, 59.53 MMac, 0.484% MACs, 
            (0): Conv2d(301.06 k, 0.254% Params, 59.01 MMac, 0.479% MACs, 224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 526.85 KMac, 0.004% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            14.78 k, 0.012% Params, 724.42 KMac, 0.006% MACs, 
            (0): Conv2d(12.1 k, 0.010% Params, 592.7 KMac, 0.005% MACs, 1344, 1344, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1344, bias=False)
            (1): BatchNorm2d(2.69 k, 0.002% Params, 131.71 KMac, 0.001% MACs, 1344, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            151.93 k, 0.128% Params, 217.78 KMac, 0.002% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 65.86 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(75.32 k, 0.064% Params, 75.32 KMac, 0.001% MACs, 1344, 56, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(76.61 k, 0.065% Params, 76.61 KMac, 0.001% MACs, 56, 1344, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            516.86 k, 0.436% Params, 25.33 MMac, 0.206% MACs, 
            (0): Conv2d(516.1 k, 0.435% Params, 25.29 MMac, 0.205% MACs, 1344, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.11898734177215191, mode=row)
      )
      (1): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.12151898734177217, mode=row)
      )
      (2): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.12405063291139241, mode=row)
      )
      (3): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.12658227848101267, mode=row)
      )
      (4): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.12911392405063293, mode=row)
      )
      (5): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.13164556962025317, mode=row)
      )
      (6): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.13417721518987344, mode=row)
      )
      (7): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.13670886075949368, mode=row)
      )
      (8): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.13924050632911392, mode=row)
      )
      (9): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.14177215189873418, mode=row)
      )
      (10): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.14430379746835442, mode=row)
      )
      (11): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1468354430379747, mode=row)
      )
      (12): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.14936708860759496, mode=row)
      )
      (13): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1518987341772152, mode=row)
      )
      (14): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.15443037974683546, mode=row)
      )
      (15): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1569620253164557, mode=row)
      )
      (16): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.15949367088607597, mode=row)
      )
      (17): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1620253164556962, mode=row)
      )
      (18): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.16455696202531644, mode=row)
      )
      (19): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1670886075949367, mode=row)
      )
      (20): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.16962025316455698, mode=row)
      )
      (21): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.17215189873417724, mode=row)
      )
      (22): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.17468354430379748, mode=row)
      )
      (23): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.17721518987341772, mode=row)
      )
      (24): MBConv(
        2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
        (block): Sequential(
          2.24 M, 1.894% Params, 88.77 MMac, 0.721% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            885.5 k, 0.747% Params, 43.39 MMac, 0.352% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(768, 0.001% Params, 37.63 KMac, 0.000% MACs, 384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.179746835443038, mode=row)
      )
    )
    (7): Sequential(
      40.03 M, 33.777% Params, 1.59 GMac, 12.886% MACs, 
      (0): MBConv(
        2.84 M, 2.392% Params, 117.69 MMac, 0.956% MACs, 
        (block): Sequential(
          2.84 M, 2.392% Params, 117.69 MMac, 0.956% MACs, 
          (0): Conv2dNormActivation(
            889.34 k, 0.750% Params, 43.58 MMac, 0.354% MACs, 
            (0): Conv2d(884.74 k, 0.747% Params, 43.35 MMac, 0.352% MACs, 384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            25.34 k, 0.021% Params, 1.24 MMac, 0.010% MACs, 
            (0): Conv2d(20.74 k, 0.017% Params, 1.02 MMac, 0.008% MACs, 2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
            (1): BatchNorm2d(4.61 k, 0.004% Params, 225.79 KMac, 0.002% MACs, 2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            444.77 k, 0.375% Params, 557.66 KMac, 0.005% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 112.9 KMac, 0.001% MACs, output_size=1)
            (fc1): Conv2d(221.28 k, 0.187% Params, 221.28 KMac, 0.002% MACs, 2304, 96, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(223.49 k, 0.189% Params, 223.49 KMac, 0.002% MACs, 96, 2304, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            1.48 M, 1.245% Params, 72.32 MMac, 0.587% MACs, 
            (0): Conv2d(1.47 M, 1.244% Params, 72.25 MMac, 0.587% MACs, 2304, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.18227848101265823, mode=row)
      )
      (1): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1848101265822785, mode=row)
      )
      (2): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.18734177215189873, mode=row)
      )
      (3): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.189873417721519, mode=row)
      )
      (4): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.19240506329113927, mode=row)
      )
      (5): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1949367088607595, mode=row)
      )
      (6): MBConv(
        6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
        (block): Sequential(
          6.2 M, 5.231% Params, 244.77 MMac, 1.988% MACs, 
          (0): Conv2dNormActivation(
            2.47 M, 2.080% Params, 120.8 MMac, 0.981% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (1): Conv2dNormActivation(
            42.24 k, 0.036% Params, 2.07 MMac, 0.017% MACs, 
            (0): Conv2d(34.56 k, 0.029% Params, 1.69 MMac, 0.014% MACs, 3840, 3840, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3840, bias=False)
            (1): BatchNorm2d(7.68 k, 0.006% Params, 376.32 KMac, 0.003% MACs, 3840, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
          )
          (2): SqueezeExcitation(
            1.23 M, 1.040% Params, 1.42 MMac, 0.012% MACs, 
            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 188.16 KMac, 0.002% MACs, output_size=1)
            (fc1): Conv2d(614.56 k, 0.519% Params, 614.56 KMac, 0.005% MACs, 3840, 160, kernel_size=(1, 1), stride=(1, 1))
            (fc2): Conv2d(618.24 k, 0.522% Params, 618.24 KMac, 0.005% MACs, 160, 3840, kernel_size=(1, 1), stride=(1, 1))
            (activation): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
            (scale_activation): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          )
          (3): Conv2dNormActivation(
            2.46 M, 2.075% Params, 120.49 MMac, 0.979% MACs, 
            (0): Conv2d(2.46 M, 2.074% Params, 120.42 MMac, 0.978% MACs, 3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1.28 k, 0.001% Params, 62.72 KMac, 0.001% MACs, 640, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.19746835443037977, mode=row)
      )
    )
    (8): Conv2dNormActivation(
      821.76 k, 0.693% Params, 40.27 MMac, 0.327% MACs, 
      (0): Conv2d(819.2 k, 0.691% Params, 40.14 MMac, 0.326% MACs, 640, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(2.56 k, 0.002% Params, 125.44 KMac, 0.001% MACs, 1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      (2): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 62.72 KMac, 0.001% MACs, output_size=1)
  (classifier): Sequential(
    1.28 M, 1.081% Params, 1.28 MMac, 0.010% MACs, 
    (0): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.4, inplace=True)
    (1): Linear(1.28 M, 1.081% Params, 1.28 MMac, 0.010% MACs, in_features=1280, out_features=1000, bias=True)
  )
)Measurement of allocated memory is only available on CUDA devices

Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:   1%|          | 1/100 [00:00<00:13,  7.32it/s]Warming up with batch_size=1:   2%|▏         | 2/100 [00:00<00:13,  7.32it/s]Warming up with batch_size=1:   3%|▎         | 3/100 [00:00<00:13,  7.32it/s]Warming up with batch_size=1:   4%|▍         | 4/100 [00:00<00:13,  7.32it/s]Warming up with batch_size=1:   5%|▌         | 5/100 [00:00<00:12,  7.32it/s]Warming up with batch_size=1:   6%|▌         | 6/100 [00:00<00:12,  7.31it/s]Warming up with batch_size=1:   7%|▋         | 7/100 [00:00<00:12,  7.31it/s]Warming up with batch_size=1:   8%|▊         | 8/100 [00:01<00:12,  7.31it/s]Warming up with batch_size=1:   9%|▉         | 9/100 [00:01<00:12,  7.31it/s]Warming up with batch_size=1:  10%|█         | 10/100 [00:01<00:12,  7.31it/s]Warming up with batch_size=1:  11%|█         | 11/100 [00:01<00:12,  7.31it/s]Warming up with batch_size=1:  12%|█▏        | 12/100 [00:01<00:12,  7.31it/s]Warming up with batch_size=1:  13%|█▎        | 13/100 [00:01<00:11,  7.31it/s]Warming up with batch_size=1:  14%|█▍        | 14/100 [00:01<00:11,  7.30it/s]Warming up with batch_size=1:  15%|█▌        | 15/100 [00:02<00:11,  7.30it/s]Warming up with batch_size=1:  16%|█▌        | 16/100 [00:02<00:11,  7.30it/s]Warming up with batch_size=1:  17%|█▋        | 17/100 [00:02<00:11,  7.30it/s]Warming up with batch_size=1:  18%|█▊        | 18/100 [00:02<00:11,  7.30it/s]Warming up with batch_size=1:  19%|█▉        | 19/100 [00:02<00:11,  7.30it/s]Warming up with batch_size=1:  20%|██        | 20/100 [00:02<00:10,  7.30it/s]Warming up with batch_size=1:  21%|██        | 21/100 [00:02<00:10,  7.30it/s]Warming up with batch_size=1:  22%|██▏       | 22/100 [00:03<00:10,  7.30it/s]Warming up with batch_size=1:  23%|██▎       | 23/100 [00:03<00:10,  7.30it/s]Warming up with batch_size=1:  24%|██▍       | 24/100 [00:03<00:10,  7.30it/s]Warming up with batch_size=1:  25%|██▌       | 25/100 [00:03<00:10,  7.30it/s]Warming up with batch_size=1:  26%|██▌       | 26/100 [00:03<00:10,  7.30it/s]Warming up with batch_size=1:  27%|██▋       | 27/100 [00:03<00:10,  7.30it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:03<00:09,  7.30it/s]Warming up with batch_size=1:  29%|██▉       | 29/100 [00:03<00:09,  7.30it/s]Warming up with batch_size=1:  30%|███       | 30/100 [00:04<00:09,  7.30it/s]Warming up with batch_size=1:  31%|███       | 31/100 [00:04<00:09,  7.30it/s]Warming up with batch_size=1:  32%|███▏      | 32/100 [00:04<00:09,  7.29it/s]Warming up with batch_size=1:  33%|███▎      | 33/100 [00:04<00:09,  7.29it/s]Warming up with batch_size=1:  34%|███▍      | 34/100 [00:04<00:09,  7.30it/s]Warming up with batch_size=1:  35%|███▌      | 35/100 [00:04<00:08,  7.30it/s]Warming up with batch_size=1:  36%|███▌      | 36/100 [00:04<00:08,  7.30it/s]Warming up with batch_size=1:  37%|███▋      | 37/100 [00:05<00:08,  7.30it/s]Warming up with batch_size=1:  38%|███▊      | 38/100 [00:05<00:08,  7.30it/s]Warming up with batch_size=1:  39%|███▉      | 39/100 [00:05<00:08,  7.30it/s]Warming up with batch_size=1:  40%|████      | 40/100 [00:05<00:08,  7.30it/s]Warming up with batch_size=1:  41%|████      | 41/100 [00:05<00:08,  7.30it/s]Warming up with batch_size=1:  42%|████▏     | 42/100 [00:05<00:07,  7.30it/s]Warming up with batch_size=1:  43%|████▎     | 43/100 [00:05<00:07,  7.30it/s]Warming up with batch_size=1:  44%|████▍     | 44/100 [00:06<00:07,  7.30it/s]Warming up with batch_size=1:  45%|████▌     | 45/100 [00:06<00:07,  7.30it/s]Warming up with batch_size=1:  46%|████▌     | 46/100 [00:06<00:07,  7.30it/s]Warming up with batch_size=1:  47%|████▋     | 47/100 [00:06<00:07,  7.30it/s]Warming up with batch_size=1:  48%|████▊     | 48/100 [00:06<00:07,  7.29it/s]Warming up with batch_size=1:  49%|████▉     | 49/100 [00:06<00:06,  7.29it/s]Warming up with batch_size=1:  50%|█████     | 50/100 [00:06<00:06,  7.29it/s]Warming up with batch_size=1:  51%|█████     | 51/100 [00:06<00:06,  7.29it/s]Warming up with batch_size=1:  52%|█████▏    | 52/100 [00:07<00:06,  7.30it/s]Warming up with batch_size=1:  53%|█████▎    | 53/100 [00:07<00:06,  7.29it/s]Warming up with batch_size=1:  54%|█████▍    | 54/100 [00:07<00:06,  7.29it/s]Warming up with batch_size=1:  55%|█████▌    | 55/100 [00:07<00:06,  7.29it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:07<00:06,  7.29it/s]Warming up with batch_size=1:  57%|█████▋    | 57/100 [00:07<00:05,  7.29it/s]Warming up with batch_size=1:  58%|█████▊    | 58/100 [00:07<00:05,  7.29it/s]Warming up with batch_size=1:  59%|█████▉    | 59/100 [00:08<00:05,  7.29it/s]Warming up with batch_size=1:  60%|██████    | 60/100 [00:08<00:05,  7.29it/s]Warming up with batch_size=1:  61%|██████    | 61/100 [00:08<00:05,  7.29it/s]Warming up with batch_size=1:  62%|██████▏   | 62/100 [00:08<00:05,  7.29it/s]Warming up with batch_size=1:  63%|██████▎   | 63/100 [00:08<00:05,  7.29it/s]Warming up with batch_size=1:  64%|██████▍   | 64/100 [00:08<00:04,  7.29it/s]Warming up with batch_size=1:  65%|██████▌   | 65/100 [00:08<00:04,  7.30it/s]Warming up with batch_size=1:  66%|██████▌   | 66/100 [00:09<00:04,  7.30it/s]Warming up with batch_size=1:  67%|██████▋   | 67/100 [00:09<00:04,  7.30it/s]Warming up with batch_size=1:  68%|██████▊   | 68/100 [00:09<00:04,  7.29it/s]Warming up with batch_size=1:  69%|██████▉   | 69/100 [00:09<00:04,  7.29it/s]Warming up with batch_size=1:  70%|███████   | 70/100 [00:09<00:04,  7.30it/s]Warming up with batch_size=1:  71%|███████   | 71/100 [00:09<00:03,  7.30it/s]Warming up with batch_size=1:  72%|███████▏  | 72/100 [00:09<00:03,  7.30it/s]Warming up with batch_size=1:  73%|███████▎  | 73/100 [00:10<00:03,  7.31it/s]Warming up with batch_size=1:  74%|███████▍  | 74/100 [00:10<00:03,  7.31it/s]Warming up with batch_size=1:  75%|███████▌  | 75/100 [00:10<00:03,  7.31it/s]Warming up with batch_size=1:  76%|███████▌  | 76/100 [00:10<00:03,  7.30it/s]Warming up with batch_size=1:  77%|███████▋  | 77/100 [00:10<00:03,  7.30it/s]Warming up with batch_size=1:  78%|███████▊  | 78/100 [00:10<00:03,  7.30it/s]Warming up with batch_size=1:  79%|███████▉  | 79/100 [00:10<00:02,  7.30it/s]Warming up with batch_size=1:  80%|████████  | 80/100 [00:10<00:02,  7.29it/s]Warming up with batch_size=1:  81%|████████  | 81/100 [00:11<00:02,  7.29it/s]Warming up with batch_size=1:  82%|████████▏ | 82/100 [00:11<00:02,  7.29it/s]Warming up with batch_size=1:  83%|████████▎ | 83/100 [00:11<00:02,  7.30it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:11<00:02,  7.28it/s]Warming up with batch_size=1:  85%|████████▌ | 85/100 [00:11<00:02,  7.29it/s]Warming up with batch_size=1:  86%|████████▌ | 86/100 [00:11<00:01,  7.29it/s]Warming up with batch_size=1:  87%|████████▋ | 87/100 [00:11<00:01,  7.29it/s]Warming up with batch_size=1:  88%|████████▊ | 88/100 [00:12<00:01,  7.30it/s]Warming up with batch_size=1:  89%|████████▉ | 89/100 [00:12<00:01,  7.30it/s]Warming up with batch_size=1:  90%|█████████ | 90/100 [00:12<00:01,  7.30it/s]Warming up with batch_size=1:  91%|█████████ | 91/100 [00:12<00:01,  7.30it/s]Warming up with batch_size=1:  92%|█████████▏| 92/100 [00:12<00:01,  7.30it/s]Warming up with batch_size=1:  93%|█████████▎| 93/100 [00:12<00:00,  7.30it/s]Warming up with batch_size=1:  94%|█████████▍| 94/100 [00:12<00:00,  7.30it/s]Warming up with batch_size=1:  95%|█████████▌| 95/100 [00:13<00:00,  7.30it/s]Warming up with batch_size=1:  96%|█████████▌| 96/100 [00:13<00:00,  7.30it/s]Warming up with batch_size=1:  97%|█████████▋| 97/100 [00:13<00:00,  7.30it/s]Warming up with batch_size=1:  98%|█████████▊| 98/100 [00:13<00:00,  7.30it/s]Warming up with batch_size=1:  99%|█████████▉| 99/100 [00:13<00:00,  7.30it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:13<00:00,  7.31it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:13<00:00,  7.30it/s]
STAGE:2024-02-23 09:46:47 176409:176409 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:46:48 176409:176409 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:46:48 176409:176409 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   0%|          | 1/1000 [00:00<02:16,  7.30it/s]Measuring inference for batch_size=1:   0%|          | 2/1000 [00:00<02:16,  7.31it/s]Measuring inference for batch_size=1:   0%|          | 3/1000 [00:00<02:16,  7.30it/s]Measuring inference for batch_size=1:   0%|          | 4/1000 [00:00<02:16,  7.30it/s]Measuring inference for batch_size=1:   0%|          | 5/1000 [00:00<02:16,  7.31it/s]Measuring inference for batch_size=1:   1%|          | 6/1000 [00:00<02:15,  7.31it/s]Measuring inference for batch_size=1:   1%|          | 7/1000 [00:00<02:16,  7.30it/s]Measuring inference for batch_size=1:   1%|          | 8/1000 [00:01<02:15,  7.30it/s]Measuring inference for batch_size=1:   1%|          | 9/1000 [00:01<02:15,  7.31it/s]Measuring inference for batch_size=1:   1%|          | 10/1000 [00:01<02:15,  7.31it/s]Measuring inference for batch_size=1:   1%|          | 11/1000 [00:01<02:15,  7.31it/s]Measuring inference for batch_size=1:   1%|          | 12/1000 [00:01<02:15,  7.31it/s]Measuring inference for batch_size=1:   1%|▏         | 13/1000 [00:01<02:15,  7.31it/s]Measuring inference for batch_size=1:   1%|▏         | 14/1000 [00:01<02:14,  7.31it/s]Measuring inference for batch_size=1:   2%|▏         | 15/1000 [00:02<02:14,  7.31it/s]Measuring inference for batch_size=1:   2%|▏         | 16/1000 [00:02<02:14,  7.31it/s]Measuring inference for batch_size=1:   2%|▏         | 17/1000 [00:02<02:14,  7.31it/s]Measuring inference for batch_size=1:   2%|▏         | 18/1000 [00:02<02:14,  7.30it/s]Measuring inference for batch_size=1:   2%|▏         | 19/1000 [00:02<02:14,  7.31it/s]Measuring inference for batch_size=1:   2%|▏         | 20/1000 [00:02<02:14,  7.31it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:02<02:14,  7.30it/s]Measuring inference for batch_size=1:   2%|▏         | 22/1000 [00:03<02:13,  7.30it/s]Measuring inference for batch_size=1:   2%|▏         | 23/1000 [00:03<02:13,  7.31it/s]Measuring inference for batch_size=1:   2%|▏         | 24/1000 [00:03<02:13,  7.30it/s]Measuring inference for batch_size=1:   2%|▎         | 25/1000 [00:03<02:13,  7.30it/s]Measuring inference for batch_size=1:   3%|▎         | 26/1000 [00:03<02:13,  7.30it/s]Measuring inference for batch_size=1:   3%|▎         | 27/1000 [00:03<02:13,  7.30it/s]Measuring inference for batch_size=1:   3%|▎         | 28/1000 [00:03<02:13,  7.31it/s]Measuring inference for batch_size=1:   3%|▎         | 29/1000 [00:03<02:12,  7.31it/s]Measuring inference for batch_size=1:   3%|▎         | 30/1000 [00:04<02:12,  7.30it/s]Measuring inference for batch_size=1:   3%|▎         | 31/1000 [00:04<02:12,  7.30it/s]Measuring inference for batch_size=1:   3%|▎         | 32/1000 [00:04<02:12,  7.31it/s]Measuring inference for batch_size=1:   3%|▎         | 33/1000 [00:04<02:12,  7.31it/s]Measuring inference for batch_size=1:   3%|▎         | 34/1000 [00:04<02:12,  7.31it/s]Measuring inference for batch_size=1:   4%|▎         | 35/1000 [00:04<02:12,  7.31it/s]Measuring inference for batch_size=1:   4%|▎         | 36/1000 [00:04<02:11,  7.31it/s]Measuring inference for batch_size=1:   4%|▎         | 37/1000 [00:05<02:11,  7.30it/s]Measuring inference for batch_size=1:   4%|▍         | 38/1000 [00:05<02:11,  7.30it/s]Measuring inference for batch_size=1:   4%|▍         | 39/1000 [00:05<02:11,  7.31it/s]Measuring inference for batch_size=1:   4%|▍         | 40/1000 [00:05<02:11,  7.31it/s]Measuring inference for batch_size=1:   4%|▍         | 41/1000 [00:05<02:11,  7.31it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:05<02:11,  7.31it/s]Measuring inference for batch_size=1:   4%|▍         | 43/1000 [00:05<02:10,  7.31it/s]Measuring inference for batch_size=1:   4%|▍         | 44/1000 [00:06<02:10,  7.31it/s]Measuring inference for batch_size=1:   4%|▍         | 45/1000 [00:06<02:10,  7.31it/s]Measuring inference for batch_size=1:   5%|▍         | 46/1000 [00:06<02:10,  7.31it/s]Measuring inference for batch_size=1:   5%|▍         | 47/1000 [00:06<02:10,  7.31it/s]Measuring inference for batch_size=1:   5%|▍         | 48/1000 [00:06<02:10,  7.31it/s]Measuring inference for batch_size=1:   5%|▍         | 49/1000 [00:06<02:10,  7.31it/s]Measuring inference for batch_size=1:   5%|▌         | 50/1000 [00:06<02:09,  7.31it/s]Measuring inference for batch_size=1:   5%|▌         | 51/1000 [00:06<02:09,  7.31it/s]Measuring inference for batch_size=1:   5%|▌         | 52/1000 [00:07<02:09,  7.31it/s]Measuring inference for batch_size=1:   5%|▌         | 53/1000 [00:07<02:09,  7.31it/s]Measuring inference for batch_size=1:   5%|▌         | 54/1000 [00:07<02:09,  7.31it/s]Measuring inference for batch_size=1:   6%|▌         | 55/1000 [00:07<02:09,  7.31it/s]Measuring inference for batch_size=1:   6%|▌         | 56/1000 [00:07<02:09,  7.31it/s]Measuring inference for batch_size=1:   6%|▌         | 57/1000 [00:07<02:09,  7.31it/s]Measuring inference for batch_size=1:   6%|▌         | 58/1000 [00:07<02:08,  7.31it/s]Measuring inference for batch_size=1:   6%|▌         | 59/1000 [00:08<02:08,  7.31it/s]Measuring inference for batch_size=1:   6%|▌         | 60/1000 [00:08<02:08,  7.31it/s]Measuring inference for batch_size=1:   6%|▌         | 61/1000 [00:08<02:08,  7.31it/s]Measuring inference for batch_size=1:   6%|▌         | 62/1000 [00:08<02:08,  7.31it/s]Measuring inference for batch_size=1:   6%|▋         | 63/1000 [00:08<02:08,  7.31it/s]Measuring inference for batch_size=1:   6%|▋         | 64/1000 [00:08<02:08,  7.31it/s]Measuring inference for batch_size=1:   6%|▋         | 65/1000 [00:08<02:07,  7.31it/s]Measuring inference for batch_size=1:   7%|▋         | 66/1000 [00:09<02:07,  7.32it/s]Measuring inference for batch_size=1:   7%|▋         | 67/1000 [00:09<02:07,  7.31it/s]Measuring inference for batch_size=1:   7%|▋         | 68/1000 [00:09<02:07,  7.32it/s]Measuring inference for batch_size=1:   7%|▋         | 69/1000 [00:09<02:07,  7.31it/s]Measuring inference for batch_size=1:   7%|▋         | 70/1000 [00:09<02:07,  7.32it/s]Measuring inference for batch_size=1:   7%|▋         | 71/1000 [00:09<02:07,  7.31it/s]Measuring inference for batch_size=1:   7%|▋         | 72/1000 [00:09<02:06,  7.31it/s]Measuring inference for batch_size=1:   7%|▋         | 73/1000 [00:09<02:06,  7.31it/s]Measuring inference for batch_size=1:   7%|▋         | 74/1000 [00:10<02:06,  7.31it/s]Measuring inference for batch_size=1:   8%|▊         | 75/1000 [00:10<02:06,  7.31it/s]Measuring inference for batch_size=1:   8%|▊         | 76/1000 [00:10<02:06,  7.30it/s]Measuring inference for batch_size=1:   8%|▊         | 77/1000 [00:10<02:06,  7.31it/s]Measuring inference for batch_size=1:   8%|▊         | 78/1000 [00:10<02:06,  7.31it/s]Measuring inference for batch_size=1:   8%|▊         | 79/1000 [00:10<02:05,  7.31it/s]Measuring inference for batch_size=1:   8%|▊         | 80/1000 [00:10<02:05,  7.31it/s]Measuring inference for batch_size=1:   8%|▊         | 81/1000 [00:11<02:05,  7.31it/s]Measuring inference for batch_size=1:   8%|▊         | 82/1000 [00:11<02:05,  7.31it/s]Measuring inference for batch_size=1:   8%|▊         | 83/1000 [00:11<02:05,  7.31it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:11<02:05,  7.31it/s]Measuring inference for batch_size=1:   8%|▊         | 85/1000 [00:11<02:05,  7.31it/s]Measuring inference for batch_size=1:   9%|▊         | 86/1000 [00:11<02:04,  7.31it/s]Measuring inference for batch_size=1:   9%|▊         | 87/1000 [00:11<02:04,  7.31it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:12<02:04,  7.31it/s]Measuring inference for batch_size=1:   9%|▉         | 89/1000 [00:12<02:04,  7.31it/s]Measuring inference for batch_size=1:   9%|▉         | 90/1000 [00:12<02:04,  7.31it/s]Measuring inference for batch_size=1:   9%|▉         | 91/1000 [00:12<02:04,  7.31it/s]Measuring inference for batch_size=1:   9%|▉         | 92/1000 [00:12<02:04,  7.31it/s]Measuring inference for batch_size=1:   9%|▉         | 93/1000 [00:12<02:04,  7.30it/s]Measuring inference for batch_size=1:   9%|▉         | 94/1000 [00:12<02:04,  7.31it/s]Measuring inference for batch_size=1:  10%|▉         | 95/1000 [00:12<02:03,  7.30it/s]Measuring inference for batch_size=1:  10%|▉         | 96/1000 [00:13<02:03,  7.31it/s]Measuring inference for batch_size=1:  10%|▉         | 97/1000 [00:13<02:03,  7.31it/s]Measuring inference for batch_size=1:  10%|▉         | 98/1000 [00:13<02:03,  7.31it/s]Measuring inference for batch_size=1:  10%|▉         | 99/1000 [00:13<02:03,  7.31it/s]Measuring inference for batch_size=1:  10%|█         | 100/1000 [00:13<02:03,  7.30it/s]Measuring inference for batch_size=1:  10%|█         | 101/1000 [00:13<02:03,  7.31it/s]Measuring inference for batch_size=1:  10%|█         | 102/1000 [00:13<02:02,  7.30it/s]Measuring inference for batch_size=1:  10%|█         | 103/1000 [00:14<02:02,  7.31it/s]Measuring inference for batch_size=1:  10%|█         | 104/1000 [00:14<02:02,  7.31it/s]Measuring inference for batch_size=1:  10%|█         | 105/1000 [00:14<02:02,  7.31it/s]Measuring inference for batch_size=1:  11%|█         | 106/1000 [00:14<02:02,  7.30it/s]Measuring inference for batch_size=1:  11%|█         | 107/1000 [00:14<02:02,  7.31it/s]Measuring inference for batch_size=1:  11%|█         | 108/1000 [00:14<02:02,  7.31it/s]Measuring inference for batch_size=1:  11%|█         | 109/1000 [00:14<02:01,  7.31it/s]Measuring inference for batch_size=1:  11%|█         | 110/1000 [00:15<02:01,  7.31it/s]Measuring inference for batch_size=1:  11%|█         | 111/1000 [00:15<02:01,  7.31it/s]Measuring inference for batch_size=1:  11%|█         | 112/1000 [00:15<02:01,  7.31it/s]Measuring inference for batch_size=1:  11%|█▏        | 113/1000 [00:15<02:01,  7.31it/s]Measuring inference for batch_size=1:  11%|█▏        | 114/1000 [00:15<02:01,  7.30it/s]Measuring inference for batch_size=1:  12%|█▏        | 115/1000 [00:15<02:01,  7.30it/s]Measuring inference for batch_size=1:  12%|█▏        | 116/1000 [00:15<02:01,  7.30it/s]Measuring inference for batch_size=1:  12%|█▏        | 117/1000 [00:16<02:00,  7.31it/s]Measuring inference for batch_size=1:  12%|█▏        | 118/1000 [00:16<02:00,  7.31it/s]Measuring inference for batch_size=1:  12%|█▏        | 119/1000 [00:16<02:00,  7.30it/s]Measuring inference for batch_size=1:  12%|█▏        | 120/1000 [00:16<02:00,  7.30it/s]Measuring inference for batch_size=1:  12%|█▏        | 121/1000 [00:16<02:00,  7.30it/s]Measuring inference for batch_size=1:  12%|█▏        | 122/1000 [00:16<02:00,  7.30it/s]Measuring inference for batch_size=1:  12%|█▏        | 123/1000 [00:16<02:00,  7.31it/s]Measuring inference for batch_size=1:  12%|█▏        | 124/1000 [00:16<01:59,  7.30it/s]Measuring inference for batch_size=1:  12%|█▎        | 125/1000 [00:17<01:59,  7.30it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:17<01:59,  7.30it/s]Measuring inference for batch_size=1:  13%|█▎        | 127/1000 [00:17<01:59,  7.31it/s]Measuring inference for batch_size=1:  13%|█▎        | 128/1000 [00:17<01:59,  7.31it/s]Measuring inference for batch_size=1:  13%|█▎        | 129/1000 [00:17<01:59,  7.30it/s]Measuring inference for batch_size=1:  13%|█▎        | 130/1000 [00:17<01:59,  7.30it/s]Measuring inference for batch_size=1:  13%|█▎        | 131/1000 [00:17<01:58,  7.30it/s]Measuring inference for batch_size=1:  13%|█▎        | 132/1000 [00:18<01:58,  7.30it/s]Measuring inference for batch_size=1:  13%|█▎        | 133/1000 [00:18<01:58,  7.30it/s]Measuring inference for batch_size=1:  13%|█▎        | 134/1000 [00:18<01:58,  7.30it/s]Measuring inference for batch_size=1:  14%|█▎        | 135/1000 [00:18<01:58,  7.30it/s]Measuring inference for batch_size=1:  14%|█▎        | 136/1000 [00:18<01:58,  7.30it/s]Measuring inference for batch_size=1:  14%|█▎        | 137/1000 [00:18<01:58,  7.30it/s]Measuring inference for batch_size=1:  14%|█▍        | 138/1000 [00:18<01:58,  7.30it/s]Measuring inference for batch_size=1:  14%|█▍        | 139/1000 [00:19<01:57,  7.30it/s]Measuring inference for batch_size=1:  14%|█▍        | 140/1000 [00:19<01:57,  7.30it/s]Measuring inference for batch_size=1:  14%|█▍        | 141/1000 [00:19<01:57,  7.30it/s]Measuring inference for batch_size=1:  14%|█▍        | 142/1000 [00:19<01:57,  7.31it/s]Measuring inference for batch_size=1:  14%|█▍        | 143/1000 [00:19<01:57,  7.30it/s]Measuring inference for batch_size=1:  14%|█▍        | 144/1000 [00:19<01:57,  7.31it/s]Measuring inference for batch_size=1:  14%|█▍        | 145/1000 [00:19<01:57,  7.30it/s]Measuring inference for batch_size=1:  15%|█▍        | 146/1000 [00:19<01:56,  7.30it/s]Measuring inference for batch_size=1:  15%|█▍        | 147/1000 [00:20<01:56,  7.31it/s]Measuring inference for batch_size=1:  15%|█▍        | 148/1000 [00:20<01:56,  7.30it/s]Measuring inference for batch_size=1:  15%|█▍        | 149/1000 [00:20<01:56,  7.30it/s]Measuring inference for batch_size=1:  15%|█▌        | 150/1000 [00:20<01:56,  7.30it/s]Measuring inference for batch_size=1:  15%|█▌        | 151/1000 [00:20<01:56,  7.30it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:20<01:56,  7.30it/s]Measuring inference for batch_size=1:  15%|█▌        | 153/1000 [00:20<01:55,  7.30it/s]Measuring inference for batch_size=1:  15%|█▌        | 154/1000 [00:21<01:55,  7.30it/s]Measuring inference for batch_size=1:  16%|█▌        | 155/1000 [00:21<01:55,  7.30it/s]Measuring inference for batch_size=1:  16%|█▌        | 156/1000 [00:21<01:55,  7.30it/s]Measuring inference for batch_size=1:  16%|█▌        | 157/1000 [00:21<01:55,  7.30it/s]Measuring inference for batch_size=1:  16%|█▌        | 158/1000 [00:21<01:55,  7.31it/s]Measuring inference for batch_size=1:  16%|█▌        | 159/1000 [00:21<01:55,  7.30it/s]Measuring inference for batch_size=1:  16%|█▌        | 160/1000 [00:21<01:54,  7.31it/s]Measuring inference for batch_size=1:  16%|█▌        | 161/1000 [00:22<01:54,  7.31it/s]Measuring inference for batch_size=1:  16%|█▌        | 162/1000 [00:22<01:54,  7.31it/s]Measuring inference for batch_size=1:  16%|█▋        | 163/1000 [00:22<01:54,  7.30it/s]Measuring inference for batch_size=1:  16%|█▋        | 164/1000 [00:22<01:54,  7.31it/s]Measuring inference for batch_size=1:  16%|█▋        | 165/1000 [00:22<01:54,  7.30it/s]Measuring inference for batch_size=1:  17%|█▋        | 166/1000 [00:22<01:54,  7.30it/s]Measuring inference for batch_size=1:  17%|█▋        | 167/1000 [00:22<01:54,  7.30it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:22<01:54,  7.29it/s]Measuring inference for batch_size=1:  17%|█▋        | 169/1000 [00:23<01:53,  7.30it/s]Measuring inference for batch_size=1:  17%|█▋        | 170/1000 [00:23<01:53,  7.30it/s]Measuring inference for batch_size=1:  17%|█▋        | 171/1000 [00:23<01:53,  7.30it/s]Measuring inference for batch_size=1:  17%|█▋        | 172/1000 [00:23<01:53,  7.30it/s]Measuring inference for batch_size=1:  17%|█▋        | 173/1000 [00:23<01:53,  7.30it/s]Measuring inference for batch_size=1:  17%|█▋        | 174/1000 [00:23<01:53,  7.30it/s]Measuring inference for batch_size=1:  18%|█▊        | 175/1000 [00:23<01:52,  7.30it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:24<01:52,  7.30it/s]Measuring inference for batch_size=1:  18%|█▊        | 177/1000 [00:24<01:52,  7.30it/s]Measuring inference for batch_size=1:  18%|█▊        | 178/1000 [00:24<01:52,  7.30it/s]Measuring inference for batch_size=1:  18%|█▊        | 179/1000 [00:24<01:52,  7.29it/s]Measuring inference for batch_size=1:  18%|█▊        | 180/1000 [00:24<01:52,  7.30it/s]Measuring inference for batch_size=1:  18%|█▊        | 181/1000 [00:24<01:52,  7.30it/s]Measuring inference for batch_size=1:  18%|█▊        | 182/1000 [00:24<01:52,  7.30it/s]Measuring inference for batch_size=1:  18%|█▊        | 183/1000 [00:25<01:51,  7.30it/s]Measuring inference for batch_size=1:  18%|█▊        | 184/1000 [00:25<01:51,  7.30it/s]Measuring inference for batch_size=1:  18%|█▊        | 185/1000 [00:25<01:51,  7.30it/s]Measuring inference for batch_size=1:  19%|█▊        | 186/1000 [00:25<01:51,  7.30it/s]Measuring inference for batch_size=1:  19%|█▊        | 187/1000 [00:25<01:51,  7.30it/s]Measuring inference for batch_size=1:  19%|█▉        | 188/1000 [00:25<01:51,  7.30it/s]Measuring inference for batch_size=1:  19%|█▉        | 189/1000 [00:25<01:51,  7.30it/s]Measuring inference for batch_size=1:  19%|█▉        | 190/1000 [00:26<01:50,  7.31it/s]Measuring inference for batch_size=1:  19%|█▉        | 191/1000 [00:26<01:50,  7.30it/s]Measuring inference for batch_size=1:  19%|█▉        | 192/1000 [00:26<01:50,  7.31it/s]Measuring inference for batch_size=1:  19%|█▉        | 193/1000 [00:26<01:50,  7.30it/s]Measuring inference for batch_size=1:  19%|█▉        | 194/1000 [00:26<01:50,  7.31it/s]Measuring inference for batch_size=1:  20%|█▉        | 195/1000 [00:26<01:50,  7.31it/s]Measuring inference for batch_size=1:  20%|█▉        | 196/1000 [00:26<01:50,  7.31it/s]Measuring inference for batch_size=1:  20%|█▉        | 197/1000 [00:26<01:49,  7.30it/s]Measuring inference for batch_size=1:  20%|█▉        | 198/1000 [00:27<01:49,  7.30it/s]Measuring inference for batch_size=1:  20%|█▉        | 199/1000 [00:27<01:49,  7.30it/s]Measuring inference for batch_size=1:  20%|██        | 200/1000 [00:27<01:49,  7.31it/s]Measuring inference for batch_size=1:  20%|██        | 201/1000 [00:27<01:49,  7.31it/s]Measuring inference for batch_size=1:  20%|██        | 202/1000 [00:27<01:49,  7.31it/s]Measuring inference for batch_size=1:  20%|██        | 203/1000 [00:27<01:49,  7.31it/s]Measuring inference for batch_size=1:  20%|██        | 204/1000 [00:27<01:48,  7.30it/s]Measuring inference for batch_size=1:  20%|██        | 205/1000 [00:28<01:48,  7.30it/s]Measuring inference for batch_size=1:  21%|██        | 206/1000 [00:28<01:48,  7.30it/s]Measuring inference for batch_size=1:  21%|██        | 207/1000 [00:28<01:48,  7.29it/s]Measuring inference for batch_size=1:  21%|██        | 208/1000 [00:28<01:48,  7.30it/s]Measuring inference for batch_size=1:  21%|██        | 209/1000 [00:28<01:48,  7.30it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:28<01:48,  7.30it/s]Measuring inference for batch_size=1:  21%|██        | 211/1000 [00:28<01:48,  7.30it/s]Measuring inference for batch_size=1:  21%|██        | 212/1000 [00:29<01:47,  7.30it/s]Measuring inference for batch_size=1:  21%|██▏       | 213/1000 [00:29<01:47,  7.31it/s]Measuring inference for batch_size=1:  21%|██▏       | 214/1000 [00:29<01:47,  7.31it/s]Measuring inference for batch_size=1:  22%|██▏       | 215/1000 [00:29<01:47,  7.31it/s]Measuring inference for batch_size=1:  22%|██▏       | 216/1000 [00:29<01:47,  7.30it/s]Measuring inference for batch_size=1:  22%|██▏       | 217/1000 [00:29<01:47,  7.30it/s]Measuring inference for batch_size=1:  22%|██▏       | 218/1000 [00:29<01:47,  7.29it/s]Measuring inference for batch_size=1:  22%|██▏       | 219/1000 [00:29<01:47,  7.29it/s]Measuring inference for batch_size=1:  22%|██▏       | 220/1000 [00:30<01:46,  7.30it/s]Measuring inference for batch_size=1:  22%|██▏       | 221/1000 [00:30<01:46,  7.30it/s]Measuring inference for batch_size=1:  22%|██▏       | 222/1000 [00:30<01:46,  7.30it/s]Measuring inference for batch_size=1:  22%|██▏       | 223/1000 [00:30<01:46,  7.30it/s]Measuring inference for batch_size=1:  22%|██▏       | 224/1000 [00:30<01:46,  7.30it/s]Measuring inference for batch_size=1:  22%|██▎       | 225/1000 [00:30<01:46,  7.30it/s]Measuring inference for batch_size=1:  23%|██▎       | 226/1000 [00:30<01:46,  7.30it/s]Measuring inference for batch_size=1:  23%|██▎       | 227/1000 [00:31<01:45,  7.30it/s]Measuring inference for batch_size=1:  23%|██▎       | 228/1000 [00:31<01:45,  7.30it/s]Measuring inference for batch_size=1:  23%|██▎       | 229/1000 [00:31<01:45,  7.30it/s]Measuring inference for batch_size=1:  23%|██▎       | 230/1000 [00:31<01:45,  7.31it/s]Measuring inference for batch_size=1:  23%|██▎       | 231/1000 [00:31<01:45,  7.31it/s]Measuring inference for batch_size=1:  23%|██▎       | 232/1000 [00:31<01:45,  7.31it/s]Measuring inference for batch_size=1:  23%|██▎       | 233/1000 [00:31<01:44,  7.31it/s]Measuring inference for batch_size=1:  23%|██▎       | 234/1000 [00:32<01:44,  7.31it/s]Measuring inference for batch_size=1:  24%|██▎       | 235/1000 [00:32<01:44,  7.31it/s]Measuring inference for batch_size=1:  24%|██▎       | 236/1000 [00:32<01:44,  7.30it/s]Measuring inference for batch_size=1:  24%|██▎       | 237/1000 [00:32<01:44,  7.31it/s]Measuring inference for batch_size=1:  24%|██▍       | 238/1000 [00:32<01:44,  7.30it/s]Measuring inference for batch_size=1:  24%|██▍       | 239/1000 [00:32<01:44,  7.30it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:32<01:44,  7.31it/s]Measuring inference for batch_size=1:  24%|██▍       | 241/1000 [00:32<01:43,  7.31it/s]Measuring inference for batch_size=1:  24%|██▍       | 242/1000 [00:33<01:43,  7.31it/s]Measuring inference for batch_size=1:  24%|██▍       | 243/1000 [00:33<01:43,  7.31it/s]Measuring inference for batch_size=1:  24%|██▍       | 244/1000 [00:33<01:43,  7.31it/s]Measuring inference for batch_size=1:  24%|██▍       | 245/1000 [00:33<01:43,  7.31it/s]Measuring inference for batch_size=1:  25%|██▍       | 246/1000 [00:33<01:43,  7.31it/s]Measuring inference for batch_size=1:  25%|██▍       | 247/1000 [00:33<01:43,  7.30it/s]Measuring inference for batch_size=1:  25%|██▍       | 248/1000 [00:33<01:42,  7.31it/s]Measuring inference for batch_size=1:  25%|██▍       | 249/1000 [00:34<01:42,  7.30it/s]Measuring inference for batch_size=1:  25%|██▌       | 250/1000 [00:34<01:42,  7.31it/s]Measuring inference for batch_size=1:  25%|██▌       | 251/1000 [00:34<01:42,  7.30it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:34<01:42,  7.30it/s]Measuring inference for batch_size=1:  25%|██▌       | 253/1000 [00:34<01:42,  7.30it/s]Measuring inference for batch_size=1:  25%|██▌       | 254/1000 [00:34<01:42,  7.31it/s]Measuring inference for batch_size=1:  26%|██▌       | 255/1000 [00:34<01:42,  7.30it/s]Measuring inference for batch_size=1:  26%|██▌       | 256/1000 [00:35<01:41,  7.30it/s]Measuring inference for batch_size=1:  26%|██▌       | 257/1000 [00:35<01:41,  7.31it/s]Measuring inference for batch_size=1:  26%|██▌       | 258/1000 [00:35<01:41,  7.31it/s]Measuring inference for batch_size=1:  26%|██▌       | 259/1000 [00:35<01:41,  7.31it/s]Measuring inference for batch_size=1:  26%|██▌       | 260/1000 [00:35<01:41,  7.31it/s]Measuring inference for batch_size=1:  26%|██▌       | 261/1000 [00:35<01:41,  7.31it/s]Measuring inference for batch_size=1:  26%|██▌       | 262/1000 [00:35<01:40,  7.31it/s]Measuring inference for batch_size=1:  26%|██▋       | 263/1000 [00:36<01:40,  7.31it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:36<01:40,  7.31it/s]Measuring inference for batch_size=1:  26%|██▋       | 265/1000 [00:36<01:40,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 266/1000 [00:36<01:40,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 267/1000 [00:36<01:40,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 268/1000 [00:36<01:40,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 269/1000 [00:36<01:40,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 270/1000 [00:36<01:39,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 271/1000 [00:37<01:39,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 272/1000 [00:37<01:39,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 273/1000 [00:37<01:39,  7.31it/s]Measuring inference for batch_size=1:  27%|██▋       | 274/1000 [00:37<01:39,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 275/1000 [00:37<01:39,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 276/1000 [00:37<01:39,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 277/1000 [00:37<01:38,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 278/1000 [00:38<01:38,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 279/1000 [00:38<01:38,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 280/1000 [00:38<01:38,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 281/1000 [00:38<01:38,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 282/1000 [00:38<01:38,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 283/1000 [00:38<01:38,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 284/1000 [00:38<01:37,  7.31it/s]Measuring inference for batch_size=1:  28%|██▊       | 285/1000 [00:39<01:37,  7.30it/s]Measuring inference for batch_size=1:  29%|██▊       | 286/1000 [00:39<01:37,  7.30it/s]Measuring inference for batch_size=1:  29%|██▊       | 287/1000 [00:39<01:37,  7.31it/s]Measuring inference for batch_size=1:  29%|██▉       | 288/1000 [00:39<01:37,  7.31it/s]Measuring inference for batch_size=1:  29%|██▉       | 289/1000 [00:39<01:37,  7.31it/s]Measuring inference for batch_size=1:  29%|██▉       | 290/1000 [00:39<01:37,  7.31it/s]Measuring inference for batch_size=1:  29%|██▉       | 291/1000 [00:39<01:36,  7.31it/s]Measuring inference for batch_size=1:  29%|██▉       | 292/1000 [00:39<01:36,  7.31it/s]Measuring inference for batch_size=1:  29%|██▉       | 293/1000 [00:40<01:36,  7.31it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:40<01:36,  7.31it/s]Measuring inference for batch_size=1:  30%|██▉       | 295/1000 [00:40<01:36,  7.32it/s]Measuring inference for batch_size=1:  30%|██▉       | 296/1000 [00:40<01:36,  7.31it/s]Measuring inference for batch_size=1:  30%|██▉       | 297/1000 [00:40<01:36,  7.31it/s]Measuring inference for batch_size=1:  30%|██▉       | 298/1000 [00:40<01:36,  7.31it/s]Measuring inference for batch_size=1:  30%|██▉       | 299/1000 [00:40<01:35,  7.31it/s]Measuring inference for batch_size=1:  30%|███       | 300/1000 [00:41<01:35,  7.31it/s]Measuring inference for batch_size=1:  30%|███       | 301/1000 [00:41<01:35,  7.31it/s]Measuring inference for batch_size=1:  30%|███       | 302/1000 [00:41<01:35,  7.31it/s]Measuring inference for batch_size=1:  30%|███       | 303/1000 [00:41<01:35,  7.31it/s]Measuring inference for batch_size=1:  30%|███       | 304/1000 [00:41<01:35,  7.31it/s]Measuring inference for batch_size=1:  30%|███       | 305/1000 [00:41<01:35,  7.31it/s]Measuring inference for batch_size=1:  31%|███       | 306/1000 [00:41<01:34,  7.31it/s]Measuring inference for batch_size=1:  31%|███       | 307/1000 [00:42<01:34,  7.31it/s]Measuring inference for batch_size=1:  31%|███       | 308/1000 [00:42<01:34,  7.31it/s]Measuring inference for batch_size=1:  31%|███       | 309/1000 [00:42<01:34,  7.31it/s]Measuring inference for batch_size=1:  31%|███       | 310/1000 [00:42<01:34,  7.31it/s]Measuring inference for batch_size=1:  31%|███       | 311/1000 [00:42<01:34,  7.31it/s]Measuring inference for batch_size=1:  31%|███       | 312/1000 [00:42<01:34,  7.31it/s]Measuring inference for batch_size=1:  31%|███▏      | 313/1000 [00:42<01:33,  7.31it/s]Measuring inference for batch_size=1:  31%|███▏      | 314/1000 [00:42<01:33,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 315/1000 [00:43<01:33,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 316/1000 [00:43<01:33,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 317/1000 [00:43<01:33,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 318/1000 [00:43<01:33,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 319/1000 [00:43<01:33,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 320/1000 [00:43<01:33,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 321/1000 [00:43<01:32,  7.30it/s]Measuring inference for batch_size=1:  32%|███▏      | 322/1000 [00:44<01:32,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 323/1000 [00:44<01:32,  7.31it/s]Measuring inference for batch_size=1:  32%|███▏      | 324/1000 [00:44<01:32,  7.31it/s]Measuring inference for batch_size=1:  32%|███▎      | 325/1000 [00:44<01:32,  7.30it/s]Measuring inference for batch_size=1:  33%|███▎      | 326/1000 [00:44<01:32,  7.31it/s]Measuring inference for batch_size=1:  33%|███▎      | 327/1000 [00:44<01:32,  7.30it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:44<01:31,  7.31it/s]Measuring inference for batch_size=1:  33%|███▎      | 329/1000 [00:45<01:31,  7.31it/s]Measuring inference for batch_size=1:  33%|███▎      | 330/1000 [00:45<01:31,  7.30it/s]Measuring inference for batch_size=1:  33%|███▎      | 331/1000 [00:45<01:31,  7.31it/s]Measuring inference for batch_size=1:  33%|███▎      | 332/1000 [00:45<01:31,  7.30it/s]Measuring inference for batch_size=1:  33%|███▎      | 333/1000 [00:45<01:31,  7.31it/s]Measuring inference for batch_size=1:  33%|███▎      | 334/1000 [00:45<01:31,  7.31it/s]Measuring inference for batch_size=1:  34%|███▎      | 335/1000 [00:45<01:31,  7.31it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:45<01:30,  7.31it/s]Measuring inference for batch_size=1:  34%|███▎      | 337/1000 [00:46<01:30,  7.31it/s]Measuring inference for batch_size=1:  34%|███▍      | 338/1000 [00:46<01:30,  7.31it/s]Measuring inference for batch_size=1:  34%|███▍      | 339/1000 [00:46<01:30,  7.31it/s]Measuring inference for batch_size=1:  34%|███▍      | 340/1000 [00:46<01:30,  7.31it/s]Measuring inference for batch_size=1:  34%|███▍      | 341/1000 [00:46<01:30,  7.30it/s]Measuring inference for batch_size=1:  34%|███▍      | 342/1000 [00:46<01:30,  7.30it/s]Measuring inference for batch_size=1:  34%|███▍      | 343/1000 [00:46<01:29,  7.30it/s]Measuring inference for batch_size=1:  34%|███▍      | 344/1000 [00:47<01:29,  7.31it/s]Measuring inference for batch_size=1:  34%|███▍      | 345/1000 [00:47<01:29,  7.31it/s]Measuring inference for batch_size=1:  35%|███▍      | 346/1000 [00:47<01:29,  7.31it/s]Measuring inference for batch_size=1:  35%|███▍      | 347/1000 [00:47<01:29,  7.31it/s]Measuring inference for batch_size=1:  35%|███▍      | 348/1000 [00:47<01:29,  7.31it/s]Measuring inference for batch_size=1:  35%|███▍      | 349/1000 [00:47<01:29,  7.30it/s]Measuring inference for batch_size=1:  35%|███▌      | 350/1000 [00:47<01:28,  7.31it/s]Measuring inference for batch_size=1:  35%|███▌      | 351/1000 [00:48<01:28,  7.30it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:48<01:28,  7.30it/s]Measuring inference for batch_size=1:  35%|███▌      | 353/1000 [00:48<01:28,  7.30it/s]Measuring inference for batch_size=1:  35%|███▌      | 354/1000 [00:48<01:28,  7.30it/s]Measuring inference for batch_size=1:  36%|███▌      | 355/1000 [00:48<01:28,  7.30it/s]Measuring inference for batch_size=1:  36%|███▌      | 356/1000 [00:48<01:28,  7.31it/s]Measuring inference for batch_size=1:  36%|███▌      | 357/1000 [00:48<01:28,  7.31it/s]Measuring inference for batch_size=1:  36%|███▌      | 358/1000 [00:49<01:27,  7.31it/s]Measuring inference for batch_size=1:  36%|███▌      | 359/1000 [00:49<01:27,  7.31it/s]Measuring inference for batch_size=1:  36%|███▌      | 360/1000 [00:49<01:27,  7.31it/s]Measuring inference for batch_size=1:  36%|███▌      | 361/1000 [00:49<01:27,  7.31it/s]Measuring inference for batch_size=1:  36%|███▌      | 362/1000 [00:49<01:27,  7.31it/s]Measuring inference for batch_size=1:  36%|███▋      | 363/1000 [00:49<01:27,  7.30it/s]Measuring inference for batch_size=1:  36%|███▋      | 364/1000 [00:49<01:27,  7.30it/s]Measuring inference for batch_size=1:  36%|███▋      | 365/1000 [00:49<01:26,  7.30it/s]Measuring inference for batch_size=1:  37%|███▋      | 366/1000 [00:50<01:26,  7.30it/s]Measuring inference for batch_size=1:  37%|███▋      | 367/1000 [00:50<01:26,  7.30it/s]Measuring inference for batch_size=1:  37%|███▋      | 368/1000 [00:50<01:26,  7.30it/s]Measuring inference for batch_size=1:  37%|███▋      | 369/1000 [00:50<01:26,  7.31it/s]Measuring inference for batch_size=1:  37%|███▋      | 370/1000 [00:50<01:26,  7.30it/s]Measuring inference for batch_size=1:  37%|███▋      | 371/1000 [00:50<01:26,  7.30it/s]Measuring inference for batch_size=1:  37%|███▋      | 372/1000 [00:50<01:26,  7.30it/s]Measuring inference for batch_size=1:  37%|███▋      | 373/1000 [00:51<01:25,  7.30it/s]Measuring inference for batch_size=1:  37%|███▋      | 374/1000 [00:51<01:25,  7.30it/s]Measuring inference for batch_size=1:  38%|███▊      | 375/1000 [00:51<01:25,  7.30it/s]Measuring inference for batch_size=1:  38%|███▊      | 376/1000 [00:51<01:25,  7.30it/s]Measuring inference for batch_size=1:  38%|███▊      | 377/1000 [00:51<01:25,  7.30it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:51<01:25,  7.30it/s]Measuring inference for batch_size=1:  38%|███▊      | 379/1000 [00:51<01:25,  7.30it/s]Measuring inference for batch_size=1:  38%|███▊      | 380/1000 [00:52<01:24,  7.30it/s]Measuring inference for batch_size=1:  38%|███▊      | 381/1000 [00:52<01:24,  7.30it/s]Measuring inference for batch_size=1:  38%|███▊      | 382/1000 [00:52<01:24,  7.30it/s]Measuring inference for batch_size=1:  38%|███▊      | 383/1000 [00:52<01:24,  7.31it/s]Measuring inference for batch_size=1:  38%|███▊      | 384/1000 [00:52<01:24,  7.31it/s]Measuring inference for batch_size=1:  38%|███▊      | 385/1000 [00:52<01:24,  7.31it/s]Measuring inference for batch_size=1:  39%|███▊      | 386/1000 [00:52<01:24,  7.31it/s]Measuring inference for batch_size=1:  39%|███▊      | 387/1000 [00:52<01:23,  7.31it/s]Measuring inference for batch_size=1:  39%|███▉      | 388/1000 [00:53<01:23,  7.31it/s]Measuring inference for batch_size=1:  39%|███▉      | 389/1000 [00:53<01:23,  7.31it/s]Measuring inference for batch_size=1:  39%|███▉      | 390/1000 [00:53<01:23,  7.30it/s]Measuring inference for batch_size=1:  39%|███▉      | 391/1000 [00:53<01:23,  7.30it/s]Measuring inference for batch_size=1:  39%|███▉      | 392/1000 [00:53<01:23,  7.30it/s]Measuring inference for batch_size=1:  39%|███▉      | 393/1000 [00:53<01:23,  7.30it/s]Measuring inference for batch_size=1:  39%|███▉      | 394/1000 [00:53<01:22,  7.31it/s]Measuring inference for batch_size=1:  40%|███▉      | 395/1000 [00:54<01:22,  7.30it/s]Measuring inference for batch_size=1:  40%|███▉      | 396/1000 [00:54<01:22,  7.30it/s]Measuring inference for batch_size=1:  40%|███▉      | 397/1000 [00:54<01:22,  7.30it/s]Measuring inference for batch_size=1:  40%|███▉      | 398/1000 [00:54<01:22,  7.30it/s]Measuring inference for batch_size=1:  40%|███▉      | 399/1000 [00:54<01:22,  7.31it/s]Measuring inference for batch_size=1:  40%|████      | 400/1000 [00:54<01:22,  7.30it/s]Measuring inference for batch_size=1:  40%|████      | 401/1000 [00:54<01:21,  7.31it/s]Measuring inference for batch_size=1:  40%|████      | 402/1000 [00:55<01:21,  7.30it/s]Measuring inference for batch_size=1:  40%|████      | 403/1000 [00:55<01:21,  7.30it/s]Measuring inference for batch_size=1:  40%|████      | 404/1000 [00:55<01:21,  7.30it/s]Measuring inference for batch_size=1:  40%|████      | 405/1000 [00:55<01:21,  7.30it/s]Measuring inference for batch_size=1:  41%|████      | 406/1000 [00:55<01:21,  7.31it/s]Measuring inference for batch_size=1:  41%|████      | 407/1000 [00:55<01:21,  7.31it/s]Measuring inference for batch_size=1:  41%|████      | 408/1000 [00:55<01:21,  7.31it/s]Measuring inference for batch_size=1:  41%|████      | 409/1000 [00:55<01:20,  7.31it/s]Measuring inference for batch_size=1:  41%|████      | 410/1000 [00:56<01:20,  7.31it/s]Measuring inference for batch_size=1:  41%|████      | 411/1000 [00:56<01:20,  7.30it/s]Measuring inference for batch_size=1:  41%|████      | 412/1000 [00:56<01:20,  7.30it/s]Measuring inference for batch_size=1:  41%|████▏     | 413/1000 [00:56<01:20,  7.29it/s]Measuring inference for batch_size=1:  41%|████▏     | 414/1000 [00:56<01:20,  7.30it/s]Measuring inference for batch_size=1:  42%|████▏     | 415/1000 [00:56<01:20,  7.30it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:56<01:19,  7.30it/s]Measuring inference for batch_size=1:  42%|████▏     | 417/1000 [00:57<01:19,  7.30it/s]Measuring inference for batch_size=1:  42%|████▏     | 418/1000 [00:57<01:19,  7.30it/s]Measuring inference for batch_size=1:  42%|████▏     | 419/1000 [00:57<01:19,  7.30it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:57<01:19,  7.30it/s]Measuring inference for batch_size=1:  42%|████▏     | 421/1000 [00:57<01:19,  7.30it/s]Measuring inference for batch_size=1:  42%|████▏     | 422/1000 [00:57<01:19,  7.30it/s]Measuring inference for batch_size=1:  42%|████▏     | 423/1000 [00:57<01:19,  7.30it/s]Measuring inference for batch_size=1:  42%|████▏     | 424/1000 [00:58<01:18,  7.30it/s]Measuring inference for batch_size=1:  42%|████▎     | 425/1000 [00:58<01:18,  7.30it/s]Measuring inference for batch_size=1:  43%|████▎     | 426/1000 [00:58<01:18,  7.30it/s]Measuring inference for batch_size=1:  43%|████▎     | 427/1000 [00:58<01:18,  7.30it/s]Measuring inference for batch_size=1:  43%|████▎     | 428/1000 [00:58<01:18,  7.31it/s]Measuring inference for batch_size=1:  43%|████▎     | 429/1000 [00:58<01:18,  7.31it/s]Measuring inference for batch_size=1:  43%|████▎     | 430/1000 [00:58<01:18,  7.30it/s]Measuring inference for batch_size=1:  43%|████▎     | 431/1000 [00:58<01:17,  7.31it/s]Measuring inference for batch_size=1:  43%|████▎     | 432/1000 [00:59<01:17,  7.31it/s]Measuring inference for batch_size=1:  43%|████▎     | 433/1000 [00:59<01:17,  7.30it/s]Measuring inference for batch_size=1:  43%|████▎     | 434/1000 [00:59<01:17,  7.30it/s]Measuring inference for batch_size=1:  44%|████▎     | 435/1000 [00:59<01:17,  7.30it/s]Measuring inference for batch_size=1:  44%|████▎     | 436/1000 [00:59<01:17,  7.31it/s]Measuring inference for batch_size=1:  44%|████▎     | 437/1000 [00:59<01:17,  7.30it/s]Measuring inference for batch_size=1:  44%|████▍     | 438/1000 [00:59<01:16,  7.30it/s]Measuring inference for batch_size=1:  44%|████▍     | 439/1000 [01:00<01:16,  7.30it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [01:00<01:16,  7.31it/s]Measuring inference for batch_size=1:  44%|████▍     | 441/1000 [01:00<01:16,  7.30it/s]Measuring inference for batch_size=1:  44%|████▍     | 442/1000 [01:00<01:16,  7.30it/s]Measuring inference for batch_size=1:  44%|████▍     | 443/1000 [01:00<01:16,  7.30it/s]Measuring inference for batch_size=1:  44%|████▍     | 444/1000 [01:00<01:16,  7.31it/s]Measuring inference for batch_size=1:  44%|████▍     | 445/1000 [01:00<01:15,  7.30it/s]Measuring inference for batch_size=1:  45%|████▍     | 446/1000 [01:01<01:15,  7.31it/s]Measuring inference for batch_size=1:  45%|████▍     | 447/1000 [01:01<01:15,  7.30it/s]Measuring inference for batch_size=1:  45%|████▍     | 448/1000 [01:01<01:15,  7.30it/s]Measuring inference for batch_size=1:  45%|████▍     | 449/1000 [01:01<01:15,  7.31it/s]Measuring inference for batch_size=1:  45%|████▌     | 450/1000 [01:01<01:15,  7.31it/s]Measuring inference for batch_size=1:  45%|████▌     | 451/1000 [01:01<01:15,  7.31it/s]Measuring inference for batch_size=1:  45%|████▌     | 452/1000 [01:01<01:14,  7.31it/s]Measuring inference for batch_size=1:  45%|████▌     | 453/1000 [01:02<01:14,  7.31it/s]Measuring inference for batch_size=1:  45%|████▌     | 454/1000 [01:02<01:14,  7.31it/s]Measuring inference for batch_size=1:  46%|████▌     | 455/1000 [01:02<01:14,  7.31it/s]Measuring inference for batch_size=1:  46%|████▌     | 456/1000 [01:02<01:14,  7.31it/s]Measuring inference for batch_size=1:  46%|████▌     | 457/1000 [01:02<01:14,  7.31it/s]Measuring inference for batch_size=1:  46%|████▌     | 458/1000 [01:02<01:14,  7.31it/s]Measuring inference for batch_size=1:  46%|████▌     | 459/1000 [01:02<01:14,  7.31it/s]Measuring inference for batch_size=1:  46%|████▌     | 460/1000 [01:02<01:13,  7.31it/s]Measuring inference for batch_size=1:  46%|████▌     | 461/1000 [01:03<01:13,  7.31it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [01:03<01:13,  7.31it/s]Measuring inference for batch_size=1:  46%|████▋     | 463/1000 [01:03<01:13,  7.31it/s]Measuring inference for batch_size=1:  46%|████▋     | 464/1000 [01:03<01:13,  7.31it/s]Measuring inference for batch_size=1:  46%|████▋     | 465/1000 [01:03<01:13,  7.31it/s]Measuring inference for batch_size=1:  47%|████▋     | 466/1000 [01:03<01:13,  7.31it/s]Measuring inference for batch_size=1:  47%|████▋     | 467/1000 [01:03<01:12,  7.31it/s]Measuring inference for batch_size=1:  47%|████▋     | 468/1000 [01:04<01:12,  7.30it/s]Measuring inference for batch_size=1:  47%|████▋     | 469/1000 [01:04<01:12,  7.30it/s]Measuring inference for batch_size=1:  47%|████▋     | 470/1000 [01:04<01:12,  7.30it/s]Measuring inference for batch_size=1:  47%|████▋     | 471/1000 [01:04<01:12,  7.30it/s]Measuring inference for batch_size=1:  47%|████▋     | 472/1000 [01:04<01:12,  7.31it/s]Measuring inference for batch_size=1:  47%|████▋     | 473/1000 [01:04<01:12,  7.31it/s]Measuring inference for batch_size=1:  47%|████▋     | 474/1000 [01:04<01:11,  7.31it/s]Measuring inference for batch_size=1:  48%|████▊     | 475/1000 [01:05<01:11,  7.31it/s]Measuring inference for batch_size=1:  48%|████▊     | 476/1000 [01:05<01:11,  7.31it/s]Measuring inference for batch_size=1:  48%|████▊     | 477/1000 [01:05<01:11,  7.30it/s]Measuring inference for batch_size=1:  48%|████▊     | 478/1000 [01:05<01:11,  7.30it/s]Measuring inference for batch_size=1:  48%|████▊     | 479/1000 [01:05<01:11,  7.31it/s]Measuring inference for batch_size=1:  48%|████▊     | 480/1000 [01:05<01:11,  7.31it/s]Measuring inference for batch_size=1:  48%|████▊     | 481/1000 [01:05<01:10,  7.31it/s]Measuring inference for batch_size=1:  48%|████▊     | 482/1000 [01:05<01:10,  7.31it/s]Measuring inference for batch_size=1:  48%|████▊     | 483/1000 [01:06<01:10,  7.31it/s]Measuring inference for batch_size=1:  48%|████▊     | 484/1000 [01:06<01:10,  7.31it/s]Measuring inference for batch_size=1:  48%|████▊     | 485/1000 [01:06<01:10,  7.30it/s]Measuring inference for batch_size=1:  49%|████▊     | 486/1000 [01:06<01:10,  7.31it/s]Measuring inference for batch_size=1:  49%|████▊     | 487/1000 [01:06<01:10,  7.31it/s]Measuring inference for batch_size=1:  49%|████▉     | 488/1000 [01:06<01:10,  7.31it/s]Measuring inference for batch_size=1:  49%|████▉     | 489/1000 [01:06<01:09,  7.31it/s]Measuring inference for batch_size=1:  49%|████▉     | 490/1000 [01:07<01:09,  7.31it/s]Measuring inference for batch_size=1:  49%|████▉     | 491/1000 [01:07<01:09,  7.31it/s]Measuring inference for batch_size=1:  49%|████▉     | 492/1000 [01:07<01:09,  7.30it/s]Measuring inference for batch_size=1:  49%|████▉     | 493/1000 [01:07<01:09,  7.30it/s]Measuring inference for batch_size=1:  49%|████▉     | 494/1000 [01:07<01:09,  7.31it/s]Measuring inference for batch_size=1:  50%|████▉     | 495/1000 [01:07<01:09,  7.30it/s]Measuring inference for batch_size=1:  50%|████▉     | 496/1000 [01:07<01:09,  7.30it/s]Measuring inference for batch_size=1:  50%|████▉     | 497/1000 [01:08<01:08,  7.30it/s]Measuring inference for batch_size=1:  50%|████▉     | 498/1000 [01:08<01:08,  7.31it/s]Measuring inference for batch_size=1:  50%|████▉     | 499/1000 [01:08<01:08,  7.30it/s]Measuring inference for batch_size=1:  50%|█████     | 500/1000 [01:08<01:08,  7.30it/s]Measuring inference for batch_size=1:  50%|█████     | 501/1000 [01:08<01:08,  7.30it/s]Measuring inference for batch_size=1:  50%|█████     | 502/1000 [01:08<01:08,  7.30it/s]Measuring inference for batch_size=1:  50%|█████     | 503/1000 [01:08<01:07,  7.31it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [01:08<01:07,  7.30it/s]Measuring inference for batch_size=1:  50%|█████     | 505/1000 [01:09<01:07,  7.30it/s]Measuring inference for batch_size=1:  51%|█████     | 506/1000 [01:09<01:07,  7.31it/s]Measuring inference for batch_size=1:  51%|█████     | 507/1000 [01:09<01:07,  7.31it/s]Measuring inference for batch_size=1:  51%|█████     | 508/1000 [01:09<01:07,  7.31it/s]Measuring inference for batch_size=1:  51%|█████     | 509/1000 [01:09<01:07,  7.31it/s]Measuring inference for batch_size=1:  51%|█████     | 510/1000 [01:09<01:07,  7.30it/s]Measuring inference for batch_size=1:  51%|█████     | 511/1000 [01:09<01:06,  7.30it/s]Measuring inference for batch_size=1:  51%|█████     | 512/1000 [01:10<01:06,  7.30it/s]Measuring inference for batch_size=1:  51%|█████▏    | 513/1000 [01:10<01:06,  7.30it/s]Measuring inference for batch_size=1:  51%|█████▏    | 514/1000 [01:10<01:06,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 515/1000 [01:10<01:06,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 516/1000 [01:10<01:06,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 517/1000 [01:10<01:06,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 518/1000 [01:10<01:05,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 519/1000 [01:11<01:05,  7.30it/s]Measuring inference for batch_size=1:  52%|█████▏    | 520/1000 [01:11<01:05,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 521/1000 [01:11<01:05,  7.31it/s]Measuring inference for batch_size=1:  52%|█████▏    | 522/1000 [01:11<01:05,  7.30it/s]Measuring inference for batch_size=1:  52%|█████▏    | 523/1000 [01:11<01:05,  7.30it/s]Measuring inference for batch_size=1:  52%|█████▏    | 524/1000 [01:11<01:05,  7.30it/s]Measuring inference for batch_size=1:  52%|█████▎    | 525/1000 [01:11<01:05,  7.31it/s]Measuring inference for batch_size=1:  53%|█████▎    | 526/1000 [01:11<01:04,  7.30it/s]Measuring inference for batch_size=1:  53%|█████▎    | 527/1000 [01:12<01:04,  7.30it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [01:12<01:04,  7.31it/s]Measuring inference for batch_size=1:  53%|█████▎    | 529/1000 [01:12<01:04,  7.31it/s]Measuring inference for batch_size=1:  53%|█████▎    | 530/1000 [01:12<01:04,  7.31it/s]Measuring inference for batch_size=1:  53%|█████▎    | 531/1000 [01:12<01:04,  7.31it/s]Measuring inference for batch_size=1:  53%|█████▎    | 532/1000 [01:12<01:03,  7.31it/s]Measuring inference for batch_size=1:  53%|█████▎    | 533/1000 [01:12<01:03,  7.31it/s]Measuring inference for batch_size=1:  53%|█████▎    | 534/1000 [01:13<01:03,  7.31it/s]Measuring inference for batch_size=1:  54%|█████▎    | 535/1000 [01:13<01:03,  7.30it/s]Measuring inference for batch_size=1:  54%|█████▎    | 536/1000 [01:13<01:03,  7.30it/s]Measuring inference for batch_size=1:  54%|█████▎    | 537/1000 [01:13<01:03,  7.29it/s]Measuring inference for batch_size=1:  54%|█████▍    | 538/1000 [01:13<01:03,  7.30it/s]Measuring inference for batch_size=1:  54%|█████▍    | 539/1000 [01:13<01:03,  7.30it/s]Measuring inference for batch_size=1:  54%|█████▍    | 540/1000 [01:13<01:03,  7.30it/s]Measuring inference for batch_size=1:  54%|█████▍    | 541/1000 [01:14<01:02,  7.30it/s]Measuring inference for batch_size=1:  54%|█████▍    | 542/1000 [01:14<01:02,  7.30it/s]Measuring inference for batch_size=1:  54%|█████▍    | 543/1000 [01:14<01:02,  7.30it/s]Measuring inference for batch_size=1:  54%|█████▍    | 544/1000 [01:14<01:02,  7.30it/s]Measuring inference for batch_size=1:  55%|█████▍    | 545/1000 [01:14<01:02,  7.30it/s]Measuring inference for batch_size=1:  55%|█████▍    | 546/1000 [01:14<01:02,  7.29it/s]Measuring inference for batch_size=1:  55%|█████▍    | 547/1000 [01:14<01:02,  7.29it/s]Measuring inference for batch_size=1:  55%|█████▍    | 548/1000 [01:15<01:01,  7.29it/s]Measuring inference for batch_size=1:  55%|█████▍    | 549/1000 [01:15<01:01,  7.30it/s]Measuring inference for batch_size=1:  55%|█████▌    | 550/1000 [01:15<01:01,  7.29it/s]Measuring inference for batch_size=1:  55%|█████▌    | 551/1000 [01:15<01:01,  7.29it/s]Measuring inference for batch_size=1:  55%|█████▌    | 552/1000 [01:15<01:01,  7.29it/s]Measuring inference for batch_size=1:  55%|█████▌    | 553/1000 [01:15<01:01,  7.30it/s]Measuring inference for batch_size=1:  55%|█████▌    | 554/1000 [01:15<01:01,  7.29it/s]Measuring inference for batch_size=1:  56%|█████▌    | 555/1000 [01:15<01:01,  7.29it/s]Measuring inference for batch_size=1:  56%|█████▌    | 556/1000 [01:16<01:00,  7.29it/s]Measuring inference for batch_size=1:  56%|█████▌    | 557/1000 [01:16<01:00,  7.29it/s]Measuring inference for batch_size=1:  56%|█████▌    | 558/1000 [01:16<01:00,  7.29it/s]Measuring inference for batch_size=1:  56%|█████▌    | 559/1000 [01:16<01:00,  7.29it/s]Measuring inference for batch_size=1:  56%|█████▌    | 560/1000 [01:16<01:00,  7.30it/s]Measuring inference for batch_size=1:  56%|█████▌    | 561/1000 [01:16<01:00,  7.30it/s]Measuring inference for batch_size=1:  56%|█████▌    | 562/1000 [01:16<00:59,  7.30it/s]Measuring inference for batch_size=1:  56%|█████▋    | 563/1000 [01:17<00:59,  7.30it/s]Measuring inference for batch_size=1:  56%|█████▋    | 564/1000 [01:17<00:59,  7.31it/s]Measuring inference for batch_size=1:  56%|█████▋    | 565/1000 [01:17<00:59,  7.30it/s]Measuring inference for batch_size=1:  57%|█████▋    | 566/1000 [01:17<00:59,  7.30it/s]Measuring inference for batch_size=1:  57%|█████▋    | 567/1000 [01:17<00:59,  7.30it/s]Measuring inference for batch_size=1:  57%|█████▋    | 568/1000 [01:17<00:59,  7.30it/s]Measuring inference for batch_size=1:  57%|█████▋    | 569/1000 [01:17<00:59,  7.29it/s]Measuring inference for batch_size=1:  57%|█████▋    | 570/1000 [01:18<00:58,  7.29it/s]Measuring inference for batch_size=1:  57%|█████▋    | 571/1000 [01:18<00:58,  7.30it/s]Measuring inference for batch_size=1:  57%|█████▋    | 572/1000 [01:18<00:58,  7.30it/s]Measuring inference for batch_size=1:  57%|█████▋    | 573/1000 [01:18<00:58,  7.30it/s]Measuring inference for batch_size=1:  57%|█████▋    | 574/1000 [01:18<00:58,  7.30it/s]Measuring inference for batch_size=1:  57%|█████▊    | 575/1000 [01:18<00:58,  7.30it/s]Measuring inference for batch_size=1:  58%|█████▊    | 576/1000 [01:18<00:58,  7.30it/s]Measuring inference for batch_size=1:  58%|█████▊    | 577/1000 [01:18<00:57,  7.29it/s]Measuring inference for batch_size=1:  58%|█████▊    | 578/1000 [01:19<00:57,  7.30it/s]Measuring inference for batch_size=1:  58%|█████▊    | 579/1000 [01:19<00:57,  7.30it/s]Measuring inference for batch_size=1:  58%|█████▊    | 580/1000 [01:19<00:57,  7.30it/s]Measuring inference for batch_size=1:  58%|█████▊    | 581/1000 [01:19<00:57,  7.30it/s]Measuring inference for batch_size=1:  58%|█████▊    | 582/1000 [01:19<00:57,  7.29it/s]Measuring inference for batch_size=1:  58%|█████▊    | 583/1000 [01:19<00:57,  7.30it/s]Measuring inference for batch_size=1:  58%|█████▊    | 584/1000 [01:19<00:56,  7.30it/s]Measuring inference for batch_size=1:  58%|█████▊    | 585/1000 [01:20<00:56,  7.30it/s]Measuring inference for batch_size=1:  59%|█████▊    | 586/1000 [01:20<00:56,  7.30it/s]Measuring inference for batch_size=1:  59%|█████▊    | 587/1000 [01:20<00:56,  7.30it/s]Measuring inference for batch_size=1:  59%|█████▉    | 588/1000 [01:20<00:56,  7.31it/s]Measuring inference for batch_size=1:  59%|█████▉    | 589/1000 [01:20<00:56,  7.30it/s]Measuring inference for batch_size=1:  59%|█████▉    | 590/1000 [01:20<00:56,  7.31it/s]Measuring inference for batch_size=1:  59%|█████▉    | 591/1000 [01:20<00:56,  7.30it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [01:21<00:55,  7.30it/s]Measuring inference for batch_size=1:  59%|█████▉    | 593/1000 [01:21<00:55,  7.30it/s]Measuring inference for batch_size=1:  59%|█████▉    | 594/1000 [01:21<00:55,  7.31it/s]Measuring inference for batch_size=1:  60%|█████▉    | 595/1000 [01:21<00:55,  7.30it/s]Measuring inference for batch_size=1:  60%|█████▉    | 596/1000 [01:21<00:55,  7.30it/s]Measuring inference for batch_size=1:  60%|█████▉    | 597/1000 [01:21<00:55,  7.31it/s]Measuring inference for batch_size=1:  60%|█████▉    | 598/1000 [01:21<00:55,  7.31it/s]Measuring inference for batch_size=1:  60%|█████▉    | 599/1000 [01:22<00:54,  7.30it/s]Measuring inference for batch_size=1:  60%|██████    | 600/1000 [01:22<00:54,  7.31it/s]Measuring inference for batch_size=1:  60%|██████    | 601/1000 [01:22<00:54,  7.31it/s]Measuring inference for batch_size=1:  60%|██████    | 602/1000 [01:22<00:54,  7.31it/s]Measuring inference for batch_size=1:  60%|██████    | 603/1000 [01:22<00:54,  7.30it/s]Measuring inference for batch_size=1:  60%|██████    | 604/1000 [01:22<00:54,  7.30it/s]Measuring inference for batch_size=1:  60%|██████    | 605/1000 [01:22<00:54,  7.30it/s]Measuring inference for batch_size=1:  61%|██████    | 606/1000 [01:22<00:53,  7.30it/s]Measuring inference for batch_size=1:  61%|██████    | 607/1000 [01:23<00:53,  7.31it/s]Measuring inference for batch_size=1:  61%|██████    | 608/1000 [01:23<00:53,  7.31it/s]Measuring inference for batch_size=1:  61%|██████    | 609/1000 [01:23<00:53,  7.30it/s]Measuring inference for batch_size=1:  61%|██████    | 610/1000 [01:23<00:53,  7.30it/s]Measuring inference for batch_size=1:  61%|██████    | 611/1000 [01:23<00:53,  7.30it/s]Measuring inference for batch_size=1:  61%|██████    | 612/1000 [01:23<00:53,  7.30it/s]Measuring inference for batch_size=1:  61%|██████▏   | 613/1000 [01:23<00:53,  7.30it/s]Measuring inference for batch_size=1:  61%|██████▏   | 614/1000 [01:24<00:52,  7.30it/s]Measuring inference for batch_size=1:  62%|██████▏   | 615/1000 [01:24<00:52,  7.30it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [01:24<00:52,  7.30it/s]Measuring inference for batch_size=1:  62%|██████▏   | 617/1000 [01:24<00:52,  7.30it/s]Measuring inference for batch_size=1:  62%|██████▏   | 618/1000 [01:24<00:52,  7.30it/s]Measuring inference for batch_size=1:  62%|██████▏   | 619/1000 [01:24<00:52,  7.30it/s]Measuring inference for batch_size=1:  62%|██████▏   | 620/1000 [01:24<00:52,  7.30it/s]Measuring inference for batch_size=1:  62%|██████▏   | 621/1000 [01:25<00:51,  7.30it/s]Measuring inference for batch_size=1:  62%|██████▏   | 622/1000 [01:25<00:51,  7.30it/s]Measuring inference for batch_size=1:  62%|██████▏   | 623/1000 [01:25<00:51,  7.30it/s]Measuring inference for batch_size=1:  62%|██████▏   | 624/1000 [01:25<00:51,  7.30it/s]Measuring inference for batch_size=1:  62%|██████▎   | 625/1000 [01:25<00:51,  7.30it/s]Measuring inference for batch_size=1:  63%|██████▎   | 626/1000 [01:25<00:51,  7.30it/s]Measuring inference for batch_size=1:  63%|██████▎   | 627/1000 [01:25<00:51,  7.30it/s]Measuring inference for batch_size=1:  63%|██████▎   | 628/1000 [01:25<00:50,  7.30it/s]Measuring inference for batch_size=1:  63%|██████▎   | 629/1000 [01:26<00:50,  7.30it/s]Measuring inference for batch_size=1:  63%|██████▎   | 630/1000 [01:26<00:50,  7.31it/s]Measuring inference for batch_size=1:  63%|██████▎   | 631/1000 [01:26<00:50,  7.31it/s]Measuring inference for batch_size=1:  63%|██████▎   | 632/1000 [01:26<00:50,  7.31it/s]Measuring inference for batch_size=1:  63%|██████▎   | 633/1000 [01:26<00:50,  7.31it/s]Measuring inference for batch_size=1:  63%|██████▎   | 634/1000 [01:26<00:50,  7.31it/s]Measuring inference for batch_size=1:  64%|██████▎   | 635/1000 [01:26<00:49,  7.30it/s]Measuring inference for batch_size=1:  64%|██████▎   | 636/1000 [01:27<00:49,  7.30it/s]Measuring inference for batch_size=1:  64%|██████▎   | 637/1000 [01:27<00:49,  7.30it/s]Measuring inference for batch_size=1:  64%|██████▍   | 638/1000 [01:27<00:49,  7.31it/s]Measuring inference for batch_size=1:  64%|██████▍   | 639/1000 [01:27<00:49,  7.31it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [01:27<00:49,  7.30it/s]Measuring inference for batch_size=1:  64%|██████▍   | 641/1000 [01:27<00:49,  7.29it/s]Measuring inference for batch_size=1:  64%|██████▍   | 642/1000 [01:27<00:49,  7.30it/s]Measuring inference for batch_size=1:  64%|██████▍   | 643/1000 [01:28<00:48,  7.30it/s]Measuring inference for batch_size=1:  64%|██████▍   | 644/1000 [01:28<00:48,  7.30it/s]Measuring inference for batch_size=1:  64%|██████▍   | 645/1000 [01:28<00:48,  7.30it/s]Measuring inference for batch_size=1:  65%|██████▍   | 646/1000 [01:28<00:48,  7.30it/s]Measuring inference for batch_size=1:  65%|██████▍   | 647/1000 [01:28<00:48,  7.30it/s]Measuring inference for batch_size=1:  65%|██████▍   | 648/1000 [01:28<00:48,  7.30it/s]Measuring inference for batch_size=1:  65%|██████▍   | 649/1000 [01:28<00:48,  7.30it/s]Measuring inference for batch_size=1:  65%|██████▌   | 650/1000 [01:28<00:47,  7.30it/s]Measuring inference for batch_size=1:  65%|██████▌   | 651/1000 [01:29<00:47,  7.30it/s]Measuring inference for batch_size=1:  65%|██████▌   | 652/1000 [01:29<00:47,  7.31it/s]Measuring inference for batch_size=1:  65%|██████▌   | 653/1000 [01:29<00:47,  7.30it/s]Measuring inference for batch_size=1:  65%|██████▌   | 654/1000 [01:29<00:47,  7.30it/s]Measuring inference for batch_size=1:  66%|██████▌   | 655/1000 [01:29<00:47,  7.30it/s]Measuring inference for batch_size=1:  66%|██████▌   | 656/1000 [01:29<00:47,  7.30it/s]Measuring inference for batch_size=1:  66%|██████▌   | 657/1000 [01:29<00:47,  7.30it/s]Measuring inference for batch_size=1:  66%|██████▌   | 658/1000 [01:30<00:46,  7.30it/s]Measuring inference for batch_size=1:  66%|██████▌   | 659/1000 [01:30<00:46,  7.30it/s]Measuring inference for batch_size=1:  66%|██████▌   | 660/1000 [01:30<00:46,  7.30it/s]Measuring inference for batch_size=1:  66%|██████▌   | 661/1000 [01:30<00:46,  7.30it/s]Measuring inference for batch_size=1:  66%|██████▌   | 662/1000 [01:30<00:46,  7.30it/s]Measuring inference for batch_size=1:  66%|██████▋   | 663/1000 [01:30<00:46,  7.30it/s]Measuring inference for batch_size=1:  66%|██████▋   | 664/1000 [01:30<00:46,  7.30it/s]Measuring inference for batch_size=1:  66%|██████▋   | 665/1000 [01:31<00:45,  7.30it/s]Measuring inference for batch_size=1:  67%|██████▋   | 666/1000 [01:31<00:45,  7.30it/s]Measuring inference for batch_size=1:  67%|██████▋   | 667/1000 [01:31<00:45,  7.30it/s]Measuring inference for batch_size=1:  67%|██████▋   | 668/1000 [01:31<00:45,  7.30it/s]Measuring inference for batch_size=1:  67%|██████▋   | 669/1000 [01:31<00:45,  7.30it/s]Measuring inference for batch_size=1:  67%|██████▋   | 670/1000 [01:31<00:45,  7.30it/s]Measuring inference for batch_size=1:  67%|██████▋   | 671/1000 [01:31<00:45,  7.30it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [01:31<00:44,  7.30it/s]Measuring inference for batch_size=1:  67%|██████▋   | 673/1000 [01:32<00:44,  7.29it/s]Measuring inference for batch_size=1:  67%|██████▋   | 674/1000 [01:32<00:44,  7.30it/s]Measuring inference for batch_size=1:  68%|██████▊   | 675/1000 [01:32<00:44,  7.30it/s]Measuring inference for batch_size=1:  68%|██████▊   | 676/1000 [01:32<00:44,  7.29it/s]Measuring inference for batch_size=1:  68%|██████▊   | 677/1000 [01:32<00:44,  7.29it/s]Measuring inference for batch_size=1:  68%|██████▊   | 678/1000 [01:32<00:44,  7.29it/s]Measuring inference for batch_size=1:  68%|██████▊   | 679/1000 [01:32<00:43,  7.30it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [01:33<00:43,  7.30it/s]Measuring inference for batch_size=1:  68%|██████▊   | 681/1000 [01:33<00:43,  7.30it/s]Measuring inference for batch_size=1:  68%|██████▊   | 682/1000 [01:33<00:43,  7.30it/s]Measuring inference for batch_size=1:  68%|██████▊   | 683/1000 [01:33<00:43,  7.30it/s]Measuring inference for batch_size=1:  68%|██████▊   | 684/1000 [01:33<00:43,  7.30it/s]Measuring inference for batch_size=1:  68%|██████▊   | 685/1000 [01:33<00:43,  7.30it/s]Measuring inference for batch_size=1:  69%|██████▊   | 686/1000 [01:33<00:43,  7.30it/s]Measuring inference for batch_size=1:  69%|██████▊   | 687/1000 [01:34<00:42,  7.30it/s]Measuring inference for batch_size=1:  69%|██████▉   | 688/1000 [01:34<00:42,  7.30it/s]Measuring inference for batch_size=1:  69%|██████▉   | 689/1000 [01:34<00:42,  7.30it/s]Measuring inference for batch_size=1:  69%|██████▉   | 690/1000 [01:34<00:42,  7.30it/s]Measuring inference for batch_size=1:  69%|██████▉   | 691/1000 [01:34<00:42,  7.30it/s]Measuring inference for batch_size=1:  69%|██████▉   | 692/1000 [01:34<00:42,  7.29it/s]Measuring inference for batch_size=1:  69%|██████▉   | 693/1000 [01:34<00:42,  7.30it/s]Measuring inference for batch_size=1:  69%|██████▉   | 694/1000 [01:35<00:41,  7.30it/s]Measuring inference for batch_size=1:  70%|██████▉   | 695/1000 [01:35<00:41,  7.30it/s]Measuring inference for batch_size=1:  70%|██████▉   | 696/1000 [01:35<00:41,  7.31it/s]Measuring inference for batch_size=1:  70%|██████▉   | 697/1000 [01:35<00:41,  7.31it/s]Measuring inference for batch_size=1:  70%|██████▉   | 698/1000 [01:35<00:41,  7.31it/s]Measuring inference for batch_size=1:  70%|██████▉   | 699/1000 [01:35<00:41,  7.31it/s]Measuring inference for batch_size=1:  70%|███████   | 700/1000 [01:35<00:41,  7.31it/s]Measuring inference for batch_size=1:  70%|███████   | 701/1000 [01:35<00:40,  7.31it/s]Measuring inference for batch_size=1:  70%|███████   | 702/1000 [01:36<00:40,  7.31it/s]Measuring inference for batch_size=1:  70%|███████   | 703/1000 [01:36<00:40,  7.31it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [01:36<00:40,  7.30it/s]Measuring inference for batch_size=1:  70%|███████   | 705/1000 [01:36<00:40,  7.30it/s]Measuring inference for batch_size=1:  71%|███████   | 706/1000 [01:36<00:40,  7.30it/s]Measuring inference for batch_size=1:  71%|███████   | 707/1000 [01:36<00:40,  7.30it/s]Measuring inference for batch_size=1:  71%|███████   | 708/1000 [01:36<00:39,  7.30it/s]Measuring inference for batch_size=1:  71%|███████   | 709/1000 [01:37<00:39,  7.31it/s]Measuring inference for batch_size=1:  71%|███████   | 710/1000 [01:37<00:39,  7.30it/s]Measuring inference for batch_size=1:  71%|███████   | 711/1000 [01:37<00:39,  7.30it/s]Measuring inference for batch_size=1:  71%|███████   | 712/1000 [01:37<00:39,  7.30it/s]Measuring inference for batch_size=1:  71%|███████▏  | 713/1000 [01:37<00:39,  7.30it/s]Measuring inference for batch_size=1:  71%|███████▏  | 714/1000 [01:37<00:39,  7.30it/s]Measuring inference for batch_size=1:  72%|███████▏  | 715/1000 [01:37<00:39,  7.30it/s]Measuring inference for batch_size=1:  72%|███████▏  | 716/1000 [01:38<00:38,  7.30it/s]Measuring inference for batch_size=1:  72%|███████▏  | 717/1000 [01:38<00:38,  7.30it/s]Measuring inference for batch_size=1:  72%|███████▏  | 718/1000 [01:38<00:38,  7.30it/s]Measuring inference for batch_size=1:  72%|███████▏  | 719/1000 [01:38<00:38,  7.29it/s]Measuring inference for batch_size=1:  72%|███████▏  | 720/1000 [01:38<00:38,  7.30it/s]Measuring inference for batch_size=1:  72%|███████▏  | 721/1000 [01:38<00:38,  7.30it/s]Measuring inference for batch_size=1:  72%|███████▏  | 722/1000 [01:38<00:38,  7.30it/s]Measuring inference for batch_size=1:  72%|███████▏  | 723/1000 [01:38<00:37,  7.31it/s]Measuring inference for batch_size=1:  72%|███████▏  | 724/1000 [01:39<00:37,  7.31it/s]Measuring inference for batch_size=1:  72%|███████▎  | 725/1000 [01:39<00:37,  7.31it/s]Measuring inference for batch_size=1:  73%|███████▎  | 726/1000 [01:39<00:37,  7.31it/s]Measuring inference for batch_size=1:  73%|███████▎  | 727/1000 [01:39<00:37,  7.31it/s]Measuring inference for batch_size=1:  73%|███████▎  | 728/1000 [01:39<00:37,  7.31it/s]Measuring inference for batch_size=1:  73%|███████▎  | 729/1000 [01:39<00:37,  7.31it/s]Measuring inference for batch_size=1:  73%|███████▎  | 730/1000 [01:39<00:36,  7.31it/s]Measuring inference for batch_size=1:  73%|███████▎  | 731/1000 [01:40<00:36,  7.31it/s]Measuring inference for batch_size=1:  73%|███████▎  | 732/1000 [01:40<00:36,  7.31it/s]Measuring inference for batch_size=1:  73%|███████▎  | 733/1000 [01:40<00:36,  7.30it/s]Measuring inference for batch_size=1:  73%|███████▎  | 734/1000 [01:40<00:36,  7.31it/s]Measuring inference for batch_size=1:  74%|███████▎  | 735/1000 [01:40<00:36,  7.31it/s]Measuring inference for batch_size=1:  74%|███████▎  | 736/1000 [01:40<00:36,  7.31it/s]Measuring inference for batch_size=1:  74%|███████▎  | 737/1000 [01:40<00:35,  7.31it/s]Measuring inference for batch_size=1:  74%|███████▍  | 738/1000 [01:41<00:35,  7.31it/s]Measuring inference for batch_size=1:  74%|███████▍  | 739/1000 [01:41<00:35,  7.31it/s]Measuring inference for batch_size=1:  74%|███████▍  | 740/1000 [01:41<00:35,  7.30it/s]Measuring inference for batch_size=1:  74%|███████▍  | 741/1000 [01:41<00:35,  7.30it/s]Measuring inference for batch_size=1:  74%|███████▍  | 742/1000 [01:41<00:35,  7.30it/s]Measuring inference for batch_size=1:  74%|███████▍  | 743/1000 [01:41<00:35,  7.31it/s]Measuring inference for batch_size=1:  74%|███████▍  | 744/1000 [01:41<00:35,  7.31it/s]Measuring inference for batch_size=1:  74%|███████▍  | 745/1000 [01:41<00:34,  7.31it/s]Measuring inference for batch_size=1:  75%|███████▍  | 746/1000 [01:42<00:34,  7.30it/s]Measuring inference for batch_size=1:  75%|███████▍  | 747/1000 [01:42<00:34,  7.30it/s]Measuring inference for batch_size=1:  75%|███████▍  | 748/1000 [01:42<00:34,  7.30it/s]Measuring inference for batch_size=1:  75%|███████▍  | 749/1000 [01:42<00:34,  7.29it/s]Measuring inference for batch_size=1:  75%|███████▌  | 750/1000 [01:42<00:34,  7.30it/s]Measuring inference for batch_size=1:  75%|███████▌  | 751/1000 [01:42<00:34,  7.30it/s]Measuring inference for batch_size=1:  75%|███████▌  | 752/1000 [01:42<00:33,  7.30it/s]Measuring inference for batch_size=1:  75%|███████▌  | 753/1000 [01:43<00:33,  7.31it/s]Measuring inference for batch_size=1:  75%|███████▌  | 754/1000 [01:43<00:33,  7.31it/s]Measuring inference for batch_size=1:  76%|███████▌  | 755/1000 [01:43<00:33,  7.30it/s]Measuring inference for batch_size=1:  76%|███████▌  | 756/1000 [01:43<00:33,  7.31it/s]Measuring inference for batch_size=1:  76%|███████▌  | 757/1000 [01:43<00:33,  7.31it/s]Measuring inference for batch_size=1:  76%|███████▌  | 758/1000 [01:43<00:33,  7.30it/s]Measuring inference for batch_size=1:  76%|███████▌  | 759/1000 [01:43<00:33,  7.30it/s]Measuring inference for batch_size=1:  76%|███████▌  | 760/1000 [01:44<00:32,  7.30it/s]Measuring inference for batch_size=1:  76%|███████▌  | 761/1000 [01:44<00:32,  7.30it/s]Measuring inference for batch_size=1:  76%|███████▌  | 762/1000 [01:44<00:32,  7.30it/s]Measuring inference for batch_size=1:  76%|███████▋  | 763/1000 [01:44<00:32,  7.30it/s]Measuring inference for batch_size=1:  76%|███████▋  | 764/1000 [01:44<00:32,  7.30it/s]Measuring inference for batch_size=1:  76%|███████▋  | 765/1000 [01:44<00:32,  7.30it/s]Measuring inference for batch_size=1:  77%|███████▋  | 766/1000 [01:44<00:32,  7.31it/s]Measuring inference for batch_size=1:  77%|███████▋  | 767/1000 [01:45<00:31,  7.30it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [01:45<00:31,  7.30it/s]Measuring inference for batch_size=1:  77%|███████▋  | 769/1000 [01:45<00:31,  7.30it/s]Measuring inference for batch_size=1:  77%|███████▋  | 770/1000 [01:45<00:31,  7.30it/s]Measuring inference for batch_size=1:  77%|███████▋  | 771/1000 [01:45<00:31,  7.30it/s]Measuring inference for batch_size=1:  77%|███████▋  | 772/1000 [01:45<00:31,  7.30it/s]Measuring inference for batch_size=1:  77%|███████▋  | 773/1000 [01:45<00:31,  7.30it/s]Measuring inference for batch_size=1:  77%|███████▋  | 774/1000 [01:45<00:30,  7.31it/s]Measuring inference for batch_size=1:  78%|███████▊  | 775/1000 [01:46<00:30,  7.30it/s]Measuring inference for batch_size=1:  78%|███████▊  | 776/1000 [01:46<00:30,  7.30it/s]Measuring inference for batch_size=1:  78%|███████▊  | 777/1000 [01:46<00:30,  7.30it/s]Measuring inference for batch_size=1:  78%|███████▊  | 778/1000 [01:46<00:30,  7.30it/s]Measuring inference for batch_size=1:  78%|███████▊  | 779/1000 [01:46<00:30,  7.31it/s]Measuring inference for batch_size=1:  78%|███████▊  | 780/1000 [01:46<00:30,  7.31it/s]Measuring inference for batch_size=1:  78%|███████▊  | 781/1000 [01:46<00:29,  7.30it/s]Measuring inference for batch_size=1:  78%|███████▊  | 782/1000 [01:47<00:29,  7.30it/s]Measuring inference for batch_size=1:  78%|███████▊  | 783/1000 [01:47<00:29,  7.30it/s]Measuring inference for batch_size=1:  78%|███████▊  | 784/1000 [01:47<00:29,  7.29it/s]Measuring inference for batch_size=1:  78%|███████▊  | 785/1000 [01:47<00:29,  7.30it/s]Measuring inference for batch_size=1:  79%|███████▊  | 786/1000 [01:47<00:29,  7.30it/s]Measuring inference for batch_size=1:  79%|███████▊  | 787/1000 [01:47<00:29,  7.30it/s]Measuring inference for batch_size=1:  79%|███████▉  | 788/1000 [01:47<00:29,  7.30it/s]Measuring inference for batch_size=1:  79%|███████▉  | 789/1000 [01:48<00:28,  7.30it/s]Measuring inference for batch_size=1:  79%|███████▉  | 790/1000 [01:48<00:28,  7.30it/s]Measuring inference for batch_size=1:  79%|███████▉  | 791/1000 [01:48<00:28,  7.30it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [01:48<00:28,  7.30it/s]Measuring inference for batch_size=1:  79%|███████▉  | 793/1000 [01:48<00:28,  7.30it/s]Measuring inference for batch_size=1:  79%|███████▉  | 794/1000 [01:48<00:28,  7.30it/s]Measuring inference for batch_size=1:  80%|███████▉  | 795/1000 [01:48<00:28,  7.30it/s]Measuring inference for batch_size=1:  80%|███████▉  | 796/1000 [01:48<00:27,  7.30it/s]Measuring inference for batch_size=1:  80%|███████▉  | 797/1000 [01:49<00:27,  7.30it/s]Measuring inference for batch_size=1:  80%|███████▉  | 798/1000 [01:49<00:27,  7.29it/s]Measuring inference for batch_size=1:  80%|███████▉  | 799/1000 [01:49<00:27,  7.30it/s]Measuring inference for batch_size=1:  80%|████████  | 800/1000 [01:49<00:27,  7.30it/s]Measuring inference for batch_size=1:  80%|████████  | 801/1000 [01:49<00:27,  7.30it/s]Measuring inference for batch_size=1:  80%|████████  | 802/1000 [01:49<00:27,  7.30it/s]Measuring inference for batch_size=1:  80%|████████  | 803/1000 [01:49<00:26,  7.30it/s]Measuring inference for batch_size=1:  80%|████████  | 804/1000 [01:50<00:26,  7.30it/s]Measuring inference for batch_size=1:  80%|████████  | 805/1000 [01:50<00:26,  7.30it/s]Measuring inference for batch_size=1:  81%|████████  | 806/1000 [01:50<00:26,  7.30it/s]Measuring inference for batch_size=1:  81%|████████  | 807/1000 [01:50<00:26,  7.30it/s]Measuring inference for batch_size=1:  81%|████████  | 808/1000 [01:50<00:26,  7.30it/s]Measuring inference for batch_size=1:  81%|████████  | 809/1000 [01:50<00:26,  7.30it/s]Measuring inference for batch_size=1:  81%|████████  | 810/1000 [01:50<00:26,  7.30it/s]Measuring inference for batch_size=1:  81%|████████  | 811/1000 [01:51<00:25,  7.30it/s]Measuring inference for batch_size=1:  81%|████████  | 812/1000 [01:51<00:25,  7.30it/s]Measuring inference for batch_size=1:  81%|████████▏ | 813/1000 [01:51<00:25,  7.30it/s]Measuring inference for batch_size=1:  81%|████████▏ | 814/1000 [01:51<00:25,  7.30it/s]Measuring inference for batch_size=1:  82%|████████▏ | 815/1000 [01:51<00:25,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▏ | 816/1000 [01:51<00:25,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▏ | 817/1000 [01:51<00:25,  7.30it/s]Measuring inference for batch_size=1:  82%|████████▏ | 818/1000 [01:51<00:24,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▏ | 819/1000 [01:52<00:24,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▏ | 820/1000 [01:52<00:24,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▏ | 821/1000 [01:52<00:24,  7.30it/s]Measuring inference for batch_size=1:  82%|████████▏ | 822/1000 [01:52<00:24,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▏ | 823/1000 [01:52<00:24,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▏ | 824/1000 [01:52<00:24,  7.31it/s]Measuring inference for batch_size=1:  82%|████████▎ | 825/1000 [01:52<00:23,  7.31it/s]Measuring inference for batch_size=1:  83%|████████▎ | 826/1000 [01:53<00:23,  7.31it/s]Measuring inference for batch_size=1:  83%|████████▎ | 827/1000 [01:53<00:23,  7.31it/s]Measuring inference for batch_size=1:  83%|████████▎ | 828/1000 [01:53<00:23,  7.30it/s]Measuring inference for batch_size=1:  83%|████████▎ | 829/1000 [01:53<00:23,  7.30it/s]Measuring inference for batch_size=1:  83%|████████▎ | 830/1000 [01:53<00:23,  7.30it/s]Measuring inference for batch_size=1:  83%|████████▎ | 831/1000 [01:53<00:23,  7.30it/s]Measuring inference for batch_size=1:  83%|████████▎ | 832/1000 [01:53<00:23,  7.30it/s]Measuring inference for batch_size=1:  83%|████████▎ | 833/1000 [01:54<00:22,  7.30it/s]Measuring inference for batch_size=1:  83%|████████▎ | 834/1000 [01:54<00:22,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▎ | 835/1000 [01:54<00:22,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▎ | 836/1000 [01:54<00:22,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▎ | 837/1000 [01:54<00:22,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▍ | 838/1000 [01:54<00:22,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▍ | 839/1000 [01:54<00:22,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [01:55<00:21,  7.31it/s]Measuring inference for batch_size=1:  84%|████████▍ | 841/1000 [01:55<00:21,  7.30it/s]Measuring inference for batch_size=1:  84%|████████▍ | 842/1000 [01:55<00:21,  7.31it/s]Measuring inference for batch_size=1:  84%|████████▍ | 843/1000 [01:55<00:21,  7.31it/s]Measuring inference for batch_size=1:  84%|████████▍ | 844/1000 [01:55<00:21,  7.31it/s]Measuring inference for batch_size=1:  84%|████████▍ | 845/1000 [01:55<00:21,  7.31it/s]Measuring inference for batch_size=1:  85%|████████▍ | 846/1000 [01:55<00:21,  7.31it/s]Measuring inference for batch_size=1:  85%|████████▍ | 847/1000 [01:55<00:20,  7.31it/s]Measuring inference for batch_size=1:  85%|████████▍ | 848/1000 [01:56<00:20,  7.31it/s]Measuring inference for batch_size=1:  85%|████████▍ | 849/1000 [01:56<00:20,  7.31it/s]Measuring inference for batch_size=1:  85%|████████▌ | 850/1000 [01:56<00:20,  7.30it/s]Measuring inference for batch_size=1:  85%|████████▌ | 851/1000 [01:56<00:20,  7.31it/s]Measuring inference for batch_size=1:  85%|████████▌ | 852/1000 [01:56<00:20,  7.30it/s]Measuring inference for batch_size=1:  85%|████████▌ | 853/1000 [01:56<00:20,  7.30it/s]Measuring inference for batch_size=1:  85%|████████▌ | 854/1000 [01:56<00:19,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▌ | 855/1000 [01:57<00:19,  7.31it/s]Measuring inference for batch_size=1:  86%|████████▌ | 856/1000 [01:57<00:19,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▌ | 857/1000 [01:57<00:19,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▌ | 858/1000 [01:57<00:19,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▌ | 859/1000 [01:57<00:19,  7.31it/s]Measuring inference for batch_size=1:  86%|████████▌ | 860/1000 [01:57<00:19,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▌ | 861/1000 [01:57<00:19,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▌ | 862/1000 [01:58<00:18,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▋ | 863/1000 [01:58<00:18,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▋ | 864/1000 [01:58<00:18,  7.30it/s]Measuring inference for batch_size=1:  86%|████████▋ | 865/1000 [01:58<00:18,  7.30it/s]Measuring inference for batch_size=1:  87%|████████▋ | 866/1000 [01:58<00:18,  7.30it/s]Measuring inference for batch_size=1:  87%|████████▋ | 867/1000 [01:58<00:18,  7.30it/s]Measuring inference for batch_size=1:  87%|████████▋ | 868/1000 [01:58<00:18,  7.30it/s]Measuring inference for batch_size=1:  87%|████████▋ | 869/1000 [01:58<00:17,  7.31it/s]Measuring inference for batch_size=1:  87%|████████▋ | 870/1000 [01:59<00:17,  7.31it/s]Measuring inference for batch_size=1:  87%|████████▋ | 871/1000 [01:59<00:17,  7.31it/s]Measuring inference for batch_size=1:  87%|████████▋ | 872/1000 [01:59<00:17,  7.30it/s]Measuring inference for batch_size=1:  87%|████████▋ | 873/1000 [01:59<00:17,  7.31it/s]Measuring inference for batch_size=1:  87%|████████▋ | 874/1000 [01:59<00:17,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 875/1000 [01:59<00:17,  7.30it/s]Measuring inference for batch_size=1:  88%|████████▊ | 876/1000 [01:59<00:16,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 877/1000 [02:00<00:16,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 878/1000 [02:00<00:16,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 879/1000 [02:00<00:16,  7.30it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [02:00<00:16,  7.30it/s]Measuring inference for batch_size=1:  88%|████████▊ | 881/1000 [02:00<00:16,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 882/1000 [02:00<00:16,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 883/1000 [02:00<00:16,  7.31it/s]Measuring inference for batch_size=1:  88%|████████▊ | 884/1000 [02:01<00:15,  7.30it/s]Measuring inference for batch_size=1:  88%|████████▊ | 885/1000 [02:01<00:15,  7.30it/s]Measuring inference for batch_size=1:  89%|████████▊ | 886/1000 [02:01<00:15,  7.30it/s]Measuring inference for batch_size=1:  89%|████████▊ | 887/1000 [02:01<00:15,  7.31it/s]Measuring inference for batch_size=1:  89%|████████▉ | 888/1000 [02:01<00:15,  7.30it/s]Measuring inference for batch_size=1:  89%|████████▉ | 889/1000 [02:01<00:15,  7.31it/s]Measuring inference for batch_size=1:  89%|████████▉ | 890/1000 [02:01<00:15,  7.30it/s]Measuring inference for batch_size=1:  89%|████████▉ | 891/1000 [02:01<00:14,  7.31it/s]Measuring inference for batch_size=1:  89%|████████▉ | 892/1000 [02:02<00:14,  7.31it/s]Measuring inference for batch_size=1:  89%|████████▉ | 893/1000 [02:02<00:14,  7.30it/s]Measuring inference for batch_size=1:  89%|████████▉ | 894/1000 [02:02<00:14,  7.30it/s]Measuring inference for batch_size=1:  90%|████████▉ | 895/1000 [02:02<00:14,  7.31it/s]Measuring inference for batch_size=1:  90%|████████▉ | 896/1000 [02:02<00:14,  7.31it/s]Measuring inference for batch_size=1:  90%|████████▉ | 897/1000 [02:02<00:14,  7.31it/s]Measuring inference for batch_size=1:  90%|████████▉ | 898/1000 [02:02<00:13,  7.31it/s]Measuring inference for batch_size=1:  90%|████████▉ | 899/1000 [02:03<00:13,  7.31it/s]Measuring inference for batch_size=1:  90%|█████████ | 900/1000 [02:03<00:13,  7.31it/s]Measuring inference for batch_size=1:  90%|█████████ | 901/1000 [02:03<00:13,  7.31it/s]Measuring inference for batch_size=1:  90%|█████████ | 902/1000 [02:03<00:13,  7.31it/s]Measuring inference for batch_size=1:  90%|█████████ | 903/1000 [02:03<00:13,  7.31it/s]Measuring inference for batch_size=1:  90%|█████████ | 904/1000 [02:03<00:13,  7.31it/s]Measuring inference for batch_size=1:  90%|█████████ | 905/1000 [02:03<00:12,  7.31it/s]Measuring inference for batch_size=1:  91%|█████████ | 906/1000 [02:04<00:12,  7.31it/s]Measuring inference for batch_size=1:  91%|█████████ | 907/1000 [02:04<00:12,  7.31it/s]Measuring inference for batch_size=1:  91%|█████████ | 908/1000 [02:04<00:12,  7.30it/s]Measuring inference for batch_size=1:  91%|█████████ | 909/1000 [02:04<00:12,  7.31it/s]Measuring inference for batch_size=1:  91%|█████████ | 910/1000 [02:04<00:12,  7.31it/s]Measuring inference for batch_size=1:  91%|█████████ | 911/1000 [02:04<00:12,  7.31it/s]Measuring inference for batch_size=1:  91%|█████████ | 912/1000 [02:04<00:12,  7.31it/s]Measuring inference for batch_size=1:  91%|█████████▏| 913/1000 [02:04<00:11,  7.31it/s]Measuring inference for batch_size=1:  91%|█████████▏| 914/1000 [02:05<00:11,  7.31it/s]Measuring inference for batch_size=1:  92%|█████████▏| 915/1000 [02:05<00:11,  7.31it/s]Measuring inference for batch_size=1:  92%|█████████▏| 916/1000 [02:05<00:11,  7.31it/s]Measuring inference for batch_size=1:  92%|█████████▏| 917/1000 [02:05<00:11,  7.31it/s]Measuring inference for batch_size=1:  92%|█████████▏| 918/1000 [02:05<00:11,  7.31it/s]Measuring inference for batch_size=1:  92%|█████████▏| 919/1000 [02:05<00:11,  7.30it/s]Measuring inference for batch_size=1:  92%|█████████▏| 920/1000 [02:05<00:10,  7.31it/s]Measuring inference for batch_size=1:  92%|█████████▏| 921/1000 [02:06<00:10,  7.31it/s]Measuring inference for batch_size=1:  92%|█████████▏| 922/1000 [02:06<00:10,  7.30it/s]Measuring inference for batch_size=1:  92%|█████████▏| 923/1000 [02:06<00:10,  7.31it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [02:06<00:10,  7.30it/s]Measuring inference for batch_size=1:  92%|█████████▎| 925/1000 [02:06<00:10,  7.30it/s]Measuring inference for batch_size=1:  93%|█████████▎| 926/1000 [02:06<00:10,  7.30it/s]Measuring inference for batch_size=1:  93%|█████████▎| 927/1000 [02:06<00:10,  7.30it/s]Measuring inference for batch_size=1:  93%|█████████▎| 928/1000 [02:07<00:09,  7.30it/s]Measuring inference for batch_size=1:  93%|█████████▎| 929/1000 [02:07<00:09,  7.30it/s]Measuring inference for batch_size=1:  93%|█████████▎| 930/1000 [02:07<00:09,  7.30it/s]Measuring inference for batch_size=1:  93%|█████████▎| 931/1000 [02:07<00:09,  7.31it/s]Measuring inference for batch_size=1:  93%|█████████▎| 932/1000 [02:07<00:09,  7.31it/s]Measuring inference for batch_size=1:  93%|█████████▎| 933/1000 [02:07<00:09,  7.31it/s]Measuring inference for batch_size=1:  93%|█████████▎| 934/1000 [02:07<00:09,  7.31it/s]Measuring inference for batch_size=1:  94%|█████████▎| 935/1000 [02:08<00:08,  7.31it/s]Measuring inference for batch_size=1:  94%|█████████▎| 936/1000 [02:08<00:08,  7.31it/s]Measuring inference for batch_size=1:  94%|█████████▎| 937/1000 [02:08<00:08,  7.31it/s]Measuring inference for batch_size=1:  94%|█████████▍| 938/1000 [02:08<00:08,  7.30it/s]Measuring inference for batch_size=1:  94%|█████████▍| 939/1000 [02:08<00:08,  7.30it/s]Measuring inference for batch_size=1:  94%|█████████▍| 940/1000 [02:08<00:08,  7.30it/s]Measuring inference for batch_size=1:  94%|█████████▍| 941/1000 [02:08<00:08,  7.31it/s]Measuring inference for batch_size=1:  94%|█████████▍| 942/1000 [02:08<00:07,  7.31it/s]Measuring inference for batch_size=1:  94%|█████████▍| 943/1000 [02:09<00:07,  7.31it/s]Measuring inference for batch_size=1:  94%|█████████▍| 944/1000 [02:09<00:07,  7.31it/s]Measuring inference for batch_size=1:  94%|█████████▍| 945/1000 [02:09<00:07,  7.30it/s]Measuring inference for batch_size=1:  95%|█████████▍| 946/1000 [02:09<00:07,  7.30it/s]Measuring inference for batch_size=1:  95%|█████████▍| 947/1000 [02:09<00:07,  7.31it/s]Measuring inference for batch_size=1:  95%|█████████▍| 948/1000 [02:09<00:07,  7.30it/s]Measuring inference for batch_size=1:  95%|█████████▍| 949/1000 [02:09<00:06,  7.30it/s]Measuring inference for batch_size=1:  95%|█████████▌| 950/1000 [02:10<00:06,  7.30it/s]Measuring inference for batch_size=1:  95%|█████████▌| 951/1000 [02:10<00:06,  7.30it/s]Measuring inference for batch_size=1:  95%|█████████▌| 952/1000 [02:10<00:06,  7.30it/s]Measuring inference for batch_size=1:  95%|█████████▌| 953/1000 [02:10<00:06,  7.31it/s]Measuring inference for batch_size=1:  95%|█████████▌| 954/1000 [02:10<00:06,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▌| 955/1000 [02:10<00:06,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▌| 956/1000 [02:10<00:06,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▌| 957/1000 [02:11<00:05,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▌| 958/1000 [02:11<00:05,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▌| 959/1000 [02:11<00:05,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▌| 960/1000 [02:11<00:05,  7.31it/s]Measuring inference for batch_size=1:  96%|█████████▌| 961/1000 [02:11<00:05,  7.31it/s]Measuring inference for batch_size=1:  96%|█████████▌| 962/1000 [02:11<00:05,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▋| 963/1000 [02:11<00:05,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▋| 964/1000 [02:11<00:04,  7.30it/s]Measuring inference for batch_size=1:  96%|█████████▋| 965/1000 [02:12<00:04,  7.30it/s]Measuring inference for batch_size=1:  97%|█████████▋| 966/1000 [02:12<00:04,  7.31it/s]Measuring inference for batch_size=1:  97%|█████████▋| 967/1000 [02:12<00:04,  7.31it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [02:12<00:04,  7.30it/s]Measuring inference for batch_size=1:  97%|█████████▋| 969/1000 [02:12<00:04,  7.30it/s]Measuring inference for batch_size=1:  97%|█████████▋| 970/1000 [02:12<00:04,  7.31it/s]Measuring inference for batch_size=1:  97%|█████████▋| 971/1000 [02:12<00:03,  7.30it/s]Measuring inference for batch_size=1:  97%|█████████▋| 972/1000 [02:13<00:03,  7.31it/s]Measuring inference for batch_size=1:  97%|█████████▋| 973/1000 [02:13<00:03,  7.30it/s]Measuring inference for batch_size=1:  97%|█████████▋| 974/1000 [02:13<00:03,  7.30it/s]Measuring inference for batch_size=1:  98%|█████████▊| 975/1000 [02:13<00:03,  7.31it/s]Measuring inference for batch_size=1:  98%|█████████▊| 976/1000 [02:13<00:03,  7.30it/s]Measuring inference for batch_size=1:  98%|█████████▊| 977/1000 [02:13<00:03,  7.30it/s]Measuring inference for batch_size=1:  98%|█████████▊| 978/1000 [02:13<00:03,  7.30it/s]Measuring inference for batch_size=1:  98%|█████████▊| 979/1000 [02:14<00:02,  7.30it/s]Measuring inference for batch_size=1:  98%|█████████▊| 980/1000 [02:14<00:02,  7.30it/s]Measuring inference for batch_size=1:  98%|█████████▊| 981/1000 [02:14<00:02,  7.30it/s]Measuring inference for batch_size=1:  98%|█████████▊| 982/1000 [02:14<00:02,  7.30it/s]Measuring inference for batch_size=1:  98%|█████████▊| 983/1000 [02:14<00:02,  7.30it/s]Measuring inference for batch_size=1:  98%|█████████▊| 984/1000 [02:14<00:02,  7.30it/s]Measuring inference for batch_size=1:  98%|█████████▊| 985/1000 [02:14<00:02,  7.30it/s]Measuring inference for batch_size=1:  99%|█████████▊| 986/1000 [02:14<00:01,  7.30it/s]Measuring inference for batch_size=1:  99%|█████████▊| 987/1000 [02:15<00:01,  7.30it/s]Measuring inference for batch_size=1:  99%|█████████▉| 988/1000 [02:15<00:01,  7.30it/s]Measuring inference for batch_size=1:  99%|█████████▉| 989/1000 [02:15<00:01,  7.30it/s]Measuring inference for batch_size=1:  99%|█████████▉| 990/1000 [02:15<00:01,  7.30it/s]Measuring inference for batch_size=1:  99%|█████████▉| 991/1000 [02:15<00:01,  7.30it/s]Measuring inference for batch_size=1:  99%|█████████▉| 992/1000 [02:15<00:01,  7.30it/s]Measuring inference for batch_size=1:  99%|█████████▉| 993/1000 [02:15<00:00,  7.29it/s]Measuring inference for batch_size=1:  99%|█████████▉| 994/1000 [02:16<00:00,  7.30it/s]Measuring inference for batch_size=1: 100%|█████████▉| 995/1000 [02:16<00:00,  7.30it/s]Measuring inference for batch_size=1: 100%|█████████▉| 996/1000 [02:16<00:00,  7.30it/s]Measuring inference for batch_size=1: 100%|█████████▉| 997/1000 [02:16<00:00,  7.30it/s]Measuring inference for batch_size=1: 100%|█████████▉| 998/1000 [02:16<00:00,  7.30it/s]Measuring inference for batch_size=1: 100%|█████████▉| 999/1000 [02:16<00:00,  7.30it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [02:16<00:00,  7.30it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [02:16<00:00,  7.30it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Measurement of allocated memory is only available on CUDA devices
Warming up with batch_size=512:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=512:   1%|          | 1/100 [00:00<00:18,  5.41it/s]Warming up with batch_size=512:   2%|▏         | 2/100 [00:00<00:17,  5.55it/s]Warming up with batch_size=512:   3%|▎         | 3/100 [00:00<00:17,  5.60it/s]Warming up with batch_size=512:   4%|▍         | 4/100 [00:00<00:17,  5.61it/s]Warming up with batch_size=512:   5%|▌         | 5/100 [00:00<00:16,  5.62it/s]Warming up with batch_size=512:   6%|▌         | 6/100 [00:01<00:16,  5.63it/s]Warming up with batch_size=512:   7%|▋         | 7/100 [00:01<00:16,  5.63it/s]Warming up with batch_size=512:   8%|▊         | 8/100 [00:01<00:16,  5.63it/s]Warming up with batch_size=512:   9%|▉         | 9/100 [00:01<00:16,  5.63it/s]Warming up with batch_size=512:  10%|█         | 10/100 [00:01<00:15,  5.63it/s]Warming up with batch_size=512:  11%|█         | 11/100 [00:01<00:15,  5.63it/s]Warming up with batch_size=512:  12%|█▏        | 12/100 [00:02<00:15,  5.63it/s]Warming up with batch_size=512:  13%|█▎        | 13/100 [00:02<00:15,  5.63it/s]Warming up with batch_size=512:  14%|█▍        | 14/100 [00:02<00:15,  5.63it/s]Warming up with batch_size=512:  15%|█▌        | 15/100 [00:02<00:15,  5.63it/s]Warming up with batch_size=512:  16%|█▌        | 16/100 [00:02<00:14,  5.63it/s]Warming up with batch_size=512:  17%|█▋        | 17/100 [00:03<00:14,  5.63it/s]Warming up with batch_size=512:  18%|█▊        | 18/100 [00:03<00:14,  5.63it/s]Warming up with batch_size=512:  19%|█▉        | 19/100 [00:03<00:14,  5.61it/s]Warming up with batch_size=512:  20%|██        | 20/100 [00:03<00:14,  5.62it/s]Warming up with batch_size=512:  21%|██        | 21/100 [00:03<00:14,  5.63it/s]Warming up with batch_size=512:  22%|██▏       | 22/100 [00:03<00:13,  5.63it/s]Warming up with batch_size=512:  23%|██▎       | 23/100 [00:04<00:13,  5.63it/s]Warming up with batch_size=512:  24%|██▍       | 24/100 [00:04<00:13,  5.63it/s]Warming up with batch_size=512:  25%|██▌       | 25/100 [00:04<00:13,  5.63it/s]Warming up with batch_size=512:  26%|██▌       | 26/100 [00:04<00:13,  5.63it/s]Warming up with batch_size=512:  27%|██▋       | 27/100 [00:04<00:12,  5.63it/s]Warming up with batch_size=512:  28%|██▊       | 28/100 [00:04<00:12,  5.64it/s]Warming up with batch_size=512:  29%|██▉       | 29/100 [00:05<00:12,  5.64it/s]Warming up with batch_size=512:  30%|███       | 30/100 [00:05<00:12,  5.64it/s]Warming up with batch_size=512:  31%|███       | 31/100 [00:05<00:12,  5.64it/s]Warming up with batch_size=512:  32%|███▏      | 32/100 [00:05<00:12,  5.64it/s]Warming up with batch_size=512:  33%|███▎      | 33/100 [00:05<00:11,  5.63it/s]Warming up with batch_size=512:  34%|███▍      | 34/100 [00:06<00:11,  5.63it/s]Warming up with batch_size=512:  35%|███▌      | 35/100 [00:06<00:11,  5.63it/s]Warming up with batch_size=512:  36%|███▌      | 36/100 [00:06<00:11,  5.64it/s]Warming up with batch_size=512:  37%|███▋      | 37/100 [00:06<00:11,  5.63it/s]Warming up with batch_size=512:  38%|███▊      | 38/100 [00:06<00:11,  5.63it/s]Warming up with batch_size=512:  39%|███▉      | 39/100 [00:06<00:10,  5.63it/s]Warming up with batch_size=512:  40%|████      | 40/100 [00:07<00:10,  5.63it/s]Warming up with batch_size=512:  41%|████      | 41/100 [00:07<00:10,  5.64it/s]Warming up with batch_size=512:  42%|████▏     | 42/100 [00:07<00:10,  5.64it/s]Warming up with batch_size=512:  43%|████▎     | 43/100 [00:07<00:10,  5.64it/s]Warming up with batch_size=512:  44%|████▍     | 44/100 [00:07<00:09,  5.64it/s]Warming up with batch_size=512:  45%|████▌     | 45/100 [00:07<00:09,  5.64it/s]Warming up with batch_size=512:  46%|████▌     | 46/100 [00:08<00:09,  5.64it/s]Warming up with batch_size=512:  47%|████▋     | 47/100 [00:08<00:09,  5.64it/s]Warming up with batch_size=512:  48%|████▊     | 48/100 [00:08<00:09,  5.63it/s]Warming up with batch_size=512:  49%|████▉     | 49/100 [00:08<00:09,  5.64it/s]Warming up with batch_size=512:  50%|█████     | 50/100 [00:08<00:08,  5.63it/s]Warming up with batch_size=512:  51%|█████     | 51/100 [00:09<00:08,  5.63it/s]Warming up with batch_size=512:  52%|█████▏    | 52/100 [00:09<00:08,  5.63it/s]Warming up with batch_size=512:  53%|█████▎    | 53/100 [00:09<00:08,  5.63it/s]Warming up with batch_size=512:  54%|█████▍    | 54/100 [00:09<00:08,  5.64it/s]Warming up with batch_size=512:  55%|█████▌    | 55/100 [00:09<00:07,  5.64it/s]Warming up with batch_size=512:  56%|█████▌    | 56/100 [00:09<00:07,  5.64it/s]Warming up with batch_size=512:  57%|█████▋    | 57/100 [00:10<00:07,  5.64it/s]Warming up with batch_size=512:  58%|█████▊    | 58/100 [00:10<00:07,  5.64it/s]Warming up with batch_size=512:  59%|█████▉    | 59/100 [00:10<00:07,  5.64it/s]Warming up with batch_size=512:  60%|██████    | 60/100 [00:10<00:07,  5.64it/s]Warming up with batch_size=512:  61%|██████    | 61/100 [00:10<00:06,  5.64it/s]Warming up with batch_size=512:  62%|██████▏   | 62/100 [00:11<00:06,  5.64it/s]Warming up with batch_size=512:  63%|██████▎   | 63/100 [00:11<00:06,  5.64it/s]Warming up with batch_size=512:  64%|██████▍   | 64/100 [00:11<00:06,  5.63it/s]Warming up with batch_size=512:  65%|██████▌   | 65/100 [00:11<00:06,  5.63it/s]Warming up with batch_size=512:  66%|██████▌   | 66/100 [00:11<00:06,  5.63it/s]Warming up with batch_size=512:  67%|██████▋   | 67/100 [00:11<00:05,  5.64it/s]Warming up with batch_size=512:  68%|██████▊   | 68/100 [00:12<00:05,  5.64it/s]Warming up with batch_size=512:  69%|██████▉   | 69/100 [00:12<00:05,  5.64it/s]Warming up with batch_size=512:  70%|███████   | 70/100 [00:12<00:05,  5.64it/s]Warming up with batch_size=512:  71%|███████   | 71/100 [00:12<00:05,  5.64it/s]Warming up with batch_size=512:  72%|███████▏  | 72/100 [00:12<00:04,  5.64it/s]Warming up with batch_size=512:  73%|███████▎  | 73/100 [00:12<00:04,  5.63it/s]Warming up with batch_size=512:  74%|███████▍  | 74/100 [00:13<00:04,  5.63it/s]Warming up with batch_size=512:  75%|███████▌  | 75/100 [00:13<00:04,  5.63it/s]Warming up with batch_size=512:  76%|███████▌  | 76/100 [00:13<00:04,  5.63it/s]Warming up with batch_size=512:  77%|███████▋  | 77/100 [00:13<00:04,  5.64it/s]Warming up with batch_size=512:  78%|███████▊  | 78/100 [00:13<00:03,  5.64it/s]Warming up with batch_size=512:  79%|███████▉  | 79/100 [00:14<00:03,  5.64it/s]Warming up with batch_size=512:  80%|████████  | 80/100 [00:14<00:03,  5.64it/s]Warming up with batch_size=512:  81%|████████  | 81/100 [00:14<00:03,  5.64it/s]Warming up with batch_size=512:  82%|████████▏ | 82/100 [00:14<00:03,  5.63it/s]Warming up with batch_size=512:  83%|████████▎ | 83/100 [00:14<00:03,  5.63it/s]Warming up with batch_size=512:  84%|████████▍ | 84/100 [00:14<00:02,  5.63it/s]Warming up with batch_size=512:  85%|████████▌ | 85/100 [00:15<00:02,  5.63it/s]Warming up with batch_size=512:  86%|████████▌ | 86/100 [00:15<00:02,  5.63it/s]Warming up with batch_size=512:  87%|████████▋ | 87/100 [00:15<00:02,  5.63it/s]Warming up with batch_size=512:  88%|████████▊ | 88/100 [00:15<00:02,  5.64it/s]Warming up with batch_size=512:  89%|████████▉ | 89/100 [00:15<00:01,  5.64it/s]Warming up with batch_size=512:  90%|█████████ | 90/100 [00:15<00:01,  5.64it/s]Warming up with batch_size=512:  91%|█████████ | 91/100 [00:16<00:01,  5.64it/s]Warming up with batch_size=512:  92%|█████████▏| 92/100 [00:16<00:01,  5.65it/s]Warming up with batch_size=512:  93%|█████████▎| 93/100 [00:16<00:01,  5.64it/s]Warming up with batch_size=512:  94%|█████████▍| 94/100 [00:16<00:01,  5.64it/s]Warming up with batch_size=512:  95%|█████████▌| 95/100 [00:16<00:00,  5.63it/s]Warming up with batch_size=512:  96%|█████████▌| 96/100 [00:17<00:00,  5.63it/s]Warming up with batch_size=512:  97%|█████████▋| 97/100 [00:17<00:00,  5.64it/s]Warming up with batch_size=512:  98%|█████████▊| 98/100 [00:17<00:00,  5.64it/s]Warming up with batch_size=512:  99%|█████████▉| 99/100 [00:17<00:00,  5.64it/s]Warming up with batch_size=512: 100%|██████████| 100/100 [00:17<00:00,  5.64it/s]Warming up with batch_size=512: 100%|██████████| 100/100 [00:17<00:00,  5.63it/s]
STAGE:2024-02-23 09:49:23 176409:176409 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:49:23 176409:176409 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:49:23 176409:176409 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=512:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=512:   0%|          | 1/1000 [00:00<02:58,  5.59it/s]Measuring inference for batch_size=512:   0%|          | 2/1000 [00:00<02:57,  5.62it/s]Measuring inference for batch_size=512:   0%|          | 3/1000 [00:00<02:57,  5.63it/s]Measuring inference for batch_size=512:   0%|          | 4/1000 [00:00<02:56,  5.63it/s]Measuring inference for batch_size=512:   0%|          | 5/1000 [00:00<02:56,  5.64it/s]Measuring inference for batch_size=512:   1%|          | 6/1000 [00:01<02:56,  5.64it/s]Measuring inference for batch_size=512:   1%|          | 7/1000 [00:01<02:56,  5.64it/s]Measuring inference for batch_size=512:   1%|          | 8/1000 [00:01<02:55,  5.64it/s]Measuring inference for batch_size=512:   1%|          | 9/1000 [00:01<02:55,  5.64it/s]Measuring inference for batch_size=512:   1%|          | 10/1000 [00:01<02:55,  5.63it/s]Measuring inference for batch_size=512:   1%|          | 11/1000 [00:01<02:55,  5.63it/s]Measuring inference for batch_size=512:   1%|          | 12/1000 [00:02<02:55,  5.64it/s]Measuring inference for batch_size=512:   1%|▏         | 13/1000 [00:02<02:55,  5.64it/s]Measuring inference for batch_size=512:   1%|▏         | 14/1000 [00:02<02:54,  5.64it/s]Measuring inference for batch_size=512:   2%|▏         | 15/1000 [00:02<02:54,  5.64it/s]Measuring inference for batch_size=512:   2%|▏         | 16/1000 [00:02<02:54,  5.63it/s]Measuring inference for batch_size=512:   2%|▏         | 17/1000 [00:03<02:54,  5.63it/s]Measuring inference for batch_size=512:   2%|▏         | 18/1000 [00:03<02:54,  5.63it/s]Measuring inference for batch_size=512:   2%|▏         | 19/1000 [00:03<02:54,  5.63it/s]Measuring inference for batch_size=512:   2%|▏         | 20/1000 [00:03<02:54,  5.62it/s]Measuring inference for batch_size=512:   2%|▏         | 21/1000 [00:03<02:53,  5.63it/s]Measuring inference for batch_size=512:   2%|▏         | 22/1000 [00:03<02:53,  5.63it/s]Measuring inference for batch_size=512:   2%|▏         | 23/1000 [00:04<02:53,  5.63it/s]Measuring inference for batch_size=512:   2%|▏         | 24/1000 [00:04<02:53,  5.63it/s]Measuring inference for batch_size=512:   2%|▎         | 25/1000 [00:04<02:53,  5.63it/s]Measuring inference for batch_size=512:   3%|▎         | 26/1000 [00:04<02:52,  5.64it/s]Measuring inference for batch_size=512:   3%|▎         | 27/1000 [00:04<02:52,  5.64it/s]Measuring inference for batch_size=512:   3%|▎         | 28/1000 [00:04<02:52,  5.63it/s]Measuring inference for batch_size=512:   3%|▎         | 29/1000 [00:05<02:52,  5.64it/s]Measuring inference for batch_size=512:   3%|▎         | 30/1000 [00:05<02:52,  5.63it/s]Measuring inference for batch_size=512:   3%|▎         | 31/1000 [00:05<02:51,  5.64it/s]Measuring inference for batch_size=512:   3%|▎         | 32/1000 [00:05<02:51,  5.63it/s]Measuring inference for batch_size=512:   3%|▎         | 33/1000 [00:05<02:51,  5.64it/s]Measuring inference for batch_size=512:   3%|▎         | 34/1000 [00:06<02:51,  5.63it/s]Measuring inference for batch_size=512:   4%|▎         | 35/1000 [00:06<02:51,  5.64it/s]Measuring inference for batch_size=512:   4%|▎         | 36/1000 [00:06<02:51,  5.63it/s]Measuring inference for batch_size=512:   4%|▎         | 37/1000 [00:06<02:50,  5.63it/s]Measuring inference for batch_size=512:   4%|▍         | 38/1000 [00:06<02:50,  5.64it/s]Measuring inference for batch_size=512:   4%|▍         | 39/1000 [00:06<02:50,  5.64it/s]Measuring inference for batch_size=512:   4%|▍         | 40/1000 [00:07<02:50,  5.63it/s]Measuring inference for batch_size=512:   4%|▍         | 41/1000 [00:07<02:50,  5.63it/s]Measuring inference for batch_size=512:   4%|▍         | 42/1000 [00:07<02:49,  5.64it/s]Measuring inference for batch_size=512:   4%|▍         | 43/1000 [00:07<02:49,  5.64it/s]Measuring inference for batch_size=512:   4%|▍         | 44/1000 [00:07<02:49,  5.63it/s]Measuring inference for batch_size=512:   4%|▍         | 45/1000 [00:07<02:49,  5.63it/s]Measuring inference for batch_size=512:   5%|▍         | 46/1000 [00:08<02:49,  5.63it/s]Measuring inference for batch_size=512:   5%|▍         | 47/1000 [00:08<02:49,  5.63it/s]Measuring inference for batch_size=512:   5%|▍         | 48/1000 [00:08<02:48,  5.63it/s]Measuring inference for batch_size=512:   5%|▍         | 49/1000 [00:08<02:48,  5.63it/s]Measuring inference for batch_size=512:   5%|▌         | 50/1000 [00:08<02:48,  5.64it/s]Measuring inference for batch_size=512:   5%|▌         | 51/1000 [00:09<02:48,  5.63it/s]Measuring inference for batch_size=512:   5%|▌         | 52/1000 [00:09<02:48,  5.63it/s]Measuring inference for batch_size=512:   5%|▌         | 53/1000 [00:09<02:48,  5.63it/s]Measuring inference for batch_size=512:   5%|▌         | 54/1000 [00:09<02:47,  5.63it/s]Measuring inference for batch_size=512:   6%|▌         | 55/1000 [00:09<02:47,  5.63it/s]Measuring inference for batch_size=512:   6%|▌         | 56/1000 [00:09<02:47,  5.64it/s]Measuring inference for batch_size=512:   6%|▌         | 57/1000 [00:10<02:47,  5.64it/s]Measuring inference for batch_size=512:   6%|▌         | 58/1000 [00:10<02:47,  5.64it/s]Measuring inference for batch_size=512:   6%|▌         | 59/1000 [00:10<02:46,  5.64it/s]Measuring inference for batch_size=512:   6%|▌         | 60/1000 [00:10<02:46,  5.63it/s]Measuring inference for batch_size=512:   6%|▌         | 61/1000 [00:10<02:46,  5.63it/s]Measuring inference for batch_size=512:   6%|▌         | 62/1000 [00:11<02:46,  5.64it/s]Measuring inference for batch_size=512:   6%|▋         | 63/1000 [00:11<02:46,  5.63it/s]Measuring inference for batch_size=512:   6%|▋         | 64/1000 [00:11<02:46,  5.64it/s]Measuring inference for batch_size=512:   6%|▋         | 65/1000 [00:11<02:45,  5.64it/s]Measuring inference for batch_size=512:   7%|▋         | 66/1000 [00:11<02:45,  5.63it/s]Measuring inference for batch_size=512:   7%|▋         | 67/1000 [00:11<02:45,  5.63it/s]Measuring inference for batch_size=512:   7%|▋         | 68/1000 [00:12<02:45,  5.63it/s]Measuring inference for batch_size=512:   7%|▋         | 69/1000 [00:12<02:45,  5.64it/s]Measuring inference for batch_size=512:   7%|▋         | 70/1000 [00:12<02:45,  5.62it/s]Measuring inference for batch_size=512:   7%|▋         | 71/1000 [00:12<02:45,  5.62it/s]Measuring inference for batch_size=512:   7%|▋         | 72/1000 [00:12<02:44,  5.63it/s]Measuring inference for batch_size=512:   7%|▋         | 73/1000 [00:12<02:44,  5.63it/s]Measuring inference for batch_size=512:   7%|▋         | 74/1000 [00:13<02:44,  5.63it/s]Measuring inference for batch_size=512:   8%|▊         | 75/1000 [00:13<02:44,  5.64it/s]Measuring inference for batch_size=512:   8%|▊         | 76/1000 [00:13<02:43,  5.63it/s]Measuring inference for batch_size=512:   8%|▊         | 77/1000 [00:13<02:43,  5.63it/s]Measuring inference for batch_size=512:   8%|▊         | 78/1000 [00:13<02:43,  5.64it/s]Measuring inference for batch_size=512:   8%|▊         | 79/1000 [00:14<02:43,  5.64it/s]Measuring inference for batch_size=512:   8%|▊         | 80/1000 [00:14<02:43,  5.63it/s]Measuring inference for batch_size=512:   8%|▊         | 81/1000 [00:14<02:43,  5.63it/s]Measuring inference for batch_size=512:   8%|▊         | 82/1000 [00:14<02:42,  5.63it/s]Measuring inference for batch_size=512:   8%|▊         | 83/1000 [00:14<02:42,  5.64it/s]Measuring inference for batch_size=512:   8%|▊         | 84/1000 [00:14<02:42,  5.63it/s]Measuring inference for batch_size=512:   8%|▊         | 85/1000 [00:15<02:42,  5.63it/s]Measuring inference for batch_size=512:   9%|▊         | 86/1000 [00:15<02:42,  5.63it/s]Measuring inference for batch_size=512:   9%|▊         | 87/1000 [00:15<02:42,  5.63it/s]Measuring inference for batch_size=512:   9%|▉         | 88/1000 [00:15<02:41,  5.63it/s]Measuring inference for batch_size=512:   9%|▉         | 89/1000 [00:15<02:41,  5.63it/s]Measuring inference for batch_size=512:   9%|▉         | 90/1000 [00:15<02:41,  5.63it/s]Measuring inference for batch_size=512:   9%|▉         | 91/1000 [00:16<02:41,  5.64it/s]Measuring inference for batch_size=512:   9%|▉         | 92/1000 [00:16<02:41,  5.61it/s]Measuring inference for batch_size=512:   9%|▉         | 93/1000 [00:16<02:41,  5.62it/s]Measuring inference for batch_size=512:   9%|▉         | 94/1000 [00:16<02:41,  5.62it/s]Measuring inference for batch_size=512:  10%|▉         | 95/1000 [00:16<02:40,  5.62it/s]Measuring inference for batch_size=512:  10%|▉         | 96/1000 [00:17<02:40,  5.62it/s]Measuring inference for batch_size=512:  10%|▉         | 97/1000 [00:17<02:40,  5.63it/s]Measuring inference for batch_size=512:  10%|▉         | 98/1000 [00:17<02:40,  5.63it/s]Measuring inference for batch_size=512:  10%|▉         | 99/1000 [00:17<02:39,  5.63it/s]Measuring inference for batch_size=512:  10%|█         | 100/1000 [00:17<02:39,  5.63it/s]Measuring inference for batch_size=512:  10%|█         | 101/1000 [00:17<02:39,  5.63it/s]Measuring inference for batch_size=512:  10%|█         | 102/1000 [00:18<02:39,  5.63it/s]Measuring inference for batch_size=512:  10%|█         | 103/1000 [00:18<02:39,  5.63it/s]Measuring inference for batch_size=512:  10%|█         | 104/1000 [00:18<02:39,  5.63it/s]Measuring inference for batch_size=512:  10%|█         | 105/1000 [00:18<02:39,  5.63it/s]Measuring inference for batch_size=512:  11%|█         | 106/1000 [00:18<02:38,  5.63it/s]Measuring inference for batch_size=512:  11%|█         | 107/1000 [00:18<02:38,  5.63it/s]Measuring inference for batch_size=512:  11%|█         | 108/1000 [00:19<02:38,  5.63it/s]Measuring inference for batch_size=512:  11%|█         | 109/1000 [00:19<02:38,  5.63it/s]Measuring inference for batch_size=512:  11%|█         | 110/1000 [00:19<02:38,  5.63it/s]Measuring inference for batch_size=512:  11%|█         | 111/1000 [00:19<02:37,  5.63it/s]Measuring inference for batch_size=512:  11%|█         | 112/1000 [00:19<02:37,  5.63it/s]Measuring inference for batch_size=512:  11%|█▏        | 113/1000 [00:20<02:38,  5.61it/s]Measuring inference for batch_size=512:  11%|█▏        | 114/1000 [00:20<02:39,  5.56it/s]Measuring inference for batch_size=512:  12%|█▏        | 115/1000 [00:20<02:38,  5.58it/s]Measuring inference for batch_size=512:  12%|█▏        | 116/1000 [00:20<02:38,  5.57it/s]Measuring inference for batch_size=512:  12%|█▏        | 117/1000 [00:20<02:38,  5.59it/s]Measuring inference for batch_size=512:  12%|█▏        | 118/1000 [00:20<02:37,  5.60it/s]Measuring inference for batch_size=512:  12%|█▏        | 119/1000 [00:21<02:37,  5.61it/s]Measuring inference for batch_size=512:  12%|█▏        | 120/1000 [00:21<02:36,  5.62it/s]Measuring inference for batch_size=512:  12%|█▏        | 121/1000 [00:21<02:36,  5.62it/s]Measuring inference for batch_size=512:  12%|█▏        | 122/1000 [00:21<02:35,  5.63it/s]Measuring inference for batch_size=512:  12%|█▏        | 123/1000 [00:21<02:35,  5.63it/s]Measuring inference for batch_size=512:  12%|█▏        | 124/1000 [00:22<02:35,  5.63it/s]Measuring inference for batch_size=512:  12%|█▎        | 125/1000 [00:22<02:35,  5.63it/s]Measuring inference for batch_size=512:  13%|█▎        | 126/1000 [00:22<02:35,  5.63it/s]Measuring inference for batch_size=512:  13%|█▎        | 127/1000 [00:22<02:34,  5.63it/s]Measuring inference for batch_size=512:  13%|█▎        | 128/1000 [00:22<02:34,  5.63it/s]Measuring inference for batch_size=512:  13%|█▎        | 129/1000 [00:22<02:34,  5.64it/s]Measuring inference for batch_size=512:  13%|█▎        | 130/1000 [00:23<02:34,  5.64it/s]Measuring inference for batch_size=512:  13%|█▎        | 131/1000 [00:23<02:34,  5.64it/s]Measuring inference for batch_size=512:  13%|█▎        | 132/1000 [00:23<02:34,  5.63it/s]Measuring inference for batch_size=512:  13%|█▎        | 133/1000 [00:23<02:33,  5.63it/s]Measuring inference for batch_size=512:  13%|█▎        | 134/1000 [00:23<02:33,  5.63it/s]Measuring inference for batch_size=512:  14%|█▎        | 135/1000 [00:23<02:33,  5.63it/s]Measuring inference for batch_size=512:  14%|█▎        | 136/1000 [00:24<02:33,  5.63it/s]Measuring inference for batch_size=512:  14%|█▎        | 137/1000 [00:24<02:33,  5.63it/s]Measuring inference for batch_size=512:  14%|█▍        | 138/1000 [00:24<02:33,  5.63it/s]Measuring inference for batch_size=512:  14%|█▍        | 139/1000 [00:24<02:32,  5.63it/s]Measuring inference for batch_size=512:  14%|█▍        | 140/1000 [00:24<02:32,  5.63it/s]Measuring inference for batch_size=512:  14%|█▍        | 141/1000 [00:25<02:32,  5.63it/s]Measuring inference for batch_size=512:  14%|█▍        | 142/1000 [00:25<02:32,  5.63it/s]Measuring inference for batch_size=512:  14%|█▍        | 143/1000 [00:25<02:32,  5.63it/s]Measuring inference for batch_size=512:  14%|█▍        | 144/1000 [00:25<02:32,  5.63it/s]Measuring inference for batch_size=512:  14%|█▍        | 145/1000 [00:25<02:31,  5.63it/s]Measuring inference for batch_size=512:  15%|█▍        | 146/1000 [00:25<02:31,  5.63it/s]Measuring inference for batch_size=512:  15%|█▍        | 147/1000 [00:26<02:31,  5.63it/s]Measuring inference for batch_size=512:  15%|█▍        | 148/1000 [00:26<02:31,  5.63it/s]Measuring inference for batch_size=512:  15%|█▍        | 149/1000 [00:26<02:31,  5.63it/s]Measuring inference for batch_size=512:  15%|█▌        | 150/1000 [00:26<02:30,  5.63it/s]Measuring inference for batch_size=512:  15%|█▌        | 151/1000 [00:26<02:30,  5.63it/s]Measuring inference for batch_size=512:  15%|█▌        | 152/1000 [00:26<02:30,  5.63it/s]Measuring inference for batch_size=512:  15%|█▌        | 153/1000 [00:27<02:30,  5.63it/s]Measuring inference for batch_size=512:  15%|█▌        | 154/1000 [00:27<02:30,  5.64it/s]Measuring inference for batch_size=512:  16%|█▌        | 155/1000 [00:27<02:29,  5.64it/s]Measuring inference for batch_size=512:  16%|█▌        | 156/1000 [00:27<02:29,  5.64it/s]Measuring inference for batch_size=512:  16%|█▌        | 157/1000 [00:27<02:29,  5.63it/s]Measuring inference for batch_size=512:  16%|█▌        | 158/1000 [00:28<02:29,  5.64it/s]Measuring inference for batch_size=512:  16%|█▌        | 159/1000 [00:28<02:29,  5.64it/s]Measuring inference for batch_size=512:  16%|█▌        | 160/1000 [00:28<02:29,  5.64it/s]Measuring inference for batch_size=512:  16%|█▌        | 161/1000 [00:28<02:28,  5.64it/s]Measuring inference for batch_size=512:  16%|█▌        | 162/1000 [00:28<02:28,  5.63it/s]Measuring inference for batch_size=512:  16%|█▋        | 163/1000 [00:28<02:28,  5.64it/s]Measuring inference for batch_size=512:  16%|█▋        | 164/1000 [00:29<02:28,  5.63it/s]Measuring inference for batch_size=512:  16%|█▋        | 165/1000 [00:29<02:28,  5.64it/s]Measuring inference for batch_size=512:  17%|█▋        | 166/1000 [00:29<02:28,  5.63it/s]Measuring inference for batch_size=512:  17%|█▋        | 167/1000 [00:29<02:27,  5.63it/s]Measuring inference for batch_size=512:  17%|█▋        | 168/1000 [00:29<02:27,  5.63it/s]Measuring inference for batch_size=512:  17%|█▋        | 169/1000 [00:30<02:27,  5.63it/s]Measuring inference for batch_size=512:  17%|█▋        | 170/1000 [00:30<02:27,  5.63it/s]Measuring inference for batch_size=512:  17%|█▋        | 171/1000 [00:30<02:27,  5.63it/s]Measuring inference for batch_size=512:  17%|█▋        | 172/1000 [00:30<02:26,  5.63it/s]Measuring inference for batch_size=512:  17%|█▋        | 173/1000 [00:30<02:26,  5.63it/s]Measuring inference for batch_size=512:  17%|█▋        | 174/1000 [00:30<02:26,  5.64it/s]Measuring inference for batch_size=512:  18%|█▊        | 175/1000 [00:31<02:26,  5.64it/s]Measuring inference for batch_size=512:  18%|█▊        | 176/1000 [00:31<02:26,  5.64it/s]Measuring inference for batch_size=512:  18%|█▊        | 177/1000 [00:31<02:26,  5.64it/s]Measuring inference for batch_size=512:  18%|█▊        | 178/1000 [00:31<02:25,  5.63it/s]Measuring inference for batch_size=512:  18%|█▊        | 179/1000 [00:31<02:25,  5.64it/s]Measuring inference for batch_size=512:  18%|█▊        | 180/1000 [00:31<02:25,  5.63it/s]Measuring inference for batch_size=512:  18%|█▊        | 181/1000 [00:32<02:25,  5.63it/s]Measuring inference for batch_size=512:  18%|█▊        | 182/1000 [00:32<02:25,  5.63it/s]Measuring inference for batch_size=512:  18%|█▊        | 183/1000 [00:32<02:24,  5.63it/s]Measuring inference for batch_size=512:  18%|█▊        | 184/1000 [00:32<02:24,  5.63it/s]Measuring inference for batch_size=512:  18%|█▊        | 185/1000 [00:32<02:24,  5.64it/s]Measuring inference for batch_size=512:  19%|█▊        | 186/1000 [00:33<02:24,  5.64it/s]Measuring inference for batch_size=512:  19%|█▊        | 187/1000 [00:33<02:24,  5.63it/s]Measuring inference for batch_size=512:  19%|█▉        | 188/1000 [00:33<02:24,  5.63it/s]Measuring inference for batch_size=512:  19%|█▉        | 189/1000 [00:33<02:24,  5.63it/s]Measuring inference for batch_size=512:  19%|█▉        | 190/1000 [00:33<02:23,  5.63it/s]Measuring inference for batch_size=512:  19%|█▉        | 191/1000 [00:33<02:23,  5.63it/s]Measuring inference for batch_size=512:  19%|█▉        | 192/1000 [00:34<02:23,  5.63it/s]Measuring inference for batch_size=512:  19%|█▉        | 193/1000 [00:34<02:23,  5.63it/s]Measuring inference for batch_size=512:  19%|█▉        | 194/1000 [00:34<02:23,  5.63it/s]Measuring inference for batch_size=512:  20%|█▉        | 195/1000 [00:34<02:22,  5.63it/s]Measuring inference for batch_size=512:  20%|█▉        | 196/1000 [00:34<02:22,  5.64it/s]Measuring inference for batch_size=512:  20%|█▉        | 197/1000 [00:34<02:22,  5.64it/s]Measuring inference for batch_size=512:  20%|█▉        | 198/1000 [00:35<02:22,  5.63it/s]Measuring inference for batch_size=512:  20%|█▉        | 199/1000 [00:35<02:22,  5.64it/s]Measuring inference for batch_size=512:  20%|██        | 200/1000 [00:35<02:22,  5.63it/s]Measuring inference for batch_size=512:  20%|██        | 201/1000 [00:35<02:21,  5.64it/s]Measuring inference for batch_size=512:  20%|██        | 202/1000 [00:35<02:21,  5.64it/s]Measuring inference for batch_size=512:  20%|██        | 203/1000 [00:36<02:21,  5.64it/s]Measuring inference for batch_size=512:  20%|██        | 204/1000 [00:36<02:21,  5.64it/s]Measuring inference for batch_size=512:  20%|██        | 205/1000 [00:36<02:21,  5.64it/s]Measuring inference for batch_size=512:  21%|██        | 206/1000 [00:36<02:20,  5.63it/s]Measuring inference for batch_size=512:  21%|██        | 207/1000 [00:36<02:20,  5.64it/s]Measuring inference for batch_size=512:  21%|██        | 208/1000 [00:36<02:20,  5.64it/s]Measuring inference for batch_size=512:  21%|██        | 209/1000 [00:37<02:20,  5.64it/s]Measuring inference for batch_size=512:  21%|██        | 210/1000 [00:37<02:20,  5.64it/s]Measuring inference for batch_size=512:  21%|██        | 211/1000 [00:37<02:19,  5.64it/s]Measuring inference for batch_size=512:  21%|██        | 212/1000 [00:37<02:19,  5.64it/s]Measuring inference for batch_size=512:  21%|██▏       | 213/1000 [00:37<02:19,  5.64it/s]Measuring inference for batch_size=512:  21%|██▏       | 214/1000 [00:38<02:19,  5.64it/s]Measuring inference for batch_size=512:  22%|██▏       | 215/1000 [00:38<02:19,  5.64it/s]Measuring inference for batch_size=512:  22%|██▏       | 216/1000 [00:38<02:19,  5.64it/s]Measuring inference for batch_size=512:  22%|██▏       | 217/1000 [00:38<02:18,  5.64it/s]Measuring inference for batch_size=512:  22%|██▏       | 218/1000 [00:38<02:18,  5.64it/s]Measuring inference for batch_size=512:  22%|██▏       | 219/1000 [00:38<02:18,  5.64it/s]Measuring inference for batch_size=512:  22%|██▏       | 220/1000 [00:39<02:18,  5.64it/s]Measuring inference for batch_size=512:  22%|██▏       | 221/1000 [00:39<02:18,  5.64it/s]Measuring inference for batch_size=512:  22%|██▏       | 222/1000 [00:39<02:18,  5.64it/s]Measuring inference for batch_size=512:  22%|██▏       | 223/1000 [00:39<02:17,  5.63it/s]Measuring inference for batch_size=512:  22%|██▏       | 224/1000 [00:39<02:17,  5.64it/s]Measuring inference for batch_size=512:  22%|██▎       | 225/1000 [00:39<02:17,  5.64it/s]Measuring inference for batch_size=512:  23%|██▎       | 226/1000 [00:40<02:17,  5.64it/s]Measuring inference for batch_size=512:  23%|██▎       | 227/1000 [00:40<02:17,  5.64it/s]Measuring inference for batch_size=512:  23%|██▎       | 228/1000 [00:40<02:16,  5.64it/s]Measuring inference for batch_size=512:  23%|██▎       | 229/1000 [00:40<02:16,  5.64it/s]Measuring inference for batch_size=512:  23%|██▎       | 230/1000 [00:40<02:16,  5.64it/s]Measuring inference for batch_size=512:  23%|██▎       | 231/1000 [00:41<02:16,  5.64it/s]Measuring inference for batch_size=512:  23%|██▎       | 232/1000 [00:41<02:16,  5.64it/s]Measuring inference for batch_size=512:  23%|██▎       | 233/1000 [00:41<02:16,  5.64it/s]Measuring inference for batch_size=512:  23%|██▎       | 234/1000 [00:41<02:15,  5.63it/s]Measuring inference for batch_size=512:  24%|██▎       | 235/1000 [00:41<02:15,  5.64it/s]Measuring inference for batch_size=512:  24%|██▎       | 236/1000 [00:41<02:15,  5.63it/s]Measuring inference for batch_size=512:  24%|██▎       | 237/1000 [00:42<02:15,  5.63it/s]Measuring inference for batch_size=512:  24%|██▍       | 238/1000 [00:42<02:15,  5.64it/s]Measuring inference for batch_size=512:  24%|██▍       | 239/1000 [00:42<02:15,  5.63it/s]Measuring inference for batch_size=512:  24%|██▍       | 240/1000 [00:42<02:14,  5.63it/s]Measuring inference for batch_size=512:  24%|██▍       | 241/1000 [00:42<02:14,  5.63it/s]Measuring inference for batch_size=512:  24%|██▍       | 242/1000 [00:42<02:14,  5.63it/s]Measuring inference for batch_size=512:  24%|██▍       | 243/1000 [00:43<02:14,  5.63it/s]Measuring inference for batch_size=512:  24%|██▍       | 244/1000 [00:43<02:14,  5.64it/s]Measuring inference for batch_size=512:  24%|██▍       | 245/1000 [00:43<02:14,  5.63it/s]Measuring inference for batch_size=512:  25%|██▍       | 246/1000 [00:43<02:13,  5.63it/s]Measuring inference for batch_size=512:  25%|██▍       | 247/1000 [00:43<02:13,  5.63it/s]Measuring inference for batch_size=512:  25%|██▍       | 248/1000 [00:44<02:13,  5.63it/s]Measuring inference for batch_size=512:  25%|██▍       | 249/1000 [00:44<02:13,  5.63it/s]Measuring inference for batch_size=512:  25%|██▌       | 250/1000 [00:44<02:13,  5.63it/s]Measuring inference for batch_size=512:  25%|██▌       | 251/1000 [00:44<02:12,  5.64it/s]Measuring inference for batch_size=512:  25%|██▌       | 252/1000 [00:44<02:12,  5.63it/s]Measuring inference for batch_size=512:  25%|██▌       | 253/1000 [00:44<02:12,  5.63it/s]Measuring inference for batch_size=512:  25%|██▌       | 254/1000 [00:45<02:12,  5.63it/s]Measuring inference for batch_size=512:  26%|██▌       | 255/1000 [00:45<02:12,  5.63it/s]Measuring inference for batch_size=512:  26%|██▌       | 256/1000 [00:45<02:12,  5.63it/s]Measuring inference for batch_size=512:  26%|██▌       | 257/1000 [00:45<02:11,  5.64it/s]Measuring inference for batch_size=512:  26%|██▌       | 258/1000 [00:45<02:11,  5.63it/s]Measuring inference for batch_size=512:  26%|██▌       | 259/1000 [00:45<02:11,  5.64it/s]Measuring inference for batch_size=512:  26%|██▌       | 260/1000 [00:46<02:11,  5.63it/s]Measuring inference for batch_size=512:  26%|██▌       | 261/1000 [00:46<02:11,  5.63it/s]Measuring inference for batch_size=512:  26%|██▌       | 262/1000 [00:46<02:11,  5.63it/s]Measuring inference for batch_size=512:  26%|██▋       | 263/1000 [00:46<02:10,  5.63it/s]Measuring inference for batch_size=512:  26%|██▋       | 264/1000 [00:46<02:10,  5.63it/s]Measuring inference for batch_size=512:  26%|██▋       | 265/1000 [00:47<02:10,  5.63it/s]Measuring inference for batch_size=512:  27%|██▋       | 266/1000 [00:47<02:10,  5.62it/s]Measuring inference for batch_size=512:  27%|██▋       | 267/1000 [00:47<02:10,  5.63it/s]Measuring inference for batch_size=512:  27%|██▋       | 268/1000 [00:47<02:10,  5.63it/s]Measuring inference for batch_size=512:  27%|██▋       | 269/1000 [00:47<02:09,  5.63it/s]Measuring inference for batch_size=512:  27%|██▋       | 270/1000 [00:47<02:09,  5.63it/s]Measuring inference for batch_size=512:  27%|██▋       | 271/1000 [00:48<02:09,  5.64it/s]Measuring inference for batch_size=512:  27%|██▋       | 272/1000 [00:48<02:09,  5.63it/s]Measuring inference for batch_size=512:  27%|██▋       | 273/1000 [00:48<02:09,  5.63it/s]Measuring inference for batch_size=512:  27%|██▋       | 274/1000 [00:48<02:09,  5.63it/s]Measuring inference for batch_size=512:  28%|██▊       | 275/1000 [00:48<02:08,  5.63it/s]Measuring inference for batch_size=512:  28%|██▊       | 276/1000 [00:49<02:08,  5.63it/s]Measuring inference for batch_size=512:  28%|██▊       | 277/1000 [00:49<02:08,  5.63it/s]Measuring inference for batch_size=512:  28%|██▊       | 278/1000 [00:49<02:08,  5.63it/s]Measuring inference for batch_size=512:  28%|██▊       | 279/1000 [00:49<02:07,  5.64it/s]Measuring inference for batch_size=512:  28%|██▊       | 280/1000 [00:49<02:07,  5.63it/s]Measuring inference for batch_size=512:  28%|██▊       | 281/1000 [00:49<02:07,  5.63it/s]Measuring inference for batch_size=512:  28%|██▊       | 282/1000 [00:50<02:07,  5.63it/s]Measuring inference for batch_size=512:  28%|██▊       | 283/1000 [00:50<02:07,  5.64it/s]Measuring inference for batch_size=512:  28%|██▊       | 284/1000 [00:50<02:07,  5.64it/s]Measuring inference for batch_size=512:  28%|██▊       | 285/1000 [00:50<02:06,  5.64it/s]Measuring inference for batch_size=512:  29%|██▊       | 286/1000 [00:50<02:06,  5.64it/s]Measuring inference for batch_size=512:  29%|██▊       | 287/1000 [00:50<02:06,  5.64it/s]Measuring inference for batch_size=512:  29%|██▉       | 288/1000 [00:51<02:06,  5.64it/s]Measuring inference for batch_size=512:  29%|██▉       | 289/1000 [00:51<02:06,  5.64it/s]Measuring inference for batch_size=512:  29%|██▉       | 290/1000 [00:51<02:05,  5.64it/s]Measuring inference for batch_size=512:  29%|██▉       | 291/1000 [00:51<02:05,  5.63it/s]Measuring inference for batch_size=512:  29%|██▉       | 292/1000 [00:51<02:05,  5.63it/s]Measuring inference for batch_size=512:  29%|██▉       | 293/1000 [00:52<02:05,  5.64it/s]Measuring inference for batch_size=512:  29%|██▉       | 294/1000 [00:52<02:05,  5.63it/s]Measuring inference for batch_size=512:  30%|██▉       | 295/1000 [00:52<02:05,  5.64it/s]Measuring inference for batch_size=512:  30%|██▉       | 296/1000 [00:52<02:05,  5.63it/s]Measuring inference for batch_size=512:  30%|██▉       | 297/1000 [00:52<02:04,  5.63it/s]Measuring inference for batch_size=512:  30%|██▉       | 298/1000 [00:52<02:04,  5.63it/s]Measuring inference for batch_size=512:  30%|██▉       | 299/1000 [00:53<02:04,  5.63it/s]Measuring inference for batch_size=512:  30%|███       | 300/1000 [00:53<02:04,  5.63it/s]Measuring inference for batch_size=512:  30%|███       | 301/1000 [00:53<02:04,  5.63it/s]Measuring inference for batch_size=512:  30%|███       | 302/1000 [00:53<02:04,  5.63it/s]Measuring inference for batch_size=512:  30%|███       | 303/1000 [00:53<02:03,  5.63it/s]Measuring inference for batch_size=512:  30%|███       | 304/1000 [00:53<02:03,  5.63it/s]Measuring inference for batch_size=512:  30%|███       | 305/1000 [00:54<02:03,  5.63it/s]Measuring inference for batch_size=512:  31%|███       | 306/1000 [00:54<02:03,  5.63it/s]Measuring inference for batch_size=512:  31%|███       | 307/1000 [00:54<02:03,  5.63it/s]Measuring inference for batch_size=512:  31%|███       | 308/1000 [00:54<02:02,  5.63it/s]Measuring inference for batch_size=512:  31%|███       | 309/1000 [00:54<02:02,  5.63it/s]Measuring inference for batch_size=512:  31%|███       | 310/1000 [00:55<02:02,  5.63it/s]Measuring inference for batch_size=512:  31%|███       | 311/1000 [00:55<02:02,  5.63it/s]Measuring inference for batch_size=512:  31%|███       | 312/1000 [00:55<02:02,  5.63it/s]Measuring inference for batch_size=512:  31%|███▏      | 313/1000 [00:55<02:01,  5.63it/s]Measuring inference for batch_size=512:  31%|███▏      | 314/1000 [00:55<02:01,  5.63it/s]Measuring inference for batch_size=512:  32%|███▏      | 315/1000 [00:55<02:01,  5.64it/s]Measuring inference for batch_size=512:  32%|███▏      | 316/1000 [00:56<02:01,  5.63it/s]Measuring inference for batch_size=512:  32%|███▏      | 317/1000 [00:56<02:01,  5.64it/s]Measuring inference for batch_size=512:  32%|███▏      | 318/1000 [00:56<02:01,  5.63it/s]Measuring inference for batch_size=512:  32%|███▏      | 319/1000 [00:56<02:00,  5.63it/s]Measuring inference for batch_size=512:  32%|███▏      | 320/1000 [00:56<02:00,  5.63it/s]Measuring inference for batch_size=512:  32%|███▏      | 321/1000 [00:56<02:00,  5.63it/s]Measuring inference for batch_size=512:  32%|███▏      | 322/1000 [00:57<02:00,  5.63it/s]Measuring inference for batch_size=512:  32%|███▏      | 323/1000 [00:57<02:00,  5.63it/s]Measuring inference for batch_size=512:  32%|███▏      | 324/1000 [00:57<02:00,  5.63it/s]Measuring inference for batch_size=512:  32%|███▎      | 325/1000 [00:57<02:00,  5.62it/s]Measuring inference for batch_size=512:  33%|███▎      | 326/1000 [00:57<01:59,  5.62it/s]Measuring inference for batch_size=512:  33%|███▎      | 327/1000 [00:58<01:59,  5.63it/s]Measuring inference for batch_size=512:  33%|███▎      | 328/1000 [00:58<01:59,  5.63it/s]Measuring inference for batch_size=512:  33%|███▎      | 329/1000 [00:58<01:59,  5.63it/s]Measuring inference for batch_size=512:  33%|███▎      | 330/1000 [00:58<01:59,  5.63it/s]Measuring inference for batch_size=512:  33%|███▎      | 331/1000 [00:58<01:58,  5.63it/s]Measuring inference for batch_size=512:  33%|███▎      | 332/1000 [00:58<01:58,  5.63it/s]Measuring inference for batch_size=512:  33%|███▎      | 333/1000 [00:59<01:58,  5.63it/s]Measuring inference for batch_size=512:  33%|███▎      | 334/1000 [00:59<01:58,  5.63it/s]Measuring inference for batch_size=512:  34%|███▎      | 335/1000 [00:59<01:58,  5.63it/s]Measuring inference for batch_size=512:  34%|███▎      | 336/1000 [00:59<01:58,  5.63it/s]Measuring inference for batch_size=512:  34%|███▎      | 337/1000 [00:59<01:57,  5.63it/s]Measuring inference for batch_size=512:  34%|███▍      | 338/1000 [01:00<01:57,  5.63it/s]Measuring inference for batch_size=512:  34%|███▍      | 339/1000 [01:00<01:57,  5.63it/s]Measuring inference for batch_size=512:  34%|███▍      | 340/1000 [01:00<01:57,  5.63it/s]Measuring inference for batch_size=512:  34%|███▍      | 341/1000 [01:00<01:56,  5.63it/s]Measuring inference for batch_size=512:  34%|███▍      | 342/1000 [01:00<01:56,  5.63it/s]Measuring inference for batch_size=512:  34%|███▍      | 343/1000 [01:00<01:56,  5.64it/s]Measuring inference for batch_size=512:  34%|███▍      | 344/1000 [01:01<01:56,  5.63it/s]Measuring inference for batch_size=512:  34%|███▍      | 345/1000 [01:01<01:56,  5.63it/s]Measuring inference for batch_size=512:  35%|███▍      | 346/1000 [01:01<01:56,  5.63it/s]Measuring inference for batch_size=512:  35%|███▍      | 347/1000 [01:01<01:55,  5.63it/s]Measuring inference for batch_size=512:  35%|███▍      | 348/1000 [01:01<01:55,  5.64it/s]Measuring inference for batch_size=512:  35%|███▍      | 349/1000 [01:01<01:55,  5.63it/s]Measuring inference for batch_size=512:  35%|███▌      | 350/1000 [01:02<01:55,  5.63it/s]Measuring inference for batch_size=512:  35%|███▌      | 351/1000 [01:02<01:55,  5.63it/s]Measuring inference for batch_size=512:  35%|███▌      | 352/1000 [01:02<01:55,  5.63it/s]Measuring inference for batch_size=512:  35%|███▌      | 353/1000 [01:02<01:54,  5.63it/s]Measuring inference for batch_size=512:  35%|███▌      | 354/1000 [01:02<01:54,  5.63it/s]Measuring inference for batch_size=512:  36%|███▌      | 355/1000 [01:03<01:54,  5.63it/s]Measuring inference for batch_size=512:  36%|███▌      | 356/1000 [01:03<01:54,  5.63it/s]Measuring inference for batch_size=512:  36%|███▌      | 357/1000 [01:03<01:54,  5.63it/s]Measuring inference for batch_size=512:  36%|███▌      | 358/1000 [01:03<01:54,  5.63it/s]Measuring inference for batch_size=512:  36%|███▌      | 359/1000 [01:03<01:53,  5.63it/s]Measuring inference for batch_size=512:  36%|███▌      | 360/1000 [01:03<01:53,  5.63it/s]Measuring inference for batch_size=512:  36%|███▌      | 361/1000 [01:04<01:53,  5.63it/s]Measuring inference for batch_size=512:  36%|███▌      | 362/1000 [01:04<01:53,  5.63it/s]Measuring inference for batch_size=512:  36%|███▋      | 363/1000 [01:04<01:53,  5.63it/s]Measuring inference for batch_size=512:  36%|███▋      | 364/1000 [01:04<01:53,  5.63it/s]Measuring inference for batch_size=512:  36%|███▋      | 365/1000 [01:04<01:52,  5.62it/s]Measuring inference for batch_size=512:  37%|███▋      | 366/1000 [01:04<01:52,  5.63it/s]Measuring inference for batch_size=512:  37%|███▋      | 367/1000 [01:05<01:52,  5.63it/s]Measuring inference for batch_size=512:  37%|███▋      | 368/1000 [01:05<01:52,  5.62it/s]Measuring inference for batch_size=512:  37%|███▋      | 369/1000 [01:05<01:52,  5.63it/s]Measuring inference for batch_size=512:  37%|███▋      | 370/1000 [01:05<01:52,  5.62it/s]Measuring inference for batch_size=512:  37%|███▋      | 371/1000 [01:05<01:51,  5.62it/s]Measuring inference for batch_size=512:  37%|███▋      | 372/1000 [01:06<01:51,  5.62it/s]Measuring inference for batch_size=512:  37%|███▋      | 373/1000 [01:06<01:51,  5.63it/s]Measuring inference for batch_size=512:  37%|███▋      | 374/1000 [01:06<01:51,  5.63it/s]Measuring inference for batch_size=512:  38%|███▊      | 375/1000 [01:06<01:51,  5.62it/s]Measuring inference for batch_size=512:  38%|███▊      | 376/1000 [01:06<01:50,  5.62it/s]Measuring inference for batch_size=512:  38%|███▊      | 377/1000 [01:06<01:50,  5.63it/s]Measuring inference for batch_size=512:  38%|███▊      | 378/1000 [01:07<01:50,  5.63it/s]Measuring inference for batch_size=512:  38%|███▊      | 379/1000 [01:07<01:50,  5.63it/s]Measuring inference for batch_size=512:  38%|███▊      | 380/1000 [01:07<01:50,  5.63it/s]Measuring inference for batch_size=512:  38%|███▊      | 381/1000 [01:07<01:49,  5.63it/s]Measuring inference for batch_size=512:  38%|███▊      | 382/1000 [01:07<01:49,  5.63it/s]Measuring inference for batch_size=512:  38%|███▊      | 383/1000 [01:08<01:49,  5.63it/s]Measuring inference for batch_size=512:  38%|███▊      | 384/1000 [01:08<01:49,  5.63it/s]Measuring inference for batch_size=512:  38%|███▊      | 385/1000 [01:08<01:49,  5.63it/s]Measuring inference for batch_size=512:  39%|███▊      | 386/1000 [01:08<01:49,  5.63it/s]Measuring inference for batch_size=512:  39%|███▊      | 387/1000 [01:08<01:48,  5.63it/s]Measuring inference for batch_size=512:  39%|███▉      | 388/1000 [01:08<01:48,  5.63it/s]Measuring inference for batch_size=512:  39%|███▉      | 389/1000 [01:09<01:48,  5.63it/s]Measuring inference for batch_size=512:  39%|███▉      | 390/1000 [01:09<01:48,  5.63it/s]Measuring inference for batch_size=512:  39%|███▉      | 391/1000 [01:09<01:48,  5.63it/s]Measuring inference for batch_size=512:  39%|███▉      | 392/1000 [01:09<01:47,  5.63it/s]Measuring inference for batch_size=512:  39%|███▉      | 393/1000 [01:09<01:47,  5.63it/s]Measuring inference for batch_size=512:  39%|███▉      | 394/1000 [01:09<01:47,  5.63it/s]Measuring inference for batch_size=512:  40%|███▉      | 395/1000 [01:10<01:47,  5.64it/s]Measuring inference for batch_size=512:  40%|███▉      | 396/1000 [01:10<01:47,  5.64it/s]Measuring inference for batch_size=512:  40%|███▉      | 397/1000 [01:10<01:46,  5.64it/s]Measuring inference for batch_size=512:  40%|███▉      | 398/1000 [01:10<01:46,  5.64it/s]Measuring inference for batch_size=512:  40%|███▉      | 399/1000 [01:10<01:46,  5.63it/s]Measuring inference for batch_size=512:  40%|████      | 400/1000 [01:11<01:46,  5.64it/s]Measuring inference for batch_size=512:  40%|████      | 401/1000 [01:11<01:46,  5.63it/s]Measuring inference for batch_size=512:  40%|████      | 402/1000 [01:11<01:46,  5.63it/s]Measuring inference for batch_size=512:  40%|████      | 403/1000 [01:11<01:46,  5.63it/s]Measuring inference for batch_size=512:  40%|████      | 404/1000 [01:11<01:45,  5.63it/s]Measuring inference for batch_size=512:  40%|████      | 405/1000 [01:11<01:45,  5.63it/s]Measuring inference for batch_size=512:  41%|████      | 406/1000 [01:12<01:45,  5.63it/s]Measuring inference for batch_size=512:  41%|████      | 407/1000 [01:12<01:45,  5.64it/s]Measuring inference for batch_size=512:  41%|████      | 408/1000 [01:12<01:45,  5.63it/s]Measuring inference for batch_size=512:  41%|████      | 409/1000 [01:12<01:45,  5.63it/s]Measuring inference for batch_size=512:  41%|████      | 410/1000 [01:12<01:44,  5.63it/s]Measuring inference for batch_size=512:  41%|████      | 411/1000 [01:12<01:44,  5.63it/s]Measuring inference for batch_size=512:  41%|████      | 412/1000 [01:13<01:44,  5.63it/s]Measuring inference for batch_size=512:  41%|████▏     | 413/1000 [01:13<01:44,  5.64it/s]Measuring inference for batch_size=512:  41%|████▏     | 414/1000 [01:13<01:44,  5.63it/s]Measuring inference for batch_size=512:  42%|████▏     | 415/1000 [01:13<01:43,  5.64it/s]Measuring inference for batch_size=512:  42%|████▏     | 416/1000 [01:13<01:43,  5.64it/s]Measuring inference for batch_size=512:  42%|████▏     | 417/1000 [01:14<01:43,  5.64it/s]Measuring inference for batch_size=512:  42%|████▏     | 418/1000 [01:14<01:43,  5.64it/s]Measuring inference for batch_size=512:  42%|████▏     | 419/1000 [01:14<01:43,  5.64it/s]Measuring inference for batch_size=512:  42%|████▏     | 420/1000 [01:14<01:42,  5.63it/s]Measuring inference for batch_size=512:  42%|████▏     | 421/1000 [01:14<01:42,  5.63it/s]Measuring inference for batch_size=512:  42%|████▏     | 422/1000 [01:14<01:42,  5.63it/s]Measuring inference for batch_size=512:  42%|████▏     | 423/1000 [01:15<01:42,  5.63it/s]Measuring inference for batch_size=512:  42%|████▏     | 424/1000 [01:15<01:42,  5.63it/s]Measuring inference for batch_size=512:  42%|████▎     | 425/1000 [01:15<01:42,  5.63it/s]Measuring inference for batch_size=512:  43%|████▎     | 426/1000 [01:15<01:41,  5.63it/s]Measuring inference for batch_size=512:  43%|████▎     | 427/1000 [01:15<01:41,  5.63it/s]Measuring inference for batch_size=512:  43%|████▎     | 428/1000 [01:16<01:41,  5.63it/s]Measuring inference for batch_size=512:  43%|████▎     | 429/1000 [01:16<01:41,  5.63it/s]Measuring inference for batch_size=512:  43%|████▎     | 430/1000 [01:16<01:41,  5.63it/s]Measuring inference for batch_size=512:  43%|████▎     | 431/1000 [01:16<01:40,  5.63it/s]Measuring inference for batch_size=512:  43%|████▎     | 432/1000 [01:16<01:40,  5.63it/s]Measuring inference for batch_size=512:  43%|████▎     | 433/1000 [01:16<01:40,  5.63it/s]Measuring inference for batch_size=512:  43%|████▎     | 434/1000 [01:17<01:40,  5.63it/s]Measuring inference for batch_size=512:  44%|████▎     | 435/1000 [01:17<01:40,  5.63it/s]Measuring inference for batch_size=512:  44%|████▎     | 436/1000 [01:17<01:40,  5.63it/s]Measuring inference for batch_size=512:  44%|████▎     | 437/1000 [01:17<01:39,  5.63it/s]Measuring inference for batch_size=512:  44%|████▍     | 438/1000 [01:17<01:39,  5.63it/s]Measuring inference for batch_size=512:  44%|████▍     | 439/1000 [01:17<01:39,  5.64it/s]Measuring inference for batch_size=512:  44%|████▍     | 440/1000 [01:18<01:39,  5.64it/s]Measuring inference for batch_size=512:  44%|████▍     | 441/1000 [01:18<01:39,  5.63it/s]Measuring inference for batch_size=512:  44%|████▍     | 442/1000 [01:18<01:39,  5.63it/s]Measuring inference for batch_size=512:  44%|████▍     | 443/1000 [01:18<01:38,  5.63it/s]Measuring inference for batch_size=512:  44%|████▍     | 444/1000 [01:18<01:38,  5.63it/s]Measuring inference for batch_size=512:  44%|████▍     | 445/1000 [01:19<01:38,  5.63it/s]Measuring inference for batch_size=512:  45%|████▍     | 446/1000 [01:19<01:38,  5.63it/s]Measuring inference for batch_size=512:  45%|████▍     | 447/1000 [01:19<01:38,  5.63it/s]Measuring inference for batch_size=512:  45%|████▍     | 448/1000 [01:19<01:38,  5.63it/s]Measuring inference for batch_size=512:  45%|████▍     | 449/1000 [01:19<01:37,  5.63it/s]Measuring inference for batch_size=512:  45%|████▌     | 450/1000 [01:19<01:37,  5.63it/s]Measuring inference for batch_size=512:  45%|████▌     | 451/1000 [01:20<01:37,  5.63it/s]Measuring inference for batch_size=512:  45%|████▌     | 452/1000 [01:20<01:37,  5.63it/s]Measuring inference for batch_size=512:  45%|████▌     | 453/1000 [01:20<01:37,  5.63it/s]Measuring inference for batch_size=512:  45%|████▌     | 454/1000 [01:20<01:36,  5.63it/s]Measuring inference for batch_size=512:  46%|████▌     | 455/1000 [01:20<01:36,  5.64it/s]Measuring inference for batch_size=512:  46%|████▌     | 456/1000 [01:20<01:36,  5.64it/s]Measuring inference for batch_size=512:  46%|████▌     | 457/1000 [01:21<01:36,  5.64it/s]Measuring inference for batch_size=512:  46%|████▌     | 458/1000 [01:21<01:36,  5.64it/s]Measuring inference for batch_size=512:  46%|████▌     | 459/1000 [01:21<01:35,  5.64it/s]Measuring inference for batch_size=512:  46%|████▌     | 460/1000 [01:21<01:35,  5.64it/s]Measuring inference for batch_size=512:  46%|████▌     | 461/1000 [01:21<01:35,  5.64it/s]Measuring inference for batch_size=512:  46%|████▌     | 462/1000 [01:22<01:35,  5.64it/s]Measuring inference for batch_size=512:  46%|████▋     | 463/1000 [01:22<01:35,  5.64it/s]Measuring inference for batch_size=512:  46%|████▋     | 464/1000 [01:22<01:35,  5.64it/s]Measuring inference for batch_size=512:  46%|████▋     | 465/1000 [01:22<01:34,  5.64it/s]Measuring inference for batch_size=512:  47%|████▋     | 466/1000 [01:22<01:34,  5.64it/s]Measuring inference for batch_size=512:  47%|████▋     | 467/1000 [01:22<01:34,  5.63it/s]Measuring inference for batch_size=512:  47%|████▋     | 468/1000 [01:23<01:34,  5.63it/s]Measuring inference for batch_size=512:  47%|████▋     | 469/1000 [01:23<01:34,  5.63it/s]Measuring inference for batch_size=512:  47%|████▋     | 470/1000 [01:23<01:34,  5.64it/s]Measuring inference for batch_size=512:  47%|████▋     | 471/1000 [01:23<01:33,  5.63it/s]Measuring inference for batch_size=512:  47%|████▋     | 472/1000 [01:23<01:34,  5.60it/s]Measuring inference for batch_size=512:  47%|████▋     | 473/1000 [01:23<01:33,  5.61it/s]Measuring inference for batch_size=512:  47%|████▋     | 474/1000 [01:24<01:33,  5.61it/s]Measuring inference for batch_size=512:  48%|████▊     | 475/1000 [01:24<01:33,  5.62it/s]Measuring inference for batch_size=512:  48%|████▊     | 476/1000 [01:24<01:33,  5.62it/s]Measuring inference for batch_size=512:  48%|████▊     | 477/1000 [01:24<01:32,  5.62it/s]Measuring inference for batch_size=512:  48%|████▊     | 478/1000 [01:24<01:32,  5.62it/s]Measuring inference for batch_size=512:  48%|████▊     | 479/1000 [01:25<01:32,  5.63it/s]Measuring inference for batch_size=512:  48%|████▊     | 480/1000 [01:25<01:32,  5.62it/s]Measuring inference for batch_size=512:  48%|████▊     | 481/1000 [01:25<01:32,  5.62it/s]Measuring inference for batch_size=512:  48%|████▊     | 482/1000 [01:25<01:32,  5.62it/s]Measuring inference for batch_size=512:  48%|████▊     | 483/1000 [01:25<01:31,  5.62it/s]Measuring inference for batch_size=512:  48%|████▊     | 484/1000 [01:25<01:31,  5.63it/s]Measuring inference for batch_size=512:  48%|████▊     | 485/1000 [01:26<01:31,  5.63it/s]Measuring inference for batch_size=512:  49%|████▊     | 486/1000 [01:26<01:31,  5.63it/s]Measuring inference for batch_size=512:  49%|████▊     | 487/1000 [01:26<01:31,  5.63it/s]Measuring inference for batch_size=512:  49%|████▉     | 488/1000 [01:26<01:31,  5.62it/s]Measuring inference for batch_size=512:  49%|████▉     | 489/1000 [01:26<01:30,  5.63it/s]Measuring inference for batch_size=512:  49%|████▉     | 490/1000 [01:27<01:30,  5.62it/s]Measuring inference for batch_size=512:  49%|████▉     | 491/1000 [01:27<01:30,  5.63it/s]Measuring inference for batch_size=512:  49%|████▉     | 492/1000 [01:27<01:30,  5.63it/s]Measuring inference for batch_size=512:  49%|████▉     | 493/1000 [01:27<01:30,  5.63it/s]Measuring inference for batch_size=512:  49%|████▉     | 494/1000 [01:27<01:29,  5.63it/s]Measuring inference for batch_size=512:  50%|████▉     | 495/1000 [01:27<01:29,  5.63it/s]Measuring inference for batch_size=512:  50%|████▉     | 496/1000 [01:28<01:29,  5.64it/s]Measuring inference for batch_size=512:  50%|████▉     | 497/1000 [01:28<01:29,  5.64it/s]Measuring inference for batch_size=512:  50%|████▉     | 498/1000 [01:28<01:29,  5.64it/s]Measuring inference for batch_size=512:  50%|████▉     | 499/1000 [01:28<01:28,  5.63it/s]Measuring inference for batch_size=512:  50%|█████     | 500/1000 [01:28<01:28,  5.64it/s]Measuring inference for batch_size=512:  50%|█████     | 501/1000 [01:28<01:28,  5.63it/s]Measuring inference for batch_size=512:  50%|█████     | 502/1000 [01:29<01:28,  5.63it/s]Measuring inference for batch_size=512:  50%|█████     | 503/1000 [01:29<01:28,  5.63it/s]Measuring inference for batch_size=512:  50%|█████     | 504/1000 [01:29<01:28,  5.63it/s]Measuring inference for batch_size=512:  50%|█████     | 505/1000 [01:29<01:27,  5.63it/s]Measuring inference for batch_size=512:  51%|█████     | 506/1000 [01:29<01:27,  5.63it/s]Measuring inference for batch_size=512:  51%|█████     | 507/1000 [01:30<01:27,  5.63it/s]Measuring inference for batch_size=512:  51%|█████     | 508/1000 [01:30<01:27,  5.63it/s]Measuring inference for batch_size=512:  51%|█████     | 509/1000 [01:30<01:27,  5.63it/s]Measuring inference for batch_size=512:  51%|█████     | 510/1000 [01:30<01:27,  5.63it/s]Measuring inference for batch_size=512:  51%|█████     | 511/1000 [01:30<01:26,  5.62it/s]Measuring inference for batch_size=512:  51%|█████     | 512/1000 [01:30<01:26,  5.63it/s]Measuring inference for batch_size=512:  51%|█████▏    | 513/1000 [01:31<01:26,  5.63it/s]Measuring inference for batch_size=512:  51%|█████▏    | 514/1000 [01:31<01:26,  5.63it/s]Measuring inference for batch_size=512:  52%|█████▏    | 515/1000 [01:31<01:26,  5.63it/s]Measuring inference for batch_size=512:  52%|█████▏    | 516/1000 [01:31<01:25,  5.63it/s]Measuring inference for batch_size=512:  52%|█████▏    | 517/1000 [01:31<01:25,  5.63it/s]Measuring inference for batch_size=512:  52%|█████▏    | 518/1000 [01:31<01:25,  5.63it/s]Measuring inference for batch_size=512:  52%|█████▏    | 519/1000 [01:32<01:25,  5.63it/s]Measuring inference for batch_size=512:  52%|█████▏    | 520/1000 [01:32<01:25,  5.63it/s]Measuring inference for batch_size=512:  52%|█████▏    | 521/1000 [01:32<01:25,  5.63it/s]Measuring inference for batch_size=512:  52%|█████▏    | 522/1000 [01:32<01:24,  5.63it/s]Measuring inference for batch_size=512:  52%|█████▏    | 523/1000 [01:32<01:24,  5.63it/s]Measuring inference for batch_size=512:  52%|█████▏    | 524/1000 [01:33<01:24,  5.63it/s]Measuring inference for batch_size=512:  52%|█████▎    | 525/1000 [01:33<01:24,  5.63it/s]Measuring inference for batch_size=512:  53%|█████▎    | 526/1000 [01:33<01:24,  5.63it/s]Measuring inference for batch_size=512:  53%|█████▎    | 527/1000 [01:33<01:24,  5.63it/s]Measuring inference for batch_size=512:  53%|█████▎    | 528/1000 [01:33<01:23,  5.62it/s]Measuring inference for batch_size=512:  53%|█████▎    | 529/1000 [01:33<01:23,  5.63it/s]Measuring inference for batch_size=512:  53%|█████▎    | 530/1000 [01:34<01:23,  5.62it/s]Measuring inference for batch_size=512:  53%|█████▎    | 531/1000 [01:34<01:23,  5.63it/s]Measuring inference for batch_size=512:  53%|█████▎    | 532/1000 [01:34<01:23,  5.63it/s]Measuring inference for batch_size=512:  53%|█████▎    | 533/1000 [01:34<01:23,  5.62it/s]Measuring inference for batch_size=512:  53%|█████▎    | 534/1000 [01:34<01:22,  5.62it/s]Measuring inference for batch_size=512:  54%|█████▎    | 535/1000 [01:35<01:22,  5.62it/s]Measuring inference for batch_size=512:  54%|█████▎    | 536/1000 [01:35<01:22,  5.62it/s]Measuring inference for batch_size=512:  54%|█████▎    | 537/1000 [01:35<01:22,  5.62it/s]Measuring inference for batch_size=512:  54%|█████▍    | 538/1000 [01:35<01:22,  5.62it/s]Measuring inference for batch_size=512:  54%|█████▍    | 539/1000 [01:35<01:21,  5.63it/s]Measuring inference for batch_size=512:  54%|█████▍    | 540/1000 [01:35<01:21,  5.63it/s]Measuring inference for batch_size=512:  54%|█████▍    | 541/1000 [01:36<01:21,  5.64it/s]Measuring inference for batch_size=512:  54%|█████▍    | 542/1000 [01:36<01:21,  5.63it/s]Measuring inference for batch_size=512:  54%|█████▍    | 543/1000 [01:36<01:21,  5.63it/s]Measuring inference for batch_size=512:  54%|█████▍    | 544/1000 [01:36<01:20,  5.63it/s]Measuring inference for batch_size=512:  55%|█████▍    | 545/1000 [01:36<01:20,  5.63it/s]Measuring inference for batch_size=512:  55%|█████▍    | 546/1000 [01:36<01:20,  5.63it/s]Measuring inference for batch_size=512:  55%|█████▍    | 547/1000 [01:37<01:20,  5.63it/s]Measuring inference for batch_size=512:  55%|█████▍    | 548/1000 [01:37<01:20,  5.63it/s]Measuring inference for batch_size=512:  55%|█████▍    | 549/1000 [01:37<01:20,  5.63it/s]Measuring inference for batch_size=512:  55%|█████▌    | 550/1000 [01:37<01:19,  5.63it/s]Measuring inference for batch_size=512:  55%|█████▌    | 551/1000 [01:37<01:19,  5.63it/s]Measuring inference for batch_size=512:  55%|█████▌    | 552/1000 [01:38<01:19,  5.62it/s]Measuring inference for batch_size=512:  55%|█████▌    | 553/1000 [01:38<01:19,  5.63it/s]Measuring inference for batch_size=512:  55%|█████▌    | 554/1000 [01:38<01:19,  5.63it/s]Measuring inference for batch_size=512:  56%|█████▌    | 555/1000 [01:38<01:19,  5.63it/s]Measuring inference for batch_size=512:  56%|█████▌    | 556/1000 [01:38<01:18,  5.63it/s]Measuring inference for batch_size=512:  56%|█████▌    | 557/1000 [01:38<01:18,  5.63it/s]Measuring inference for batch_size=512:  56%|█████▌    | 558/1000 [01:39<01:18,  5.63it/s]Measuring inference for batch_size=512:  56%|█████▌    | 559/1000 [01:39<01:18,  5.63it/s]Measuring inference for batch_size=512:  56%|█████▌    | 560/1000 [01:39<01:18,  5.63it/s]Measuring inference for batch_size=512:  56%|█████▌    | 561/1000 [01:39<01:17,  5.63it/s]Measuring inference for batch_size=512:  56%|█████▌    | 562/1000 [01:39<01:17,  5.63it/s]Measuring inference for batch_size=512:  56%|█████▋    | 563/1000 [01:39<01:17,  5.63it/s]Measuring inference for batch_size=512:  56%|█████▋    | 564/1000 [01:40<01:17,  5.61it/s]Measuring inference for batch_size=512:  56%|█████▋    | 565/1000 [01:40<01:17,  5.62it/s]Measuring inference for batch_size=512:  57%|█████▋    | 566/1000 [01:40<01:17,  5.62it/s]Measuring inference for batch_size=512:  57%|█████▋    | 567/1000 [01:40<01:17,  5.62it/s]Measuring inference for batch_size=512:  57%|█████▋    | 568/1000 [01:40<01:16,  5.62it/s]Measuring inference for batch_size=512:  57%|█████▋    | 569/1000 [01:41<01:16,  5.63it/s]Measuring inference for batch_size=512:  57%|█████▋    | 570/1000 [01:41<01:16,  5.62it/s]Measuring inference for batch_size=512:  57%|█████▋    | 571/1000 [01:41<01:16,  5.62it/s]Measuring inference for batch_size=512:  57%|█████▋    | 572/1000 [01:41<01:16,  5.62it/s]Measuring inference for batch_size=512:  57%|█████▋    | 573/1000 [01:41<01:15,  5.62it/s]Measuring inference for batch_size=512:  57%|█████▋    | 574/1000 [01:41<01:15,  5.62it/s]Measuring inference for batch_size=512:  57%|█████▊    | 575/1000 [01:42<01:15,  5.63it/s]Measuring inference for batch_size=512:  58%|█████▊    | 576/1000 [01:42<01:15,  5.63it/s]Measuring inference for batch_size=512:  58%|█████▊    | 577/1000 [01:42<01:15,  5.62it/s]Measuring inference for batch_size=512:  58%|█████▊    | 578/1000 [01:42<01:15,  5.62it/s]Measuring inference for batch_size=512:  58%|█████▊    | 579/1000 [01:42<01:14,  5.63it/s]Measuring inference for batch_size=512:  58%|█████▊    | 580/1000 [01:43<01:14,  5.63it/s]Measuring inference for batch_size=512:  58%|█████▊    | 581/1000 [01:43<01:14,  5.63it/s]Measuring inference for batch_size=512:  58%|█████▊    | 582/1000 [01:43<01:14,  5.63it/s]Measuring inference for batch_size=512:  58%|█████▊    | 583/1000 [01:43<01:14,  5.63it/s]Measuring inference for batch_size=512:  58%|█████▊    | 584/1000 [01:43<01:13,  5.63it/s]Measuring inference for batch_size=512:  58%|█████▊    | 585/1000 [01:43<01:13,  5.63it/s]Measuring inference for batch_size=512:  59%|█████▊    | 586/1000 [01:44<01:13,  5.63it/s]Measuring inference for batch_size=512:  59%|█████▊    | 587/1000 [01:44<01:13,  5.63it/s]Measuring inference for batch_size=512:  59%|█████▉    | 588/1000 [01:44<01:13,  5.63it/s]Measuring inference for batch_size=512:  59%|█████▉    | 589/1000 [01:44<01:12,  5.63it/s]Measuring inference for batch_size=512:  59%|█████▉    | 590/1000 [01:44<01:12,  5.63it/s]Measuring inference for batch_size=512:  59%|█████▉    | 591/1000 [01:44<01:12,  5.63it/s]Measuring inference for batch_size=512:  59%|█████▉    | 592/1000 [01:45<01:12,  5.63it/s]Measuring inference for batch_size=512:  59%|█████▉    | 593/1000 [01:45<01:12,  5.63it/s]Measuring inference for batch_size=512:  59%|█████▉    | 594/1000 [01:45<01:12,  5.63it/s]Measuring inference for batch_size=512:  60%|█████▉    | 595/1000 [01:45<01:11,  5.63it/s]Measuring inference for batch_size=512:  60%|█████▉    | 596/1000 [01:45<01:11,  5.63it/s]Measuring inference for batch_size=512:  60%|█████▉    | 597/1000 [01:46<01:11,  5.63it/s]Measuring inference for batch_size=512:  60%|█████▉    | 598/1000 [01:46<01:11,  5.63it/s]Measuring inference for batch_size=512:  60%|█████▉    | 599/1000 [01:46<01:11,  5.63it/s]Measuring inference for batch_size=512:  60%|██████    | 600/1000 [01:46<01:11,  5.63it/s]Measuring inference for batch_size=512:  60%|██████    | 601/1000 [01:46<01:10,  5.63it/s]Measuring inference for batch_size=512:  60%|██████    | 602/1000 [01:46<01:10,  5.64it/s]Measuring inference for batch_size=512:  60%|██████    | 603/1000 [01:47<01:10,  5.64it/s]Measuring inference for batch_size=512:  60%|██████    | 604/1000 [01:47<01:10,  5.64it/s]Measuring inference for batch_size=512:  60%|██████    | 605/1000 [01:47<01:10,  5.64it/s]Measuring inference for batch_size=512:  61%|██████    | 606/1000 [01:47<01:09,  5.63it/s]Measuring inference for batch_size=512:  61%|██████    | 607/1000 [01:47<01:09,  5.63it/s]Measuring inference for batch_size=512:  61%|██████    | 608/1000 [01:47<01:09,  5.63it/s]Measuring inference for batch_size=512:  61%|██████    | 609/1000 [01:48<01:09,  5.64it/s]Measuring inference for batch_size=512:  61%|██████    | 610/1000 [01:48<01:09,  5.64it/s]Measuring inference for batch_size=512:  61%|██████    | 611/1000 [01:48<01:08,  5.64it/s]Measuring inference for batch_size=512:  61%|██████    | 612/1000 [01:48<01:08,  5.64it/s]Measuring inference for batch_size=512:  61%|██████▏   | 613/1000 [01:48<01:08,  5.64it/s]Measuring inference for batch_size=512:  61%|██████▏   | 614/1000 [01:49<01:08,  5.63it/s]Measuring inference for batch_size=512:  62%|██████▏   | 615/1000 [01:49<01:08,  5.64it/s]Measuring inference for batch_size=512:  62%|██████▏   | 616/1000 [01:49<01:08,  5.63it/s]Measuring inference for batch_size=512:  62%|██████▏   | 617/1000 [01:49<01:08,  5.63it/s]Measuring inference for batch_size=512:  62%|██████▏   | 618/1000 [01:49<01:07,  5.63it/s]Measuring inference for batch_size=512:  62%|██████▏   | 619/1000 [01:49<01:07,  5.63it/s]Measuring inference for batch_size=512:  62%|██████▏   | 620/1000 [01:50<01:07,  5.63it/s]Measuring inference for batch_size=512:  62%|██████▏   | 621/1000 [01:50<01:07,  5.64it/s]Measuring inference for batch_size=512:  62%|██████▏   | 622/1000 [01:50<01:07,  5.63it/s]Measuring inference for batch_size=512:  62%|██████▏   | 623/1000 [01:50<01:06,  5.63it/s]Measuring inference for batch_size=512:  62%|██████▏   | 624/1000 [01:50<01:06,  5.63it/s]Measuring inference for batch_size=512:  62%|██████▎   | 625/1000 [01:50<01:06,  5.63it/s]Measuring inference for batch_size=512:  63%|██████▎   | 626/1000 [01:51<01:06,  5.63it/s]Measuring inference for batch_size=512:  63%|██████▎   | 627/1000 [01:51<01:06,  5.63it/s]Measuring inference for batch_size=512:  63%|██████▎   | 628/1000 [01:51<01:06,  5.63it/s]Measuring inference for batch_size=512:  63%|██████▎   | 629/1000 [01:51<01:05,  5.63it/s]Measuring inference for batch_size=512:  63%|██████▎   | 630/1000 [01:51<01:05,  5.63it/s]Measuring inference for batch_size=512:  63%|██████▎   | 631/1000 [01:52<01:05,  5.63it/s]Measuring inference for batch_size=512:  63%|██████▎   | 632/1000 [01:52<01:05,  5.63it/s]Measuring inference for batch_size=512:  63%|██████▎   | 633/1000 [01:52<01:05,  5.64it/s]Measuring inference for batch_size=512:  63%|██████▎   | 634/1000 [01:52<01:04,  5.64it/s]Measuring inference for batch_size=512:  64%|██████▎   | 635/1000 [01:52<01:04,  5.64it/s]Measuring inference for batch_size=512:  64%|██████▎   | 636/1000 [01:52<01:04,  5.63it/s]Measuring inference for batch_size=512:  64%|██████▎   | 637/1000 [01:53<01:04,  5.64it/s]Measuring inference for batch_size=512:  64%|██████▍   | 638/1000 [01:53<01:04,  5.64it/s]Measuring inference for batch_size=512:  64%|██████▍   | 639/1000 [01:53<01:04,  5.64it/s]Measuring inference for batch_size=512:  64%|██████▍   | 640/1000 [01:53<01:03,  5.64it/s]Measuring inference for batch_size=512:  64%|██████▍   | 641/1000 [01:53<01:03,  5.63it/s]Measuring inference for batch_size=512:  64%|██████▍   | 642/1000 [01:54<01:03,  5.63it/s]Measuring inference for batch_size=512:  64%|██████▍   | 643/1000 [01:54<01:03,  5.63it/s]Measuring inference for batch_size=512:  64%|██████▍   | 644/1000 [01:54<01:03,  5.63it/s]Measuring inference for batch_size=512:  64%|██████▍   | 645/1000 [01:54<01:03,  5.63it/s]Measuring inference for batch_size=512:  65%|██████▍   | 646/1000 [01:54<01:02,  5.63it/s]Measuring inference for batch_size=512:  65%|██████▍   | 647/1000 [01:54<01:02,  5.63it/s]Measuring inference for batch_size=512:  65%|██████▍   | 648/1000 [01:55<01:02,  5.63it/s]Measuring inference for batch_size=512:  65%|██████▍   | 649/1000 [01:55<01:02,  5.63it/s]Measuring inference for batch_size=512:  65%|██████▌   | 650/1000 [01:55<01:02,  5.63it/s]Measuring inference for batch_size=512:  65%|██████▌   | 651/1000 [01:55<01:02,  5.63it/s]Measuring inference for batch_size=512:  65%|██████▌   | 652/1000 [01:55<01:01,  5.63it/s]Measuring inference for batch_size=512:  65%|██████▌   | 653/1000 [01:55<01:01,  5.63it/s]Measuring inference for batch_size=512:  65%|██████▌   | 654/1000 [01:56<01:01,  5.63it/s]Measuring inference for batch_size=512:  66%|██████▌   | 655/1000 [01:56<01:01,  5.63it/s]Measuring inference for batch_size=512:  66%|██████▌   | 656/1000 [01:56<01:01,  5.61it/s]Measuring inference for batch_size=512:  66%|██████▌   | 657/1000 [01:56<01:01,  5.62it/s]Measuring inference for batch_size=512:  66%|██████▌   | 658/1000 [01:56<01:00,  5.62it/s]Measuring inference for batch_size=512:  66%|██████▌   | 659/1000 [01:57<01:00,  5.62it/s]Measuring inference for batch_size=512:  66%|██████▌   | 660/1000 [01:57<01:00,  5.62it/s]Measuring inference for batch_size=512:  66%|██████▌   | 661/1000 [01:57<01:00,  5.62it/s]Measuring inference for batch_size=512:  66%|██████▌   | 662/1000 [01:57<01:00,  5.63it/s]Measuring inference for batch_size=512:  66%|██████▋   | 663/1000 [01:57<00:59,  5.63it/s]Measuring inference for batch_size=512:  66%|██████▋   | 664/1000 [01:57<00:59,  5.63it/s]Measuring inference for batch_size=512:  66%|██████▋   | 665/1000 [01:58<00:59,  5.63it/s]Measuring inference for batch_size=512:  67%|██████▋   | 666/1000 [01:58<00:59,  5.63it/s]Measuring inference for batch_size=512:  67%|██████▋   | 667/1000 [01:58<00:59,  5.63it/s]Measuring inference for batch_size=512:  67%|██████▋   | 668/1000 [01:58<00:58,  5.63it/s]Measuring inference for batch_size=512:  67%|██████▋   | 669/1000 [01:58<00:58,  5.63it/s]Measuring inference for batch_size=512:  67%|██████▋   | 670/1000 [01:58<00:58,  5.63it/s]Measuring inference for batch_size=512:  67%|██████▋   | 671/1000 [01:59<00:58,  5.63it/s]Measuring inference for batch_size=512:  67%|██████▋   | 672/1000 [01:59<00:58,  5.63it/s]Measuring inference for batch_size=512:  67%|██████▋   | 673/1000 [01:59<00:58,  5.63it/s]Measuring inference for batch_size=512:  67%|██████▋   | 674/1000 [01:59<00:57,  5.62it/s]Measuring inference for batch_size=512:  68%|██████▊   | 675/1000 [01:59<00:57,  5.62it/s]Measuring inference for batch_size=512:  68%|██████▊   | 676/1000 [02:00<00:57,  5.63it/s]Measuring inference for batch_size=512:  68%|██████▊   | 677/1000 [02:00<00:57,  5.63it/s]Measuring inference for batch_size=512:  68%|██████▊   | 678/1000 [02:00<00:57,  5.63it/s]Measuring inference for batch_size=512:  68%|██████▊   | 679/1000 [02:00<00:57,  5.63it/s]Measuring inference for batch_size=512:  68%|██████▊   | 680/1000 [02:00<00:56,  5.62it/s]Measuring inference for batch_size=512:  68%|██████▊   | 681/1000 [02:00<00:56,  5.63it/s]Measuring inference for batch_size=512:  68%|██████▊   | 682/1000 [02:01<00:56,  5.63it/s]Measuring inference for batch_size=512:  68%|██████▊   | 683/1000 [02:01<00:56,  5.63it/s]Measuring inference for batch_size=512:  68%|██████▊   | 684/1000 [02:01<00:56,  5.63it/s]Measuring inference for batch_size=512:  68%|██████▊   | 685/1000 [02:01<00:55,  5.63it/s]Measuring inference for batch_size=512:  69%|██████▊   | 686/1000 [02:01<00:55,  5.63it/s]Measuring inference for batch_size=512:  69%|██████▊   | 687/1000 [02:02<00:55,  5.64it/s]Measuring inference for batch_size=512:  69%|██████▉   | 688/1000 [02:02<00:55,  5.64it/s]Measuring inference for batch_size=512:  69%|██████▉   | 689/1000 [02:02<00:55,  5.64it/s]Measuring inference for batch_size=512:  69%|██████▉   | 690/1000 [02:02<00:55,  5.63it/s]Measuring inference for batch_size=512:  69%|██████▉   | 691/1000 [02:02<00:54,  5.63it/s]Measuring inference for batch_size=512:  69%|██████▉   | 692/1000 [02:02<00:54,  5.63it/s]Measuring inference for batch_size=512:  69%|██████▉   | 693/1000 [02:03<00:54,  5.63it/s]Measuring inference for batch_size=512:  69%|██████▉   | 694/1000 [02:03<00:54,  5.64it/s]Measuring inference for batch_size=512:  70%|██████▉   | 695/1000 [02:03<00:54,  5.64it/s]Measuring inference for batch_size=512:  70%|██████▉   | 696/1000 [02:03<00:53,  5.63it/s]Measuring inference for batch_size=512:  70%|██████▉   | 697/1000 [02:03<00:53,  5.63it/s]Measuring inference for batch_size=512:  70%|██████▉   | 698/1000 [02:03<00:53,  5.63it/s]Measuring inference for batch_size=512:  70%|██████▉   | 699/1000 [02:04<00:53,  5.64it/s]Measuring inference for batch_size=512:  70%|███████   | 700/1000 [02:04<00:53,  5.64it/s]Measuring inference for batch_size=512:  70%|███████   | 701/1000 [02:04<00:53,  5.63it/s]Measuring inference for batch_size=512:  70%|███████   | 702/1000 [02:04<00:52,  5.63it/s]Measuring inference for batch_size=512:  70%|███████   | 703/1000 [02:04<00:52,  5.63it/s]Measuring inference for batch_size=512:  70%|███████   | 704/1000 [02:05<00:52,  5.63it/s]Measuring inference for batch_size=512:  70%|███████   | 705/1000 [02:05<00:52,  5.63it/s]Measuring inference for batch_size=512:  71%|███████   | 706/1000 [02:05<00:52,  5.63it/s]Measuring inference for batch_size=512:  71%|███████   | 707/1000 [02:05<00:52,  5.63it/s]Measuring inference for batch_size=512:  71%|███████   | 708/1000 [02:05<00:51,  5.63it/s]Measuring inference for batch_size=512:  71%|███████   | 709/1000 [02:05<00:51,  5.63it/s]Measuring inference for batch_size=512:  71%|███████   | 710/1000 [02:06<00:51,  5.63it/s]Measuring inference for batch_size=512:  71%|███████   | 711/1000 [02:06<00:51,  5.64it/s]Measuring inference for batch_size=512:  71%|███████   | 712/1000 [02:06<00:51,  5.63it/s]Measuring inference for batch_size=512:  71%|███████▏  | 713/1000 [02:06<00:50,  5.64it/s]Measuring inference for batch_size=512:  71%|███████▏  | 714/1000 [02:06<00:50,  5.64it/s]Measuring inference for batch_size=512:  72%|███████▏  | 715/1000 [02:06<00:50,  5.64it/s]Measuring inference for batch_size=512:  72%|███████▏  | 716/1000 [02:07<00:50,  5.64it/s]Measuring inference for batch_size=512:  72%|███████▏  | 717/1000 [02:07<00:50,  5.64it/s]Measuring inference for batch_size=512:  72%|███████▏  | 718/1000 [02:07<00:50,  5.64it/s]Measuring inference for batch_size=512:  72%|███████▏  | 719/1000 [02:07<00:49,  5.64it/s]Measuring inference for batch_size=512:  72%|███████▏  | 720/1000 [02:07<00:49,  5.64it/s]Measuring inference for batch_size=512:  72%|███████▏  | 721/1000 [02:08<00:49,  5.64it/s]Measuring inference for batch_size=512:  72%|███████▏  | 722/1000 [02:08<00:49,  5.64it/s]Measuring inference for batch_size=512:  72%|███████▏  | 723/1000 [02:08<00:49,  5.64it/s]Measuring inference for batch_size=512:  72%|███████▏  | 724/1000 [02:08<00:48,  5.63it/s]Measuring inference for batch_size=512:  72%|███████▎  | 725/1000 [02:08<00:48,  5.64it/s]Measuring inference for batch_size=512:  73%|███████▎  | 726/1000 [02:08<00:48,  5.64it/s]Measuring inference for batch_size=512:  73%|███████▎  | 727/1000 [02:09<00:48,  5.64it/s]Measuring inference for batch_size=512:  73%|███████▎  | 728/1000 [02:09<00:48,  5.63it/s]Measuring inference for batch_size=512:  73%|███████▎  | 729/1000 [02:09<00:48,  5.63it/s]Measuring inference for batch_size=512:  73%|███████▎  | 730/1000 [02:09<00:48,  5.62it/s]Measuring inference for batch_size=512:  73%|███████▎  | 731/1000 [02:09<00:47,  5.62it/s]Measuring inference for batch_size=512:  73%|███████▎  | 732/1000 [02:09<00:47,  5.62it/s]Measuring inference for batch_size=512:  73%|███████▎  | 733/1000 [02:10<00:47,  5.63it/s]Measuring inference for batch_size=512:  73%|███████▎  | 734/1000 [02:10<00:47,  5.63it/s]Measuring inference for batch_size=512:  74%|███████▎  | 735/1000 [02:10<00:47,  5.64it/s]Measuring inference for batch_size=512:  74%|███████▎  | 736/1000 [02:10<00:46,  5.63it/s]Measuring inference for batch_size=512:  74%|███████▎  | 737/1000 [02:10<00:46,  5.63it/s]Measuring inference for batch_size=512:  74%|███████▍  | 738/1000 [02:11<00:46,  5.63it/s]Measuring inference for batch_size=512:  74%|███████▍  | 739/1000 [02:11<00:46,  5.63it/s]Measuring inference for batch_size=512:  74%|███████▍  | 740/1000 [02:11<00:46,  5.63it/s]Measuring inference for batch_size=512:  74%|███████▍  | 741/1000 [02:11<00:45,  5.63it/s]Measuring inference for batch_size=512:  74%|███████▍  | 742/1000 [02:11<00:45,  5.63it/s]Measuring inference for batch_size=512:  74%|███████▍  | 743/1000 [02:11<00:45,  5.63it/s]Measuring inference for batch_size=512:  74%|███████▍  | 744/1000 [02:12<00:45,  5.63it/s]Measuring inference for batch_size=512:  74%|███████▍  | 745/1000 [02:12<00:45,  5.63it/s]Measuring inference for batch_size=512:  75%|███████▍  | 746/1000 [02:12<00:45,  5.63it/s]Measuring inference for batch_size=512:  75%|███████▍  | 747/1000 [02:12<00:44,  5.63it/s]Measuring inference for batch_size=512:  75%|███████▍  | 748/1000 [02:12<00:44,  5.63it/s]Measuring inference for batch_size=512:  75%|███████▍  | 749/1000 [02:13<00:44,  5.64it/s]Measuring inference for batch_size=512:  75%|███████▌  | 750/1000 [02:13<00:44,  5.63it/s]Measuring inference for batch_size=512:  75%|███████▌  | 751/1000 [02:13<00:44,  5.63it/s]Measuring inference for batch_size=512:  75%|███████▌  | 752/1000 [02:13<00:44,  5.63it/s]Measuring inference for batch_size=512:  75%|███████▌  | 753/1000 [02:13<00:43,  5.63it/s]Measuring inference for batch_size=512:  75%|███████▌  | 754/1000 [02:13<00:43,  5.63it/s]Measuring inference for batch_size=512:  76%|███████▌  | 755/1000 [02:14<00:43,  5.63it/s]Measuring inference for batch_size=512:  76%|███████▌  | 756/1000 [02:14<00:43,  5.63it/s]Measuring inference for batch_size=512:  76%|███████▌  | 757/1000 [02:14<00:43,  5.63it/s]Measuring inference for batch_size=512:  76%|███████▌  | 758/1000 [02:14<00:42,  5.63it/s]Measuring inference for batch_size=512:  76%|███████▌  | 759/1000 [02:14<00:42,  5.63it/s]Measuring inference for batch_size=512:  76%|███████▌  | 760/1000 [02:14<00:42,  5.63it/s]Measuring inference for batch_size=512:  76%|███████▌  | 761/1000 [02:15<00:42,  5.63it/s]Measuring inference for batch_size=512:  76%|███████▌  | 762/1000 [02:15<00:42,  5.64it/s]Measuring inference for batch_size=512:  76%|███████▋  | 763/1000 [02:15<00:42,  5.64it/s]Measuring inference for batch_size=512:  76%|███████▋  | 764/1000 [02:15<00:41,  5.64it/s]Measuring inference for batch_size=512:  76%|███████▋  | 765/1000 [02:15<00:41,  5.63it/s]Measuring inference for batch_size=512:  77%|███████▋  | 766/1000 [02:16<00:41,  5.63it/s]Measuring inference for batch_size=512:  77%|███████▋  | 767/1000 [02:16<00:41,  5.63it/s]Measuring inference for batch_size=512:  77%|███████▋  | 768/1000 [02:16<00:41,  5.63it/s]Measuring inference for batch_size=512:  77%|███████▋  | 769/1000 [02:16<00:41,  5.63it/s]Measuring inference for batch_size=512:  77%|███████▋  | 770/1000 [02:16<00:40,  5.63it/s]Measuring inference for batch_size=512:  77%|███████▋  | 771/1000 [02:16<00:40,  5.63it/s]Measuring inference for batch_size=512:  77%|███████▋  | 772/1000 [02:17<00:40,  5.63it/s]Measuring inference for batch_size=512:  77%|███████▋  | 773/1000 [02:17<00:40,  5.64it/s]Measuring inference for batch_size=512:  77%|███████▋  | 774/1000 [02:17<00:40,  5.64it/s]Measuring inference for batch_size=512:  78%|███████▊  | 775/1000 [02:17<00:39,  5.64it/s]Measuring inference for batch_size=512:  78%|███████▊  | 776/1000 [02:17<00:39,  5.63it/s]Measuring inference for batch_size=512:  78%|███████▊  | 777/1000 [02:17<00:39,  5.63it/s]Measuring inference for batch_size=512:  78%|███████▊  | 778/1000 [02:18<00:39,  5.63it/s]Measuring inference for batch_size=512:  78%|███████▊  | 779/1000 [02:18<00:39,  5.63it/s]Measuring inference for batch_size=512:  78%|███████▊  | 780/1000 [02:18<00:39,  5.63it/s]Measuring inference for batch_size=512:  78%|███████▊  | 781/1000 [02:18<00:38,  5.63it/s]Measuring inference for batch_size=512:  78%|███████▊  | 782/1000 [02:18<00:38,  5.63it/s]Measuring inference for batch_size=512:  78%|███████▊  | 783/1000 [02:19<00:38,  5.63it/s]Measuring inference for batch_size=512:  78%|███████▊  | 784/1000 [02:19<00:38,  5.63it/s]Measuring inference for batch_size=512:  78%|███████▊  | 785/1000 [02:19<00:38,  5.63it/s]Measuring inference for batch_size=512:  79%|███████▊  | 786/1000 [02:19<00:37,  5.63it/s]Measuring inference for batch_size=512:  79%|███████▊  | 787/1000 [02:19<00:37,  5.64it/s]Measuring inference for batch_size=512:  79%|███████▉  | 788/1000 [02:19<00:37,  5.64it/s]Measuring inference for batch_size=512:  79%|███████▉  | 789/1000 [02:20<00:37,  5.64it/s]Measuring inference for batch_size=512:  79%|███████▉  | 790/1000 [02:20<00:37,  5.64it/s]Measuring inference for batch_size=512:  79%|███████▉  | 791/1000 [02:20<00:37,  5.64it/s]Measuring inference for batch_size=512:  79%|███████▉  | 792/1000 [02:20<00:36,  5.63it/s]Measuring inference for batch_size=512:  79%|███████▉  | 793/1000 [02:20<00:36,  5.63it/s]Measuring inference for batch_size=512:  79%|███████▉  | 794/1000 [02:21<00:36,  5.63it/s]Measuring inference for batch_size=512:  80%|███████▉  | 795/1000 [02:21<00:36,  5.63it/s]Measuring inference for batch_size=512:  80%|███████▉  | 796/1000 [02:21<00:36,  5.63it/s]Measuring inference for batch_size=512:  80%|███████▉  | 797/1000 [02:21<00:36,  5.63it/s]Measuring inference for batch_size=512:  80%|███████▉  | 798/1000 [02:21<00:35,  5.63it/s]Measuring inference for batch_size=512:  80%|███████▉  | 799/1000 [02:21<00:35,  5.64it/s]Measuring inference for batch_size=512:  80%|████████  | 800/1000 [02:22<00:35,  5.64it/s]Measuring inference for batch_size=512:  80%|████████  | 801/1000 [02:22<00:35,  5.64it/s]Measuring inference for batch_size=512:  80%|████████  | 802/1000 [02:22<00:35,  5.64it/s]Measuring inference for batch_size=512:  80%|████████  | 803/1000 [02:22<00:34,  5.63it/s]Measuring inference for batch_size=512:  80%|████████  | 804/1000 [02:22<00:34,  5.64it/s]Measuring inference for batch_size=512:  80%|████████  | 805/1000 [02:22<00:34,  5.64it/s]Measuring inference for batch_size=512:  81%|████████  | 806/1000 [02:23<00:34,  5.63it/s]Measuring inference for batch_size=512:  81%|████████  | 807/1000 [02:23<00:34,  5.63it/s]Measuring inference for batch_size=512:  81%|████████  | 808/1000 [02:23<00:34,  5.60it/s]Measuring inference for batch_size=512:  81%|████████  | 809/1000 [02:23<00:34,  5.61it/s]Measuring inference for batch_size=512:  81%|████████  | 810/1000 [02:23<00:33,  5.62it/s]Measuring inference for batch_size=512:  81%|████████  | 811/1000 [02:24<00:33,  5.62it/s]Measuring inference for batch_size=512:  81%|████████  | 812/1000 [02:24<00:33,  5.63it/s]Measuring inference for batch_size=512:  81%|████████▏ | 813/1000 [02:24<00:33,  5.63it/s]Measuring inference for batch_size=512:  81%|████████▏ | 814/1000 [02:24<00:33,  5.63it/s]Measuring inference for batch_size=512:  82%|████████▏ | 815/1000 [02:24<00:32,  5.63it/s]Measuring inference for batch_size=512:  82%|████████▏ | 816/1000 [02:24<00:32,  5.64it/s]Measuring inference for batch_size=512:  82%|████████▏ | 817/1000 [02:25<00:32,  5.63it/s]Measuring inference for batch_size=512:  82%|████████▏ | 818/1000 [02:25<00:32,  5.63it/s]Measuring inference for batch_size=512:  82%|████████▏ | 819/1000 [02:25<00:32,  5.62it/s]Measuring inference for batch_size=512:  82%|████████▏ | 820/1000 [02:25<00:31,  5.63it/s]Measuring inference for batch_size=512:  82%|████████▏ | 821/1000 [02:25<00:31,  5.63it/s]Measuring inference for batch_size=512:  82%|████████▏ | 822/1000 [02:25<00:31,  5.63it/s]Measuring inference for batch_size=512:  82%|████████▏ | 823/1000 [02:26<00:31,  5.63it/s]Measuring inference for batch_size=512:  82%|████████▏ | 824/1000 [02:26<00:31,  5.63it/s]Measuring inference for batch_size=512:  82%|████████▎ | 825/1000 [02:26<00:31,  5.63it/s]Measuring inference for batch_size=512:  83%|████████▎ | 826/1000 [02:26<00:30,  5.63it/s]Measuring inference for batch_size=512:  83%|████████▎ | 827/1000 [02:26<00:30,  5.64it/s]Measuring inference for batch_size=512:  83%|████████▎ | 828/1000 [02:27<00:30,  5.63it/s]Measuring inference for batch_size=512:  83%|████████▎ | 829/1000 [02:27<00:30,  5.63it/s]Measuring inference for batch_size=512:  83%|████████▎ | 830/1000 [02:27<00:30,  5.63it/s]Measuring inference for batch_size=512:  83%|████████▎ | 831/1000 [02:27<00:29,  5.64it/s]Measuring inference for batch_size=512:  83%|████████▎ | 832/1000 [02:27<00:29,  5.63it/s]Measuring inference for batch_size=512:  83%|████████▎ | 833/1000 [02:27<00:29,  5.63it/s]Measuring inference for batch_size=512:  83%|████████▎ | 834/1000 [02:28<00:29,  5.63it/s]Measuring inference for batch_size=512:  84%|████████▎ | 835/1000 [02:28<00:29,  5.63it/s]Measuring inference for batch_size=512:  84%|████████▎ | 836/1000 [02:28<00:29,  5.63it/s]Measuring inference for batch_size=512:  84%|████████▎ | 837/1000 [02:28<00:28,  5.63it/s]Measuring inference for batch_size=512:  84%|████████▍ | 838/1000 [02:28<00:28,  5.63it/s]Measuring inference for batch_size=512:  84%|████████▍ | 839/1000 [02:28<00:28,  5.63it/s]Measuring inference for batch_size=512:  84%|████████▍ | 840/1000 [02:29<00:28,  5.63it/s]Measuring inference for batch_size=512:  84%|████████▍ | 841/1000 [02:29<00:28,  5.64it/s]Measuring inference for batch_size=512:  84%|████████▍ | 842/1000 [02:29<00:28,  5.64it/s]Measuring inference for batch_size=512:  84%|████████▍ | 843/1000 [02:29<00:27,  5.64it/s]Measuring inference for batch_size=512:  84%|████████▍ | 844/1000 [02:29<00:27,  5.64it/s]Measuring inference for batch_size=512:  84%|████████▍ | 845/1000 [02:30<00:27,  5.64it/s]Measuring inference for batch_size=512:  85%|████████▍ | 846/1000 [02:30<00:27,  5.64it/s]Measuring inference for batch_size=512:  85%|████████▍ | 847/1000 [02:30<00:27,  5.64it/s]Measuring inference for batch_size=512:  85%|████████▍ | 848/1000 [02:30<00:26,  5.64it/s]Measuring inference for batch_size=512:  85%|████████▍ | 849/1000 [02:30<00:26,  5.64it/s]Measuring inference for batch_size=512:  85%|████████▌ | 850/1000 [02:30<00:26,  5.63it/s]Measuring inference for batch_size=512:  85%|████████▌ | 851/1000 [02:31<00:26,  5.63it/s]Measuring inference for batch_size=512:  85%|████████▌ | 852/1000 [02:31<00:26,  5.63it/s]Measuring inference for batch_size=512:  85%|████████▌ | 853/1000 [02:31<00:26,  5.63it/s]Measuring inference for batch_size=512:  85%|████████▌ | 854/1000 [02:31<00:25,  5.63it/s]Measuring inference for batch_size=512:  86%|████████▌ | 855/1000 [02:31<00:25,  5.63it/s]Measuring inference for batch_size=512:  86%|████████▌ | 856/1000 [02:32<00:25,  5.63it/s]Measuring inference for batch_size=512:  86%|████████▌ | 857/1000 [02:32<00:25,  5.63it/s]Measuring inference for batch_size=512:  86%|████████▌ | 858/1000 [02:32<00:25,  5.63it/s]Measuring inference for batch_size=512:  86%|████████▌ | 859/1000 [02:32<00:25,  5.64it/s]Measuring inference for batch_size=512:  86%|████████▌ | 860/1000 [02:32<00:24,  5.63it/s]Measuring inference for batch_size=512:  86%|████████▌ | 861/1000 [02:32<00:24,  5.63it/s]Measuring inference for batch_size=512:  86%|████████▌ | 862/1000 [02:33<00:24,  5.63it/s]Measuring inference for batch_size=512:  86%|████████▋ | 863/1000 [02:33<00:24,  5.64it/s]Measuring inference for batch_size=512:  86%|████████▋ | 864/1000 [02:33<00:24,  5.63it/s]Measuring inference for batch_size=512:  86%|████████▋ | 865/1000 [02:33<00:23,  5.63it/s]Measuring inference for batch_size=512:  87%|████████▋ | 866/1000 [02:33<00:23,  5.63it/s]Measuring inference for batch_size=512:  87%|████████▋ | 867/1000 [02:33<00:23,  5.63it/s]Measuring inference for batch_size=512:  87%|████████▋ | 868/1000 [02:34<00:23,  5.63it/s]Measuring inference for batch_size=512:  87%|████████▋ | 869/1000 [02:34<00:23,  5.63it/s]Measuring inference for batch_size=512:  87%|████████▋ | 870/1000 [02:34<00:23,  5.63it/s]Measuring inference for batch_size=512:  87%|████████▋ | 871/1000 [02:34<00:22,  5.63it/s]Measuring inference for batch_size=512:  87%|████████▋ | 872/1000 [02:34<00:22,  5.63it/s]Measuring inference for batch_size=512:  87%|████████▋ | 873/1000 [02:35<00:22,  5.63it/s]Measuring inference for batch_size=512:  87%|████████▋ | 874/1000 [02:35<00:22,  5.63it/s]Measuring inference for batch_size=512:  88%|████████▊ | 875/1000 [02:35<00:22,  5.64it/s]Measuring inference for batch_size=512:  88%|████████▊ | 876/1000 [02:35<00:21,  5.64it/s]Measuring inference for batch_size=512:  88%|████████▊ | 877/1000 [02:35<00:21,  5.64it/s]Measuring inference for batch_size=512:  88%|████████▊ | 878/1000 [02:35<00:21,  5.64it/s]Measuring inference for batch_size=512:  88%|████████▊ | 879/1000 [02:36<00:21,  5.64it/s]Measuring inference for batch_size=512:  88%|████████▊ | 880/1000 [02:36<00:21,  5.64it/s]Measuring inference for batch_size=512:  88%|████████▊ | 881/1000 [02:36<00:21,  5.64it/s]Measuring inference for batch_size=512:  88%|████████▊ | 882/1000 [02:36<00:20,  5.63it/s]Measuring inference for batch_size=512:  88%|████████▊ | 883/1000 [02:36<00:20,  5.64it/s]Measuring inference for batch_size=512:  88%|████████▊ | 884/1000 [02:36<00:20,  5.63it/s]Measuring inference for batch_size=512:  88%|████████▊ | 885/1000 [02:37<00:20,  5.63it/s]Measuring inference for batch_size=512:  89%|████████▊ | 886/1000 [02:37<00:20,  5.63it/s]Measuring inference for batch_size=512:  89%|████████▊ | 887/1000 [02:37<00:20,  5.64it/s]Measuring inference for batch_size=512:  89%|████████▉ | 888/1000 [02:37<00:19,  5.64it/s]Measuring inference for batch_size=512:  89%|████████▉ | 889/1000 [02:37<00:19,  5.64it/s]Measuring inference for batch_size=512:  89%|████████▉ | 890/1000 [02:38<00:19,  5.64it/s]Measuring inference for batch_size=512:  89%|████████▉ | 891/1000 [02:38<00:19,  5.63it/s]Measuring inference for batch_size=512:  89%|████████▉ | 892/1000 [02:38<00:19,  5.64it/s]Measuring inference for batch_size=512:  89%|████████▉ | 893/1000 [02:38<00:18,  5.64it/s]Measuring inference for batch_size=512:  89%|████████▉ | 894/1000 [02:38<00:18,  5.63it/s]Measuring inference for batch_size=512:  90%|████████▉ | 895/1000 [02:38<00:18,  5.63it/s]Measuring inference for batch_size=512:  90%|████████▉ | 896/1000 [02:39<00:18,  5.63it/s]Measuring inference for batch_size=512:  90%|████████▉ | 897/1000 [02:39<00:18,  5.64it/s]Measuring inference for batch_size=512:  90%|████████▉ | 898/1000 [02:39<00:18,  5.64it/s]Measuring inference for batch_size=512:  90%|████████▉ | 899/1000 [02:39<00:17,  5.64it/s]Measuring inference for batch_size=512:  90%|█████████ | 900/1000 [02:39<00:17,  5.63it/s]Measuring inference for batch_size=512:  90%|█████████ | 901/1000 [02:40<00:17,  5.63it/s]Measuring inference for batch_size=512:  90%|█████████ | 902/1000 [02:40<00:17,  5.63it/s]Measuring inference for batch_size=512:  90%|█████████ | 903/1000 [02:40<00:17,  5.63it/s]Measuring inference for batch_size=512:  90%|█████████ | 904/1000 [02:40<00:17,  5.63it/s]Measuring inference for batch_size=512:  90%|█████████ | 905/1000 [02:40<00:16,  5.63it/s]Measuring inference for batch_size=512:  91%|█████████ | 906/1000 [02:40<00:16,  5.63it/s]Measuring inference for batch_size=512:  91%|█████████ | 907/1000 [02:41<00:16,  5.63it/s]Measuring inference for batch_size=512:  91%|█████████ | 908/1000 [02:41<00:16,  5.63it/s]Measuring inference for batch_size=512:  91%|█████████ | 909/1000 [02:41<00:16,  5.63it/s]Measuring inference for batch_size=512:  91%|█████████ | 910/1000 [02:41<00:15,  5.63it/s]Measuring inference for batch_size=512:  91%|█████████ | 911/1000 [02:41<00:15,  5.64it/s]Measuring inference for batch_size=512:  91%|█████████ | 912/1000 [02:41<00:15,  5.64it/s]Measuring inference for batch_size=512:  91%|█████████▏| 913/1000 [02:42<00:15,  5.64it/s]Measuring inference for batch_size=512:  91%|█████████▏| 914/1000 [02:42<00:15,  5.64it/s]Measuring inference for batch_size=512:  92%|█████████▏| 915/1000 [02:42<00:15,  5.64it/s]Measuring inference for batch_size=512:  92%|█████████▏| 916/1000 [02:42<00:14,  5.63it/s]Measuring inference for batch_size=512:  92%|█████████▏| 917/1000 [02:42<00:14,  5.63it/s]Measuring inference for batch_size=512:  92%|█████████▏| 918/1000 [02:43<00:14,  5.63it/s]Measuring inference for batch_size=512:  92%|█████████▏| 919/1000 [02:43<00:14,  5.64it/s]Measuring inference for batch_size=512:  92%|█████████▏| 920/1000 [02:43<00:14,  5.64it/s]Measuring inference for batch_size=512:  92%|█████████▏| 921/1000 [02:43<00:14,  5.64it/s]Measuring inference for batch_size=512:  92%|█████████▏| 922/1000 [02:43<00:13,  5.64it/s]Measuring inference for batch_size=512:  92%|█████████▏| 923/1000 [02:43<00:13,  5.64it/s]Measuring inference for batch_size=512:  92%|█████████▏| 924/1000 [02:44<00:13,  5.64it/s]Measuring inference for batch_size=512:  92%|█████████▎| 925/1000 [02:44<00:13,  5.64it/s]Measuring inference for batch_size=512:  93%|█████████▎| 926/1000 [02:44<00:13,  5.63it/s]Measuring inference for batch_size=512:  93%|█████████▎| 927/1000 [02:44<00:12,  5.63it/s]Measuring inference for batch_size=512:  93%|█████████▎| 928/1000 [02:44<00:12,  5.63it/s]Measuring inference for batch_size=512:  93%|█████████▎| 929/1000 [02:44<00:12,  5.64it/s]Measuring inference for batch_size=512:  93%|█████████▎| 930/1000 [02:45<00:12,  5.63it/s]Measuring inference for batch_size=512:  93%|█████████▎| 931/1000 [02:45<00:12,  5.63it/s]Measuring inference for batch_size=512:  93%|█████████▎| 932/1000 [02:45<00:12,  5.63it/s]Measuring inference for batch_size=512:  93%|█████████▎| 933/1000 [02:45<00:11,  5.63it/s]Measuring inference for batch_size=512:  93%|█████████▎| 934/1000 [02:45<00:11,  5.62it/s]Measuring inference for batch_size=512:  94%|█████████▎| 935/1000 [02:46<00:11,  5.63it/s]Measuring inference for batch_size=512:  94%|█████████▎| 936/1000 [02:46<00:11,  5.63it/s]Measuring inference for batch_size=512:  94%|█████████▎| 937/1000 [02:46<00:11,  5.64it/s]Measuring inference for batch_size=512:  94%|█████████▍| 938/1000 [02:46<00:11,  5.64it/s]Measuring inference for batch_size=512:  94%|█████████▍| 939/1000 [02:46<00:10,  5.64it/s]Measuring inference for batch_size=512:  94%|█████████▍| 940/1000 [02:46<00:10,  5.64it/s]Measuring inference for batch_size=512:  94%|█████████▍| 941/1000 [02:47<00:10,  5.64it/s]Measuring inference for batch_size=512:  94%|█████████▍| 942/1000 [02:47<00:10,  5.63it/s]Measuring inference for batch_size=512:  94%|█████████▍| 943/1000 [02:47<00:10,  5.63it/s]Measuring inference for batch_size=512:  94%|█████████▍| 944/1000 [02:47<00:09,  5.63it/s]Measuring inference for batch_size=512:  94%|█████████▍| 945/1000 [02:47<00:09,  5.63it/s]Measuring inference for batch_size=512:  95%|█████████▍| 946/1000 [02:47<00:09,  5.64it/s]Measuring inference for batch_size=512:  95%|█████████▍| 947/1000 [02:48<00:09,  5.64it/s]Measuring inference for batch_size=512:  95%|█████████▍| 948/1000 [02:48<00:09,  5.64it/s]Measuring inference for batch_size=512:  95%|█████████▍| 949/1000 [02:48<00:09,  5.64it/s]Measuring inference for batch_size=512:  95%|█████████▌| 950/1000 [02:48<00:08,  5.64it/s]Measuring inference for batch_size=512:  95%|█████████▌| 951/1000 [02:48<00:08,  5.64it/s]Measuring inference for batch_size=512:  95%|█████████▌| 952/1000 [02:49<00:08,  5.63it/s]Measuring inference for batch_size=512:  95%|█████████▌| 953/1000 [02:49<00:08,  5.64it/s]Measuring inference for batch_size=512:  95%|█████████▌| 954/1000 [02:49<00:08,  5.64it/s]Measuring inference for batch_size=512:  96%|█████████▌| 955/1000 [02:49<00:07,  5.63it/s]Measuring inference for batch_size=512:  96%|█████████▌| 956/1000 [02:49<00:07,  5.63it/s]Measuring inference for batch_size=512:  96%|█████████▌| 957/1000 [02:49<00:07,  5.63it/s]Measuring inference for batch_size=512:  96%|█████████▌| 958/1000 [02:50<00:07,  5.63it/s]Measuring inference for batch_size=512:  96%|█████████▌| 959/1000 [02:50<00:07,  5.63it/s]Measuring inference for batch_size=512:  96%|█████████▌| 960/1000 [02:50<00:07,  5.63it/s]Measuring inference for batch_size=512:  96%|█████████▌| 961/1000 [02:50<00:06,  5.64it/s]Measuring inference for batch_size=512:  96%|█████████▌| 962/1000 [02:50<00:06,  5.64it/s]Measuring inference for batch_size=512:  96%|█████████▋| 963/1000 [02:51<00:06,  5.64it/s]Measuring inference for batch_size=512:  96%|█████████▋| 964/1000 [02:51<00:06,  5.64it/s]Measuring inference for batch_size=512:  96%|█████████▋| 965/1000 [02:51<00:06,  5.64it/s]Measuring inference for batch_size=512:  97%|█████████▋| 966/1000 [02:51<00:06,  5.63it/s]Measuring inference for batch_size=512:  97%|█████████▋| 967/1000 [02:51<00:05,  5.63it/s]Measuring inference for batch_size=512:  97%|█████████▋| 968/1000 [02:51<00:05,  5.63it/s]Measuring inference for batch_size=512:  97%|█████████▋| 969/1000 [02:52<00:05,  5.63it/s]Measuring inference for batch_size=512:  97%|█████████▋| 970/1000 [02:52<00:05,  5.63it/s]Measuring inference for batch_size=512:  97%|█████████▋| 971/1000 [02:52<00:05,  5.63it/s]Measuring inference for batch_size=512:  97%|█████████▋| 972/1000 [02:52<00:04,  5.63it/s]Measuring inference for batch_size=512:  97%|█████████▋| 973/1000 [02:52<00:04,  5.63it/s]Measuring inference for batch_size=512:  97%|█████████▋| 974/1000 [02:52<00:04,  5.63it/s]Measuring inference for batch_size=512:  98%|█████████▊| 975/1000 [02:53<00:04,  5.63it/s]Measuring inference for batch_size=512:  98%|█████████▊| 976/1000 [02:53<00:04,  5.63it/s]Measuring inference for batch_size=512:  98%|█████████▊| 977/1000 [02:53<00:04,  5.62it/s]Measuring inference for batch_size=512:  98%|█████████▊| 978/1000 [02:53<00:03,  5.63it/s]Measuring inference for batch_size=512:  98%|█████████▊| 979/1000 [02:53<00:03,  5.63it/s]Measuring inference for batch_size=512:  98%|█████████▊| 980/1000 [02:54<00:03,  5.64it/s]Measuring inference for batch_size=512:  98%|█████████▊| 981/1000 [02:54<00:03,  5.63it/s]Measuring inference for batch_size=512:  98%|█████████▊| 982/1000 [02:54<00:03,  5.63it/s]Measuring inference for batch_size=512:  98%|█████████▊| 983/1000 [02:54<00:03,  5.63it/s]Measuring inference for batch_size=512:  98%|█████████▊| 984/1000 [02:54<00:02,  5.64it/s]Measuring inference for batch_size=512:  98%|█████████▊| 985/1000 [02:54<00:02,  5.64it/s]Measuring inference for batch_size=512:  99%|█████████▊| 986/1000 [02:55<00:02,  5.64it/s]Measuring inference for batch_size=512:  99%|█████████▊| 987/1000 [02:55<00:02,  5.63it/s]Measuring inference for batch_size=512:  99%|█████████▉| 988/1000 [02:55<00:02,  5.63it/s]Measuring inference for batch_size=512:  99%|█████████▉| 989/1000 [02:55<00:01,  5.63it/s]Measuring inference for batch_size=512:  99%|█████████▉| 990/1000 [02:55<00:01,  5.63it/s]Measuring inference for batch_size=512:  99%|█████████▉| 991/1000 [02:55<00:01,  5.63it/s]Measuring inference for batch_size=512:  99%|█████████▉| 992/1000 [02:56<00:01,  5.63it/s]Measuring inference for batch_size=512:  99%|█████████▉| 993/1000 [02:56<00:01,  5.63it/s]Measuring inference for batch_size=512:  99%|█████████▉| 994/1000 [02:56<00:01,  5.63it/s]Measuring inference for batch_size=512: 100%|█████████▉| 995/1000 [02:56<00:00,  5.63it/s]Measuring inference for batch_size=512: 100%|█████████▉| 996/1000 [02:56<00:00,  5.63it/s]Measuring inference for batch_size=512: 100%|█████████▉| 997/1000 [02:57<00:00,  5.63it/s]Measuring inference for batch_size=512: 100%|█████████▉| 998/1000 [02:57<00:00,  5.63it/s]Measuring inference for batch_size=512: 100%|█████████▉| 999/1000 [02:57<00:00,  5.63it/s]Measuring inference for batch_size=512: 100%|██████████| 1000/1000 [02:57<00:00,  5.63it/s]Measuring inference for batch_size=512: 100%|██████████| 1000/1000 [02:57<00:00,  5.63it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cpu
flops: 12310546408
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.22 GB
    total: 31.28 GB
    used: 3.60 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
params: 118515272
timing:
  batch_size_1:
    on_device_inference:
      human_readable:
        batch_latency: 136.783 ms +/- 184.615 us [136.274 ms, 137.518 ms]
        batches_per_second: 7.31 +/- 0.01 [7.27, 7.34]
      metrics:
        batches_per_second_max: 7.338138760199868
        batches_per_second_mean: 7.310889228416088
        batches_per_second_min: 7.271779081540659
        batches_per_second_std: 0.009864536405712561
        seconds_per_batch_max: 0.13751792907714844
        seconds_per_batch_mean: 0.13678251576423645
        seconds_per_batch_min: 0.1362743377685547
        seconds_per_batch_std: 0.00018461488942003666
  batch_size_512:
    on_device_inference:
      human_readable:
        batch_latency: 177.442 ms +/- 412.421 us [176.609 ms, 183.472 ms]
        batches_per_second: 5.64 +/- 0.01 [5.45, 5.66]
      metrics:
        batches_per_second_max: 5.662216690313775
        batches_per_second_mean: 5.635670978980586
        batches_per_second_min: 5.45041830292981
        batches_per_second_std: 0.012944247069725093
        seconds_per_batch_max: 0.1834721565246582
        seconds_per_batch_mean: 0.1774421079158783
        seconds_per_batch_min: 0.17660927772521973
        seconds_per_batch_std: 0.0004124209667049759


