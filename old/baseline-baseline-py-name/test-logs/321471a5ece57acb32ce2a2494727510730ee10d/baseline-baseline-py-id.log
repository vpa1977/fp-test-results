#####
baseline-baseline-py-id - Run 1
2024-02-23 09:54:45
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 274.25it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:00<00:00, 275.37it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:00<00:00, 276.61it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 276.38it/s]
STAGE:2024-02-23 09:54:32 176748:176748 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:54:32 176748:176748 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:54:32 176748:176748 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:00<00:04, 207.52it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:00<00:04, 207.87it/s]Measuring inference for batch_size=1:   6%|▋         | 63/1000 [00:00<00:04, 208.02it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:00<00:04, 208.05it/s]Measuring inference for batch_size=1:  10%|█         | 105/1000 [00:00<00:04, 208.11it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:00<00:04, 208.21it/s]Measuring inference for batch_size=1:  15%|█▍        | 147/1000 [00:00<00:04, 208.28it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:00<00:03, 208.39it/s]Measuring inference for batch_size=1:  19%|█▉        | 189/1000 [00:00<00:03, 208.54it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:01<00:03, 208.46it/s]Measuring inference for batch_size=1:  23%|██▎       | 231/1000 [00:01<00:03, 208.38it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:01<00:03, 208.42it/s]Measuring inference for batch_size=1:  27%|██▋       | 273/1000 [00:01<00:03, 208.41it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:01<00:03, 208.42it/s]Measuring inference for batch_size=1:  32%|███▏      | 315/1000 [00:01<00:03, 208.36it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:01<00:03, 208.40it/s]Measuring inference for batch_size=1:  36%|███▌      | 357/1000 [00:01<00:03, 208.36it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:01<00:02, 208.29it/s]Measuring inference for batch_size=1:  40%|███▉      | 399/1000 [00:01<00:02, 208.31it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:02<00:02, 208.30it/s]Measuring inference for batch_size=1:  44%|████▍     | 441/1000 [00:02<00:02, 208.25it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:02<00:02, 208.17it/s]Measuring inference for batch_size=1:  48%|████▊     | 483/1000 [00:02<00:02, 208.16it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:02<00:02, 208.22it/s]Measuring inference for batch_size=1:  52%|█████▎    | 525/1000 [00:02<00:02, 208.32it/s]Measuring inference for batch_size=1:  55%|█████▍    | 546/1000 [00:02<00:02, 208.35it/s]Measuring inference for batch_size=1:  57%|█████▋    | 567/1000 [00:02<00:02, 208.37it/s]Measuring inference for batch_size=1:  59%|█████▉    | 588/1000 [00:02<00:01, 208.31it/s]Measuring inference for batch_size=1:  61%|██████    | 609/1000 [00:02<00:01, 208.36it/s]Measuring inference for batch_size=1:  63%|██████▎   | 630/1000 [00:03<00:01, 208.30it/s]Measuring inference for batch_size=1:  65%|██████▌   | 651/1000 [00:03<00:01, 208.33it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:03<00:01, 208.31it/s]Measuring inference for batch_size=1:  69%|██████▉   | 693/1000 [00:03<00:01, 208.22it/s]Measuring inference for batch_size=1:  71%|███████▏  | 714/1000 [00:03<00:01, 208.23it/s]Measuring inference for batch_size=1:  74%|███████▎  | 735/1000 [00:03<00:01, 208.25it/s]Measuring inference for batch_size=1:  76%|███████▌  | 756/1000 [00:03<00:01, 208.28it/s]Measuring inference for batch_size=1:  78%|███████▊  | 777/1000 [00:03<00:01, 208.31it/s]Measuring inference for batch_size=1:  80%|███████▉  | 798/1000 [00:03<00:00, 208.33it/s]Measuring inference for batch_size=1:  82%|████████▏ | 819/1000 [00:03<00:00, 208.30it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:04<00:00, 208.17it/s]Measuring inference for batch_size=1:  86%|████████▌ | 861/1000 [00:04<00:00, 208.11it/s]Measuring inference for batch_size=1:  88%|████████▊ | 882/1000 [00:04<00:00, 208.09it/s]Measuring inference for batch_size=1:  90%|█████████ | 903/1000 [00:04<00:00, 208.08it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [00:04<00:00, 208.06it/s]Measuring inference for batch_size=1:  94%|█████████▍| 945/1000 [00:04<00:00, 207.95it/s]Measuring inference for batch_size=1:  97%|█████████▋| 966/1000 [00:04<00:00, 208.01it/s]Measuring inference for batch_size=1:  99%|█████████▊| 987/1000 [00:04<00:00, 208.09it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 208.24it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=16:  21%|██        | 21/100 [00:00<00:00, 202.85it/s]Warming up with batch_size=16:  42%|████▏     | 42/100 [00:00<00:00, 203.25it/s]Warming up with batch_size=16:  63%|██████▎   | 63/100 [00:00<00:00, 203.30it/s]Warming up with batch_size=16:  84%|████████▍ | 84/100 [00:00<00:00, 203.34it/s]Warming up with batch_size=16: 100%|██████████| 100/100 [00:00<00:00, 203.27it/s]
STAGE:2024-02-23 09:54:37 176748:176748 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:54:37 176748:176748 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:54:37 176748:176748 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=16:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=16:   2%|▏         | 20/1000 [00:00<00:04, 199.33it/s]Measuring inference for batch_size=16:   4%|▍         | 41/1000 [00:00<00:04, 199.98it/s]Measuring inference for batch_size=16:   6%|▌         | 62/1000 [00:00<00:04, 200.25it/s]Measuring inference for batch_size=16:   8%|▊         | 83/1000 [00:00<00:04, 200.24it/s]Measuring inference for batch_size=16:  10%|█         | 104/1000 [00:00<00:04, 200.25it/s]Measuring inference for batch_size=16:  12%|█▎        | 125/1000 [00:00<00:04, 200.20it/s]Measuring inference for batch_size=16:  15%|█▍        | 146/1000 [00:00<00:04, 200.19it/s]Measuring inference for batch_size=16:  17%|█▋        | 167/1000 [00:00<00:04, 200.07it/s]Measuring inference for batch_size=16:  19%|█▉        | 188/1000 [00:00<00:04, 200.06it/s]Measuring inference for batch_size=16:  21%|██        | 209/1000 [00:01<00:03, 199.99it/s]Measuring inference for batch_size=16:  23%|██▎       | 229/1000 [00:01<00:03, 199.94it/s]Measuring inference for batch_size=16:  25%|██▌       | 250/1000 [00:01<00:03, 200.00it/s]Measuring inference for batch_size=16:  27%|██▋       | 271/1000 [00:01<00:03, 200.06it/s]Measuring inference for batch_size=16:  29%|██▉       | 292/1000 [00:01<00:03, 200.14it/s]Measuring inference for batch_size=16:  31%|███▏      | 313/1000 [00:01<00:03, 200.20it/s]Measuring inference for batch_size=16:  33%|███▎      | 334/1000 [00:01<00:03, 200.17it/s]Measuring inference for batch_size=16:  36%|███▌      | 355/1000 [00:01<00:03, 200.13it/s]Measuring inference for batch_size=16:  38%|███▊      | 376/1000 [00:01<00:03, 200.09it/s]Measuring inference for batch_size=16:  40%|███▉      | 397/1000 [00:01<00:03, 200.10it/s]Measuring inference for batch_size=16:  42%|████▏     | 418/1000 [00:02<00:02, 200.05it/s]Measuring inference for batch_size=16:  44%|████▍     | 439/1000 [00:02<00:02, 200.00it/s]Measuring inference for batch_size=16:  46%|████▌     | 460/1000 [00:02<00:02, 200.01it/s]Measuring inference for batch_size=16:  48%|████▊     | 481/1000 [00:02<00:02, 200.03it/s]Measuring inference for batch_size=16:  50%|█████     | 502/1000 [00:02<00:02, 199.98it/s]Measuring inference for batch_size=16:  52%|█████▏    | 522/1000 [00:02<00:02, 199.92it/s]Measuring inference for batch_size=16:  54%|█████▍    | 542/1000 [00:02<00:02, 199.91it/s]Measuring inference for batch_size=16:  56%|█████▌    | 562/1000 [00:02<00:02, 199.91it/s]Measuring inference for batch_size=16:  58%|█████▊    | 583/1000 [00:02<00:02, 199.96it/s]Measuring inference for batch_size=16:  60%|██████    | 603/1000 [00:03<00:01, 199.96it/s]Measuring inference for batch_size=16:  62%|██████▏   | 624/1000 [00:03<00:01, 200.00it/s]Measuring inference for batch_size=16:  64%|██████▍   | 645/1000 [00:03<00:01, 200.03it/s]Measuring inference for batch_size=16:  67%|██████▋   | 666/1000 [00:03<00:01, 200.09it/s]Measuring inference for batch_size=16:  69%|██████▊   | 687/1000 [00:03<00:01, 200.13it/s]Measuring inference for batch_size=16:  71%|███████   | 708/1000 [00:03<00:01, 200.11it/s]Measuring inference for batch_size=16:  73%|███████▎  | 729/1000 [00:03<00:01, 200.13it/s]Measuring inference for batch_size=16:  75%|███████▌  | 750/1000 [00:03<00:01, 200.18it/s]Measuring inference for batch_size=16:  77%|███████▋  | 771/1000 [00:03<00:01, 200.17it/s]Measuring inference for batch_size=16:  79%|███████▉  | 792/1000 [00:03<00:01, 200.11it/s]Measuring inference for batch_size=16:  81%|████████▏ | 813/1000 [00:04<00:00, 200.20it/s]Measuring inference for batch_size=16:  83%|████████▎ | 834/1000 [00:04<00:00, 200.20it/s]Measuring inference for batch_size=16:  86%|████████▌ | 855/1000 [00:04<00:00, 200.32it/s]Measuring inference for batch_size=16:  88%|████████▊ | 876/1000 [00:04<00:00, 200.39it/s]Measuring inference for batch_size=16:  90%|████████▉ | 897/1000 [00:04<00:00, 200.43it/s]Measuring inference for batch_size=16:  92%|█████████▏| 918/1000 [00:04<00:00, 200.40it/s]Measuring inference for batch_size=16:  94%|█████████▍| 939/1000 [00:04<00:00, 200.38it/s]Measuring inference for batch_size=16:  96%|█████████▌| 960/1000 [00:04<00:00, 200.39it/s]Measuring inference for batch_size=16:  98%|█████████▊| 981/1000 [00:04<00:00, 200.38it/s]Measuring inference for batch_size=16: 100%|██████████| 1000/1000 [00:04<00:00, 200.14it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.53 GB
    total: 31.28 GB
    used: 3.28 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_16:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 94.197 us +/- 3.370 us [92.268 us, 144.482 us]
        batches_per_second: 10.63 K +/- 324.37 [6.92 K, 10.84 K]
      metrics:
        batches_per_second_max: 10837.994832041344
        batches_per_second_mean: 10627.554012394148
        batches_per_second_min: 6921.293729372937
        batches_per_second_std: 324.36736062661976
        seconds_per_batch_max: 0.00014448165893554688
        seconds_per_batch_mean: 9.419703483581543e-05
        seconds_per_batch_min: 9.226799011230469e-05
        seconds_per_batch_std: 3.3699327091361674e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.095 us +/- 0.659 us [22.411 us, 32.902 us]
        batches_per_second: 43.33 K +/- 1.06 K [30.39 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 43329.1806016137
        batches_per_second_min: 30393.507246376812
        batches_per_second_std: 1058.3452737851144
        seconds_per_batch_max: 3.2901763916015625e-05
        seconds_per_batch_mean: 2.3095130920410155e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 6.593718292865745e-07
    on_device_inference:
      human_readable:
        batch_latency: -4674660.930 us +/- 12.626 ms [-4904704.094 us, -4637599.945
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.22, -0.20]
      metrics:
        batches_per_second_max: -0.20388589828221323
        batches_per_second_mean: -0.21392079466324274
        batches_per_second_min: -0.21562877605762515
        batches_per_second_std: 0.0005688825939407174
        seconds_per_batch_max: -4.637599945068359
        seconds_per_batch_mean: -4.67466092967987
        seconds_per_batch_min: -4.9047040939331055
        seconds_per_batch_std: 0.012625937802821365
    total:
      human_readable:
        batch_latency: 4.799 ms +/- 14.434 us [4.759 ms, 5.090 ms]
        batches_per_second: 208.40 +/- 0.61 [196.45, 210.11]
      metrics:
        batches_per_second_max: 210.11441739304678
        batches_per_second_mean: 208.39912602928678
        batches_per_second_min: 196.44531872043464
        batches_per_second_std: 0.6122455548276537
        seconds_per_batch_max: 0.005090475082397461
        seconds_per_batch_mean: 0.004798527002334595
        seconds_per_batch_min: 0.004759311676025391
        seconds_per_batch_std: 1.4434010506396195e-05
  batch_size_16:
    cpu_to_gpu:
      human_readable:
        batch_latency: 146.628 us +/- 4.701 us [144.243 us, 247.478 us]
        batches_per_second: 6.83 K +/- 173.71 [4.04 K, 6.93 K]
      metrics:
        batches_per_second_max: 6932.7338842975205
        batches_per_second_mean: 6825.413352709498
        batches_per_second_min: 4040.7552986512524
        batches_per_second_std: 173.7085444979065
        seconds_per_batch_max: 0.0002474784851074219
        seconds_per_batch_mean: 0.00014662814140319825
        seconds_per_batch_min: 0.0001442432403564453
        seconds_per_batch_std: 4.7014728681051355e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.235 us +/- 0.732 us [22.411 us, 30.279 us]
        batches_per_second: 43.08 K +/- 1.21 K [33.03 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 43076.33801539552
        batches_per_second_min: 33026.015748031496
        batches_per_second_std: 1209.8947717044468
        seconds_per_batch_max: 3.0279159545898438e-05
        seconds_per_batch_mean: 2.3235082626342773e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 7.316760879963335e-07
    on_device_inference:
      human_readable:
        batch_latency: -4817658.142 us +/- 14.888 ms [-5148352.146 us, -4785727.978
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.19]
      metrics:
        batches_per_second_max: -0.19423690757984924
        batches_per_second_mean: -0.20757164810179846
        batches_per_second_min: -0.20895462605661652
        batches_per_second_std: 0.0006210156981404935
        seconds_per_batch_max: -4.7857279777526855
        seconds_per_batch_mean: -4.817658141613006
        seconds_per_batch_min: -5.148352146148682
        seconds_per_batch_std: 0.014887516767772184
    total:
      human_readable:
        batch_latency: 4.993 ms +/- 18.006 us [4.959 ms, 5.430 ms]
        batches_per_second: 200.29 +/- 0.69 [184.16, 201.64]
      metrics:
        batches_per_second_max: 201.63953656074227
        batches_per_second_mean: 200.28653349911994
        batches_per_second_min: 184.162634467618
        batches_per_second_std: 0.688072001817357
        seconds_per_batch_max: 0.005429983139038086
        seconds_per_batch_mean: 0.004992908716201782
        seconds_per_batch_min: 0.0049593448638916016
        seconds_per_batch_std: 1.8005574517128856e-05


#####
baseline-baseline-py-id - Run 2
2024-02-23 09:55:03
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 274.61it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:00<00:00, 276.46it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:00<00:00, 277.39it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 277.18it/s]
STAGE:2024-02-23 09:54:50 176794:176794 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:54:50 176794:176794 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:54:50 176794:176794 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:00<00:04, 207.88it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:00<00:04, 208.22it/s]Measuring inference for batch_size=1:   6%|▋         | 63/1000 [00:00<00:04, 208.52it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:00<00:04, 208.52it/s]Measuring inference for batch_size=1:  10%|█         | 105/1000 [00:00<00:04, 208.55it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:00<00:04, 208.62it/s]Measuring inference for batch_size=1:  15%|█▍        | 147/1000 [00:00<00:04, 208.70it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:00<00:03, 208.70it/s]Measuring inference for batch_size=1:  19%|█▉        | 189/1000 [00:00<00:03, 208.63it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:01<00:03, 208.56it/s]Measuring inference for batch_size=1:  23%|██▎       | 231/1000 [00:01<00:03, 208.55it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:01<00:03, 208.54it/s]Measuring inference for batch_size=1:  27%|██▋       | 273/1000 [00:01<00:03, 208.54it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:01<00:03, 208.54it/s]Measuring inference for batch_size=1:  32%|███▏      | 315/1000 [00:01<00:03, 208.73it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:01<00:03, 208.89it/s]Measuring inference for batch_size=1:  36%|███▌      | 357/1000 [00:01<00:03, 208.90it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:01<00:02, 208.77it/s]Measuring inference for batch_size=1:  40%|███▉      | 399/1000 [00:01<00:02, 208.66it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:02<00:02, 208.54it/s]Measuring inference for batch_size=1:  44%|████▍     | 441/1000 [00:02<00:02, 208.49it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:02<00:02, 208.42it/s]Measuring inference for batch_size=1:  48%|████▊     | 483/1000 [00:02<00:02, 208.40it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:02<00:02, 208.46it/s]Measuring inference for batch_size=1:  52%|█████▎    | 525/1000 [00:02<00:02, 208.47it/s]Measuring inference for batch_size=1:  55%|█████▍    | 546/1000 [00:02<00:02, 208.48it/s]Measuring inference for batch_size=1:  57%|█████▋    | 567/1000 [00:02<00:02, 208.49it/s]Measuring inference for batch_size=1:  59%|█████▉    | 588/1000 [00:02<00:01, 208.50it/s]Measuring inference for batch_size=1:  61%|██████    | 609/1000 [00:02<00:01, 208.40it/s]Measuring inference for batch_size=1:  63%|██████▎   | 630/1000 [00:03<00:01, 208.33it/s]Measuring inference for batch_size=1:  65%|██████▌   | 651/1000 [00:03<00:01, 208.30it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:03<00:01, 208.33it/s]Measuring inference for batch_size=1:  69%|██████▉   | 693/1000 [00:03<00:01, 208.29it/s]Measuring inference for batch_size=1:  71%|███████▏  | 714/1000 [00:03<00:01, 208.45it/s]Measuring inference for batch_size=1:  74%|███████▎  | 735/1000 [00:03<00:01, 208.60it/s]Measuring inference for batch_size=1:  76%|███████▌  | 756/1000 [00:03<00:01, 208.90it/s]Measuring inference for batch_size=1:  78%|███████▊  | 777/1000 [00:03<00:01, 208.99it/s]Measuring inference for batch_size=1:  80%|███████▉  | 798/1000 [00:03<00:00, 208.91it/s]Measuring inference for batch_size=1:  82%|████████▏ | 819/1000 [00:03<00:00, 208.80it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:04<00:00, 208.80it/s]Measuring inference for batch_size=1:  86%|████████▌ | 861/1000 [00:04<00:00, 208.90it/s]Measuring inference for batch_size=1:  88%|████████▊ | 882/1000 [00:04<00:00, 208.98it/s]Measuring inference for batch_size=1:  90%|█████████ | 903/1000 [00:04<00:00, 208.97it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [00:04<00:00, 208.93it/s]Measuring inference for batch_size=1:  94%|█████████▍| 945/1000 [00:04<00:00, 208.98it/s]Measuring inference for batch_size=1:  97%|█████████▋| 966/1000 [00:04<00:00, 209.11it/s]Measuring inference for batch_size=1:  99%|█████████▊| 987/1000 [00:04<00:00, 209.08it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 208.66it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=16:  21%|██        | 21/100 [00:00<00:00, 203.40it/s]Warming up with batch_size=16:  42%|████▏     | 42/100 [00:00<00:00, 203.77it/s]Warming up with batch_size=16:  63%|██████▎   | 63/100 [00:00<00:00, 203.94it/s]Warming up with batch_size=16:  84%|████████▍ | 84/100 [00:00<00:00, 203.99it/s]Warming up with batch_size=16: 100%|██████████| 100/100 [00:00<00:00, 203.93it/s]
STAGE:2024-02-23 09:54:55 176794:176794 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:54:55 176794:176794 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:54:55 176794:176794 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=16:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=16:   2%|▏         | 20/1000 [00:00<00:04, 199.78it/s]Measuring inference for batch_size=16:   4%|▍         | 41/1000 [00:00<00:04, 200.73it/s]Measuring inference for batch_size=16:   6%|▌         | 62/1000 [00:00<00:04, 200.93it/s]Measuring inference for batch_size=16:   8%|▊         | 83/1000 [00:00<00:04, 200.78it/s]Measuring inference for batch_size=16:  10%|█         | 104/1000 [00:00<00:04, 200.78it/s]Measuring inference for batch_size=16:  12%|█▎        | 125/1000 [00:00<00:04, 200.74it/s]Measuring inference for batch_size=16:  15%|█▍        | 146/1000 [00:00<00:04, 200.73it/s]Measuring inference for batch_size=16:  17%|█▋        | 167/1000 [00:00<00:04, 200.67it/s]Measuring inference for batch_size=16:  19%|█▉        | 188/1000 [00:00<00:04, 200.73it/s]Measuring inference for batch_size=16:  21%|██        | 209/1000 [00:01<00:03, 200.76it/s]Measuring inference for batch_size=16:  23%|██▎       | 230/1000 [00:01<00:03, 200.83it/s]Measuring inference for batch_size=16:  25%|██▌       | 251/1000 [00:01<00:03, 200.78it/s]Measuring inference for batch_size=16:  27%|██▋       | 272/1000 [00:01<00:03, 200.78it/s]Measuring inference for batch_size=16:  29%|██▉       | 293/1000 [00:01<00:03, 200.68it/s]Measuring inference for batch_size=16:  31%|███▏      | 314/1000 [00:01<00:03, 200.62it/s]Measuring inference for batch_size=16:  34%|███▎      | 335/1000 [00:01<00:03, 200.57it/s]Measuring inference for batch_size=16:  36%|███▌      | 356/1000 [00:01<00:03, 200.63it/s]Measuring inference for batch_size=16:  38%|███▊      | 377/1000 [00:01<00:03, 200.65it/s]Measuring inference for batch_size=16:  40%|███▉      | 398/1000 [00:01<00:02, 200.70it/s]Measuring inference for batch_size=16:  42%|████▏     | 419/1000 [00:02<00:02, 200.66it/s]Measuring inference for batch_size=16:  44%|████▍     | 440/1000 [00:02<00:02, 200.73it/s]Measuring inference for batch_size=16:  46%|████▌     | 461/1000 [00:02<00:02, 200.68it/s]Measuring inference for batch_size=16:  48%|████▊     | 482/1000 [00:02<00:02, 200.64it/s]Measuring inference for batch_size=16:  50%|█████     | 503/1000 [00:02<00:02, 200.56it/s]Measuring inference for batch_size=16:  52%|█████▏    | 524/1000 [00:02<00:02, 200.52it/s]Measuring inference for batch_size=16:  55%|█████▍    | 545/1000 [00:02<00:02, 200.54it/s]Measuring inference for batch_size=16:  57%|█████▋    | 566/1000 [00:02<00:02, 200.57it/s]Measuring inference for batch_size=16:  59%|█████▊    | 587/1000 [00:02<00:02, 200.55it/s]Measuring inference for batch_size=16:  61%|██████    | 608/1000 [00:03<00:01, 200.48it/s]Measuring inference for batch_size=16:  63%|██████▎   | 629/1000 [00:03<00:01, 200.55it/s]Measuring inference for batch_size=16:  65%|██████▌   | 650/1000 [00:03<00:01, 200.68it/s]Measuring inference for batch_size=16:  67%|██████▋   | 671/1000 [00:03<00:01, 200.65it/s]Measuring inference for batch_size=16:  69%|██████▉   | 692/1000 [00:03<00:01, 200.68it/s]Measuring inference for batch_size=16:  71%|███████▏  | 713/1000 [00:03<00:01, 200.74it/s]Measuring inference for batch_size=16:  73%|███████▎  | 734/1000 [00:03<00:01, 200.71it/s]Measuring inference for batch_size=16:  76%|███████▌  | 755/1000 [00:03<00:01, 200.65it/s]Measuring inference for batch_size=16:  78%|███████▊  | 776/1000 [00:03<00:01, 200.65it/s]Measuring inference for batch_size=16:  80%|███████▉  | 797/1000 [00:03<00:01, 200.54it/s]Measuring inference for batch_size=16:  82%|████████▏ | 818/1000 [00:04<00:00, 200.54it/s]Measuring inference for batch_size=16:  84%|████████▍ | 839/1000 [00:04<00:00, 200.55it/s]Measuring inference for batch_size=16:  86%|████████▌ | 860/1000 [00:04<00:00, 200.52it/s]Measuring inference for batch_size=16:  88%|████████▊ | 881/1000 [00:04<00:00, 200.47it/s]Measuring inference for batch_size=16:  90%|█████████ | 902/1000 [00:04<00:00, 200.45it/s]Measuring inference for batch_size=16:  92%|█████████▏| 923/1000 [00:04<00:00, 200.43it/s]Measuring inference for batch_size=16:  94%|█████████▍| 944/1000 [00:04<00:00, 200.42it/s]Measuring inference for batch_size=16:  96%|█████████▋| 965/1000 [00:04<00:00, 200.39it/s]Measuring inference for batch_size=16:  99%|█████████▊| 986/1000 [00:04<00:00, 200.25it/s]Measuring inference for batch_size=16: 100%|██████████| 1000/1000 [00:04<00:00, 200.59it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.53 GB
    total: 31.28 GB
    used: 3.28 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_16:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 92.315 us +/- 2.988 us [90.599 us, 143.528 us]
        batches_per_second: 10.84 K +/- 301.34 [6.97 K, 11.04 K]
      metrics:
        batches_per_second_max: 11037.642105263158
        batches_per_second_mean: 10842.058577925236
        batches_per_second_min: 6967.282392026578
        batches_per_second_std: 301.3373023651029
        seconds_per_batch_max: 0.00014352798461914062
        seconds_per_batch_mean: 9.23154354095459e-05
        seconds_per_batch_min: 9.059906005859375e-05
        seconds_per_batch_std: 2.9875607887755756e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 22.985 us +/- 0.632 us [22.173 us, 29.325 us]
        batches_per_second: 43.54 K +/- 1.06 K [34.10 K, 45.10 K]
      metrics:
        batches_per_second_max: 45100.04301075269
        batches_per_second_mean: 43536.70192351836
        batches_per_second_min: 34100.03252032521
        batches_per_second_std: 1063.3879618184528
        seconds_per_batch_max: 2.9325485229492188e-05
        seconds_per_batch_mean: 2.298450469970703e-05
        seconds_per_batch_min: 2.2172927856445312e-05
        seconds_per_batch_std: 6.31732036264791e-07
    on_device_inference:
      human_readable:
        batch_latency: -4666675.815 us +/- 15.892 ms [-4885407.925 us, -4619552.135
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.22, -0.20]
      metrics:
        batches_per_second_max: -0.20469119783302683
        batches_per_second_mean: -0.2142877592932445
        batches_per_second_min: -0.21647120124964092
        batches_per_second_std: 0.0007239787904562953
        seconds_per_batch_max: -4.619552135467529
        seconds_per_batch_mean: -4.666675815105438
        seconds_per_batch_min: -4.8854079246521
        seconds_per_batch_std: 0.01589234576772251
    total:
      human_readable:
        batch_latency: 4.789 ms +/- 17.177 us [4.740 ms, 5.069 ms]
        batches_per_second: 208.82 +/- 0.74 [197.27, 210.99]
      metrics:
        batches_per_second_max: 210.99169978369133
        batches_per_second_mean: 208.8212332290424
        batches_per_second_min: 197.26761358291787
        batches_per_second_std: 0.7385041914399053
        seconds_per_batch_max: 0.005069255828857422
        seconds_per_batch_mean: 0.004788845777511597
        seconds_per_batch_min: 0.004739522933959961
        seconds_per_batch_std: 1.7176729773970396e-05
  batch_size_16:
    cpu_to_gpu:
      human_readable:
        batch_latency: 143.201 us +/- 4.145 us [141.382 us, 244.617 us]
        batches_per_second: 6.99 K +/- 152.04 [4.09 K, 7.07 K]
      metrics:
        batches_per_second_max: 7073.025295109612
        batches_per_second_mean: 6987.462822194308
        batches_per_second_min: 4088.01559454191
        batches_per_second_std: 152.0379289214605
        seconds_per_batch_max: 0.0002446174621582031
        seconds_per_batch_mean: 0.0001432011127471924
        seconds_per_batch_min: 0.00014138221740722656
        seconds_per_batch_std: 4.145298748138104e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.064 us +/- 0.586 us [22.173 us, 28.849 us]
        batches_per_second: 43.38 K +/- 995.03 [34.66 K, 45.10 K]
      metrics:
        batches_per_second_max: 45100.04301075269
        batches_per_second_mean: 43382.13223045738
        batches_per_second_min: 34663.669421487604
        batches_per_second_std: 995.0297326821405
        seconds_per_batch_max: 2.8848648071289062e-05
        seconds_per_batch_mean: 2.3064374923706056e-05
        seconds_per_batch_min: 2.2172927856445312e-05
        seconds_per_batch_std: 5.864747645662746e-07
    on_device_inference:
      human_readable:
        batch_latency: -4810015.200 us +/- 15.349 ms [-5130623.817 us, -4771200.180
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.19]
      metrics:
        batches_per_second_max: -0.1949080726986947
        batches_per_second_mean: -0.20790160961057438
        batches_per_second_min: -0.20959087069550342
        batches_per_second_std: 0.0006454787951041772
        seconds_per_batch_max: -4.771200180053711
        seconds_per_batch_mean: -4.810015200138092
        seconds_per_batch_min: -5.130623817443848
        seconds_per_batch_std: 0.015348852418385548
    total:
      human_readable:
        batch_latency: 4.982 ms +/- 18.148 us [4.942 ms, 5.409 ms]
        batches_per_second: 200.74 +/- 0.70 [184.88, 202.36]
      metrics:
        batches_per_second_max: 202.35943455396344
        batches_per_second_mean: 200.74011036127195
        batches_per_second_min: 184.8769780050249
        batches_per_second_std: 0.699618227852132
        seconds_per_batch_max: 0.0054090023040771484
        seconds_per_batch_mean: 0.004981628656387329
        seconds_per_batch_min: 0.004941701889038086
        seconds_per_batch_std: 1.8148184784211756e-05


#####
baseline-baseline-py-id - Run 3
2024-02-23 09:55:19
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.91it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.91it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 272.62it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:00<00:00, 274.00it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:00<00:00, 274.76it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 274.59it/s]
STAGE:2024-02-23 09:55:08 176840:176840 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:55:08 176840:176840 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:55:08 176840:176840 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:00<00:04, 206.12it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:00<00:04, 207.00it/s]Measuring inference for batch_size=1:   6%|▋         | 63/1000 [00:00<00:04, 207.10it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:00<00:04, 207.15it/s]Measuring inference for batch_size=1:  10%|█         | 105/1000 [00:00<00:04, 207.21it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:00<00:04, 207.25it/s]Measuring inference for batch_size=1:  15%|█▍        | 147/1000 [00:00<00:04, 207.32it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:00<00:04, 207.30it/s]Measuring inference for batch_size=1:  19%|█▉        | 189/1000 [00:00<00:03, 207.24it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:01<00:03, 207.27it/s]Measuring inference for batch_size=1:  23%|██▎       | 231/1000 [00:01<00:03, 207.25it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:01<00:03, 207.26it/s]Measuring inference for batch_size=1:  27%|██▋       | 273/1000 [00:01<00:03, 207.25it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:01<00:03, 207.19it/s]Measuring inference for batch_size=1:  32%|███▏      | 315/1000 [00:01<00:03, 207.22it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:01<00:03, 207.25it/s]Measuring inference for batch_size=1:  36%|███▌      | 357/1000 [00:01<00:03, 207.20it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:01<00:03, 207.19it/s]Measuring inference for batch_size=1:  40%|███▉      | 399/1000 [00:01<00:02, 207.23it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:02<00:02, 207.28it/s]Measuring inference for batch_size=1:  44%|████▍     | 441/1000 [00:02<00:02, 207.34it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:02<00:02, 207.40it/s]Measuring inference for batch_size=1:  48%|████▊     | 483/1000 [00:02<00:02, 207.38it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:02<00:02, 207.38it/s]Measuring inference for batch_size=1:  52%|█████▎    | 525/1000 [00:02<00:02, 207.31it/s]Measuring inference for batch_size=1:  55%|█████▍    | 546/1000 [00:02<00:02, 207.36it/s]Measuring inference for batch_size=1:  57%|█████▋    | 567/1000 [00:02<00:02, 207.44it/s]Measuring inference for batch_size=1:  59%|█████▉    | 588/1000 [00:02<00:01, 207.40it/s]Measuring inference for batch_size=1:  61%|██████    | 609/1000 [00:02<00:01, 207.38it/s]Measuring inference for batch_size=1:  63%|██████▎   | 630/1000 [00:03<00:01, 207.41it/s]Measuring inference for batch_size=1:  65%|██████▌   | 651/1000 [00:03<00:01, 207.38it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:03<00:01, 207.40it/s]Measuring inference for batch_size=1:  69%|██████▉   | 693/1000 [00:03<00:01, 207.41it/s]Measuring inference for batch_size=1:  71%|███████▏  | 714/1000 [00:03<00:01, 207.35it/s]Measuring inference for batch_size=1:  74%|███████▎  | 735/1000 [00:03<00:01, 207.36it/s]Measuring inference for batch_size=1:  76%|███████▌  | 756/1000 [00:03<00:01, 207.38it/s]Measuring inference for batch_size=1:  78%|███████▊  | 777/1000 [00:03<00:01, 207.31it/s]Measuring inference for batch_size=1:  80%|███████▉  | 798/1000 [00:03<00:00, 207.22it/s]Measuring inference for batch_size=1:  82%|████████▏ | 819/1000 [00:03<00:00, 207.21it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:04<00:00, 207.27it/s]Measuring inference for batch_size=1:  86%|████████▌ | 861/1000 [00:04<00:00, 207.22it/s]Measuring inference for batch_size=1:  88%|████████▊ | 882/1000 [00:04<00:00, 207.26it/s]Measuring inference for batch_size=1:  90%|█████████ | 903/1000 [00:04<00:00, 207.25it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [00:04<00:00, 207.17it/s]Measuring inference for batch_size=1:  94%|█████████▍| 945/1000 [00:04<00:00, 207.21it/s]Measuring inference for batch_size=1:  97%|█████████▋| 966/1000 [00:04<00:00, 207.31it/s]Measuring inference for batch_size=1:  99%|█████████▊| 987/1000 [00:04<00:00, 207.32it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 207.28it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=16:  21%|██        | 21/100 [00:00<00:00, 201.85it/s]Warming up with batch_size=16:  42%|████▏     | 42/100 [00:00<00:00, 202.24it/s]Warming up with batch_size=16:  63%|██████▎   | 63/100 [00:00<00:00, 202.29it/s]Warming up with batch_size=16:  84%|████████▍ | 84/100 [00:00<00:00, 202.42it/s]Warming up with batch_size=16: 100%|██████████| 100/100 [00:00<00:00, 202.33it/s]
STAGE:2024-02-23 09:55:13 176840:176840 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:55:14 176840:176840 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:55:14 176840:176840 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=16:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=16:   2%|▏         | 20/1000 [00:00<00:04, 198.41it/s]Measuring inference for batch_size=16:   4%|▍         | 40/1000 [00:00<00:04, 198.95it/s]Measuring inference for batch_size=16:   6%|▌         | 60/1000 [00:00<00:04, 199.19it/s]Measuring inference for batch_size=16:   8%|▊         | 80/1000 [00:00<00:04, 199.28it/s]Measuring inference for batch_size=16:  10%|█         | 100/1000 [00:00<00:04, 199.48it/s]Measuring inference for batch_size=16:  12%|█▏        | 120/1000 [00:00<00:04, 199.46it/s]Measuring inference for batch_size=16:  14%|█▍        | 140/1000 [00:00<00:04, 199.60it/s]Measuring inference for batch_size=16:  16%|█▌        | 160/1000 [00:00<00:04, 199.58it/s]Measuring inference for batch_size=16:  18%|█▊        | 180/1000 [00:00<00:04, 199.65it/s]Measuring inference for batch_size=16:  20%|██        | 200/1000 [00:01<00:04, 199.62it/s]Measuring inference for batch_size=16:  22%|██▏       | 220/1000 [00:01<00:03, 199.71it/s]Measuring inference for batch_size=16:  24%|██▍       | 240/1000 [00:01<00:03, 199.75it/s]Measuring inference for batch_size=16:  26%|██▌       | 260/1000 [00:01<00:03, 199.69it/s]Measuring inference for batch_size=16:  28%|██▊       | 280/1000 [00:01<00:03, 199.75it/s]Measuring inference for batch_size=16:  30%|███       | 300/1000 [00:01<00:03, 199.64it/s]Measuring inference for batch_size=16:  32%|███▏      | 320/1000 [00:01<00:03, 199.67it/s]Measuring inference for batch_size=16:  34%|███▍      | 340/1000 [00:01<00:03, 199.69it/s]Measuring inference for batch_size=16:  36%|███▌      | 360/1000 [00:01<00:03, 199.72it/s]Measuring inference for batch_size=16:  38%|███▊      | 380/1000 [00:01<00:03, 199.73it/s]Measuring inference for batch_size=16:  40%|████      | 400/1000 [00:02<00:03, 199.78it/s]Measuring inference for batch_size=16:  42%|████▏     | 420/1000 [00:02<00:02, 199.81it/s]Measuring inference for batch_size=16:  44%|████▍     | 440/1000 [00:02<00:02, 199.81it/s]Measuring inference for batch_size=16:  46%|████▌     | 460/1000 [00:02<00:02, 199.75it/s]Measuring inference for batch_size=16:  48%|████▊     | 480/1000 [00:02<00:02, 199.72it/s]Measuring inference for batch_size=16:  50%|█████     | 500/1000 [00:02<00:02, 199.76it/s]Measuring inference for batch_size=16:  52%|█████▏    | 520/1000 [00:02<00:02, 199.68it/s]Measuring inference for batch_size=16:  54%|█████▍    | 540/1000 [00:02<00:02, 199.70it/s]Measuring inference for batch_size=16:  56%|█████▌    | 560/1000 [00:02<00:02, 199.78it/s]Measuring inference for batch_size=16:  58%|█████▊    | 580/1000 [00:02<00:02, 199.83it/s]Measuring inference for batch_size=16:  60%|██████    | 600/1000 [00:03<00:02, 199.81it/s]Measuring inference for batch_size=16:  62%|██████▏   | 621/1000 [00:03<00:01, 199.89it/s]Measuring inference for batch_size=16:  64%|██████▍   | 642/1000 [00:03<00:01, 199.95it/s]Measuring inference for batch_size=16:  66%|██████▋   | 663/1000 [00:03<00:01, 200.00it/s]Measuring inference for batch_size=16:  68%|██████▊   | 683/1000 [00:03<00:01, 199.99it/s]Measuring inference for batch_size=16:  70%|███████   | 703/1000 [00:03<00:01, 199.94it/s]Measuring inference for batch_size=16:  72%|███████▏  | 723/1000 [00:03<00:01, 199.89it/s]Measuring inference for batch_size=16:  74%|███████▍  | 743/1000 [00:03<00:01, 199.78it/s]Measuring inference for batch_size=16:  76%|███████▋  | 763/1000 [00:03<00:01, 199.76it/s]Measuring inference for batch_size=16:  78%|███████▊  | 783/1000 [00:03<00:01, 199.75it/s]Measuring inference for batch_size=16:  80%|████████  | 803/1000 [00:04<00:00, 199.72it/s]Measuring inference for batch_size=16:  82%|████████▏ | 823/1000 [00:04<00:00, 199.76it/s]Measuring inference for batch_size=16:  84%|████████▍ | 843/1000 [00:04<00:00, 199.80it/s]Measuring inference for batch_size=16:  86%|████████▋ | 863/1000 [00:04<00:00, 199.86it/s]Measuring inference for batch_size=16:  88%|████████▊ | 883/1000 [00:04<00:00, 199.89it/s]Measuring inference for batch_size=16:  90%|█████████ | 903/1000 [00:04<00:00, 199.86it/s]Measuring inference for batch_size=16:  92%|█████████▏| 923/1000 [00:04<00:00, 199.89it/s]Measuring inference for batch_size=16:  94%|█████████▍| 944/1000 [00:04<00:00, 199.93it/s]Measuring inference for batch_size=16:  96%|█████████▋| 964/1000 [00:04<00:00, 199.89it/s]Measuring inference for batch_size=16:  98%|█████████▊| 984/1000 [00:04<00:00, 199.77it/s]Measuring inference for batch_size=16: 100%|██████████| 1000/1000 [00:05<00:00, 199.73it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.53 GB
    total: 31.28 GB
    used: 3.28 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_16:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 92.456 us +/- 3.052 us [90.599 us, 144.005 us]
        batches_per_second: 10.83 K +/- 304.81 [6.94 K, 11.04 K]
      metrics:
        batches_per_second_max: 11037.642105263158
        batches_per_second_mean: 10825.91834609058
        batches_per_second_min: 6944.211920529801
        batches_per_second_std: 304.8084475177985
        seconds_per_batch_max: 0.00014400482177734375
        seconds_per_batch_mean: 9.245586395263672e-05
        seconds_per_batch_min: 9.059906005859375e-05
        seconds_per_batch_std: 3.0523263878453026e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.264 us +/- 0.596 us [22.411 us, 31.948 us]
        batches_per_second: 43.01 K +/- 937.54 [31.30 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 43007.98974219664
        batches_per_second_min: 31300.776119402984
        batches_per_second_std: 937.5401925274733
        seconds_per_batch_max: 3.1948089599609375e-05
        seconds_per_batch_mean: 2.3264408111572266e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 5.958516756353233e-07
    on_device_inference:
      human_readable:
        batch_latency: -4698311.028 us +/- 14.301 ms [-4943039.894 us, -4660223.961
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.20]
      metrics:
        batches_per_second_max: -0.2023046589595175
        batches_per_second_mean: -0.2128443865520916
        batches_per_second_min: -0.2145819618102488
        batches_per_second_std: 0.000638464636216494
        seconds_per_batch_max: -4.660223960876465
        seconds_per_batch_mean: -4.698311028003693
        seconds_per_batch_min: -4.943039894104004
        seconds_per_batch_std: 0.01430145040232692
    total:
      human_readable:
        batch_latency: 4.821 ms +/- 16.128 us [4.781 ms, 5.134 ms]
        batches_per_second: 207.43 +/- 0.68 [194.79, 209.16]
      metrics:
        batches_per_second_max: 209.16092355258564
        batches_per_second_mean: 207.43370958877387
        batches_per_second_min: 194.7939810514583
        batches_per_second_std: 0.6780835362943947
        seconds_per_batch_max: 0.005133628845214844
        seconds_per_batch_mean: 0.004820869922637939
        seconds_per_batch_min: 0.004781007766723633
        seconds_per_batch_std: 1.612841749522546e-05
  batch_size_16:
    cpu_to_gpu:
      human_readable:
        batch_latency: 143.010 us +/- 4.320 us [141.144 us, 248.432 us]
        batches_per_second: 7.00 K +/- 157.23 [4.03 K, 7.08 K]
      metrics:
        batches_per_second_max: 7084.972972972973
        batches_per_second_mean: 6997.1211801294685
        batches_per_second_min: 4025.243761996161
        batches_per_second_std: 157.23340413791118
        seconds_per_batch_max: 0.0002484321594238281
        seconds_per_batch_mean: 0.00014301013946533203
        seconds_per_batch_min: 0.000141143798828125
        seconds_per_batch_std: 4.3195131400305e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.373 us +/- 0.578 us [22.411 us, 30.756 us]
        batches_per_second: 42.81 K +/- 920.59 [32.51 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 42807.27507828711
        batches_per_second_min: 32513.98449612403
        batches_per_second_std: 920.5926192020835
        seconds_per_batch_max: 3.075599670410156e-05
        seconds_per_batch_mean: 2.3372888565063477e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 5.783265664939799e-07
    on_device_inference:
      human_readable:
        batch_latency: -4831195.458 us +/- 16.171 ms [-5183648.109 us, -4797984.123
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.19]
      metrics:
        batches_per_second_max: -0.19291432961655974
        batches_per_second_mean: -0.20699034849437856
        batches_per_second_min: -0.2084208647457559
        batches_per_second_std: 0.0006701934650817421
        seconds_per_batch_max: -4.7979841232299805
        seconds_per_batch_mean: -4.831195458412171
        seconds_per_batch_min: -5.183648109436035
        seconds_per_batch_std: 0.016171416500458936
    total:
      human_readable:
        batch_latency: 5.003 ms +/- 19.252 us [4.970 ms, 5.468 ms]
        batches_per_second: 199.88 +/- 0.73 [182.88, 201.21]
      metrics:
        batches_per_second_max: 201.21391220916286
        batches_per_second_mean: 199.88341472283955
        batches_per_second_min: 182.87787224765643
        batches_per_second_std: 0.7309380463735732
        seconds_per_batch_max: 0.005468130111694336
        seconds_per_batch_mean: 0.005002986669540405
        seconds_per_batch_min: 0.00496983528137207
        seconds_per_batch_std: 1.925246308172248e-05


