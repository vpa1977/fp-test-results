#####
baseline-baseline-py-id - Run 1
2024-02-23 09:55:45
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  5.03it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  5.02it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  29%|██▉       | 29/100 [00:00<00:00, 280.20it/s]Warming up with batch_size=1:  58%|█████▊    | 58/100 [00:00<00:00, 280.63it/s]Warming up with batch_size=1:  87%|████████▋ | 87/100 [00:00<00:00, 280.98it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 281.06it/s]
STAGE:2024-02-23 09:55:32 176888:176888 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:55:32 176888:176888 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:55:32 176888:176888 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 22/1000 [00:00<00:04, 213.06it/s]Measuring inference for batch_size=1:   4%|▍         | 44/1000 [00:00<00:04, 213.66it/s]Measuring inference for batch_size=1:   7%|▋         | 66/1000 [00:00<00:04, 213.78it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:00<00:04, 213.99it/s]Measuring inference for batch_size=1:  11%|█         | 110/1000 [00:00<00:04, 214.04it/s]Measuring inference for batch_size=1:  13%|█▎        | 132/1000 [00:00<00:04, 214.11it/s]Measuring inference for batch_size=1:  15%|█▌        | 154/1000 [00:00<00:03, 214.10it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:00<00:03, 213.27it/s]Measuring inference for batch_size=1:  20%|█▉        | 198/1000 [00:00<00:03, 209.89it/s]Measuring inference for batch_size=1:  22%|██▏       | 220/1000 [00:01<00:03, 207.69it/s]Measuring inference for batch_size=1:  24%|██▍       | 241/1000 [00:01<00:03, 206.22it/s]Measuring inference for batch_size=1:  26%|██▌       | 262/1000 [00:01<00:03, 205.26it/s]Measuring inference for batch_size=1:  28%|██▊       | 283/1000 [00:01<00:03, 204.74it/s]Measuring inference for batch_size=1:  30%|███       | 304/1000 [00:01<00:03, 204.34it/s]Measuring inference for batch_size=1:  32%|███▎      | 325/1000 [00:01<00:03, 203.90it/s]Measuring inference for batch_size=1:  35%|███▍      | 346/1000 [00:01<00:03, 203.56it/s]Measuring inference for batch_size=1:  37%|███▋      | 367/1000 [00:01<00:03, 203.38it/s]Measuring inference for batch_size=1:  39%|███▉      | 388/1000 [00:01<00:03, 203.26it/s]Measuring inference for batch_size=1:  41%|████      | 409/1000 [00:01<00:02, 203.19it/s]Measuring inference for batch_size=1:  43%|████▎     | 430/1000 [00:02<00:02, 203.25it/s]Measuring inference for batch_size=1:  45%|████▌     | 451/1000 [00:02<00:02, 203.30it/s]Measuring inference for batch_size=1:  47%|████▋     | 472/1000 [00:02<00:02, 203.34it/s]Measuring inference for batch_size=1:  49%|████▉     | 493/1000 [00:02<00:02, 203.36it/s]Measuring inference for batch_size=1:  51%|█████▏    | 514/1000 [00:02<00:02, 203.35it/s]Measuring inference for batch_size=1:  54%|█████▎    | 535/1000 [00:02<00:02, 203.32it/s]Measuring inference for batch_size=1:  56%|█████▌    | 556/1000 [00:02<00:02, 203.32it/s]Measuring inference for batch_size=1:  58%|█████▊    | 577/1000 [00:02<00:02, 203.27it/s]Measuring inference for batch_size=1:  60%|█████▉    | 598/1000 [00:02<00:01, 203.30it/s]Measuring inference for batch_size=1:  62%|██████▏   | 619/1000 [00:03<00:01, 203.27it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [00:03<00:01, 203.30it/s]Measuring inference for batch_size=1:  66%|██████▌   | 661/1000 [00:03<00:01, 203.06it/s]Measuring inference for batch_size=1:  68%|██████▊   | 682/1000 [00:03<00:01, 203.07it/s]Measuring inference for batch_size=1:  70%|███████   | 703/1000 [00:03<00:01, 203.05it/s]Measuring inference for batch_size=1:  72%|███████▏  | 724/1000 [00:03<00:01, 202.82it/s]Measuring inference for batch_size=1:  74%|███████▍  | 745/1000 [00:03<00:01, 202.75it/s]Measuring inference for batch_size=1:  77%|███████▋  | 766/1000 [00:03<00:01, 202.68it/s]Measuring inference for batch_size=1:  79%|███████▊  | 787/1000 [00:03<00:01, 202.75it/s]Measuring inference for batch_size=1:  81%|████████  | 808/1000 [00:03<00:00, 202.78it/s]Measuring inference for batch_size=1:  83%|████████▎ | 829/1000 [00:04<00:00, 202.89it/s]Measuring inference for batch_size=1:  85%|████████▌ | 850/1000 [00:04<00:00, 202.97it/s]Measuring inference for batch_size=1:  87%|████████▋ | 871/1000 [00:04<00:00, 202.94it/s]Measuring inference for batch_size=1:  89%|████████▉ | 892/1000 [00:04<00:00, 202.98it/s]Measuring inference for batch_size=1:  91%|█████████▏| 913/1000 [00:04<00:00, 203.01it/s]Measuring inference for batch_size=1:  93%|█████████▎| 934/1000 [00:04<00:00, 203.05it/s]Measuring inference for batch_size=1:  96%|█████████▌| 955/1000 [00:04<00:00, 203.16it/s]Measuring inference for batch_size=1:  98%|█████████▊| 976/1000 [00:04<00:00, 203.05it/s]Measuring inference for batch_size=1: 100%|█████████▉| 997/1000 [00:04<00:00, 202.99it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 204.84it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=32:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=32:  21%|██        | 21/100 [00:00<00:00, 203.99it/s]Warming up with batch_size=32:  42%|████▏     | 42/100 [00:00<00:00, 203.88it/s]Warming up with batch_size=32:  63%|██████▎   | 63/100 [00:00<00:00, 203.91it/s]Warming up with batch_size=32:  84%|████████▍ | 84/100 [00:00<00:00, 203.95it/s]Warming up with batch_size=32: 100%|██████████| 100/100 [00:00<00:00, 203.27it/s]
STAGE:2024-02-23 09:55:38 176888:176888 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:55:38 176888:176888 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:55:38 176888:176888 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=32:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=32:   2%|▏         | 20/1000 [00:00<00:04, 199.86it/s]Measuring inference for batch_size=32:   4%|▍         | 40/1000 [00:00<00:04, 197.93it/s]Measuring inference for batch_size=32:   6%|▌         | 60/1000 [00:00<00:04, 197.43it/s]Measuring inference for batch_size=32:   8%|▊         | 81/1000 [00:00<00:04, 198.45it/s]Measuring inference for batch_size=32:  10%|█         | 101/1000 [00:00<00:04, 198.71it/s]Measuring inference for batch_size=32:  12%|█▏        | 121/1000 [00:00<00:04, 197.79it/s]Measuring inference for batch_size=32:  14%|█▍        | 141/1000 [00:00<00:04, 197.67it/s]Measuring inference for batch_size=32:  16%|█▌        | 161/1000 [00:00<00:04, 198.39it/s]Measuring inference for batch_size=32:  18%|█▊        | 181/1000 [00:00<00:04, 197.76it/s]Measuring inference for batch_size=32:  20%|██        | 201/1000 [00:01<00:04, 197.59it/s]Measuring inference for batch_size=32:  22%|██▏       | 221/1000 [00:01<00:03, 198.21it/s]Measuring inference for batch_size=32:  24%|██▍       | 241/1000 [00:01<00:03, 197.51it/s]Measuring inference for batch_size=32:  26%|██▌       | 261/1000 [00:01<00:03, 197.00it/s]Measuring inference for batch_size=32:  28%|██▊       | 281/1000 [00:01<00:03, 196.60it/s]Measuring inference for batch_size=32:  30%|███       | 301/1000 [00:01<00:03, 197.13it/s]Measuring inference for batch_size=32:  32%|███▏      | 321/1000 [00:01<00:03, 197.72it/s]Measuring inference for batch_size=32:  34%|███▍      | 341/1000 [00:01<00:03, 197.57it/s]Measuring inference for batch_size=32:  36%|███▌      | 361/1000 [00:01<00:03, 197.31it/s]Measuring inference for batch_size=32:  38%|███▊      | 381/1000 [00:01<00:03, 197.07it/s]Measuring inference for batch_size=32:  40%|████      | 401/1000 [00:02<00:03, 196.94it/s]Measuring inference for batch_size=32:  42%|████▏     | 421/1000 [00:02<00:02, 196.86it/s]Measuring inference for batch_size=32:  44%|████▍     | 441/1000 [00:02<00:02, 197.60it/s]Measuring inference for batch_size=32:  46%|████▌     | 461/1000 [00:02<00:02, 197.86it/s]Measuring inference for batch_size=32:  48%|████▊     | 481/1000 [00:02<00:02, 197.52it/s]Measuring inference for batch_size=32:  50%|█████     | 501/1000 [00:02<00:02, 198.01it/s]Measuring inference for batch_size=32:  52%|█████▏    | 521/1000 [00:02<00:02, 198.13it/s]Measuring inference for batch_size=32:  54%|█████▍    | 541/1000 [00:02<00:02, 197.65it/s]Measuring inference for batch_size=32:  56%|█████▌    | 561/1000 [00:02<00:02, 197.29it/s]Measuring inference for batch_size=32:  58%|█████▊    | 581/1000 [00:02<00:02, 197.12it/s]Measuring inference for batch_size=32:  60%|██████    | 601/1000 [00:03<00:02, 196.94it/s]Measuring inference for batch_size=32:  62%|██████▏   | 621/1000 [00:03<00:01, 196.85it/s]Measuring inference for batch_size=32:  64%|██████▍   | 641/1000 [00:03<00:01, 196.78it/s]Measuring inference for batch_size=32:  66%|██████▌   | 662/1000 [00:03<00:01, 198.35it/s]Measuring inference for batch_size=32:  68%|██████▊   | 682/1000 [00:03<00:01, 197.98it/s]Measuring inference for batch_size=32:  70%|███████   | 702/1000 [00:03<00:01, 198.32it/s]Measuring inference for batch_size=32:  72%|███████▏  | 723/1000 [00:03<00:01, 199.31it/s]Measuring inference for batch_size=32:  74%|███████▍  | 744/1000 [00:03<00:01, 200.32it/s]Measuring inference for batch_size=32:  76%|███████▋  | 765/1000 [00:03<00:01, 200.42it/s]Measuring inference for batch_size=32:  79%|███████▊  | 786/1000 [00:03<00:01, 199.25it/s]Measuring inference for batch_size=32:  81%|████████  | 806/1000 [00:04<00:00, 198.47it/s]Measuring inference for batch_size=32:  83%|████████▎ | 826/1000 [00:04<00:00, 197.93it/s]Measuring inference for batch_size=32:  85%|████████▍ | 846/1000 [00:04<00:00, 197.65it/s]Measuring inference for batch_size=32:  87%|████████▋ | 866/1000 [00:04<00:00, 197.33it/s]Measuring inference for batch_size=32:  89%|████████▊ | 886/1000 [00:04<00:00, 197.10it/s]Measuring inference for batch_size=32:  91%|█████████ | 906/1000 [00:04<00:00, 197.00it/s]Measuring inference for batch_size=32:  93%|█████████▎| 926/1000 [00:04<00:00, 197.02it/s]Measuring inference for batch_size=32:  95%|█████████▍| 946/1000 [00:04<00:00, 196.90it/s]Measuring inference for batch_size=32:  97%|█████████▋| 967/1000 [00:04<00:00, 198.57it/s]Measuring inference for batch_size=32:  99%|█████████▉| 988/1000 [00:04<00:00, 199.07it/s]Measuring inference for batch_size=32: 100%|██████████| 1000/1000 [00:05<00:00, 197.85it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.52 GB
    total: 31.28 GB
    used: 3.29 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_32:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 93.180 us +/- 3.156 us [89.645 us, 146.866 us]
        batches_per_second: 10.74 K +/- 312.80 [6.81 K, 11.16 K]
      metrics:
        batches_per_second_max: 11155.063829787234
        batches_per_second_mean: 10742.374773067471
        batches_per_second_min: 6808.935064935065
        batches_per_second_std: 312.804836028087
        seconds_per_batch_max: 0.0001468658447265625
        seconds_per_batch_mean: 9.317994117736816e-05
        seconds_per_batch_min: 8.96453857421875e-05
        seconds_per_batch_std: 3.1558945517431295e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.462 us +/- 0.760 us [21.935 us, 32.425 us]
        batches_per_second: 42.66 K +/- 1.21 K [30.84 K, 45.59 K]
      metrics:
        batches_per_second_max: 45590.260869565216
        batches_per_second_mean: 42661.84535599639
        batches_per_second_min: 30840.470588235294
        batches_per_second_std: 1214.1697779821857
        seconds_per_batch_max: 3.24249267578125e-05
        seconds_per_batch_mean: 2.3461580276489258e-05
        seconds_per_batch_min: 2.193450927734375e-05
        seconds_per_batch_std: 7.602889617179019e-07
    on_device_inference:
      human_readable:
        batch_latency: -4754335.136 us +/- 95.297 ms [-4872799.873 us, -4521952.152
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.22, -0.21]
      metrics:
        batches_per_second_max: -0.20522082293358979
        batches_per_second_mean: -0.21042173637976327
        batches_per_second_min: -0.2211434279555438
        batches_per_second_std: 0.004359846406828875
        seconds_per_batch_max: -4.521952152252197
        seconds_per_batch_mean: -4.754335135936737
        seconds_per_batch_min: -4.872799873352051
        seconds_per_batch_std: 0.09529657113632017
    total:
      human_readable:
        batch_latency: 4.878 ms +/- 97.237 us [4.641 ms, 5.012 ms]
        batches_per_second: 205.08 +/- 4.22 [199.52, 215.49]
      metrics:
        batches_per_second_max: 215.49034114262227
        batches_per_second_mean: 205.07885802207517
        batches_per_second_min: 199.51974122348017
        batches_per_second_std: 4.22453870888321
        seconds_per_batch_max: 0.005012035369873047
        seconds_per_batch_mean: 0.004878175973892212
        seconds_per_batch_min: 0.0046405792236328125
        seconds_per_batch_std: 9.723712306517328e-05
  batch_size_32:
    cpu_to_gpu:
      human_readable:
        batch_latency: 144.240 us +/- 4.227 us [140.190 us, 241.280 us]
        batches_per_second: 6.94 K +/- 160.49 [4.14 K, 7.13 K]
      metrics:
        batches_per_second_max: 7133.170068027211
        batches_per_second_mean: 6937.460977604664
        batches_per_second_min: 4144.569169960474
        batches_per_second_std: 160.4946805422425
        seconds_per_batch_max: 0.00024127960205078125
        seconds_per_batch_mean: 0.00014424014091491698
        seconds_per_batch_min: 0.00014019012451171875
        seconds_per_batch_std: 4.2271743907072865e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.615 us +/- 0.748 us [21.935 us, 30.518 us]
        batches_per_second: 42.39 K +/- 1.24 K [32.77 K, 45.59 K]
      metrics:
        batches_per_second_max: 45590.260869565216
        batches_per_second_mean: 42385.596615908624
        batches_per_second_min: 32768.0
        batches_per_second_std: 1238.688154448747
        seconds_per_batch_max: 3.0517578125e-05
        seconds_per_batch_mean: 2.361464500427246e-05
        seconds_per_batch_min: 2.193450927734375e-05
        seconds_per_batch_std: 7.479199186642042e-07
    on_device_inference:
      human_readable:
        batch_latency: -4877117.761 us +/- 76.482 ms [-5057856.083 us, -4684224.129
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.20]
      metrics:
        batches_per_second_max: -0.19771222897734564
        batches_per_second_mean: -0.20509086920218314
        batches_per_second_min: -0.2134825261387709
        batches_per_second_std: 0.003299290084296891
        seconds_per_batch_max: -4.6842241287231445
        seconds_per_batch_mean: -4.8771177606582645
        seconds_per_batch_min: -5.05785608291626
        seconds_per_batch_std: 0.07648156697676863
    total:
      human_readable:
        batch_latency: 5.050 ms +/- 78.185 us [4.854 ms, 5.333 ms]
        batches_per_second: 198.05 +/- 3.14 [187.52, 206.03]
      metrics:
        batches_per_second_max: 206.02731113075941
        batches_per_second_mean: 198.05249000622567
        batches_per_second_min: 187.5219743371932
        batches_per_second_std: 3.1417879937479354
        seconds_per_batch_max: 0.0053327083587646484
        seconds_per_batch_mean: 0.005050406694412231
        seconds_per_batch_min: 0.004853725433349609
        seconds_per_batch_std: 7.818523448482697e-05


#####
baseline-baseline-py-id - Run 2
2024-02-23 09:56:03
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 276.55it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:00<00:00, 278.03it/s]Warming up with batch_size=1:  85%|████████▌ | 85/100 [00:00<00:00, 278.98it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 278.72it/s]
STAGE:2024-02-23 09:55:51 176934:176934 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:55:51 176934:176934 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:55:51 176934:176934 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:00<00:04, 208.85it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:00<00:04, 209.17it/s]Measuring inference for batch_size=1:   6%|▋         | 63/1000 [00:00<00:04, 209.23it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:00<00:04, 209.22it/s]Measuring inference for batch_size=1:  10%|█         | 105/1000 [00:00<00:04, 209.24it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:00<00:04, 209.35it/s]Measuring inference for batch_size=1:  15%|█▍        | 147/1000 [00:00<00:04, 209.41it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:00<00:03, 209.42it/s]Measuring inference for batch_size=1:  19%|█▉        | 189/1000 [00:00<00:03, 209.38it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:01<00:03, 209.39it/s]Measuring inference for batch_size=1:  23%|██▎       | 231/1000 [00:01<00:03, 209.36it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:01<00:03, 209.37it/s]Measuring inference for batch_size=1:  27%|██▋       | 273/1000 [00:01<00:03, 209.42it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:01<00:03, 209.47it/s]Measuring inference for batch_size=1:  32%|███▏      | 315/1000 [00:01<00:03, 209.46it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:01<00:03, 209.50it/s]Measuring inference for batch_size=1:  36%|███▌      | 357/1000 [00:01<00:03, 209.56it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:01<00:02, 209.62it/s]Measuring inference for batch_size=1:  40%|███▉      | 399/1000 [00:01<00:02, 209.64it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:02<00:02, 209.61it/s]Measuring inference for batch_size=1:  44%|████▍     | 441/1000 [00:02<00:02, 209.57it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:02<00:02, 209.57it/s]Measuring inference for batch_size=1:  48%|████▊     | 483/1000 [00:02<00:02, 209.57it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:02<00:02, 209.61it/s]Measuring inference for batch_size=1:  52%|█████▎    | 525/1000 [00:02<00:02, 209.62it/s]Measuring inference for batch_size=1:  55%|█████▍    | 546/1000 [00:02<00:02, 209.61it/s]Measuring inference for batch_size=1:  57%|█████▋    | 567/1000 [00:02<00:02, 209.73it/s]Measuring inference for batch_size=1:  59%|█████▉    | 589/1000 [00:02<00:01, 209.82it/s]Measuring inference for batch_size=1:  61%|██████    | 610/1000 [00:02<00:01, 209.83it/s]Measuring inference for batch_size=1:  63%|██████▎   | 631/1000 [00:03<00:01, 209.78it/s]Measuring inference for batch_size=1:  65%|██████▌   | 652/1000 [00:03<00:01, 209.80it/s]Measuring inference for batch_size=1:  67%|██████▋   | 674/1000 [00:03<00:01, 209.88it/s]Measuring inference for batch_size=1:  70%|██████▉   | 696/1000 [00:03<00:01, 209.96it/s]Measuring inference for batch_size=1:  72%|███████▏  | 718/1000 [00:03<00:01, 210.01it/s]Measuring inference for batch_size=1:  74%|███████▍  | 740/1000 [00:03<00:01, 210.10it/s]Measuring inference for batch_size=1:  76%|███████▌  | 762/1000 [00:03<00:01, 210.16it/s]Measuring inference for batch_size=1:  78%|███████▊  | 784/1000 [00:03<00:01, 210.16it/s]Measuring inference for batch_size=1:  81%|████████  | 806/1000 [00:03<00:00, 210.12it/s]Measuring inference for batch_size=1:  83%|████████▎ | 828/1000 [00:03<00:00, 210.12it/s]Measuring inference for batch_size=1:  85%|████████▌ | 850/1000 [00:04<00:00, 209.99it/s]Measuring inference for batch_size=1:  87%|████████▋ | 871/1000 [00:04<00:00, 209.91it/s]Measuring inference for batch_size=1:  89%|████████▉ | 892/1000 [00:04<00:00, 209.83it/s]Measuring inference for batch_size=1:  91%|█████████▏| 913/1000 [00:04<00:00, 209.71it/s]Measuring inference for batch_size=1:  93%|█████████▎| 934/1000 [00:04<00:00, 209.67it/s]Measuring inference for batch_size=1:  96%|█████████▌| 955/1000 [00:04<00:00, 209.69it/s]Measuring inference for batch_size=1:  98%|█████████▊| 976/1000 [00:04<00:00, 209.69it/s]Measuring inference for batch_size=1: 100%|█████████▉| 997/1000 [00:04<00:00, 209.64it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 209.66it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=32:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=32:  21%|██        | 21/100 [00:00<00:00, 204.14it/s]Warming up with batch_size=32:  42%|████▏     | 42/100 [00:00<00:00, 204.36it/s]Warming up with batch_size=32:  63%|██████▎   | 63/100 [00:00<00:00, 204.36it/s]Warming up with batch_size=32:  84%|████████▍ | 84/100 [00:00<00:00, 204.39it/s]Warming up with batch_size=32: 100%|██████████| 100/100 [00:00<00:00, 204.36it/s]
STAGE:2024-02-23 09:55:56 176934:176934 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:55:56 176934:176934 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:55:56 176934:176934 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=32:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=32:   2%|▏         | 21/1000 [00:00<00:04, 200.25it/s]Measuring inference for batch_size=32:   4%|▍         | 42/1000 [00:00<00:04, 200.80it/s]Measuring inference for batch_size=32:   6%|▋         | 63/1000 [00:00<00:04, 201.15it/s]Measuring inference for batch_size=32:   8%|▊         | 84/1000 [00:00<00:04, 201.30it/s]Measuring inference for batch_size=32:  10%|█         | 105/1000 [00:00<00:04, 201.39it/s]Measuring inference for batch_size=32:  13%|█▎        | 126/1000 [00:00<00:04, 201.37it/s]Measuring inference for batch_size=32:  15%|█▍        | 147/1000 [00:00<00:04, 201.37it/s]Measuring inference for batch_size=32:  17%|█▋        | 168/1000 [00:00<00:04, 201.45it/s]Measuring inference for batch_size=32:  19%|█▉        | 189/1000 [00:00<00:04, 201.49it/s]Measuring inference for batch_size=32:  21%|██        | 210/1000 [00:01<00:03, 201.45it/s]Measuring inference for batch_size=32:  23%|██▎       | 231/1000 [00:01<00:03, 201.38it/s]Measuring inference for batch_size=32:  25%|██▌       | 252/1000 [00:01<00:03, 201.39it/s]Measuring inference for batch_size=32:  27%|██▋       | 273/1000 [00:01<00:03, 201.41it/s]Measuring inference for batch_size=32:  29%|██▉       | 294/1000 [00:01<00:03, 201.40it/s]Measuring inference for batch_size=32:  32%|███▏      | 315/1000 [00:01<00:03, 201.34it/s]Measuring inference for batch_size=32:  34%|███▎      | 336/1000 [00:01<00:03, 201.30it/s]Measuring inference for batch_size=32:  36%|███▌      | 357/1000 [00:01<00:03, 201.17it/s]Measuring inference for batch_size=32:  38%|███▊      | 378/1000 [00:01<00:03, 201.24it/s]Measuring inference for batch_size=32:  40%|███▉      | 399/1000 [00:01<00:02, 201.25it/s]Measuring inference for batch_size=32:  42%|████▏     | 420/1000 [00:02<00:02, 201.38it/s]Measuring inference for batch_size=32:  44%|████▍     | 441/1000 [00:02<00:02, 201.43it/s]Measuring inference for batch_size=32:  46%|████▌     | 462/1000 [00:02<00:02, 201.41it/s]Measuring inference for batch_size=32:  48%|████▊     | 483/1000 [00:02<00:02, 201.46it/s]Measuring inference for batch_size=32:  50%|█████     | 504/1000 [00:02<00:02, 201.50it/s]Measuring inference for batch_size=32:  52%|█████▎    | 525/1000 [00:02<00:02, 201.47it/s]Measuring inference for batch_size=32:  55%|█████▍    | 546/1000 [00:02<00:02, 201.37it/s]Measuring inference for batch_size=32:  57%|█████▋    | 567/1000 [00:02<00:02, 201.44it/s]Measuring inference for batch_size=32:  59%|█████▉    | 588/1000 [00:02<00:02, 201.45it/s]Measuring inference for batch_size=32:  61%|██████    | 609/1000 [00:03<00:01, 201.44it/s]Measuring inference for batch_size=32:  63%|██████▎   | 630/1000 [00:03<00:01, 201.50it/s]Measuring inference for batch_size=32:  65%|██████▌   | 651/1000 [00:03<00:01, 201.49it/s]Measuring inference for batch_size=32:  67%|██████▋   | 672/1000 [00:03<00:01, 201.46it/s]Measuring inference for batch_size=32:  69%|██████▉   | 693/1000 [00:03<00:01, 201.53it/s]Measuring inference for batch_size=32:  71%|███████▏  | 714/1000 [00:03<00:01, 201.60it/s]Measuring inference for batch_size=32:  74%|███████▎  | 735/1000 [00:03<00:01, 201.51it/s]Measuring inference for batch_size=32:  76%|███████▌  | 756/1000 [00:03<00:01, 201.40it/s]Measuring inference for batch_size=32:  78%|███████▊  | 777/1000 [00:03<00:01, 201.39it/s]Measuring inference for batch_size=32:  80%|███████▉  | 798/1000 [00:03<00:01, 201.37it/s]Measuring inference for batch_size=32:  82%|████████▏ | 819/1000 [00:04<00:00, 201.34it/s]Measuring inference for batch_size=32:  84%|████████▍ | 840/1000 [00:04<00:00, 201.30it/s]Measuring inference for batch_size=32:  86%|████████▌ | 861/1000 [00:04<00:00, 201.31it/s]Measuring inference for batch_size=32:  88%|████████▊ | 882/1000 [00:04<00:00, 201.29it/s]Measuring inference for batch_size=32:  90%|█████████ | 903/1000 [00:04<00:00, 201.39it/s]Measuring inference for batch_size=32:  92%|█████████▏| 924/1000 [00:04<00:00, 201.44it/s]Measuring inference for batch_size=32:  94%|█████████▍| 945/1000 [00:04<00:00, 201.38it/s]Measuring inference for batch_size=32:  97%|█████████▋| 966/1000 [00:04<00:00, 201.41it/s]Measuring inference for batch_size=32:  99%|█████████▊| 987/1000 [00:04<00:00, 201.50it/s]Measuring inference for batch_size=32: 100%|██████████| 1000/1000 [00:04<00:00, 201.39it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.52 GB
    total: 31.28 GB
    used: 3.29 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_32:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 92.046 us +/- 2.961 us [90.361 us, 144.005 us]
        batches_per_second: 10.87 K +/- 296.15 [6.94 K, 11.07 K]
      metrics:
        batches_per_second_max: 11066.765171503957
        batches_per_second_mean: 10873.538424017135
        batches_per_second_min: 6944.211920529801
        batches_per_second_std: 296.14739161193137
        seconds_per_batch_max: 0.00014400482177734375
        seconds_per_batch_mean: 9.204602241516114e-05
        seconds_per_batch_min: 9.036064147949219e-05
        seconds_per_batch_std: 2.961080420728053e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.102 us +/- 0.612 us [22.173 us, 29.564 us]
        batches_per_second: 43.31 K +/- 999.92 [33.83 K, 45.10 K]
      metrics:
        batches_per_second_max: 45100.04301075269
        batches_per_second_mean: 43313.49513678049
        batches_per_second_min: 33825.032258064515
        batches_per_second_std: 999.9223719152983
        seconds_per_batch_max: 2.956390380859375e-05
        seconds_per_batch_mean: 2.3101568222045897e-05
        seconds_per_batch_min: 2.2172927856445312e-05
        seconds_per_batch_std: 6.11950179646621e-07
    on_device_inference:
      human_readable:
        batch_latency: -4644291.007 us +/- 15.534 ms [-4894303.799 us, -4602687.836
          us]
        batches_per_second: -0.22 +/- 0.00 [-0.22, -0.20]
      metrics:
        batches_per_second_max: -0.20431915163717732
        batches_per_second_mean: -0.2153204949549825
        batches_per_second_min: -0.2172643541552189
        batches_per_second_std: 0.0007107986312161099
        seconds_per_batch_max: -4.602687835693359
        seconds_per_batch_mean: -4.644291007041931
        seconds_per_batch_min: -4.894303798675537
        seconds_per_batch_std: 0.015533559749617067
    total:
      human_readable:
        batch_latency: 4.766 ms +/- 17.163 us [4.723 ms, 5.080 ms]
        batches_per_second: 209.82 +/- 0.74 [196.87, 211.73]
      metrics:
        batches_per_second_max: 211.72660272589602
        batches_per_second_mean: 209.8221547984719
        batches_per_second_min: 196.8694672612063
        batches_per_second_std: 0.7401186260889566
        seconds_per_batch_max: 0.005079507827758789
        seconds_per_batch_mean: 0.004766001462936402
        seconds_per_batch_min: 0.004723072052001953
        seconds_per_batch_std: 1.7162523814777065e-05
  batch_size_32:
    cpu_to_gpu:
      human_readable:
        batch_latency: 142.946 us +/- 4.231 us [140.905 us, 244.379 us]
        batches_per_second: 7.00 K +/- 157.47 [4.09 K, 7.10 K]
      metrics:
        batches_per_second_max: 7096.961082910321
        batches_per_second_mean: 7000.201096144756
        batches_per_second_min: 4092.0039024390244
        batches_per_second_std: 157.4727270432128
        seconds_per_batch_max: 0.00024437904357910156
        seconds_per_batch_mean: 0.0001429455280303955
        seconds_per_batch_min: 0.00014090538024902344
        seconds_per_batch_std: 4.2305074302892995e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.246 us +/- 0.673 us [22.411 us, 32.663 us]
        batches_per_second: 43.05 K +/- 1.06 K [30.62 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 43048.193548231626
        batches_per_second_min: 30615.357664233576
        batches_per_second_std: 1062.2812012354443
        seconds_per_batch_max: 3.266334533691406e-05
        seconds_per_batch_mean: 2.3246288299560546e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 6.731682611239823e-07
    on_device_inference:
      human_readable:
        batch_latency: -4790247.973 us +/- 16.167 ms [-5136640.072 us, -4760799.885
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.19]
      metrics:
        batches_per_second_max: -0.19467978795644206
        batches_per_second_mean: -0.20875976363717486
        batches_per_second_min: -0.21004873638851132
        batches_per_second_std: 0.0006829412664605523
        seconds_per_batch_max: -4.760799884796143
        seconds_per_batch_mean: -4.790247972965241
        seconds_per_batch_min: -5.1366400718688965
        seconds_per_batch_std: 0.016167026514363317
    total:
      human_readable:
        batch_latency: 4.962 ms +/- 19.138 us [4.932 ms, 5.416 ms]
        batches_per_second: 201.54 +/- 0.74 [184.62, 202.74]
      metrics:
        batches_per_second_max: 202.74091260634185
        batches_per_second_mean: 201.54057159962227
        batches_per_second_min: 184.62470287877454
        batches_per_second_std: 0.740952117876512
        seconds_per_batch_max: 0.005416393280029297
        seconds_per_batch_mean: 0.0049618504047393795
        seconds_per_batch_min: 0.004932403564453125
        seconds_per_batch_std: 1.9137543042956187e-05


#####
baseline-baseline-py-id - Run 3
2024-02-23 09:56:20
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.40it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.39it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 271.41it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:00<00:00, 271.86it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:00<00:00, 273.10it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 272.91it/s]
STAGE:2024-02-23 09:56:09 176980:176980 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:56:09 176980:176980 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:56:09 176980:176980 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:00<00:04, 205.34it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:00<00:04, 205.68it/s]Measuring inference for batch_size=1:   6%|▋         | 63/1000 [00:00<00:04, 206.00it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:00<00:04, 206.05it/s]Measuring inference for batch_size=1:  10%|█         | 105/1000 [00:00<00:04, 206.10it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:00<00:04, 206.13it/s]Measuring inference for batch_size=1:  15%|█▍        | 147/1000 [00:00<00:04, 206.14it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:00<00:04, 206.03it/s]Measuring inference for batch_size=1:  19%|█▉        | 189/1000 [00:00<00:03, 205.98it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:01<00:03, 205.96it/s]Measuring inference for batch_size=1:  23%|██▎       | 231/1000 [00:01<00:03, 205.95it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:01<00:03, 205.80it/s]Measuring inference for batch_size=1:  27%|██▋       | 273/1000 [00:01<00:03, 205.97it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:01<00:03, 206.00it/s]Measuring inference for batch_size=1:  32%|███▏      | 315/1000 [00:01<00:03, 206.08it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:01<00:03, 206.08it/s]Measuring inference for batch_size=1:  36%|███▌      | 357/1000 [00:01<00:03, 206.06it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:01<00:03, 206.04it/s]Measuring inference for batch_size=1:  40%|███▉      | 399/1000 [00:01<00:02, 206.07it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:02<00:02, 206.09it/s]Measuring inference for batch_size=1:  44%|████▍     | 441/1000 [00:02<00:02, 206.10it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:02<00:02, 206.09it/s]Measuring inference for batch_size=1:  48%|████▊     | 483/1000 [00:02<00:02, 206.13it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:02<00:02, 206.19it/s]Measuring inference for batch_size=1:  52%|█████▎    | 525/1000 [00:02<00:02, 206.25it/s]Measuring inference for batch_size=1:  55%|█████▍    | 546/1000 [00:02<00:02, 206.31it/s]Measuring inference for batch_size=1:  57%|█████▋    | 567/1000 [00:02<00:02, 206.33it/s]Measuring inference for batch_size=1:  59%|█████▉    | 588/1000 [00:02<00:01, 206.31it/s]Measuring inference for batch_size=1:  61%|██████    | 609/1000 [00:02<00:01, 206.25it/s]Measuring inference for batch_size=1:  63%|██████▎   | 630/1000 [00:03<00:01, 206.23it/s]Measuring inference for batch_size=1:  65%|██████▌   | 651/1000 [00:03<00:01, 206.20it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:03<00:01, 206.14it/s]Measuring inference for batch_size=1:  69%|██████▉   | 693/1000 [00:03<00:01, 205.97it/s]Measuring inference for batch_size=1:  71%|███████▏  | 714/1000 [00:03<00:01, 206.05it/s]Measuring inference for batch_size=1:  74%|███████▎  | 735/1000 [00:03<00:01, 206.05it/s]Measuring inference for batch_size=1:  76%|███████▌  | 756/1000 [00:03<00:01, 206.07it/s]Measuring inference for batch_size=1:  78%|███████▊  | 777/1000 [00:03<00:01, 206.13it/s]Measuring inference for batch_size=1:  80%|███████▉  | 798/1000 [00:03<00:00, 206.18it/s]Measuring inference for batch_size=1:  82%|████████▏ | 819/1000 [00:03<00:00, 206.26it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:04<00:00, 206.29it/s]Measuring inference for batch_size=1:  86%|████████▌ | 861/1000 [00:04<00:00, 206.21it/s]Measuring inference for batch_size=1:  88%|████████▊ | 882/1000 [00:04<00:00, 206.27it/s]Measuring inference for batch_size=1:  90%|█████████ | 903/1000 [00:04<00:00, 206.25it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [00:04<00:00, 206.18it/s]Measuring inference for batch_size=1:  94%|█████████▍| 945/1000 [00:04<00:00, 206.22it/s]Measuring inference for batch_size=1:  97%|█████████▋| 966/1000 [00:04<00:00, 206.29it/s]Measuring inference for batch_size=1:  99%|█████████▊| 987/1000 [00:04<00:00, 206.32it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 206.13it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=32:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=32:  21%|██        | 21/100 [00:00<00:00, 201.10it/s]Warming up with batch_size=32:  42%|████▏     | 42/100 [00:00<00:00, 201.38it/s]Warming up with batch_size=32:  63%|██████▎   | 63/100 [00:00<00:00, 201.43it/s]Warming up with batch_size=32:  84%|████████▍ | 84/100 [00:00<00:00, 201.36it/s]Warming up with batch_size=32: 100%|██████████| 100/100 [00:00<00:00, 201.37it/s]
STAGE:2024-02-23 09:56:14 176980:176980 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:56:14 176980:176980 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:56:14 176980:176980 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=32:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=32:   2%|▏         | 20/1000 [00:00<00:04, 197.52it/s]Measuring inference for batch_size=32:   4%|▍         | 40/1000 [00:00<00:04, 198.06it/s]Measuring inference for batch_size=32:   6%|▌         | 60/1000 [00:00<00:04, 198.17it/s]Measuring inference for batch_size=32:   8%|▊         | 80/1000 [00:00<00:04, 198.28it/s]Measuring inference for batch_size=32:  10%|█         | 100/1000 [00:00<00:04, 198.43it/s]Measuring inference for batch_size=32:  12%|█▏        | 120/1000 [00:00<00:04, 198.62it/s]Measuring inference for batch_size=32:  14%|█▍        | 140/1000 [00:00<00:04, 198.61it/s]Measuring inference for batch_size=32:  16%|█▌        | 160/1000 [00:00<00:04, 198.66it/s]Measuring inference for batch_size=32:  18%|█▊        | 180/1000 [00:00<00:04, 198.62it/s]Measuring inference for batch_size=32:  20%|██        | 200/1000 [00:01<00:04, 198.52it/s]Measuring inference for batch_size=32:  22%|██▏       | 220/1000 [00:01<00:03, 198.36it/s]Measuring inference for batch_size=32:  24%|██▍       | 240/1000 [00:01<00:03, 198.36it/s]Measuring inference for batch_size=32:  26%|██▌       | 260/1000 [00:01<00:03, 198.30it/s]Measuring inference for batch_size=32:  28%|██▊       | 280/1000 [00:01<00:03, 198.37it/s]Measuring inference for batch_size=32:  30%|███       | 300/1000 [00:01<00:03, 198.50it/s]Measuring inference for batch_size=32:  32%|███▏      | 320/1000 [00:01<00:03, 198.59it/s]Measuring inference for batch_size=32:  34%|███▍      | 340/1000 [00:01<00:03, 198.65it/s]Measuring inference for batch_size=32:  36%|███▌      | 360/1000 [00:01<00:03, 198.55it/s]Measuring inference for batch_size=32:  38%|███▊      | 380/1000 [00:01<00:03, 198.55it/s]Measuring inference for batch_size=32:  40%|████      | 400/1000 [00:02<00:03, 198.53it/s]Measuring inference for batch_size=32:  42%|████▏     | 420/1000 [00:02<00:02, 198.48it/s]Measuring inference for batch_size=32:  44%|████▍     | 440/1000 [00:02<00:02, 198.61it/s]Measuring inference for batch_size=32:  46%|████▌     | 460/1000 [00:02<00:02, 198.64it/s]Measuring inference for batch_size=32:  48%|████▊     | 480/1000 [00:02<00:02, 198.65it/s]Measuring inference for batch_size=32:  50%|█████     | 500/1000 [00:02<00:02, 198.57it/s]Measuring inference for batch_size=32:  52%|█████▏    | 520/1000 [00:02<00:02, 198.63it/s]Measuring inference for batch_size=32:  54%|█████▍    | 540/1000 [00:02<00:02, 198.60it/s]Measuring inference for batch_size=32:  56%|█████▌    | 560/1000 [00:02<00:02, 198.65it/s]Measuring inference for batch_size=32:  58%|█████▊    | 580/1000 [00:02<00:02, 198.72it/s]Measuring inference for batch_size=32:  60%|██████    | 600/1000 [00:03<00:02, 198.74it/s]Measuring inference for batch_size=32:  62%|██████▏   | 620/1000 [00:03<00:01, 198.72it/s]Measuring inference for batch_size=32:  64%|██████▍   | 640/1000 [00:03<00:01, 198.78it/s]Measuring inference for batch_size=32:  66%|██████▌   | 660/1000 [00:03<00:01, 198.75it/s]Measuring inference for batch_size=32:  68%|██████▊   | 680/1000 [00:03<00:01, 198.67it/s]Measuring inference for batch_size=32:  70%|███████   | 700/1000 [00:03<00:01, 198.69it/s]Measuring inference for batch_size=32:  72%|███████▏  | 720/1000 [00:03<00:01, 198.68it/s]Measuring inference for batch_size=32:  74%|███████▍  | 740/1000 [00:03<00:01, 198.69it/s]Measuring inference for batch_size=32:  76%|███████▌  | 760/1000 [00:03<00:01, 198.65it/s]Measuring inference for batch_size=32:  78%|███████▊  | 780/1000 [00:03<00:01, 198.68it/s]Measuring inference for batch_size=32:  80%|████████  | 800/1000 [00:04<00:01, 198.73it/s]Measuring inference for batch_size=32:  82%|████████▏ | 820/1000 [00:04<00:00, 198.74it/s]Measuring inference for batch_size=32:  84%|████████▍ | 840/1000 [00:04<00:00, 198.76it/s]Measuring inference for batch_size=32:  86%|████████▌ | 860/1000 [00:04<00:00, 198.75it/s]Measuring inference for batch_size=32:  88%|████████▊ | 880/1000 [00:04<00:00, 198.70it/s]Measuring inference for batch_size=32:  90%|█████████ | 900/1000 [00:04<00:00, 198.73it/s]Measuring inference for batch_size=32:  92%|█████████▏| 920/1000 [00:04<00:00, 198.75it/s]Measuring inference for batch_size=32:  94%|█████████▍| 940/1000 [00:04<00:00, 198.82it/s]Measuring inference for batch_size=32:  96%|█████████▌| 960/1000 [00:04<00:00, 198.83it/s]Measuring inference for batch_size=32:  98%|█████████▊| 980/1000 [00:04<00:00, 198.86it/s]Measuring inference for batch_size=32: 100%|██████████| 1000/1000 [00:05<00:00, 198.95it/s]Measuring inference for batch_size=32: 100%|██████████| 1000/1000 [00:05<00:00, 198.63it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.53 GB
    total: 31.28 GB
    used: 3.29 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_32:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 92.465 us +/- 3.038 us [90.837 us, 146.389 us]
        batches_per_second: 10.82 K +/- 296.79 [6.83 K, 11.01 K]
      metrics:
        batches_per_second_max: 11008.671916010499
        batches_per_second_mean: 10824.565045444298
        batches_per_second_min: 6831.114006514658
        batches_per_second_std: 296.78964649459033
        seconds_per_batch_max: 0.00014638900756835938
        seconds_per_batch_mean: 9.246468544006347e-05
        seconds_per_batch_min: 9.083747863769531e-05
        seconds_per_batch_std: 3.038283259605966e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.173 us +/- 0.499 us [22.411 us, 28.610 us]
        batches_per_second: 43.17 K +/- 837.88 [34.95 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 43171.04079271344
        batches_per_second_min: 34952.53333333333
        batches_per_second_std: 837.884210385532
        seconds_per_batch_max: 2.86102294921875e-05
        seconds_per_batch_mean: 2.3173332214355468e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 4.99063337248705e-07
    on_device_inference:
      human_readable:
        batch_latency: -4724665.536 us +/- 14.821 ms [-4966112.137 us, -4688543.797
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.20]
      metrics:
        batches_per_second_max: -0.2013647643156418
        batches_per_second_mean: -0.2116572479862199
        batches_per_second_min: -0.2132858395688053
        batches_per_second_std: 0.0006549841228515812
        seconds_per_batch_max: -4.688543796539307
        seconds_per_batch_mean: -4.724665535926819
        seconds_per_batch_min: -4.96611213684082
        seconds_per_batch_std: 0.014820600567205824
    total:
      human_readable:
        batch_latency: 4.848 ms +/- 16.513 us [4.810 ms, 5.155 ms]
        batches_per_second: 206.29 +/- 0.69 [193.97, 207.92]
      metrics:
        batches_per_second_max: 207.9167203688098
        batches_per_second_mean: 206.2881030358853
        batches_per_second_min: 193.97419414512325
        batches_per_second_std: 0.687771573213604
        seconds_per_batch_max: 0.005155324935913086
        seconds_per_batch_mean: 0.004847644329071045
        seconds_per_batch_min: 0.00480961799621582
        seconds_per_batch_std: 1.6513429069665183e-05
  batch_size_32:
    cpu_to_gpu:
      human_readable:
        batch_latency: 143.548 us +/- 4.200 us [141.382 us, 246.286 us]
        batches_per_second: 6.97 K +/- 153.07 [4.06 K, 7.07 K]
      metrics:
        batches_per_second_max: 7073.025295109612
        batches_per_second_mean: 6970.672408673285
        batches_per_second_min: 4060.3136495643757
        batches_per_second_std: 153.07181516571035
        seconds_per_batch_max: 0.00024628639221191406
        seconds_per_batch_mean: 0.00014354777336120605
        seconds_per_batch_min: 0.00014138221740722656
        seconds_per_batch_std: 4.200378595135778e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.348 us +/- 0.644 us [22.411 us, 32.425 us]
        batches_per_second: 42.86 K +/- 1.01 K [30.84 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 42858.305576421444
        batches_per_second_min: 30840.470588235294
        batches_per_second_std: 1014.3562816378528
        seconds_per_batch_max: 3.24249267578125e-05
        seconds_per_batch_mean: 2.3347854614257812e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 6.444721890251384e-07
    on_device_inference:
      human_readable:
        batch_latency: -4858429.529 us +/- 15.639 ms [-5190239.906 us, -4819231.987
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.19]
      metrics:
        batches_per_second_max: -0.19266932127435132
        batches_per_second_mean: -0.20582989848206382
        batches_per_second_min: -0.20750194277794193
        batches_per_second_std: 0.0006434095372801206
        seconds_per_batch_max: -4.819231986999512
        seconds_per_batch_mean: -4.858429529190063
        seconds_per_batch_min: -5.190239906311035
        seconds_per_batch_std: 0.015638609007145334
    total:
      human_readable:
        batch_latency: 5.031 ms +/- 18.486 us [4.989 ms, 5.471 ms]
        batches_per_second: 198.78 +/- 0.70 [182.77, 200.43]
      metrics:
        batches_per_second_max: 200.42547904620824
        batches_per_second_mean: 198.78325150883262
        batches_per_second_min: 182.76630790012638
        batches_per_second_std: 0.697236743120023
        seconds_per_batch_max: 0.005471467971801758
        seconds_per_batch_mean: 0.005030669689178467
        seconds_per_batch_min: 0.0049893856048583984
        seconds_per_batch_std: 1.8485893401525638e-05


