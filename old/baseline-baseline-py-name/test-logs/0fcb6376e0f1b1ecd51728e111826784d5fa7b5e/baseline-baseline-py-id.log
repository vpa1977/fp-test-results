#####
baseline-baseline-py-id - Run 1
2024-02-23 09:59:46
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  5.09it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  5.09it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  29%|██▉       | 29/100 [00:00<00:00, 280.81it/s]Warming up with batch_size=1:  58%|█████▊    | 58/100 [00:00<00:00, 281.89it/s]Warming up with batch_size=1:  87%|████████▋ | 87/100 [00:00<00:00, 283.38it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 283.08it/s]
STAGE:2024-02-23 09:59:33 177308:177308 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:59:33 177308:177308 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:59:33 177308:177308 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 22/1000 [00:00<00:04, 214.86it/s]Measuring inference for batch_size=1:   4%|▍         | 44/1000 [00:00<00:04, 215.51it/s]Measuring inference for batch_size=1:   7%|▋         | 66/1000 [00:00<00:04, 215.77it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:00<00:04, 215.97it/s]Measuring inference for batch_size=1:  11%|█         | 110/1000 [00:00<00:04, 216.00it/s]Measuring inference for batch_size=1:  13%|█▎        | 132/1000 [00:00<00:04, 216.06it/s]Measuring inference for batch_size=1:  15%|█▌        | 154/1000 [00:00<00:03, 216.13it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:00<00:03, 216.26it/s]Measuring inference for batch_size=1:  20%|█▉        | 198/1000 [00:00<00:03, 216.30it/s]Measuring inference for batch_size=1:  22%|██▏       | 220/1000 [00:01<00:03, 216.32it/s]Measuring inference for batch_size=1:  24%|██▍       | 242/1000 [00:01<00:03, 216.36it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:01<00:03, 216.44it/s]Measuring inference for batch_size=1:  29%|██▊       | 286/1000 [00:01<00:03, 216.46it/s]Measuring inference for batch_size=1:  31%|███       | 308/1000 [00:01<00:03, 216.43it/s]Measuring inference for batch_size=1:  33%|███▎      | 330/1000 [00:01<00:03, 216.35it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:01<00:02, 216.34it/s]Measuring inference for batch_size=1:  37%|███▋      | 374/1000 [00:01<00:02, 216.20it/s]Measuring inference for batch_size=1:  40%|███▉      | 396/1000 [00:01<00:02, 216.24it/s]Measuring inference for batch_size=1:  42%|████▏     | 418/1000 [00:01<00:02, 216.27it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:02<00:02, 216.39it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:02<00:02, 216.41it/s]Measuring inference for batch_size=1:  48%|████▊     | 484/1000 [00:02<00:02, 216.51it/s]Measuring inference for batch_size=1:  51%|█████     | 506/1000 [00:02<00:02, 216.50it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:02<00:02, 216.47it/s]Measuring inference for batch_size=1:  55%|█████▌    | 550/1000 [00:02<00:02, 216.38it/s]Measuring inference for batch_size=1:  57%|█████▋    | 572/1000 [00:02<00:01, 216.33it/s]Measuring inference for batch_size=1:  59%|█████▉    | 594/1000 [00:02<00:01, 216.36it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:02<00:01, 216.45it/s]Measuring inference for batch_size=1:  64%|██████▍   | 638/1000 [00:02<00:01, 216.41it/s]Measuring inference for batch_size=1:  66%|██████▌   | 660/1000 [00:03<00:01, 216.40it/s]Measuring inference for batch_size=1:  68%|██████▊   | 682/1000 [00:03<00:01, 216.43it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [00:03<00:01, 216.49it/s]Measuring inference for batch_size=1:  73%|███████▎  | 726/1000 [00:03<00:01, 216.50it/s]Measuring inference for batch_size=1:  75%|███████▍  | 748/1000 [00:03<00:01, 216.55it/s]Measuring inference for batch_size=1:  77%|███████▋  | 770/1000 [00:03<00:01, 216.53it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [00:03<00:00, 216.45it/s]Measuring inference for batch_size=1:  81%|████████▏ | 814/1000 [00:03<00:00, 216.48it/s]Measuring inference for batch_size=1:  84%|████████▎ | 836/1000 [00:03<00:00, 216.59it/s]Measuring inference for batch_size=1:  86%|████████▌ | 858/1000 [00:03<00:00, 216.61it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [00:04<00:00, 216.61it/s]Measuring inference for batch_size=1:  90%|█████████ | 902/1000 [00:04<00:00, 216.60it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [00:04<00:00, 216.69it/s]Measuring inference for batch_size=1:  95%|█████████▍| 946/1000 [00:04<00:00, 216.57it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [00:04<00:00, 216.55it/s]Measuring inference for batch_size=1:  99%|█████████▉| 990/1000 [00:04<00:00, 216.53it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 216.38it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=256:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=256:  21%|██        | 21/100 [00:00<00:00, 209.00it/s]Warming up with batch_size=256:  42%|████▏     | 42/100 [00:00<00:00, 209.24it/s]Warming up with batch_size=256:  63%|██████▎   | 63/100 [00:00<00:00, 209.41it/s]Warming up with batch_size=256:  84%|████████▍ | 84/100 [00:00<00:00, 209.32it/s]Warming up with batch_size=256: 100%|██████████| 100/100 [00:00<00:00, 209.34it/s]
STAGE:2024-02-23 09:59:39 177308:177308 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:59:39 177308:177308 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:59:39 177308:177308 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=256:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=256:   2%|▏         | 21/1000 [00:00<00:04, 205.45it/s]Measuring inference for batch_size=256:   4%|▍         | 42/1000 [00:00<00:04, 205.92it/s]Measuring inference for batch_size=256:   6%|▋         | 63/1000 [00:00<00:04, 206.15it/s]Measuring inference for batch_size=256:   8%|▊         | 84/1000 [00:00<00:04, 206.37it/s]Measuring inference for batch_size=256:  10%|█         | 105/1000 [00:00<00:04, 206.50it/s]Measuring inference for batch_size=256:  13%|█▎        | 126/1000 [00:00<00:04, 206.59it/s]Measuring inference for batch_size=256:  15%|█▍        | 147/1000 [00:00<00:04, 206.58it/s]Measuring inference for batch_size=256:  17%|█▋        | 168/1000 [00:00<00:04, 206.46it/s]Measuring inference for batch_size=256:  19%|█▉        | 189/1000 [00:00<00:03, 206.49it/s]Measuring inference for batch_size=256:  21%|██        | 210/1000 [00:01<00:03, 206.54it/s]Measuring inference for batch_size=256:  23%|██▎       | 231/1000 [00:01<00:03, 206.55it/s]Measuring inference for batch_size=256:  25%|██▌       | 252/1000 [00:01<00:03, 206.58it/s]Measuring inference for batch_size=256:  27%|██▋       | 273/1000 [00:01<00:03, 206.51it/s]Measuring inference for batch_size=256:  29%|██▉       | 294/1000 [00:01<00:03, 206.49it/s]Measuring inference for batch_size=256:  32%|███▏      | 315/1000 [00:01<00:03, 206.42it/s]Measuring inference for batch_size=256:  34%|███▎      | 336/1000 [00:01<00:03, 206.39it/s]Measuring inference for batch_size=256:  36%|███▌      | 357/1000 [00:01<00:03, 206.28it/s]Measuring inference for batch_size=256:  38%|███▊      | 378/1000 [00:01<00:03, 206.30it/s]Measuring inference for batch_size=256:  40%|███▉      | 399/1000 [00:01<00:02, 206.31it/s]Measuring inference for batch_size=256:  42%|████▏     | 420/1000 [00:02<00:02, 206.37it/s]Measuring inference for batch_size=256:  44%|████▍     | 441/1000 [00:02<00:02, 206.45it/s]Measuring inference for batch_size=256:  46%|████▌     | 462/1000 [00:02<00:02, 206.49it/s]Measuring inference for batch_size=256:  48%|████▊     | 483/1000 [00:02<00:02, 206.54it/s]Measuring inference for batch_size=256:  50%|█████     | 504/1000 [00:02<00:02, 206.52it/s]Measuring inference for batch_size=256:  52%|█████▎    | 525/1000 [00:02<00:02, 206.60it/s]Measuring inference for batch_size=256:  55%|█████▍    | 546/1000 [00:02<00:02, 206.58it/s]Measuring inference for batch_size=256:  57%|█████▋    | 567/1000 [00:02<00:02, 206.56it/s]Measuring inference for batch_size=256:  59%|█████▉    | 588/1000 [00:02<00:01, 206.48it/s]Measuring inference for batch_size=256:  61%|██████    | 609/1000 [00:02<00:01, 206.52it/s]Measuring inference for batch_size=256:  63%|██████▎   | 630/1000 [00:03<00:01, 206.56it/s]Measuring inference for batch_size=256:  65%|██████▌   | 651/1000 [00:03<00:01, 206.62it/s]Measuring inference for batch_size=256:  67%|██████▋   | 672/1000 [00:03<00:01, 206.62it/s]Measuring inference for batch_size=256:  69%|██████▉   | 693/1000 [00:03<00:01, 206.64it/s]Measuring inference for batch_size=256:  71%|███████▏  | 714/1000 [00:03<00:01, 206.65it/s]Measuring inference for batch_size=256:  74%|███████▎  | 735/1000 [00:03<00:01, 206.66it/s]Measuring inference for batch_size=256:  76%|███████▌  | 756/1000 [00:03<00:01, 206.66it/s]Measuring inference for batch_size=256:  78%|███████▊  | 777/1000 [00:03<00:01, 206.67it/s]Measuring inference for batch_size=256:  80%|███████▉  | 798/1000 [00:03<00:00, 206.64it/s]Measuring inference for batch_size=256:  82%|████████▏ | 819/1000 [00:03<00:00, 206.68it/s]Measuring inference for batch_size=256:  84%|████████▍ | 840/1000 [00:04<00:00, 206.65it/s]Measuring inference for batch_size=256:  86%|████████▌ | 861/1000 [00:04<00:00, 206.59it/s]Measuring inference for batch_size=256:  88%|████████▊ | 882/1000 [00:04<00:00, 206.21it/s]Measuring inference for batch_size=256:  90%|█████████ | 903/1000 [00:04<00:00, 205.98it/s]Measuring inference for batch_size=256:  92%|█████████▏| 924/1000 [00:04<00:00, 206.15it/s]Measuring inference for batch_size=256:  94%|█████████▍| 945/1000 [00:04<00:00, 203.97it/s]Measuring inference for batch_size=256:  97%|█████████▋| 966/1000 [00:04<00:00, 202.79it/s]Measuring inference for batch_size=256:  99%|█████████▊| 987/1000 [00:04<00:00, 202.14it/s]Measuring inference for batch_size=256: 100%|██████████| 1000/1000 [00:04<00:00, 205.95it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.53 GB
    total: 31.28 GB
    used: 3.28 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_256:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 91.080 us +/- 2.745 us [89.645 us, 145.674 us]
        batches_per_second: 10.99 K +/- 270.75 [6.86 K, 11.16 K]
      metrics:
        batches_per_second_max: 11155.063829787234
        batches_per_second_mean: 10987.363367336538
        batches_per_second_min: 6864.654664484452
        batches_per_second_std: 270.75388361335166
        seconds_per_batch_max: 0.0001456737518310547
        seconds_per_batch_mean: 9.10801887512207e-05
        seconds_per_batch_min: 8.96453857421875e-05
        seconds_per_batch_std: 2.7448976421525577e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 22.283 us +/- 0.599 us [21.696 us, 28.133 us]
        batches_per_second: 44.90 K +/- 1.03 K [35.54 K, 46.09 K]
      metrics:
        batches_per_second_max: 46091.25274725275
        batches_per_second_mean: 44904.30847785175
        batches_per_second_min: 35544.94915254237
        batches_per_second_std: 1033.0337610722966
        seconds_per_batch_max: 2.8133392333984375e-05
        seconds_per_batch_mean: 2.2283315658569334e-05
        seconds_per_batch_min: 2.1696090698242188e-05
        seconds_per_batch_std: 5.993100938244626e-07
    on_device_inference:
      human_readable:
        batch_latency: -4498819.848 us +/- 15.223 ms [-4785696.030 us, -4467616.081
          us]
        batches_per_second: -0.22 +/- 0.00 [-0.22, -0.21]
      metrics:
        batches_per_second_max: -0.2089560209845589
        batches_per_second_mean: -0.2222830028738068
        batches_per_second_min: -0.2238330200752033
        batches_per_second_std: 0.0007350301494047677
        seconds_per_batch_max: -4.467616081237793
        seconds_per_batch_mean: -4.49881984758377
        seconds_per_batch_min: -4.785696029663086
        seconds_per_batch_std: 0.015223292289949696
    total:
      human_readable:
        batch_latency: 4.618 ms +/- 17.097 us [4.586 ms, 4.974 ms]
        batches_per_second: 216.54 +/- 0.78 [201.04, 218.08]
      metrics:
        batches_per_second_max: 218.07851089273643
        batches_per_second_mean: 216.54210951143375
        batches_per_second_min: 201.04031059770887
        batches_per_second_std: 0.7757732204969103
        seconds_per_batch_max: 0.0049741268157958984
        seconds_per_batch_mean: 0.004618100643157959
        seconds_per_batch_min: 0.0045855045318603516
        seconds_per_batch_std: 1.7097053453261893e-05
  batch_size_256:
    cpu_to_gpu:
      human_readable:
        batch_latency: 142.421 us +/- 4.226 us [140.667 us, 246.048 us]
        batches_per_second: 7.03 K +/- 155.83 [4.06 K, 7.11 K]
      metrics:
        batches_per_second_max: 7108.989830508474
        batches_per_second_mean: 7025.946207440052
        batches_per_second_min: 4064.248062015504
        batches_per_second_std: 155.82623592475105
        seconds_per_batch_max: 0.0002460479736328125
        seconds_per_batch_mean: 0.00014242053031921387
        seconds_per_batch_min: 0.00014066696166992188
        seconds_per_batch_std: 4.225561393989826e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 22.397 us +/- 0.598 us [21.696 us, 27.895 us]
        batches_per_second: 44.68 K +/- 1.08 K [35.85 K, 46.09 K]
      metrics:
        batches_per_second_max: 46091.25274725275
        batches_per_second_mean: 44677.02186870424
        batches_per_second_min: 35848.75213675214
        batches_per_second_std: 1079.4610690424147
        seconds_per_batch_max: 2.7894973754882812e-05
        seconds_per_batch_mean: 2.2397279739379883e-05
        seconds_per_batch_min: 2.1696090698242188e-05
        seconds_per_batch_std: 5.984980233687828e-07
    on_device_inference:
      human_readable:
        batch_latency: -4681857.756 us +/- 47.567 ms [-5016704.082 us, -4640287.876
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.22, -0.20]
      metrics:
        batches_per_second_max: -0.19933406147883748
        batches_per_second_mean: -0.21361183633386607
        batches_per_second_min: -0.21550387103012736
        batches_per_second_std: 0.002107403753177075
        seconds_per_batch_max: -4.64028787612915
        seconds_per_batch_mean: -4.681857756137848
        seconds_per_batch_min: -5.016704082489014
        seconds_per_batch_std: 0.0475672128028503
    total:
      human_readable:
        batch_latency: 4.852 ms +/- 49.549 us [4.809 ms, 5.296 ms]
        batches_per_second: 206.12 +/- 2.04 [188.82, 207.93]
      metrics:
        batches_per_second_max: 207.92702756295856
        batches_per_second_mean: 206.11910191050802
        batches_per_second_min: 188.82204114707605
        batches_per_second_std: 2.0395110043416382
        seconds_per_batch_max: 0.005295991897583008
        seconds_per_batch_mean: 0.004852054119110108
        seconds_per_batch_min: 0.004809379577636719
        seconds_per_batch_std: 4.954937979098868e-05


#####
baseline-baseline-py-id - Run 2
2024-02-23 10:00:04
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 277.00it/s]Warming up with batch_size=1:  57%|█████▋    | 57/100 [00:00<00:00, 278.89it/s]Warming up with batch_size=1:  86%|████████▌ | 86/100 [00:00<00:00, 279.86it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 279.52it/s]
STAGE:2024-02-23 09:59:51 177355:177355 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:59:51 177355:177355 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:59:51 177355:177355 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:00<00:04, 209.70it/s]Measuring inference for batch_size=1:   4%|▍         | 43/1000 [00:00<00:04, 210.04it/s]Measuring inference for batch_size=1:   6%|▋         | 65/1000 [00:00<00:04, 210.12it/s]Measuring inference for batch_size=1:   9%|▊         | 87/1000 [00:00<00:04, 210.22it/s]Measuring inference for batch_size=1:  11%|█         | 109/1000 [00:00<00:04, 210.26it/s]Measuring inference for batch_size=1:  13%|█▎        | 131/1000 [00:00<00:04, 210.30it/s]Measuring inference for batch_size=1:  15%|█▌        | 153/1000 [00:00<00:04, 210.44it/s]Measuring inference for batch_size=1:  18%|█▊        | 175/1000 [00:00<00:03, 210.42it/s]Measuring inference for batch_size=1:  20%|█▉        | 197/1000 [00:00<00:03, 210.42it/s]Measuring inference for batch_size=1:  22%|██▏       | 219/1000 [00:01<00:03, 210.36it/s]Measuring inference for batch_size=1:  24%|██▍       | 241/1000 [00:01<00:03, 210.40it/s]Measuring inference for batch_size=1:  26%|██▋       | 263/1000 [00:01<00:03, 210.52it/s]Measuring inference for batch_size=1:  28%|██▊       | 285/1000 [00:01<00:03, 210.48it/s]Measuring inference for batch_size=1:  31%|███       | 307/1000 [00:01<00:03, 210.42it/s]Measuring inference for batch_size=1:  33%|███▎      | 329/1000 [00:01<00:03, 210.42it/s]Measuring inference for batch_size=1:  35%|███▌      | 351/1000 [00:01<00:03, 210.44it/s]Measuring inference for batch_size=1:  37%|███▋      | 373/1000 [00:01<00:02, 210.43it/s]Measuring inference for batch_size=1:  40%|███▉      | 395/1000 [00:01<00:02, 210.36it/s]Measuring inference for batch_size=1:  42%|████▏     | 417/1000 [00:01<00:02, 210.24it/s]Measuring inference for batch_size=1:  44%|████▍     | 439/1000 [00:02<00:02, 210.34it/s]Measuring inference for batch_size=1:  46%|████▌     | 461/1000 [00:02<00:02, 210.42it/s]Measuring inference for batch_size=1:  48%|████▊     | 483/1000 [00:02<00:02, 210.46it/s]Measuring inference for batch_size=1:  50%|█████     | 505/1000 [00:02<00:02, 210.39it/s]Measuring inference for batch_size=1:  53%|█████▎    | 527/1000 [00:02<00:02, 210.38it/s]Measuring inference for batch_size=1:  55%|█████▍    | 549/1000 [00:02<00:02, 210.40it/s]Measuring inference for batch_size=1:  57%|█████▋    | 571/1000 [00:02<00:02, 210.44it/s]Measuring inference for batch_size=1:  59%|█████▉    | 593/1000 [00:02<00:01, 210.49it/s]Measuring inference for batch_size=1:  62%|██████▏   | 615/1000 [00:02<00:01, 210.44it/s]Measuring inference for batch_size=1:  64%|██████▎   | 637/1000 [00:03<00:01, 210.47it/s]Measuring inference for batch_size=1:  66%|██████▌   | 659/1000 [00:03<00:01, 210.49it/s]Measuring inference for batch_size=1:  68%|██████▊   | 681/1000 [00:03<00:01, 210.52it/s]Measuring inference for batch_size=1:  70%|███████   | 703/1000 [00:03<00:01, 210.45it/s]Measuring inference for batch_size=1:  72%|███████▎  | 725/1000 [00:03<00:01, 210.44it/s]Measuring inference for batch_size=1:  75%|███████▍  | 747/1000 [00:03<00:01, 210.39it/s]Measuring inference for batch_size=1:  77%|███████▋  | 769/1000 [00:03<00:01, 210.44it/s]Measuring inference for batch_size=1:  79%|███████▉  | 791/1000 [00:03<00:00, 210.50it/s]Measuring inference for batch_size=1:  81%|████████▏ | 813/1000 [00:03<00:00, 210.54it/s]Measuring inference for batch_size=1:  84%|████████▎ | 835/1000 [00:03<00:00, 210.50it/s]Measuring inference for batch_size=1:  86%|████████▌ | 857/1000 [00:04<00:00, 210.47it/s]Measuring inference for batch_size=1:  88%|████████▊ | 879/1000 [00:04<00:00, 210.45it/s]Measuring inference for batch_size=1:  90%|█████████ | 901/1000 [00:04<00:00, 210.47it/s]Measuring inference for batch_size=1:  92%|█████████▏| 923/1000 [00:04<00:00, 210.44it/s]Measuring inference for batch_size=1:  94%|█████████▍| 945/1000 [00:04<00:00, 210.41it/s]Measuring inference for batch_size=1:  97%|█████████▋| 967/1000 [00:04<00:00, 210.40it/s]Measuring inference for batch_size=1:  99%|█████████▉| 989/1000 [00:04<00:00, 210.43it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 210.41it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=256:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=256:  21%|██        | 21/100 [00:00<00:00, 204.73it/s]Warming up with batch_size=256:  42%|████▏     | 42/100 [00:00<00:00, 204.88it/s]Warming up with batch_size=256:  63%|██████▎   | 63/100 [00:00<00:00, 205.06it/s]Warming up with batch_size=256:  84%|████████▍ | 84/100 [00:00<00:00, 204.99it/s]Warming up with batch_size=256: 100%|██████████| 100/100 [00:00<00:00, 204.97it/s]
STAGE:2024-02-23 09:59:57 177355:177355 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:59:57 177355:177355 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:59:57 177355:177355 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=256:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=256:   2%|▏         | 21/1000 [00:00<00:04, 200.78it/s]Measuring inference for batch_size=256:   4%|▍         | 42/1000 [00:00<00:04, 201.18it/s]Measuring inference for batch_size=256:   6%|▋         | 63/1000 [00:00<00:04, 201.44it/s]Measuring inference for batch_size=256:   8%|▊         | 84/1000 [00:00<00:04, 201.56it/s]Measuring inference for batch_size=256:  10%|█         | 105/1000 [00:00<00:04, 201.55it/s]Measuring inference for batch_size=256:  13%|█▎        | 126/1000 [00:00<00:04, 201.49it/s]Measuring inference for batch_size=256:  15%|█▍        | 147/1000 [00:00<00:04, 201.60it/s]Measuring inference for batch_size=256:  17%|█▋        | 168/1000 [00:00<00:04, 201.63it/s]Measuring inference for batch_size=256:  19%|█▉        | 189/1000 [00:00<00:04, 201.63it/s]Measuring inference for batch_size=256:  21%|██        | 210/1000 [00:01<00:03, 201.60it/s]Measuring inference for batch_size=256:  23%|██▎       | 231/1000 [00:01<00:03, 201.59it/s]Measuring inference for batch_size=256:  25%|██▌       | 252/1000 [00:01<00:03, 201.56it/s]Measuring inference for batch_size=256:  27%|██▋       | 273/1000 [00:01<00:03, 201.55it/s]Measuring inference for batch_size=256:  29%|██▉       | 294/1000 [00:01<00:03, 201.53it/s]Measuring inference for batch_size=256:  32%|███▏      | 315/1000 [00:01<00:03, 201.50it/s]Measuring inference for batch_size=256:  34%|███▎      | 336/1000 [00:01<00:03, 201.53it/s]Measuring inference for batch_size=256:  36%|███▌      | 357/1000 [00:01<00:03, 201.52it/s]Measuring inference for batch_size=256:  38%|███▊      | 378/1000 [00:01<00:03, 201.49it/s]Measuring inference for batch_size=256:  40%|███▉      | 399/1000 [00:01<00:02, 201.56it/s]Measuring inference for batch_size=256:  42%|████▏     | 420/1000 [00:02<00:02, 201.60it/s]Measuring inference for batch_size=256:  44%|████▍     | 441/1000 [00:02<00:02, 201.63it/s]Measuring inference for batch_size=256:  46%|████▌     | 462/1000 [00:02<00:02, 201.65it/s]Measuring inference for batch_size=256:  48%|████▊     | 483/1000 [00:02<00:02, 201.69it/s]Measuring inference for batch_size=256:  50%|█████     | 504/1000 [00:02<00:02, 201.74it/s]Measuring inference for batch_size=256:  52%|█████▎    | 525/1000 [00:02<00:02, 201.80it/s]Measuring inference for batch_size=256:  55%|█████▍    | 546/1000 [00:02<00:02, 201.88it/s]Measuring inference for batch_size=256:  57%|█████▋    | 567/1000 [00:02<00:02, 201.88it/s]Measuring inference for batch_size=256:  59%|█████▉    | 588/1000 [00:02<00:02, 201.85it/s]Measuring inference for batch_size=256:  61%|██████    | 609/1000 [00:03<00:01, 201.84it/s]Measuring inference for batch_size=256:  63%|██████▎   | 630/1000 [00:03<00:01, 201.73it/s]Measuring inference for batch_size=256:  65%|██████▌   | 651/1000 [00:03<00:01, 201.80it/s]Measuring inference for batch_size=256:  67%|██████▋   | 672/1000 [00:03<00:01, 201.83it/s]Measuring inference for batch_size=256:  69%|██████▉   | 693/1000 [00:03<00:01, 201.87it/s]Measuring inference for batch_size=256:  71%|███████▏  | 714/1000 [00:03<00:01, 201.88it/s]Measuring inference for batch_size=256:  74%|███████▎  | 735/1000 [00:03<00:01, 201.77it/s]Measuring inference for batch_size=256:  76%|███████▌  | 756/1000 [00:03<00:01, 201.81it/s]Measuring inference for batch_size=256:  78%|███████▊  | 777/1000 [00:03<00:01, 201.83it/s]Measuring inference for batch_size=256:  80%|███████▉  | 798/1000 [00:03<00:01, 201.81it/s]Measuring inference for batch_size=256:  82%|████████▏ | 819/1000 [00:04<00:00, 201.84it/s]Measuring inference for batch_size=256:  84%|████████▍ | 840/1000 [00:04<00:00, 201.80it/s]Measuring inference for batch_size=256:  86%|████████▌ | 861/1000 [00:04<00:00, 201.84it/s]Measuring inference for batch_size=256:  88%|████████▊ | 882/1000 [00:04<00:00, 201.87it/s]Measuring inference for batch_size=256:  90%|█████████ | 903/1000 [00:04<00:00, 201.84it/s]Measuring inference for batch_size=256:  92%|█████████▏| 924/1000 [00:04<00:00, 201.88it/s]Measuring inference for batch_size=256:  94%|█████████▍| 945/1000 [00:04<00:00, 201.80it/s]Measuring inference for batch_size=256:  97%|█████████▋| 966/1000 [00:04<00:00, 201.81it/s]Measuring inference for batch_size=256:  99%|█████████▊| 987/1000 [00:04<00:00, 201.79it/s]Measuring inference for batch_size=256: 100%|██████████| 1000/1000 [00:04<00:00, 201.70it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.54 GB
    total: 31.28 GB
    used: 3.28 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_256:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 91.713 us +/- 2.686 us [90.361 us, 143.290 us]
        batches_per_second: 10.91 K +/- 265.89 [6.98 K, 11.07 K]
      metrics:
        batches_per_second_max: 11066.765171503957
        batches_per_second_mean: 10911.263902110339
        batches_per_second_min: 6978.875207986689
        batches_per_second_std: 265.8941159394431
        seconds_per_batch_max: 0.00014328956604003906
        seconds_per_batch_mean: 9.171295166015626e-05
        seconds_per_batch_min: 9.036064147949219e-05
        seconds_per_batch_std: 2.6860412771123686e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 22.862 us +/- 0.506 us [22.173 us, 29.087 us]
        batches_per_second: 43.76 K +/- 841.31 [34.38 K, 45.10 K]
      metrics:
        batches_per_second_max: 45100.04301075269
        batches_per_second_mean: 43758.41133873884
        batches_per_second_min: 34379.54098360656
        batches_per_second_std: 841.3053039307514
        seconds_per_batch_max: 2.9087066650390625e-05
        seconds_per_batch_mean: 2.286243438720703e-05
        seconds_per_batch_min: 2.2172927856445312e-05
        seconds_per_batch_std: 5.060057884429945e-07
    on_device_inference:
      human_readable:
        batch_latency: -4628167.804 us +/- 13.582 ms [-4872672.081 us, -4601856.232
          us]
        batches_per_second: -0.22 +/- 0.00 [-0.22, -0.21]
      metrics:
        batches_per_second_max: -0.2052262051248227
        batches_per_second_mean: -0.21607005320520217
        batches_per_second_min: -0.2173036161177238
        batches_per_second_std: 0.0006233400455307968
        seconds_per_batch_max: -4.601856231689453
        seconds_per_batch_mean: -4.628167804241181
        seconds_per_batch_min: -4.872672080993652
        seconds_per_batch_std: 0.013581712944056783
    total:
      human_readable:
        batch_latency: 4.749 ms +/- 15.453 us [4.723 ms, 5.058 ms]
        batches_per_second: 210.56 +/- 0.67 [197.72, 211.75]
      metrics:
        batches_per_second_max: 211.7479806138934
        batches_per_second_mean: 210.56408425672433
        batches_per_second_min: 197.7232828925659
        batches_per_second_std: 0.6680335379876
        seconds_per_batch_max: 0.005057573318481445
        seconds_per_batch_mean: 0.004749197006225586
        seconds_per_batch_min: 0.00472259521484375
        seconds_per_batch_std: 1.5453048704276786e-05
  batch_size_256:
    cpu_to_gpu:
      human_readable:
        batch_latency: 142.565 us +/- 4.261 us [140.905 us, 251.770 us]
        batches_per_second: 7.02 K +/- 151.28 [3.97 K, 7.10 K]
      metrics:
        batches_per_second_max: 7096.961082910321
        batches_per_second_mean: 7018.707052788348
        batches_per_second_min: 3971.878787878788
        batches_per_second_std: 151.28133701457133
        seconds_per_batch_max: 0.00025177001953125
        seconds_per_batch_mean: 0.0001425652503967285
        seconds_per_batch_min: 0.00014090538024902344
        seconds_per_batch_std: 4.2608712467309905e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.139 us +/- 0.642 us [22.173 us, 33.140 us]
        batches_per_second: 43.25 K +/- 1.03 K [30.17 K, 45.10 K]
      metrics:
        batches_per_second_max: 45100.04301075269
        batches_per_second_mean: 43246.362635015335
        batches_per_second_min: 30174.84892086331
        batches_per_second_std: 1031.8022998269412
        seconds_per_batch_max: 3.314018249511719e-05
        seconds_per_batch_mean: 2.313852310180664e-05
        seconds_per_batch_min: 2.2172927856445312e-05
        seconds_per_batch_std: 6.418509781824019e-07
    on_device_inference:
      human_readable:
        batch_latency: -4783086.532 us +/- 16.132 ms [-5124544.144 us, -4756351.948
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.20]
      metrics:
        batches_per_second_max: -0.19513930838783253
        batches_per_second_mean: -0.20907232488760716
        batches_per_second_min: -0.2102451649873837
        batches_per_second_std: 0.000683582341293325
        seconds_per_batch_max: -4.756351947784424
        seconds_per_batch_mean: -4.783086532115936
        seconds_per_batch_min: -5.124544143676758
        seconds_per_batch_std: 0.01613184239168203
    total:
      human_readable:
        batch_latency: 4.954 ms +/- 19.176 us [4.926 ms, 5.410 ms]
        batches_per_second: 201.85 +/- 0.74 [184.84, 203.02]
      metrics:
        batches_per_second_max: 203.0156824782188
        batches_per_second_mean: 201.85311388448753
        batches_per_second_min: 184.83624184734708
        batches_per_second_std: 0.7439925276622736
        seconds_per_batch_max: 0.005410194396972656
        seconds_per_batch_mean: 0.004954168081283569
        seconds_per_batch_min: 0.004925727844238281
        seconds_per_batch_std: 1.9176160061378923e-05


#####
baseline-baseline-py-id - Run 3
2024-02-23 10:00:20
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 273.54it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:00<00:00, 274.35it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:00<00:00, 275.77it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 275.48it/s]
STAGE:2024-02-23 10:00:09 177401:177401 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 10:00:09 177401:177401 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 10:00:09 177401:177401 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:00<00:04, 206.55it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:00<00:04, 207.23it/s]Measuring inference for batch_size=1:   6%|▋         | 63/1000 [00:00<00:04, 207.51it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:00<00:04, 207.55it/s]Measuring inference for batch_size=1:  10%|█         | 105/1000 [00:00<00:04, 207.66it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:00<00:04, 207.76it/s]Measuring inference for batch_size=1:  15%|█▍        | 147/1000 [00:00<00:04, 207.75it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:00<00:04, 207.78it/s]Measuring inference for batch_size=1:  19%|█▉        | 189/1000 [00:00<00:03, 207.79it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:01<00:03, 207.74it/s]Measuring inference for batch_size=1:  23%|██▎       | 231/1000 [00:01<00:03, 207.79it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:01<00:03, 207.72it/s]Measuring inference for batch_size=1:  27%|██▋       | 273/1000 [00:01<00:03, 207.68it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:01<00:03, 207.73it/s]Measuring inference for batch_size=1:  32%|███▏      | 315/1000 [00:01<00:03, 207.75it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:01<00:03, 207.75it/s]Measuring inference for batch_size=1:  36%|███▌      | 357/1000 [00:01<00:03, 207.73it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:01<00:02, 207.71it/s]Measuring inference for batch_size=1:  40%|███▉      | 399/1000 [00:01<00:02, 207.66it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:02<00:02, 207.69it/s]Measuring inference for batch_size=1:  44%|████▍     | 441/1000 [00:02<00:02, 207.77it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:02<00:02, 207.68it/s]Measuring inference for batch_size=1:  48%|████▊     | 483/1000 [00:02<00:02, 207.66it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:02<00:02, 207.55it/s]Measuring inference for batch_size=1:  52%|█████▎    | 525/1000 [00:02<00:02, 207.54it/s]Measuring inference for batch_size=1:  55%|█████▍    | 546/1000 [00:02<00:02, 207.58it/s]Measuring inference for batch_size=1:  57%|█████▋    | 567/1000 [00:02<00:02, 207.66it/s]Measuring inference for batch_size=1:  59%|█████▉    | 588/1000 [00:02<00:01, 207.72it/s]Measuring inference for batch_size=1:  61%|██████    | 609/1000 [00:02<00:01, 207.73it/s]Measuring inference for batch_size=1:  63%|██████▎   | 630/1000 [00:03<00:01, 207.69it/s]Measuring inference for batch_size=1:  65%|██████▌   | 651/1000 [00:03<00:01, 207.69it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:03<00:01, 207.64it/s]Measuring inference for batch_size=1:  69%|██████▉   | 693/1000 [00:03<00:01, 207.59it/s]Measuring inference for batch_size=1:  71%|███████▏  | 714/1000 [00:03<00:01, 207.60it/s]Measuring inference for batch_size=1:  74%|███████▎  | 735/1000 [00:03<00:01, 207.65it/s]Measuring inference for batch_size=1:  76%|███████▌  | 756/1000 [00:03<00:01, 207.70it/s]Measuring inference for batch_size=1:  78%|███████▊  | 777/1000 [00:03<00:01, 207.64it/s]Measuring inference for batch_size=1:  80%|███████▉  | 798/1000 [00:03<00:00, 207.56it/s]Measuring inference for batch_size=1:  82%|████████▏ | 819/1000 [00:03<00:00, 207.44it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:04<00:00, 207.24it/s]Measuring inference for batch_size=1:  86%|████████▌ | 861/1000 [00:04<00:00, 207.24it/s]Measuring inference for batch_size=1:  88%|████████▊ | 882/1000 [00:04<00:00, 207.20it/s]Measuring inference for batch_size=1:  90%|█████████ | 903/1000 [00:04<00:00, 207.23it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [00:04<00:00, 207.21it/s]Measuring inference for batch_size=1:  94%|█████████▍| 945/1000 [00:04<00:00, 207.20it/s]Measuring inference for batch_size=1:  97%|█████████▋| 966/1000 [00:04<00:00, 207.06it/s]Measuring inference for batch_size=1:  99%|█████████▊| 987/1000 [00:04<00:00, 207.12it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 207.54it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=256:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=256:  21%|██        | 21/100 [00:00<00:00, 202.19it/s]Warming up with batch_size=256:  42%|████▏     | 42/100 [00:00<00:00, 202.71it/s]Warming up with batch_size=256:  63%|██████▎   | 63/100 [00:00<00:00, 202.80it/s]Warming up with batch_size=256:  84%|████████▍ | 84/100 [00:00<00:00, 202.78it/s]Warming up with batch_size=256: 100%|██████████| 100/100 [00:00<00:00, 202.68it/s]
STAGE:2024-02-23 10:00:15 177401:177401 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 10:00:15 177401:177401 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 10:00:15 177401:177401 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=256:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=256:   2%|▏         | 20/1000 [00:00<00:04, 198.79it/s]Measuring inference for batch_size=256:   4%|▍         | 40/1000 [00:00<00:04, 199.43it/s]Measuring inference for batch_size=256:   6%|▌         | 61/1000 [00:00<00:04, 199.78it/s]Measuring inference for batch_size=256:   8%|▊         | 82/1000 [00:00<00:04, 199.89it/s]Measuring inference for batch_size=256:  10%|█         | 103/1000 [00:00<00:04, 200.09it/s]Measuring inference for batch_size=256:  12%|█▏        | 124/1000 [00:00<00:04, 200.04it/s]Measuring inference for batch_size=256:  14%|█▍        | 145/1000 [00:00<00:04, 199.98it/s]Measuring inference for batch_size=256:  17%|█▋        | 166/1000 [00:00<00:04, 199.98it/s]Measuring inference for batch_size=256:  19%|█▊        | 186/1000 [00:00<00:04, 199.95it/s]Measuring inference for batch_size=256:  21%|██        | 207/1000 [00:01<00:03, 199.99it/s]Measuring inference for batch_size=256:  23%|██▎       | 228/1000 [00:01<00:03, 200.02it/s]Measuring inference for batch_size=256:  25%|██▍       | 249/1000 [00:01<00:03, 200.06it/s]Measuring inference for batch_size=256:  27%|██▋       | 270/1000 [00:01<00:03, 200.10it/s]Measuring inference for batch_size=256:  29%|██▉       | 291/1000 [00:01<00:03, 200.10it/s]Measuring inference for batch_size=256:  31%|███       | 312/1000 [00:01<00:03, 200.10it/s]Measuring inference for batch_size=256:  33%|███▎      | 333/1000 [00:01<00:03, 200.06it/s]Measuring inference for batch_size=256:  35%|███▌      | 354/1000 [00:01<00:03, 200.06it/s]Measuring inference for batch_size=256:  38%|███▊      | 375/1000 [00:01<00:03, 199.97it/s]Measuring inference for batch_size=256:  40%|███▉      | 395/1000 [00:01<00:03, 199.88it/s]Measuring inference for batch_size=256:  42%|████▏     | 416/1000 [00:02<00:02, 199.92it/s]Measuring inference for batch_size=256:  44%|████▎     | 436/1000 [00:02<00:02, 199.94it/s]Measuring inference for batch_size=256:  46%|████▌     | 457/1000 [00:02<00:02, 200.09it/s]Measuring inference for batch_size=256:  48%|████▊     | 478/1000 [00:02<00:02, 200.19it/s]Measuring inference for batch_size=256:  50%|████▉     | 499/1000 [00:02<00:02, 200.15it/s]Measuring inference for batch_size=256:  52%|█████▏    | 520/1000 [00:02<00:02, 200.17it/s]Measuring inference for batch_size=256:  54%|█████▍    | 541/1000 [00:02<00:02, 200.13it/s]Measuring inference for batch_size=256:  56%|█████▌    | 562/1000 [00:02<00:02, 200.12it/s]Measuring inference for batch_size=256:  58%|█████▊    | 583/1000 [00:02<00:02, 200.03it/s]Measuring inference for batch_size=256:  60%|██████    | 604/1000 [00:03<00:01, 199.96it/s]Measuring inference for batch_size=256:  62%|██████▎   | 625/1000 [00:03<00:01, 200.02it/s]Measuring inference for batch_size=256:  65%|██████▍   | 646/1000 [00:03<00:01, 200.06it/s]Measuring inference for batch_size=256:  67%|██████▋   | 667/1000 [00:03<00:01, 200.11it/s]Measuring inference for batch_size=256:  69%|██████▉   | 688/1000 [00:03<00:01, 200.19it/s]Measuring inference for batch_size=256:  71%|███████   | 709/1000 [00:03<00:01, 200.19it/s]Measuring inference for batch_size=256:  73%|███████▎  | 730/1000 [00:03<00:01, 200.27it/s]Measuring inference for batch_size=256:  75%|███████▌  | 751/1000 [00:03<00:01, 200.29it/s]Measuring inference for batch_size=256:  77%|███████▋  | 772/1000 [00:03<00:01, 200.26it/s]Measuring inference for batch_size=256:  79%|███████▉  | 793/1000 [00:03<00:01, 200.15it/s]Measuring inference for batch_size=256:  81%|████████▏ | 814/1000 [00:04<00:00, 200.00it/s]Measuring inference for batch_size=256:  84%|████████▎ | 835/1000 [00:04<00:00, 199.96it/s]Measuring inference for batch_size=256:  86%|████████▌ | 856/1000 [00:04<00:00, 200.01it/s]Measuring inference for batch_size=256:  88%|████████▊ | 877/1000 [00:04<00:00, 200.02it/s]Measuring inference for batch_size=256:  90%|████████▉ | 898/1000 [00:04<00:00, 200.08it/s]Measuring inference for batch_size=256:  92%|█████████▏| 919/1000 [00:04<00:00, 200.08it/s]Measuring inference for batch_size=256:  94%|█████████▍| 940/1000 [00:04<00:00, 200.13it/s]Measuring inference for batch_size=256:  96%|█████████▌| 961/1000 [00:04<00:00, 200.25it/s]Measuring inference for batch_size=256:  98%|█████████▊| 982/1000 [00:04<00:00, 200.08it/s]Measuring inference for batch_size=256: 100%|██████████| 1000/1000 [00:04<00:00, 200.04it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.53 GB
    total: 31.28 GB
    used: 3.29 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_256:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 92.213 us +/- 2.994 us [90.599 us, 144.958 us]
        batches_per_second: 10.85 K +/- 298.08 [6.90 K, 11.04 K]
      metrics:
        batches_per_second_max: 11037.642105263158
        batches_per_second_mean: 10853.966646606092
        batches_per_second_min: 6898.526315789473
        batches_per_second_std: 298.0848622470113
        seconds_per_batch_max: 0.00014495849609375
        seconds_per_batch_mean: 9.221339225769043e-05
        seconds_per_batch_min: 9.059906005859375e-05
        seconds_per_batch_std: 2.994375609855844e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.211 us +/- 0.639 us [22.411 us, 30.279 us]
        batches_per_second: 43.11 K +/- 1.01 K [33.03 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 43111.65231835871
        batches_per_second_min: 33026.015748031496
        batches_per_second_std: 1012.3107086592651
        seconds_per_batch_max: 3.0279159545898438e-05
        seconds_per_batch_mean: 2.3210525512695313e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 6.389482696013299e-07
    on_device_inference:
      human_readable:
        batch_latency: -4691959.003 us +/- 17.387 ms [-4951168.060 us, -4653728.008
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.20]
      metrics:
        batches_per_second_max: -0.20197254220024516
        batches_per_second_mean: -0.213133483665621
        batches_per_second_min: -0.214881488179557
        batches_per_second_std: 0.000780505666972617
        seconds_per_batch_max: -4.653728008270264
        seconds_per_batch_mean: -4.691959003448487
        seconds_per_batch_min: -4.951168060302734
        seconds_per_batch_std: 0.01738735077897884
    total:
      human_readable:
        batch_latency: 4.815 ms +/- 18.855 us [4.775 ms, 5.138 ms]
        batches_per_second: 207.70 +/- 0.80 [194.63, 209.44]
      metrics:
        batches_per_second_max: 209.4429241985419
        batches_per_second_mean: 207.70489552222222
        batches_per_second_min: 194.63127610208818
        batches_per_second_std: 0.7984044438691466
        seconds_per_batch_max: 0.005137920379638672
        seconds_per_batch_mean: 0.004814595460891724
        seconds_per_batch_min: 0.004774570465087891
        seconds_per_batch_std: 1.885457457905236e-05
  batch_size_256:
    cpu_to_gpu:
      human_readable:
        batch_latency: 142.706 us +/- 4.213 us [140.905 us, 248.432 us]
        batches_per_second: 7.01 K +/- 152.31 [4.03 K, 7.10 K]
      metrics:
        batches_per_second_max: 7096.961082910321
        batches_per_second_mean: 7011.776245408377
        batches_per_second_min: 4025.243761996161
        batches_per_second_std: 152.31311076916757
        seconds_per_batch_max: 0.0002484321594238281
        seconds_per_batch_mean: 0.00014270591735839844
        seconds_per_batch_min: 0.00014090538024902344
        seconds_per_batch_std: 4.212885949894521e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.323 us +/- 0.713 us [22.411 us, 31.710 us]
        batches_per_second: 42.91 K +/- 1.09 K [31.54 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 42909.14789019766
        batches_per_second_min: 31536.120300751878
        batches_per_second_std: 1091.8153671463667
        seconds_per_batch_max: 3.170967102050781e-05
        seconds_per_batch_mean: 2.332305908203125e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 7.13227256649256e-07
    on_device_inference:
      human_readable:
        batch_latency: -4823662.213 us +/- 16.863 ms [-5178495.884 us, -4790912.151
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.19]
      metrics:
        batches_per_second_max: -0.19310626529625483
        batches_per_second_mean: -0.2073138214947163
        batches_per_second_min: -0.20872851941586926
        batches_per_second_std: 0.0007029690081279861
        seconds_per_batch_max: -4.79091215133667
        seconds_per_batch_mean: -4.823662212848664
        seconds_per_batch_min: -5.17849588394165
        seconds_per_batch_std: 0.0168629469565846
    total:
      human_readable:
        batch_latency: 4.995 ms +/- 19.719 us [4.962 ms, 5.461 ms]
        batches_per_second: 200.20 +/- 0.75 [183.10, 201.51]
      metrics:
        batches_per_second_max: 201.5135966176612
        batches_per_second_mean: 200.195874394755
        batches_per_second_min: 183.1014100493299
        batches_per_second_std: 0.7530872116457658
        seconds_per_batch_max: 0.005461454391479492
        seconds_per_batch_mean: 0.004995182037353516
        seconds_per_batch_min: 0.004962444305419922
        seconds_per_batch_std: 1.9719068193493213e-05


