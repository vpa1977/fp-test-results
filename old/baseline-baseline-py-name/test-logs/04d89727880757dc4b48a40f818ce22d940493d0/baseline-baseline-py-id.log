#####
baseline-baseline-py-id - Run 1
2024-02-23 09:56:46
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.91it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.91it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 277.20it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:00<00:00, 277.48it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:00<00:00, 278.21it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 278.39it/s]
STAGE:2024-02-23 09:56:33 177028:177028 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:56:33 177028:177028 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:56:33 177028:177028 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 22/1000 [00:00<00:04, 210.47it/s]Measuring inference for batch_size=1:   4%|▍         | 44/1000 [00:00<00:04, 210.74it/s]Measuring inference for batch_size=1:   7%|▋         | 66/1000 [00:00<00:04, 210.92it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:00<00:04, 209.27it/s]Measuring inference for batch_size=1:  11%|█         | 109/1000 [00:00<00:04, 206.87it/s]Measuring inference for batch_size=1:  13%|█▎        | 130/1000 [00:00<00:04, 207.32it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:00<00:04, 208.62it/s]Measuring inference for batch_size=1:  17%|█▋        | 174/1000 [00:00<00:03, 209.42it/s]Measuring inference for batch_size=1:  20%|█▉        | 196/1000 [00:00<00:03, 209.90it/s]Measuring inference for batch_size=1:  22%|██▏       | 218/1000 [00:01<00:03, 210.18it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:01<00:03, 210.37it/s]Measuring inference for batch_size=1:  26%|██▌       | 262/1000 [00:01<00:03, 210.52it/s]Measuring inference for batch_size=1:  28%|██▊       | 284/1000 [00:01<00:03, 210.71it/s]Measuring inference for batch_size=1:  31%|███       | 306/1000 [00:01<00:03, 210.91it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:01<00:03, 211.00it/s]Measuring inference for batch_size=1:  35%|███▌      | 350/1000 [00:01<00:03, 211.00it/s]Measuring inference for batch_size=1:  37%|███▋      | 372/1000 [00:01<00:02, 211.05it/s]Measuring inference for batch_size=1:  39%|███▉      | 394/1000 [00:01<00:02, 211.02it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:01<00:02, 211.14it/s]Measuring inference for batch_size=1:  44%|████▍     | 438/1000 [00:02<00:02, 211.21it/s]Measuring inference for batch_size=1:  46%|████▌     | 460/1000 [00:02<00:02, 211.26it/s]Measuring inference for batch_size=1:  48%|████▊     | 482/1000 [00:02<00:02, 211.25it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:02<00:02, 211.29it/s]Measuring inference for batch_size=1:  53%|█████▎    | 526/1000 [00:02<00:02, 211.38it/s]Measuring inference for batch_size=1:  55%|█████▍    | 548/1000 [00:02<00:02, 211.35it/s]Measuring inference for batch_size=1:  57%|█████▋    | 570/1000 [00:02<00:02, 211.44it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [00:02<00:01, 211.37it/s]Measuring inference for batch_size=1:  61%|██████▏   | 614/1000 [00:02<00:01, 211.34it/s]Measuring inference for batch_size=1:  64%|██████▎   | 636/1000 [00:03<00:01, 211.32it/s]Measuring inference for batch_size=1:  66%|██████▌   | 658/1000 [00:03<00:01, 211.32it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [00:03<00:01, 211.27it/s]Measuring inference for batch_size=1:  70%|███████   | 702/1000 [00:03<00:01, 211.26it/s]Measuring inference for batch_size=1:  72%|███████▏  | 724/1000 [00:03<00:01, 211.26it/s]Measuring inference for batch_size=1:  75%|███████▍  | 746/1000 [00:03<00:01, 211.34it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [00:03<00:01, 211.30it/s]Measuring inference for batch_size=1:  79%|███████▉  | 790/1000 [00:03<00:00, 211.29it/s]Measuring inference for batch_size=1:  81%|████████  | 812/1000 [00:03<00:00, 211.25it/s]Measuring inference for batch_size=1:  83%|████████▎ | 834/1000 [00:03<00:00, 211.17it/s]Measuring inference for batch_size=1:  86%|████████▌ | 856/1000 [00:04<00:00, 211.16it/s]Measuring inference for batch_size=1:  88%|████████▊ | 878/1000 [00:04<00:00, 211.17it/s]Measuring inference for batch_size=1:  90%|█████████ | 900/1000 [00:04<00:00, 211.18it/s]Measuring inference for batch_size=1:  92%|█████████▏| 922/1000 [00:04<00:00, 211.17it/s]Measuring inference for batch_size=1:  94%|█████████▍| 944/1000 [00:04<00:00, 208.62it/s]Measuring inference for batch_size=1:  96%|█████████▋| 965/1000 [00:04<00:00, 206.90it/s]Measuring inference for batch_size=1:  99%|█████████▊| 986/1000 [00:04<00:00, 205.63it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 210.15it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=64:  21%|██        | 21/100 [00:00<00:00, 204.94it/s]Warming up with batch_size=64:  42%|████▏     | 42/100 [00:00<00:00, 205.17it/s]Warming up with batch_size=64:  63%|██████▎   | 63/100 [00:00<00:00, 205.38it/s]Warming up with batch_size=64:  84%|████████▍ | 84/100 [00:00<00:00, 205.42it/s]Warming up with batch_size=64: 100%|██████████| 100/100 [00:00<00:00, 205.36it/s]
STAGE:2024-02-23 09:56:38 177028:177028 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:56:38 177028:177028 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:56:38 177028:177028 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=64:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=64:   2%|▏         | 21/1000 [00:00<00:04, 201.40it/s]Measuring inference for batch_size=64:   4%|▍         | 42/1000 [00:00<00:04, 201.91it/s]Measuring inference for batch_size=64:   6%|▋         | 63/1000 [00:00<00:04, 202.20it/s]Measuring inference for batch_size=64:   8%|▊         | 84/1000 [00:00<00:04, 202.25it/s]Measuring inference for batch_size=64:  10%|█         | 105/1000 [00:00<00:04, 202.32it/s]Measuring inference for batch_size=64:  13%|█▎        | 126/1000 [00:00<00:04, 202.43it/s]Measuring inference for batch_size=64:  15%|█▍        | 147/1000 [00:00<00:04, 202.36it/s]Measuring inference for batch_size=64:  17%|█▋        | 168/1000 [00:00<00:04, 202.38it/s]Measuring inference for batch_size=64:  19%|█▉        | 189/1000 [00:00<00:04, 202.49it/s]Measuring inference for batch_size=64:  21%|██        | 210/1000 [00:01<00:03, 202.49it/s]Measuring inference for batch_size=64:  23%|██▎       | 231/1000 [00:01<00:03, 202.32it/s]Measuring inference for batch_size=64:  25%|██▌       | 252/1000 [00:01<00:03, 202.30it/s]Measuring inference for batch_size=64:  27%|██▋       | 273/1000 [00:01<00:03, 202.26it/s]Measuring inference for batch_size=64:  29%|██▉       | 294/1000 [00:01<00:03, 202.21it/s]Measuring inference for batch_size=64:  32%|███▏      | 315/1000 [00:01<00:03, 202.15it/s]Measuring inference for batch_size=64:  34%|███▎      | 336/1000 [00:01<00:03, 202.05it/s]Measuring inference for batch_size=64:  36%|███▌      | 357/1000 [00:01<00:03, 202.07it/s]Measuring inference for batch_size=64:  38%|███▊      | 378/1000 [00:01<00:03, 202.08it/s]Measuring inference for batch_size=64:  40%|███▉      | 399/1000 [00:01<00:02, 202.15it/s]Measuring inference for batch_size=64:  42%|████▏     | 420/1000 [00:02<00:02, 202.18it/s]Measuring inference for batch_size=64:  44%|████▍     | 441/1000 [00:02<00:02, 202.23it/s]Measuring inference for batch_size=64:  46%|████▌     | 462/1000 [00:02<00:02, 202.22it/s]Measuring inference for batch_size=64:  48%|████▊     | 483/1000 [00:02<00:02, 202.22it/s]Measuring inference for batch_size=64:  50%|█████     | 504/1000 [00:02<00:02, 202.20it/s]Measuring inference for batch_size=64:  52%|█████▎    | 525/1000 [00:02<00:02, 202.19it/s]Measuring inference for batch_size=64:  55%|█████▍    | 546/1000 [00:02<00:02, 202.19it/s]Measuring inference for batch_size=64:  57%|█████▋    | 567/1000 [00:02<00:02, 202.16it/s]Measuring inference for batch_size=64:  59%|█████▉    | 588/1000 [00:02<00:02, 202.12it/s]Measuring inference for batch_size=64:  61%|██████    | 609/1000 [00:03<00:01, 202.18it/s]Measuring inference for batch_size=64:  63%|██████▎   | 630/1000 [00:03<00:01, 202.26it/s]Measuring inference for batch_size=64:  65%|██████▌   | 651/1000 [00:03<00:01, 202.22it/s]Measuring inference for batch_size=64:  67%|██████▋   | 672/1000 [00:03<00:01, 202.19it/s]Measuring inference for batch_size=64:  69%|██████▉   | 693/1000 [00:03<00:01, 202.22it/s]Measuring inference for batch_size=64:  71%|███████▏  | 714/1000 [00:03<00:01, 202.23it/s]Measuring inference for batch_size=64:  74%|███████▎  | 735/1000 [00:03<00:01, 202.29it/s]Measuring inference for batch_size=64:  76%|███████▌  | 756/1000 [00:03<00:01, 202.30it/s]Measuring inference for batch_size=64:  78%|███████▊  | 777/1000 [00:03<00:01, 202.31it/s]Measuring inference for batch_size=64:  80%|███████▉  | 798/1000 [00:03<00:00, 202.36it/s]Measuring inference for batch_size=64:  82%|████████▏ | 819/1000 [00:04<00:00, 202.34it/s]Measuring inference for batch_size=64:  84%|████████▍ | 840/1000 [00:04<00:00, 202.29it/s]Measuring inference for batch_size=64:  86%|████████▌ | 861/1000 [00:04<00:00, 201.67it/s]Measuring inference for batch_size=64:  88%|████████▊ | 882/1000 [00:04<00:00, 201.46it/s]Measuring inference for batch_size=64:  90%|█████████ | 903/1000 [00:04<00:00, 201.82it/s]Measuring inference for batch_size=64:  92%|█████████▏| 924/1000 [00:04<00:00, 202.18it/s]Measuring inference for batch_size=64:  94%|█████████▍| 945/1000 [00:04<00:00, 202.42it/s]Measuring inference for batch_size=64:  97%|█████████▋| 966/1000 [00:04<00:00, 202.57it/s]Measuring inference for batch_size=64:  99%|█████████▊| 987/1000 [00:04<00:00, 202.60it/s]Measuring inference for batch_size=64: 100%|██████████| 1000/1000 [00:04<00:00, 202.24it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.53 GB
    total: 31.28 GB
    used: 3.29 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_64:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 94.455 us +/- 3.531 us [92.506 us, 148.535 us]
        batches_per_second: 10.60 K +/- 326.63 [6.73 K, 10.81 K]
      metrics:
        batches_per_second_max: 10810.061855670103
        batches_per_second_mean: 10599.144683587407
        batches_per_second_min: 6732.430176565008
        batches_per_second_std: 326.6346147832784
        seconds_per_batch_max: 0.00014853477478027344
        seconds_per_batch_mean: 9.445476531982422e-05
        seconds_per_batch_min: 9.250640869140625e-05
        seconds_per_batch_std: 3.5308851078858544e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.000 us +/- 0.454 us [22.173 us, 28.849 us]
        batches_per_second: 43.49 K +/- 793.04 [34.66 K, 45.10 K]
      metrics:
        batches_per_second_max: 45100.04301075269
        batches_per_second_mean: 43494.31734395379
        batches_per_second_min: 34663.669421487604
        batches_per_second_std: 793.0359588350011
        seconds_per_batch_max: 2.8848648071289062e-05
        seconds_per_batch_mean: 2.2999763488769532e-05
        seconds_per_batch_min: 2.2172927856445312e-05
        seconds_per_batch_std: 4.541843269024368e-07
    on_device_inference:
      human_readable:
        batch_latency: -4630892.158 us +/- 64.394 ms [-4850143.909 us, -4570528.030
          us]
        batches_per_second: -0.22 +/- 0.00 [-0.22, -0.21]
      metrics:
        batches_per_second_max: -0.2061794492428788
        batches_per_second_mean: -0.21598160505485012
        batches_per_second_min: -0.21879310078609573
        batches_per_second_std: 0.0029121882562736087
        seconds_per_batch_max: -4.570528030395508
        seconds_per_batch_mean: -4.6308921575546265
        seconds_per_batch_min: -4.850143909454346
        seconds_per_batch_std: 0.06439410577132168
    total:
      human_readable:
        batch_latency: 4.755 ms +/- 65.488 us [4.693 ms, 5.039 ms]
        batches_per_second: 210.34 +/- 2.81 [198.45, 213.10]
      metrics:
        batches_per_second_max: 213.10354638756223
        batches_per_second_mean: 210.34417928928067
        batches_per_second_min: 198.45299266619352
        batches_per_second_std: 2.8096025486675016
        seconds_per_batch_max: 0.0050389766693115234
        seconds_per_batch_mean: 0.004754987716674805
        seconds_per_batch_min: 0.004692554473876953
        seconds_per_batch_std: 6.548837832000513e-05
  batch_size_64:
    cpu_to_gpu:
      human_readable:
        batch_latency: 146.500 us +/- 4.795 us [144.482 us, 244.617 us]
        batches_per_second: 6.83 K +/- 178.66 [4.09 K, 6.92 K]
      metrics:
        batches_per_second_max: 6921.293729372937
        batches_per_second_mean: 6831.653016037277
        batches_per_second_min: 4088.01559454191
        batches_per_second_std: 178.66115737588441
        seconds_per_batch_max: 0.0002446174621582031
        seconds_per_batch_mean: 0.0001465003490447998
        seconds_per_batch_min: 0.00014448165893554688
        seconds_per_batch_std: 4.795076638198865e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.181 us +/- 0.565 us [22.411 us, 30.756 us]
        batches_per_second: 43.16 K +/- 932.40 [32.51 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 43161.451247986246
        batches_per_second_min: 32513.98449612403
        batches_per_second_std: 932.4027981987745
        seconds_per_batch_max: 3.075599670410156e-05
        seconds_per_batch_mean: 2.3180961608886717e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 5.650075661590732e-07
    on_device_inference:
      human_readable:
        batch_latency: -4766013.502 us +/- 23.658 ms [-5096864.223 us, -4730271.816
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.20]
      metrics:
        batches_per_second_max: -0.19619906596554051
        batches_per_second_mean: -0.20982397932805702
        batches_per_second_min: -0.21140434183166923
        batches_per_second_std: 0.0010112597498780971
        seconds_per_batch_max: -4.730271816253662
        seconds_per_batch_mean: -4.766013501644134
        seconds_per_batch_min: -5.096864223480225
        seconds_per_batch_std: 0.023658335344919932
    total:
      human_readable:
        batch_latency: 4.941 ms +/- 25.981 us [4.910 ms, 5.376 ms]
        batches_per_second: 202.39 +/- 1.03 [186.02, 203.68]
      metrics:
        batches_per_second_max: 203.67620065070656
        batches_per_second_mean: 202.38947790875636
        batches_per_second_min: 186.01667553663296
        batches_per_second_std: 1.0252934351798897
        seconds_per_batch_max: 0.005375862121582031
        seconds_per_batch_mean: 0.004941099882125854
        seconds_per_batch_min: 0.0049097537994384766
        seconds_per_batch_std: 2.5981433567210472e-05


#####
baseline-baseline-py-id - Run 2
2024-02-23 09:57:04
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 273.52it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:00<00:00, 275.00it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:00<00:00, 276.31it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 276.09it/s]
STAGE:2024-02-23 09:56:51 177074:177074 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:56:51 177074:177074 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:56:51 177074:177074 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:00<00:04, 208.05it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:00<00:04, 208.46it/s]Measuring inference for batch_size=1:   6%|▋         | 63/1000 [00:00<00:04, 208.61it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:00<00:04, 208.67it/s]Measuring inference for batch_size=1:  10%|█         | 105/1000 [00:00<00:04, 208.59it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:00<00:04, 208.76it/s]Measuring inference for batch_size=1:  15%|█▍        | 147/1000 [00:00<00:04, 208.74it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:00<00:03, 208.86it/s]Measuring inference for batch_size=1:  19%|█▉        | 189/1000 [00:00<00:03, 208.98it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:01<00:03, 209.00it/s]Measuring inference for batch_size=1:  23%|██▎       | 231/1000 [00:01<00:03, 209.01it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:01<00:03, 209.07it/s]Measuring inference for batch_size=1:  27%|██▋       | 273/1000 [00:01<00:03, 209.07it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:01<00:03, 209.03it/s]Measuring inference for batch_size=1:  32%|███▏      | 315/1000 [00:01<00:03, 209.03it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:01<00:03, 209.01it/s]Measuring inference for batch_size=1:  36%|███▌      | 357/1000 [00:01<00:03, 209.03it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:01<00:02, 209.06it/s]Measuring inference for batch_size=1:  40%|███▉      | 399/1000 [00:01<00:02, 209.11it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:02<00:02, 209.15it/s]Measuring inference for batch_size=1:  44%|████▍     | 441/1000 [00:02<00:02, 209.12it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:02<00:02, 209.18it/s]Measuring inference for batch_size=1:  48%|████▊     | 483/1000 [00:02<00:02, 209.21it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:02<00:02, 209.22it/s]Measuring inference for batch_size=1:  52%|█████▎    | 525/1000 [00:02<00:02, 209.26it/s]Measuring inference for batch_size=1:  55%|█████▍    | 546/1000 [00:02<00:02, 209.27it/s]Measuring inference for batch_size=1:  57%|█████▋    | 567/1000 [00:02<00:02, 209.15it/s]Measuring inference for batch_size=1:  59%|█████▉    | 588/1000 [00:02<00:01, 209.21it/s]Measuring inference for batch_size=1:  61%|██████    | 609/1000 [00:02<00:01, 209.26it/s]Measuring inference for batch_size=1:  63%|██████▎   | 630/1000 [00:03<00:01, 209.29it/s]Measuring inference for batch_size=1:  65%|██████▌   | 651/1000 [00:03<00:01, 209.34it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:03<00:01, 209.28it/s]Measuring inference for batch_size=1:  69%|██████▉   | 693/1000 [00:03<00:01, 209.35it/s]Measuring inference for batch_size=1:  71%|███████▏  | 714/1000 [00:03<00:01, 209.36it/s]Measuring inference for batch_size=1:  74%|███████▎  | 735/1000 [00:03<00:01, 209.37it/s]Measuring inference for batch_size=1:  76%|███████▌  | 756/1000 [00:03<00:01, 209.36it/s]Measuring inference for batch_size=1:  78%|███████▊  | 777/1000 [00:03<00:01, 209.41it/s]Measuring inference for batch_size=1:  80%|███████▉  | 798/1000 [00:03<00:00, 209.40it/s]Measuring inference for batch_size=1:  82%|████████▏ | 819/1000 [00:03<00:00, 209.42it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:04<00:00, 209.41it/s]Measuring inference for batch_size=1:  86%|████████▌ | 861/1000 [00:04<00:00, 209.41it/s]Measuring inference for batch_size=1:  88%|████████▊ | 882/1000 [00:04<00:00, 209.39it/s]Measuring inference for batch_size=1:  90%|█████████ | 903/1000 [00:04<00:00, 209.40it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [00:04<00:00, 209.41it/s]Measuring inference for batch_size=1:  94%|█████████▍| 945/1000 [00:04<00:00, 209.39it/s]Measuring inference for batch_size=1:  97%|█████████▋| 966/1000 [00:04<00:00, 209.37it/s]Measuring inference for batch_size=1:  99%|█████████▊| 987/1000 [00:04<00:00, 209.32it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 209.17it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=64:  21%|██        | 21/100 [00:00<00:00, 203.17it/s]Warming up with batch_size=64:  42%|████▏     | 42/100 [00:00<00:00, 203.52it/s]Warming up with batch_size=64:  63%|██████▎   | 63/100 [00:00<00:00, 203.70it/s]Warming up with batch_size=64:  84%|████████▍ | 84/100 [00:00<00:00, 203.69it/s]Warming up with batch_size=64: 100%|██████████| 100/100 [00:00<00:00, 203.64it/s]
STAGE:2024-02-23 09:56:57 177074:177074 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:56:57 177074:177074 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:56:57 177074:177074 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=64:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=64:   2%|▏         | 21/1000 [00:00<00:04, 200.40it/s]Measuring inference for batch_size=64:   4%|▍         | 42/1000 [00:00<00:04, 200.88it/s]Measuring inference for batch_size=64:   6%|▋         | 63/1000 [00:00<00:04, 201.11it/s]Measuring inference for batch_size=64:   8%|▊         | 84/1000 [00:00<00:04, 201.26it/s]Measuring inference for batch_size=64:  10%|█         | 105/1000 [00:00<00:04, 201.41it/s]Measuring inference for batch_size=64:  13%|█▎        | 126/1000 [00:00<00:04, 201.21it/s]Measuring inference for batch_size=64:  15%|█▍        | 147/1000 [00:00<00:04, 201.21it/s]Measuring inference for batch_size=64:  17%|█▋        | 168/1000 [00:00<00:04, 201.16it/s]Measuring inference for batch_size=64:  19%|█▉        | 189/1000 [00:00<00:04, 201.18it/s]Measuring inference for batch_size=64:  21%|██        | 210/1000 [00:01<00:03, 201.23it/s]Measuring inference for batch_size=64:  23%|██▎       | 231/1000 [00:01<00:03, 201.24it/s]Measuring inference for batch_size=64:  25%|██▌       | 252/1000 [00:01<00:03, 201.25it/s]Measuring inference for batch_size=64:  27%|██▋       | 273/1000 [00:01<00:03, 201.26it/s]Measuring inference for batch_size=64:  29%|██▉       | 294/1000 [00:01<00:03, 201.12it/s]Measuring inference for batch_size=64:  32%|███▏      | 315/1000 [00:01<00:03, 201.12it/s]Measuring inference for batch_size=64:  34%|███▎      | 336/1000 [00:01<00:03, 201.18it/s]Measuring inference for batch_size=64:  36%|███▌      | 357/1000 [00:01<00:03, 201.22it/s]Measuring inference for batch_size=64:  38%|███▊      | 378/1000 [00:01<00:03, 201.31it/s]Measuring inference for batch_size=64:  40%|███▉      | 399/1000 [00:01<00:02, 201.30it/s]Measuring inference for batch_size=64:  42%|████▏     | 420/1000 [00:02<00:02, 201.32it/s]Measuring inference for batch_size=64:  44%|████▍     | 441/1000 [00:02<00:02, 201.34it/s]Measuring inference for batch_size=64:  46%|████▌     | 462/1000 [00:02<00:02, 201.40it/s]Measuring inference for batch_size=64:  48%|████▊     | 483/1000 [00:02<00:02, 201.41it/s]Measuring inference for batch_size=64:  50%|█████     | 504/1000 [00:02<00:02, 201.40it/s]Measuring inference for batch_size=64:  52%|█████▎    | 525/1000 [00:02<00:02, 201.39it/s]Measuring inference for batch_size=64:  55%|█████▍    | 546/1000 [00:02<00:02, 201.41it/s]Measuring inference for batch_size=64:  57%|█████▋    | 567/1000 [00:02<00:02, 201.39it/s]Measuring inference for batch_size=64:  59%|█████▉    | 588/1000 [00:02<00:02, 201.33it/s]Measuring inference for batch_size=64:  61%|██████    | 609/1000 [00:03<00:01, 201.35it/s]Measuring inference for batch_size=64:  63%|██████▎   | 630/1000 [00:03<00:01, 201.36it/s]Measuring inference for batch_size=64:  65%|██████▌   | 651/1000 [00:03<00:01, 201.35it/s]Measuring inference for batch_size=64:  67%|██████▋   | 672/1000 [00:03<00:01, 201.37it/s]Measuring inference for batch_size=64:  69%|██████▉   | 693/1000 [00:03<00:01, 201.34it/s]Measuring inference for batch_size=64:  71%|███████▏  | 714/1000 [00:03<00:01, 201.32it/s]Measuring inference for batch_size=64:  74%|███████▎  | 735/1000 [00:03<00:01, 201.32it/s]Measuring inference for batch_size=64:  76%|███████▌  | 756/1000 [00:03<00:01, 201.35it/s]Measuring inference for batch_size=64:  78%|███████▊  | 777/1000 [00:03<00:01, 201.41it/s]Measuring inference for batch_size=64:  80%|███████▉  | 798/1000 [00:03<00:01, 201.38it/s]Measuring inference for batch_size=64:  82%|████████▏ | 819/1000 [00:04<00:00, 201.43it/s]Measuring inference for batch_size=64:  84%|████████▍ | 840/1000 [00:04<00:00, 201.46it/s]Measuring inference for batch_size=64:  86%|████████▌ | 861/1000 [00:04<00:00, 201.50it/s]Measuring inference for batch_size=64:  88%|████████▊ | 882/1000 [00:04<00:00, 201.46it/s]Measuring inference for batch_size=64:  90%|█████████ | 903/1000 [00:04<00:00, 201.46it/s]Measuring inference for batch_size=64:  92%|█████████▏| 924/1000 [00:04<00:00, 201.48it/s]Measuring inference for batch_size=64:  94%|█████████▍| 945/1000 [00:04<00:00, 201.48it/s]Measuring inference for batch_size=64:  97%|█████████▋| 966/1000 [00:04<00:00, 201.48it/s]Measuring inference for batch_size=64:  99%|█████████▊| 987/1000 [00:04<00:00, 201.50it/s]Measuring inference for batch_size=64: 100%|██████████| 1000/1000 [00:04<00:00, 201.33it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.54 GB
    total: 31.28 GB
    used: 3.28 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_64:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 92.400 us +/- 3.120 us [90.599 us, 145.197 us]
        batches_per_second: 10.83 K +/- 311.07 [6.89 K, 11.04 K]
      metrics:
        batches_per_second_max: 11037.642105263158
        batches_per_second_mean: 10832.87917215477
        batches_per_second_min: 6887.1986863711
        batches_per_second_std: 311.0665366556182
        seconds_per_batch_max: 0.00014519691467285156
        seconds_per_batch_mean: 9.240007400512695e-05
        seconds_per_batch_min: 9.059906005859375e-05
        seconds_per_batch_std: 3.12032526740734e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.202 us +/- 0.568 us [22.411 us, 29.325 us]
        batches_per_second: 43.12 K +/- 937.52 [34.10 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 43122.694090744604
        batches_per_second_min: 34100.03252032521
        batches_per_second_std: 937.5210748913828
        seconds_per_batch_max: 2.9325485229492188e-05
        seconds_per_batch_mean: 2.3201942443847657e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 5.676237165194701e-07
    on_device_inference:
      human_readable:
        batch_latency: -4654714.694 us +/- 13.933 ms [-4886879.921 us, -4623968.124
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.22, -0.20]
      metrics:
        batches_per_second_max: -0.20462954199285166
        batches_per_second_mean: -0.21483783710562904
        batches_per_second_min: -0.2162644666007505
        batches_per_second_std: 0.0006343934428290409
        seconds_per_batch_max: -4.623968124389648
        seconds_per_batch_mean: -4.6547146940231325
        seconds_per_batch_min: -4.886879920959473
        seconds_per_batch_std: 0.013933365966455075
    total:
      human_readable:
        batch_latency: 4.777 ms +/- 15.647 us [4.746 ms, 5.074 ms]
        batches_per_second: 209.33 +/- 0.67 [197.07, 210.72]
      metrics:
        batches_per_second_max: 210.71610148203968
        batches_per_second_mean: 209.32871926229458
        batches_per_second_min: 197.07296903632007
        batches_per_second_std: 0.6709800785967277
        seconds_per_batch_max: 0.005074262619018555
        seconds_per_batch_mean: 0.004777225494384765
        seconds_per_batch_min: 0.0047457218170166016
        seconds_per_batch_std: 1.564665317660301e-05
  batch_size_64:
    cpu_to_gpu:
      human_readable:
        batch_latency: 142.956 us +/- 4.178 us [141.144 us, 244.856 us]
        batches_per_second: 7.00 K +/- 153.60 [4.08 K, 7.08 K]
      metrics:
        batches_per_second_max: 7084.972972972973
        batches_per_second_mean: 6999.509661663827
        batches_per_second_min: 4084.035053554041
        batches_per_second_std: 153.59766589113866
        seconds_per_batch_max: 0.0002448558807373047
        seconds_per_batch_mean: 0.00014295625686645507
        seconds_per_batch_min: 0.000141143798828125
        seconds_per_batch_std: 4.178435048139224e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.346 us +/- 0.659 us [22.411 us, 29.087 us]
        batches_per_second: 42.86 K +/- 1.07 K [34.38 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 42863.63199762787
        batches_per_second_min: 34379.54098360656
        batches_per_second_std: 1069.0997205764015
        seconds_per_batch_max: 2.9087066650390625e-05
        seconds_per_batch_mean: 2.3346185684204103e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 6.592072799407875e-07
    on_device_inference:
      human_readable:
        batch_latency: -4791639.868 us +/- 14.566 ms [-5121503.830 us, -4762656.212
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.20]
      metrics:
        batches_per_second_max: -0.19525515028436102
        batches_per_second_mean: -0.20869868126954258
        batches_per_second_min: -0.2099668662859303
        batches_per_second_std: 0.0006128189778559413
        seconds_per_batch_max: -4.762656211853027
        seconds_per_batch_mean: -4.791639867782592
        seconds_per_batch_min: -5.121503829956055
        seconds_per_batch_std: 0.014565597824505085
    total:
      human_readable:
        batch_latency: 4.963 ms +/- 17.450 us [4.934 ms, 5.401 ms]
        batches_per_second: 201.48 +/- 0.67 [185.14, 202.68]
      metrics:
        batches_per_second_max: 202.68213008601526
        batches_per_second_mean: 201.48007369278022
        batches_per_second_min: 185.13811520635622
        batches_per_second_std: 0.6719333297555438
        seconds_per_batch_max: 0.0054013729095458984
        seconds_per_batch_mean: 0.004963328123092651
        seconds_per_batch_min: 0.004933834075927734
        seconds_per_batch_std: 1.74501050867313e-05


#####
baseline-baseline-py-id - Run 3
2024-02-23 09:57:20
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.48it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.48it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 274.52it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:00<00:00, 269.21it/s]Warming up with batch_size=1:  83%|████████▎ | 83/100 [00:00<00:00, 268.04it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 268.49it/s]
STAGE:2024-02-23 09:57:09 177120:177120 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:57:09 177120:177120 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:57:09 177120:177120 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:00<00:04, 207.95it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:00<00:04, 208.46it/s]Measuring inference for batch_size=1:   6%|▋         | 63/1000 [00:00<00:04, 208.51it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:00<00:04, 208.50it/s]Measuring inference for batch_size=1:  10%|█         | 105/1000 [00:00<00:04, 208.51it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:00<00:04, 208.48it/s]Measuring inference for batch_size=1:  15%|█▍        | 147/1000 [00:00<00:04, 208.56it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:00<00:03, 208.59it/s]Measuring inference for batch_size=1:  19%|█▉        | 189/1000 [00:00<00:03, 208.55it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:01<00:03, 208.57it/s]Measuring inference for batch_size=1:  23%|██▎       | 231/1000 [00:01<00:03, 208.52it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:01<00:03, 208.59it/s]Measuring inference for batch_size=1:  27%|██▋       | 273/1000 [00:01<00:03, 208.57it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:01<00:03, 208.44it/s]Measuring inference for batch_size=1:  32%|███▏      | 315/1000 [00:01<00:03, 208.42it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:01<00:03, 208.44it/s]Measuring inference for batch_size=1:  36%|███▌      | 357/1000 [00:01<00:03, 208.41it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:01<00:02, 208.39it/s]Measuring inference for batch_size=1:  40%|███▉      | 399/1000 [00:01<00:02, 208.46it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:02<00:02, 208.52it/s]Measuring inference for batch_size=1:  44%|████▍     | 441/1000 [00:02<00:02, 208.60it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:02<00:02, 208.75it/s]Measuring inference for batch_size=1:  48%|████▊     | 483/1000 [00:02<00:02, 208.76it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:02<00:02, 208.78it/s]Measuring inference for batch_size=1:  52%|█████▎    | 525/1000 [00:02<00:02, 208.67it/s]Measuring inference for batch_size=1:  55%|█████▍    | 546/1000 [00:02<00:02, 208.64it/s]Measuring inference for batch_size=1:  57%|█████▋    | 567/1000 [00:02<00:02, 208.54it/s]Measuring inference for batch_size=1:  59%|█████▉    | 588/1000 [00:02<00:01, 208.42it/s]Measuring inference for batch_size=1:  61%|██████    | 609/1000 [00:02<00:01, 208.35it/s]Measuring inference for batch_size=1:  63%|██████▎   | 630/1000 [00:03<00:01, 208.45it/s]Measuring inference for batch_size=1:  65%|██████▌   | 651/1000 [00:03<00:01, 208.49it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:03<00:01, 208.52it/s]Measuring inference for batch_size=1:  69%|██████▉   | 693/1000 [00:03<00:01, 208.44it/s]Measuring inference for batch_size=1:  71%|███████▏  | 714/1000 [00:03<00:01, 208.47it/s]Measuring inference for batch_size=1:  74%|███████▎  | 735/1000 [00:03<00:01, 208.48it/s]Measuring inference for batch_size=1:  76%|███████▌  | 756/1000 [00:03<00:01, 208.49it/s]Measuring inference for batch_size=1:  78%|███████▊  | 777/1000 [00:03<00:01, 208.59it/s]Measuring inference for batch_size=1:  80%|███████▉  | 798/1000 [00:03<00:00, 208.69it/s]Measuring inference for batch_size=1:  82%|████████▏ | 819/1000 [00:03<00:00, 208.75it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:04<00:00, 208.87it/s]Measuring inference for batch_size=1:  86%|████████▌ | 861/1000 [00:04<00:00, 208.92it/s]Measuring inference for batch_size=1:  88%|████████▊ | 882/1000 [00:04<00:00, 208.94it/s]Measuring inference for batch_size=1:  90%|█████████ | 903/1000 [00:04<00:00, 208.90it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [00:04<00:00, 208.82it/s]Measuring inference for batch_size=1:  94%|█████████▍| 945/1000 [00:04<00:00, 208.76it/s]Measuring inference for batch_size=1:  97%|█████████▋| 966/1000 [00:04<00:00, 208.72it/s]Measuring inference for batch_size=1:  99%|█████████▊| 987/1000 [00:04<00:00, 208.72it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 208.59it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=64:  21%|██        | 21/100 [00:00<00:00, 202.90it/s]Warming up with batch_size=64:  42%|████▏     | 42/100 [00:00<00:00, 203.49it/s]Warming up with batch_size=64:  63%|██████▎   | 63/100 [00:00<00:00, 203.72it/s]Warming up with batch_size=64:  84%|████████▍ | 84/100 [00:00<00:00, 203.65it/s]Warming up with batch_size=64: 100%|██████████| 100/100 [00:00<00:00, 203.59it/s]
STAGE:2024-02-23 09:57:15 177120:177120 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-23 09:57:15 177120:177120 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-23 09:57:15 177120:177120 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=64:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=64:   2%|▏         | 20/1000 [00:00<00:04, 198.46it/s]Measuring inference for batch_size=64:   4%|▍         | 40/1000 [00:00<00:04, 199.01it/s]Measuring inference for batch_size=64:   6%|▌         | 61/1000 [00:00<00:04, 199.78it/s]Measuring inference for batch_size=64:   8%|▊         | 82/1000 [00:00<00:04, 200.09it/s]Measuring inference for batch_size=64:  10%|█         | 103/1000 [00:00<00:04, 200.21it/s]Measuring inference for batch_size=64:  12%|█▏        | 124/1000 [00:00<00:04, 200.31it/s]Measuring inference for batch_size=64:  14%|█▍        | 145/1000 [00:00<00:04, 200.42it/s]Measuring inference for batch_size=64:  17%|█▋        | 166/1000 [00:00<00:04, 200.40it/s]Measuring inference for batch_size=64:  19%|█▊        | 187/1000 [00:00<00:04, 200.37it/s]Measuring inference for batch_size=64:  21%|██        | 208/1000 [00:01<00:03, 200.35it/s]Measuring inference for batch_size=64:  23%|██▎       | 229/1000 [00:01<00:03, 200.35it/s]Measuring inference for batch_size=64:  25%|██▌       | 250/1000 [00:01<00:03, 200.41it/s]Measuring inference for batch_size=64:  27%|██▋       | 271/1000 [00:01<00:03, 200.45it/s]Measuring inference for batch_size=64:  29%|██▉       | 292/1000 [00:01<00:03, 200.43it/s]Measuring inference for batch_size=64:  31%|███▏      | 313/1000 [00:01<00:03, 200.38it/s]Measuring inference for batch_size=64:  33%|███▎      | 334/1000 [00:01<00:03, 200.36it/s]Measuring inference for batch_size=64:  36%|███▌      | 355/1000 [00:01<00:03, 200.36it/s]Measuring inference for batch_size=64:  38%|███▊      | 376/1000 [00:01<00:03, 200.36it/s]Measuring inference for batch_size=64:  40%|███▉      | 397/1000 [00:01<00:03, 200.37it/s]Measuring inference for batch_size=64:  42%|████▏     | 418/1000 [00:02<00:02, 200.37it/s]Measuring inference for batch_size=64:  44%|████▍     | 439/1000 [00:02<00:02, 200.30it/s]Measuring inference for batch_size=64:  46%|████▌     | 460/1000 [00:02<00:02, 200.28it/s]Measuring inference for batch_size=64:  48%|████▊     | 481/1000 [00:02<00:02, 200.31it/s]Measuring inference for batch_size=64:  50%|█████     | 502/1000 [00:02<00:02, 200.30it/s]Measuring inference for batch_size=64:  52%|█████▏    | 523/1000 [00:02<00:02, 200.27it/s]Measuring inference for batch_size=64:  54%|█████▍    | 544/1000 [00:02<00:02, 200.35it/s]Measuring inference for batch_size=64:  56%|█████▋    | 565/1000 [00:02<00:02, 200.43it/s]Measuring inference for batch_size=64:  59%|█████▊    | 586/1000 [00:02<00:02, 200.46it/s]Measuring inference for batch_size=64:  61%|██████    | 607/1000 [00:03<00:01, 200.41it/s]Measuring inference for batch_size=64:  63%|██████▎   | 628/1000 [00:03<00:01, 200.32it/s]Measuring inference for batch_size=64:  65%|██████▍   | 649/1000 [00:03<00:01, 200.30it/s]Measuring inference for batch_size=64:  67%|██████▋   | 670/1000 [00:03<00:01, 200.31it/s]Measuring inference for batch_size=64:  69%|██████▉   | 691/1000 [00:03<00:01, 200.27it/s]Measuring inference for batch_size=64:  71%|███████   | 712/1000 [00:03<00:01, 200.23it/s]Measuring inference for batch_size=64:  73%|███████▎  | 733/1000 [00:03<00:01, 200.18it/s]Measuring inference for batch_size=64:  75%|███████▌  | 754/1000 [00:03<00:01, 200.27it/s]Measuring inference for batch_size=64:  78%|███████▊  | 775/1000 [00:03<00:01, 200.22it/s]Measuring inference for batch_size=64:  80%|███████▉  | 796/1000 [00:03<00:01, 200.26it/s]Measuring inference for batch_size=64:  82%|████████▏ | 817/1000 [00:04<00:00, 200.15it/s]Measuring inference for batch_size=64:  84%|████████▍ | 838/1000 [00:04<00:00, 200.12it/s]Measuring inference for batch_size=64:  86%|████████▌ | 859/1000 [00:04<00:00, 200.21it/s]Measuring inference for batch_size=64:  88%|████████▊ | 880/1000 [00:04<00:00, 200.32it/s]Measuring inference for batch_size=64:  90%|█████████ | 901/1000 [00:04<00:00, 199.48it/s]Measuring inference for batch_size=64:  92%|█████████▏| 922/1000 [00:04<00:00, 199.77it/s]Measuring inference for batch_size=64:  94%|█████████▍| 943/1000 [00:04<00:00, 199.86it/s]Measuring inference for batch_size=64:  96%|█████████▋| 964/1000 [00:04<00:00, 199.96it/s]Measuring inference for batch_size=64:  98%|█████████▊| 985/1000 [00:04<00:00, 200.12it/s]Measuring inference for batch_size=64: 100%|██████████| 1000/1000 [00:04<00:00, 200.22it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 27.53 GB
    total: 31.28 GB
    used: 3.28 GB
  system:
    node: baseline
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_64:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 94.370 us +/- 3.260 us [92.745 us, 145.912 us]
        batches_per_second: 10.61 K +/- 309.21 [6.85 K, 10.78 K]
      metrics:
        batches_per_second_max: 10782.272493573264
        batches_per_second_mean: 10607.126720244683
        batches_per_second_min: 6853.437908496732
        batches_per_second_std: 309.21005154849684
        seconds_per_batch_max: 0.00014591217041015625
        seconds_per_batch_mean: 9.437036514282226e-05
        seconds_per_batch_min: 9.274482727050781e-05
        seconds_per_batch_std: 3.2601931886169335e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.018 us +/- 0.624 us [22.173 us, 29.325 us]
        batches_per_second: 43.47 K +/- 1.03 K [34.10 K, 45.10 K]
      metrics:
        batches_per_second_max: 45100.04301075269
        batches_per_second_mean: 43471.33088910679
        batches_per_second_min: 34100.03252032521
        batches_per_second_std: 1027.549494341287
        seconds_per_batch_max: 2.9325485229492188e-05
        seconds_per_batch_mean: 2.3018360137939455e-05
        seconds_per_batch_min: 2.2172927856445312e-05
        seconds_per_batch_std: 6.237225062852728e-07
    on_device_inference:
      human_readable:
        batch_latency: -4666290.686 us +/- 14.112 ms [-4899360.180 us, -4635551.929
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.22, -0.20]
      metrics:
        batches_per_second_max: -0.2041082842005263
        batches_per_second_mean: -0.21430491322178263
        batches_per_second_min: -0.2157240421883263
        batches_per_second_std: 0.0006393460902942652
        seconds_per_batch_max: -4.635551929473877
        seconds_per_batch_mean: -4.666290685653687
        seconds_per_batch_min: -4.899360179901123
        seconds_per_batch_std: 0.014112050670298741
    total:
      human_readable:
        batch_latency: 4.790 ms +/- 15.875 us [4.758 ms, 5.089 ms]
        batches_per_second: 208.75 +/- 0.68 [196.52, 210.18]
      metrics:
        batches_per_second_max: 210.177590699539
        batches_per_second_mean: 208.74935748330654
        batches_per_second_min: 196.51895234971653
        batches_per_second_std: 0.6771501287341378
        seconds_per_batch_max: 0.0050885677337646484
        seconds_per_batch_mean: 0.004790485382080078
        seconds_per_batch_min: 0.004757881164550781
        seconds_per_batch_std: 1.587502638051683e-05
  batch_size_64:
    cpu_to_gpu:
      human_readable:
        batch_latency: 147.029 us +/- 4.768 us [144.720 us, 246.048 us]
        batches_per_second: 6.81 K +/- 175.30 [4.06 K, 6.91 K]
      metrics:
        batches_per_second_max: 6909.891268533773
        batches_per_second_mean: 6806.960897607612
        batches_per_second_min: 4064.248062015504
        batches_per_second_std: 175.29551841568315
        seconds_per_batch_max: 0.0002460479736328125
        seconds_per_batch_mean: 0.00014702868461608887
        seconds_per_batch_min: 0.00014472007751464844
        seconds_per_batch_std: 4.76790968922851e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.184 us +/- 0.638 us [22.411 us, 32.187 us]
        batches_per_second: 43.16 K +/- 1.01 K [31.07 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 43160.58846535632
        batches_per_second_min: 31068.91851851852
        batches_per_second_std: 1006.7901562271086
        seconds_per_batch_max: 3.218650817871094e-05
        seconds_per_batch_mean: 2.318406105041504e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 6.377382359809969e-07
    on_device_inference:
      human_readable:
        batch_latency: -4815189.603 us +/- 24.484 ms [-5152256.012 us, -4783743.858
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.19]
      metrics:
        batches_per_second_max: -0.19408973422091716
        batches_per_second_mean: -0.20768136433353748
        batches_per_second_min: -0.2090412926806561
        batches_per_second_std: 0.0010273256026574924
        seconds_per_batch_max: -4.783743858337402
        seconds_per_batch_mean: -4.8151896033287045
        seconds_per_batch_min: -5.152256011962891
        seconds_per_batch_std: 0.024483507102831082
    total:
      human_readable:
        batch_latency: 4.991 ms +/- 26.939 us [4.959 ms, 5.434 ms]
        batches_per_second: 200.37 +/- 1.04 [184.02, 201.67]
      metrics:
        batches_per_second_max: 201.66862198288297
        batches_per_second_mean: 200.37087802933493
        batches_per_second_min: 184.0171982626245
        batches_per_second_std: 1.0439411431534038
        seconds_per_batch_max: 0.005434274673461914
        seconds_per_batch_mean: 0.004990885496139527
        seconds_per_batch_min: 0.004958629608154297
        seconds_per_batch_std: 2.693913704182158e-05


