#####
fp-fp-py-id - Run 1
2024-02-25 22:52:45
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 277.27it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:00<00:00, 277.76it/s]Warming up with batch_size=1:  85%|████████▌ | 85/100 [00:00<00:00, 279.28it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 279.13it/s]
STAGE:2024-02-25 22:52:32 5509:5509 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:52:32 5509:5509 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:52:32 5509:5509 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:00<00:04, 209.62it/s]Measuring inference for batch_size=1:   4%|▍         | 43/1000 [00:00<00:04, 210.13it/s]Measuring inference for batch_size=1:   6%|▋         | 65/1000 [00:00<00:04, 210.25it/s]Measuring inference for batch_size=1:   9%|▊         | 87/1000 [00:00<00:04, 210.33it/s]Measuring inference for batch_size=1:  11%|█         | 109/1000 [00:00<00:04, 210.40it/s]Measuring inference for batch_size=1:  13%|█▎        | 131/1000 [00:00<00:04, 210.40it/s]Measuring inference for batch_size=1:  15%|█▌        | 153/1000 [00:00<00:04, 210.35it/s]Measuring inference for batch_size=1:  18%|█▊        | 175/1000 [00:00<00:03, 210.38it/s]Measuring inference for batch_size=1:  20%|█▉        | 197/1000 [00:00<00:03, 210.38it/s]Measuring inference for batch_size=1:  22%|██▏       | 219/1000 [00:01<00:03, 210.38it/s]Measuring inference for batch_size=1:  24%|██▍       | 241/1000 [00:01<00:03, 210.40it/s]Measuring inference for batch_size=1:  26%|██▋       | 263/1000 [00:01<00:03, 210.50it/s]Measuring inference for batch_size=1:  28%|██▊       | 285/1000 [00:01<00:03, 210.51it/s]Measuring inference for batch_size=1:  31%|███       | 307/1000 [00:01<00:03, 210.52it/s]Measuring inference for batch_size=1:  33%|███▎      | 329/1000 [00:01<00:03, 210.46it/s]Measuring inference for batch_size=1:  35%|███▌      | 351/1000 [00:01<00:03, 210.42it/s]Measuring inference for batch_size=1:  37%|███▋      | 373/1000 [00:01<00:02, 210.37it/s]Measuring inference for batch_size=1:  40%|███▉      | 395/1000 [00:01<00:02, 210.40it/s]Measuring inference for batch_size=1:  42%|████▏     | 417/1000 [00:01<00:02, 210.41it/s]Measuring inference for batch_size=1:  44%|████▍     | 439/1000 [00:02<00:02, 210.45it/s]Measuring inference for batch_size=1:  46%|████▌     | 461/1000 [00:02<00:02, 210.40it/s]Measuring inference for batch_size=1:  48%|████▊     | 483/1000 [00:02<00:02, 210.00it/s]Measuring inference for batch_size=1:  50%|█████     | 505/1000 [00:02<00:02, 207.49it/s]Measuring inference for batch_size=1:  53%|█████▎    | 526/1000 [00:02<00:02, 205.82it/s]Measuring inference for batch_size=1:  55%|█████▍    | 547/1000 [00:02<00:02, 204.67it/s]Measuring inference for batch_size=1:  57%|█████▋    | 568/1000 [00:02<00:02, 203.71it/s]Measuring inference for batch_size=1:  59%|█████▉    | 589/1000 [00:02<00:02, 202.99it/s]Measuring inference for batch_size=1:  61%|██████    | 610/1000 [00:02<00:01, 202.39it/s]Measuring inference for batch_size=1:  63%|██████▎   | 631/1000 [00:03<00:01, 202.00it/s]Measuring inference for batch_size=1:  65%|██████▌   | 652/1000 [00:03<00:01, 201.70it/s]Measuring inference for batch_size=1:  67%|██████▋   | 673/1000 [00:03<00:01, 201.56it/s]Measuring inference for batch_size=1:  69%|██████▉   | 694/1000 [00:03<00:01, 201.33it/s]Measuring inference for batch_size=1:  72%|███████▏  | 715/1000 [00:03<00:01, 201.37it/s]Measuring inference for batch_size=1:  74%|███████▎  | 736/1000 [00:03<00:01, 201.38it/s]Measuring inference for batch_size=1:  76%|███████▌  | 757/1000 [00:03<00:01, 201.49it/s]Measuring inference for batch_size=1:  78%|███████▊  | 778/1000 [00:03<00:01, 201.50it/s]Measuring inference for batch_size=1:  80%|███████▉  | 799/1000 [00:03<00:00, 201.47it/s]Measuring inference for batch_size=1:  82%|████████▏ | 820/1000 [00:03<00:00, 201.54it/s]Measuring inference for batch_size=1:  84%|████████▍ | 841/1000 [00:04<00:00, 201.65it/s]Measuring inference for batch_size=1:  86%|████████▌ | 862/1000 [00:04<00:00, 201.56it/s]Measuring inference for batch_size=1:  88%|████████▊ | 883/1000 [00:04<00:00, 201.50it/s]Measuring inference for batch_size=1:  90%|█████████ | 904/1000 [00:04<00:00, 201.45it/s]Measuring inference for batch_size=1:  92%|█████████▎| 925/1000 [00:04<00:00, 201.24it/s]Measuring inference for batch_size=1:  95%|█████████▍| 946/1000 [00:04<00:00, 201.30it/s]Measuring inference for batch_size=1:  97%|█████████▋| 967/1000 [00:04<00:00, 201.26it/s]Measuring inference for batch_size=1:  99%|█████████▉| 988/1000 [00:04<00:00, 201.20it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 205.60it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=16:  21%|██        | 21/100 [00:00<00:00, 204.84it/s]Warming up with batch_size=16:  42%|████▏     | 42/100 [00:00<00:00, 205.04it/s]Warming up with batch_size=16:  63%|██████▎   | 63/100 [00:00<00:00, 205.07it/s]Warming up with batch_size=16:  84%|████████▍ | 84/100 [00:00<00:00, 205.01it/s]Warming up with batch_size=16: 100%|██████████| 100/100 [00:00<00:00, 205.05it/s]
STAGE:2024-02-25 22:52:38 5509:5509 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:52:38 5509:5509 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:52:38 5509:5509 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=16:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=16:   2%|▏         | 21/1000 [00:00<00:04, 200.98it/s]Measuring inference for batch_size=16:   4%|▍         | 42/1000 [00:00<00:04, 201.42it/s]Measuring inference for batch_size=16:   6%|▋         | 63/1000 [00:00<00:04, 201.70it/s]Measuring inference for batch_size=16:   8%|▊         | 84/1000 [00:00<00:04, 201.84it/s]Measuring inference for batch_size=16:  10%|█         | 105/1000 [00:00<00:04, 201.93it/s]Measuring inference for batch_size=16:  13%|█▎        | 126/1000 [00:00<00:04, 201.95it/s]Measuring inference for batch_size=16:  15%|█▍        | 147/1000 [00:00<00:04, 201.81it/s]Measuring inference for batch_size=16:  17%|█▋        | 168/1000 [00:00<00:04, 201.68it/s]Measuring inference for batch_size=16:  19%|█▉        | 189/1000 [00:00<00:04, 201.68it/s]Measuring inference for batch_size=16:  21%|██        | 210/1000 [00:01<00:03, 201.69it/s]Measuring inference for batch_size=16:  23%|██▎       | 231/1000 [00:01<00:03, 201.70it/s]Measuring inference for batch_size=16:  25%|██▌       | 252/1000 [00:01<00:03, 201.70it/s]Measuring inference for batch_size=16:  27%|██▋       | 273/1000 [00:01<00:03, 201.75it/s]Measuring inference for batch_size=16:  29%|██▉       | 294/1000 [00:01<00:03, 201.77it/s]Measuring inference for batch_size=16:  32%|███▏      | 315/1000 [00:01<00:03, 201.80it/s]Measuring inference for batch_size=16:  34%|███▎      | 336/1000 [00:01<00:03, 201.74it/s]Measuring inference for batch_size=16:  36%|███▌      | 357/1000 [00:01<00:03, 201.71it/s]Measuring inference for batch_size=16:  38%|███▊      | 378/1000 [00:01<00:03, 201.67it/s]Measuring inference for batch_size=16:  40%|███▉      | 399/1000 [00:01<00:02, 201.69it/s]Measuring inference for batch_size=16:  42%|████▏     | 420/1000 [00:02<00:02, 201.69it/s]Measuring inference for batch_size=16:  44%|████▍     | 441/1000 [00:02<00:02, 201.73it/s]Measuring inference for batch_size=16:  46%|████▌     | 462/1000 [00:02<00:02, 201.71it/s]Measuring inference for batch_size=16:  48%|████▊     | 483/1000 [00:02<00:02, 201.68it/s]Measuring inference for batch_size=16:  50%|█████     | 504/1000 [00:02<00:02, 201.65it/s]Measuring inference for batch_size=16:  52%|█████▎    | 525/1000 [00:02<00:02, 201.66it/s]Measuring inference for batch_size=16:  55%|█████▍    | 546/1000 [00:02<00:02, 201.58it/s]Measuring inference for batch_size=16:  57%|█████▋    | 567/1000 [00:02<00:02, 201.58it/s]Measuring inference for batch_size=16:  59%|█████▉    | 588/1000 [00:02<00:02, 201.66it/s]Measuring inference for batch_size=16:  61%|██████    | 609/1000 [00:03<00:01, 201.72it/s]Measuring inference for batch_size=16:  63%|██████▎   | 630/1000 [00:03<00:01, 201.70it/s]Measuring inference for batch_size=16:  65%|██████▌   | 651/1000 [00:03<00:01, 201.73it/s]Measuring inference for batch_size=16:  67%|██████▋   | 672/1000 [00:03<00:01, 201.74it/s]Measuring inference for batch_size=16:  69%|██████▉   | 693/1000 [00:03<00:01, 201.68it/s]Measuring inference for batch_size=16:  71%|███████▏  | 714/1000 [00:03<00:01, 201.62it/s]Measuring inference for batch_size=16:  74%|███████▎  | 735/1000 [00:03<00:01, 201.61it/s]Measuring inference for batch_size=16:  76%|███████▌  | 756/1000 [00:03<00:01, 201.61it/s]Measuring inference for batch_size=16:  78%|███████▊  | 777/1000 [00:03<00:01, 201.58it/s]Measuring inference for batch_size=16:  80%|███████▉  | 798/1000 [00:03<00:01, 201.55it/s]Measuring inference for batch_size=16:  82%|████████▏ | 819/1000 [00:04<00:00, 201.51it/s]Measuring inference for batch_size=16:  84%|████████▍ | 840/1000 [00:04<00:00, 201.48it/s]Measuring inference for batch_size=16:  86%|████████▌ | 861/1000 [00:04<00:00, 201.52it/s]Measuring inference for batch_size=16:  88%|████████▊ | 882/1000 [00:04<00:00, 201.50it/s]Measuring inference for batch_size=16:  90%|█████████ | 903/1000 [00:04<00:00, 201.47it/s]Measuring inference for batch_size=16:  92%|█████████▏| 924/1000 [00:04<00:00, 201.46it/s]Measuring inference for batch_size=16:  94%|█████████▍| 945/1000 [00:04<00:00, 201.50it/s]Measuring inference for batch_size=16:  97%|█████████▋| 966/1000 [00:04<00:00, 201.49it/s]Measuring inference for batch_size=16:  99%|█████████▊| 987/1000 [00:04<00:00, 201.52it/s]Measuring inference for batch_size=16: 100%|██████████| 1000/1000 [00:04<00:00, 201.64it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 29.99 GB
    total: 31.28 GB
    used: 914.85 MB
  system:
    node: fp
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_16:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 95.620 us +/- 3.295 us [92.983 us, 138.998 us]
        batches_per_second: 10.47 K +/- 318.19 [7.19 K, 10.75 K]
      metrics:
        batches_per_second_max: 10754.625641025641
        batches_per_second_mean: 10468.995898760475
        batches_per_second_min: 7194.346483704974
        batches_per_second_std: 318.1941442502807
        seconds_per_batch_max: 0.00013899803161621094
        seconds_per_batch_mean: 9.561967849731445e-05
        seconds_per_batch_min: 9.298324584960938e-05
        seconds_per_batch_std: 3.29529181473087e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.594 us +/- 0.646 us [22.650 us, 29.802 us]
        batches_per_second: 42.41 K +/- 1.06 K [33.55 K, 44.15 K]
      metrics:
        batches_per_second_max: 44150.56842105263
        batches_per_second_mean: 42412.33031473075
        batches_per_second_min: 33554.432
        batches_per_second_std: 1062.4354797402823
        seconds_per_batch_max: 2.9802322387695312e-05
        seconds_per_batch_mean: 2.3594141006469726e-05
        seconds_per_batch_min: 2.2649765014648438e-05
        seconds_per_batch_std: 6.455589296618998e-07
    on_device_inference:
      human_readable:
        batch_latency: -4733296.003 us +/- 105.539 ms [-4887712.002 us, -4587327.957
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.22, -0.20]
      metrics:
        batches_per_second_max: -0.2045947059956928
        batches_per_second_mean: -0.21137452306820864
        batches_per_second_min: -0.2179918264707093
        batches_per_second_std: 0.004720493915101442
        seconds_per_batch_max: -4.58732795715332
        seconds_per_batch_mean: -4.733296002864837
        seconds_per_batch_min: -4.887712001800537
        seconds_per_batch_std: 0.10553874600650644
    total:
      human_readable:
        batch_latency: 4.860 ms +/- 107.408 us [4.711 ms, 5.017 ms]
        batches_per_second: 205.86 +/- 4.56 [199.33, 212.28]
      metrics:
        batches_per_second_max: 212.28383439619395
        batches_per_second_mean: 205.8560195664555
        batches_per_second_min: 199.33010170135918
        batches_per_second_std: 4.556433174084681
        seconds_per_batch_max: 0.005016803741455078
        seconds_per_batch_mean: 0.0048601415157318115
        seconds_per_batch_min: 0.004710674285888672
        seconds_per_batch_std: 0.00010740797820712218
  batch_size_16:
    cpu_to_gpu:
      human_readable:
        batch_latency: 146.493 us +/- 4.283 us [144.720 us, 245.571 us]
        batches_per_second: 6.83 K +/- 153.41 [4.07 K, 6.91 K]
      metrics:
        batches_per_second_max: 6909.891268533773
        batches_per_second_mean: 6830.656998358941
        batches_per_second_min: 4072.1398058252425
        batches_per_second_std: 153.40583387193496
        seconds_per_batch_max: 0.0002455711364746094
        seconds_per_batch_mean: 0.00014649271965026854
        seconds_per_batch_min: 0.00014472007751464844
        seconds_per_batch_std: 4.283326133810367e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.383 us +/- 0.628 us [22.411 us, 34.094 us]
        batches_per_second: 42.79 K +/- 969.36 [29.33 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 42791.64111745773
        batches_per_second_min: 29330.797202797203
        batches_per_second_std: 969.3550342795485
        seconds_per_batch_max: 3.409385681152344e-05
        seconds_per_batch_mean: 2.3383140563964842e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 6.282781497834186e-07
    on_device_inference:
      human_readable:
        batch_latency: -4780440.611 us +/- 14.716 ms [-5084256.172 us, -4738207.817
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.20]
      metrics:
        batches_per_second_max: -0.19668560476392966
        batches_per_second_mean: -0.20918766998593
        batches_per_second_min: -0.21105026174575128
        batches_per_second_std: 0.0006277135934619965
        seconds_per_batch_max: -4.738207817077637
        seconds_per_batch_mean: -4.780440611362457
        seconds_per_batch_min: -5.084256172180176
        seconds_per_batch_std: 0.014715656527033227
    total:
      human_readable:
        batch_latency: 4.956 ms +/- 17.430 us [4.913 ms, 5.365 ms]
        batches_per_second: 201.79 +/- 0.68 [186.38, 203.55]
      metrics:
        batches_per_second_max: 203.54770455207222
        batches_per_second_mean: 201.78799126119546
        batches_per_second_min: 186.38037682189832
        batches_per_second_std: 0.6801447687575315
        seconds_per_batch_max: 0.0053653717041015625
        seconds_per_batch_mean: 0.004955754995346069
        seconds_per_batch_min: 0.004912853240966797
        seconds_per_batch_std: 1.7429911015191496e-05


#####
fp-fp-py-id - Run 2
2024-02-25 22:53:04
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  5.00it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.99it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 272.55it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:00<00:00, 273.30it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:00<00:00, 274.74it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 274.59it/s]
STAGE:2024-02-25 22:52:51 5555:5555 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:52:51 5555:5555 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:52:51 5555:5555 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:00<00:04, 206.60it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:00<00:04, 207.07it/s]Measuring inference for batch_size=1:   6%|▋         | 63/1000 [00:00<00:04, 207.38it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:00<00:04, 207.37it/s]Measuring inference for batch_size=1:  10%|█         | 105/1000 [00:00<00:04, 207.49it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:00<00:04, 207.54it/s]Measuring inference for batch_size=1:  15%|█▍        | 147/1000 [00:00<00:04, 207.48it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:00<00:04, 207.41it/s]Measuring inference for batch_size=1:  19%|█▉        | 189/1000 [00:00<00:03, 207.36it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:01<00:03, 207.23it/s]Measuring inference for batch_size=1:  23%|██▎       | 231/1000 [00:01<00:03, 207.24it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:01<00:03, 207.27it/s]Measuring inference for batch_size=1:  27%|██▋       | 273/1000 [00:01<00:03, 207.36it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:01<00:03, 207.45it/s]Measuring inference for batch_size=1:  32%|███▏      | 315/1000 [00:01<00:03, 207.50it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:01<00:03, 207.54it/s]Measuring inference for batch_size=1:  36%|███▌      | 357/1000 [00:01<00:03, 207.49it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:01<00:03, 203.90it/s]Measuring inference for batch_size=1:  40%|███▉      | 399/1000 [00:01<00:02, 201.52it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:02<00:02, 199.96it/s]Measuring inference for batch_size=1:  44%|████▍     | 441/1000 [00:02<00:02, 198.92it/s]Measuring inference for batch_size=1:  46%|████▌     | 461/1000 [00:02<00:02, 198.23it/s]Measuring inference for batch_size=1:  48%|████▊     | 481/1000 [00:02<00:02, 197.67it/s]Measuring inference for batch_size=1:  50%|█████     | 501/1000 [00:02<00:02, 197.36it/s]Measuring inference for batch_size=1:  52%|█████▏    | 521/1000 [00:02<00:02, 196.98it/s]Measuring inference for batch_size=1:  54%|█████▍    | 541/1000 [00:02<00:02, 196.70it/s]Measuring inference for batch_size=1:  56%|█████▌    | 561/1000 [00:02<00:02, 196.57it/s]Measuring inference for batch_size=1:  58%|█████▊    | 581/1000 [00:02<00:02, 196.46it/s]Measuring inference for batch_size=1:  60%|██████    | 601/1000 [00:02<00:02, 196.39it/s]Measuring inference for batch_size=1:  62%|██████▏   | 621/1000 [00:03<00:01, 196.35it/s]Measuring inference for batch_size=1:  64%|██████▍   | 641/1000 [00:03<00:01, 196.21it/s]Measuring inference for batch_size=1:  66%|██████▌   | 661/1000 [00:03<00:01, 196.10it/s]Measuring inference for batch_size=1:  68%|██████▊   | 681/1000 [00:03<00:01, 195.97it/s]Measuring inference for batch_size=1:  70%|███████   | 701/1000 [00:03<00:01, 195.88it/s]Measuring inference for batch_size=1:  72%|███████▏  | 721/1000 [00:03<00:01, 195.81it/s]Measuring inference for batch_size=1:  74%|███████▍  | 741/1000 [00:03<00:01, 195.78it/s]Measuring inference for batch_size=1:  76%|███████▌  | 761/1000 [00:03<00:01, 195.74it/s]Measuring inference for batch_size=1:  78%|███████▊  | 781/1000 [00:03<00:01, 195.83it/s]Measuring inference for batch_size=1:  80%|████████  | 801/1000 [00:03<00:01, 195.80it/s]Measuring inference for batch_size=1:  82%|████████▏ | 821/1000 [00:04<00:00, 195.83it/s]Measuring inference for batch_size=1:  84%|████████▍ | 841/1000 [00:04<00:00, 195.68it/s]Measuring inference for batch_size=1:  86%|████████▌ | 861/1000 [00:04<00:00, 195.67it/s]Measuring inference for batch_size=1:  88%|████████▊ | 881/1000 [00:04<00:00, 195.68it/s]Measuring inference for batch_size=1:  90%|█████████ | 901/1000 [00:04<00:00, 195.58it/s]Measuring inference for batch_size=1:  92%|█████████▏| 921/1000 [00:04<00:00, 195.60it/s]Measuring inference for batch_size=1:  94%|█████████▍| 941/1000 [00:04<00:00, 195.70it/s]Measuring inference for batch_size=1:  96%|█████████▌| 961/1000 [00:04<00:00, 195.80it/s]Measuring inference for batch_size=1:  98%|█████████▊| 981/1000 [00:04<00:00, 195.86it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:05<00:00, 199.88it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=16:  21%|██        | 21/100 [00:00<00:00, 202.41it/s]Warming up with batch_size=16:  42%|████▏     | 42/100 [00:00<00:00, 202.71it/s]Warming up with batch_size=16:  63%|██████▎   | 63/100 [00:00<00:00, 202.79it/s]Warming up with batch_size=16:  84%|████████▍ | 84/100 [00:00<00:00, 202.78it/s]Warming up with batch_size=16: 100%|██████████| 100/100 [00:00<00:00, 202.74it/s]
STAGE:2024-02-25 22:52:56 5555:5555 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:52:56 5555:5555 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:52:56 5555:5555 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=16:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=16:   2%|▏         | 20/1000 [00:00<00:04, 199.20it/s]Measuring inference for batch_size=16:   4%|▍         | 40/1000 [00:00<00:04, 199.62it/s]Measuring inference for batch_size=16:   6%|▌         | 60/1000 [00:00<00:04, 199.75it/s]Measuring inference for batch_size=16:   8%|▊         | 80/1000 [00:00<00:04, 199.82it/s]Measuring inference for batch_size=16:  10%|█         | 101/1000 [00:00<00:04, 199.89it/s]Measuring inference for batch_size=16:  12%|█▏        | 122/1000 [00:00<00:04, 199.95it/s]Measuring inference for batch_size=16:  14%|█▍        | 143/1000 [00:00<00:04, 200.00it/s]Measuring inference for batch_size=16:  16%|█▋        | 163/1000 [00:00<00:04, 199.99it/s]Measuring inference for batch_size=16:  18%|█▊        | 184/1000 [00:00<00:04, 200.03it/s]Measuring inference for batch_size=16:  20%|██        | 205/1000 [00:01<00:03, 200.02it/s]Measuring inference for batch_size=16:  23%|██▎       | 226/1000 [00:01<00:03, 200.06it/s]Measuring inference for batch_size=16:  25%|██▍       | 247/1000 [00:01<00:03, 199.95it/s]Measuring inference for batch_size=16:  27%|██▋       | 267/1000 [00:01<00:03, 199.75it/s]Measuring inference for batch_size=16:  29%|██▊       | 287/1000 [00:01<00:03, 199.75it/s]Measuring inference for batch_size=16:  31%|███       | 308/1000 [00:01<00:03, 199.88it/s]Measuring inference for batch_size=16:  33%|███▎      | 328/1000 [00:01<00:03, 199.76it/s]Measuring inference for batch_size=16:  35%|███▍      | 348/1000 [00:01<00:03, 199.79it/s]Measuring inference for batch_size=16:  37%|███▋      | 368/1000 [00:01<00:03, 199.78it/s]Measuring inference for batch_size=16:  39%|███▉      | 388/1000 [00:01<00:03, 199.82it/s]Measuring inference for batch_size=16:  41%|████      | 408/1000 [00:02<00:02, 199.85it/s]Measuring inference for batch_size=16:  43%|████▎     | 428/1000 [00:02<00:02, 199.88it/s]Measuring inference for batch_size=16:  45%|████▍     | 449/1000 [00:02<00:02, 199.97it/s]Measuring inference for batch_size=16:  47%|████▋     | 470/1000 [00:02<00:02, 200.01it/s]Measuring inference for batch_size=16:  49%|████▉     | 491/1000 [00:02<00:02, 200.00it/s]Measuring inference for batch_size=16:  51%|█████     | 512/1000 [00:02<00:02, 200.17it/s]Measuring inference for batch_size=16:  53%|█████▎    | 533/1000 [00:02<00:02, 200.31it/s]Measuring inference for batch_size=16:  55%|█████▌    | 554/1000 [00:02<00:02, 200.38it/s]Measuring inference for batch_size=16:  57%|█████▊    | 575/1000 [00:02<00:02, 200.44it/s]Measuring inference for batch_size=16:  60%|█████▉    | 596/1000 [00:02<00:02, 200.49it/s]Measuring inference for batch_size=16:  62%|██████▏   | 617/1000 [00:03<00:01, 200.46it/s]Measuring inference for batch_size=16:  64%|██████▍   | 638/1000 [00:03<00:01, 200.37it/s]Measuring inference for batch_size=16:  66%|██████▌   | 659/1000 [00:03<00:01, 200.38it/s]Measuring inference for batch_size=16:  68%|██████▊   | 680/1000 [00:03<00:01, 200.33it/s]Measuring inference for batch_size=16:  70%|███████   | 701/1000 [00:03<00:01, 200.30it/s]Measuring inference for batch_size=16:  72%|███████▏  | 722/1000 [00:03<00:01, 200.32it/s]Measuring inference for batch_size=16:  74%|███████▍  | 743/1000 [00:03<00:01, 200.37it/s]Measuring inference for batch_size=16:  76%|███████▋  | 764/1000 [00:03<00:01, 200.33it/s]Measuring inference for batch_size=16:  78%|███████▊  | 785/1000 [00:03<00:01, 200.26it/s]Measuring inference for batch_size=16:  81%|████████  | 806/1000 [00:04<00:00, 200.18it/s]Measuring inference for batch_size=16:  83%|████████▎ | 827/1000 [00:04<00:00, 200.09it/s]Measuring inference for batch_size=16:  85%|████████▍ | 848/1000 [00:04<00:00, 200.16it/s]Measuring inference for batch_size=16:  87%|████████▋ | 869/1000 [00:04<00:00, 200.22it/s]Measuring inference for batch_size=16:  89%|████████▉ | 890/1000 [00:04<00:00, 200.05it/s]Measuring inference for batch_size=16:  91%|█████████ | 911/1000 [00:04<00:00, 200.12it/s]Measuring inference for batch_size=16:  93%|█████████▎| 932/1000 [00:04<00:00, 200.13it/s]Measuring inference for batch_size=16:  95%|█████████▌| 953/1000 [00:04<00:00, 200.11it/s]Measuring inference for batch_size=16:  97%|█████████▋| 974/1000 [00:04<00:00, 200.16it/s]Measuring inference for batch_size=16: 100%|█████████▉| 995/1000 [00:04<00:00, 200.19it/s]Measuring inference for batch_size=16: 100%|██████████| 1000/1000 [00:04<00:00, 200.09it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 29.98 GB
    total: 31.28 GB
    used: 917.60 MB
  system:
    node: fp
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_16:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 96.375 us +/- 3.221 us [92.983 us, 142.097 us]
        batches_per_second: 10.39 K +/- 305.13 [7.04 K, 10.75 K]
      metrics:
        batches_per_second_max: 10754.625641025641
        batches_per_second_mean: 10386.277846377037
        batches_per_second_min: 7037.422818791946
        batches_per_second_std: 305.13484253549194
        seconds_per_batch_max: 0.00014209747314453125
        seconds_per_batch_mean: 9.637475013732911e-05
        seconds_per_batch_min: 9.298324584960938e-05
        seconds_per_batch_std: 3.220525610589493e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.972 us +/- 0.746 us [22.888 us, 31.233 us]
        batches_per_second: 41.75 K +/- 1.17 K [32.02 K, 43.69 K]
      metrics:
        batches_per_second_max: 43690.666666666664
        batches_per_second_mean: 41750.90069435008
        batches_per_second_min: 32017.58778625954
        batches_per_second_std: 1166.9219213376655
        seconds_per_batch_max: 3.123283386230469e-05
        seconds_per_batch_mean: 2.3972272872924804e-05
        seconds_per_batch_min: 2.288818359375e-05
        seconds_per_batch_std: 7.455220113091489e-07
    on_device_inference:
      human_readable:
        batch_latency: -4870286.269 us +/- 133.804 ms [-5129920.006 us, -4660543.919
          us]
        batches_per_second: -0.21 +/- 0.01 [-0.21, -0.19]
      metrics:
        batches_per_second_max: -0.19493481357793138
        batches_per_second_mean: -0.2054843174527587
        batches_per_second_min: -0.21456723023400456
        batches_per_second_std: 0.005735791015344542
        seconds_per_batch_max: -4.660543918609619
        seconds_per_batch_mean: -4.870286269187927
        seconds_per_batch_min: -5.12992000579834
        seconds_per_batch_std: 0.13380370634192915
    total:
      human_readable:
        batch_latency: 4.999 ms +/- 136.159 us [4.785 ms, 5.263 ms]
        batches_per_second: 200.18 +/- 5.54 [190.02, 208.98]
      metrics:
        batches_per_second_max: 208.9837568510214
        batches_per_second_mean: 200.18314577246196
        batches_per_second_min: 190.01966203053505
        batches_per_second_std: 5.538703770277477
        seconds_per_batch_max: 0.005262613296508789
        seconds_per_batch_mean: 0.004999192714691162
        seconds_per_batch_min: 0.004785060882568359
        seconds_per_batch_std: 0.00013615903117651477
  batch_size_16:
    cpu_to_gpu:
      human_readable:
        batch_latency: 147.101 us +/- 4.241 us [145.197 us, 246.525 us]
        batches_per_second: 6.80 K +/- 149.94 [4.06 K, 6.89 K]
      metrics:
        batches_per_second_max: 6887.1986863711
        batches_per_second_mean: 6802.272821790759
        batches_per_second_min: 4056.386847195358
        batches_per_second_std: 149.9399623624606
        seconds_per_batch_max: 0.0002465248107910156
        seconds_per_batch_mean: 0.00014710092544555664
        seconds_per_batch_min: 0.00014519691467285156
        seconds_per_batch_std: 4.240687160926401e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.392 us +/- 0.560 us [22.650 us, 30.518 us]
        batches_per_second: 42.77 K +/- 870.23 [32.77 K, 44.15 K]
      metrics:
        batches_per_second_max: 44150.56842105263
        batches_per_second_mean: 42769.562738432185
        batches_per_second_min: 32768.0
        batches_per_second_std: 870.2279305471343
        seconds_per_batch_max: 3.0517578125e-05
        seconds_per_batch_mean: 2.3392438888549803e-05
        seconds_per_batch_min: 2.2649765014648438e-05
        seconds_per_batch_std: 5.595362019709599e-07
    on_device_inference:
      human_readable:
        batch_latency: -4818031.841 us +/- 18.464 ms [-5127136.230 us, -4781760.216
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.20]
      metrics:
        batches_per_second_max: -0.19504065331000084
        batches_per_second_mean: -0.20755659439852908
        batches_per_second_min: -0.20912801037247616
        batches_per_second_std: 0.000773757922175534
        seconds_per_batch_max: -4.781760215759277
        seconds_per_batch_mean: -4.818031841278076
        seconds_per_batch_min: -5.12713623046875
        seconds_per_batch_std: 0.01846413684222031
    total:
      human_readable:
        batch_latency: 4.994 ms +/- 21.117 us [4.957 ms, 5.409 ms]
        batches_per_second: 200.24 +/- 0.81 [184.87, 201.75]
      metrics:
        batches_per_second_max: 201.74622414622414
        batches_per_second_mean: 200.24321018966938
        batches_per_second_min: 184.8688293370945
        batches_per_second_std: 0.8132071882916581
        seconds_per_batch_max: 0.00540924072265625
        seconds_per_batch_mean: 0.004994012832641601
        seconds_per_batch_min: 0.004956722259521484
        seconds_per_batch_std: 2.111729161594413e-05


#####
fp-fp-py-id - Run 3
2024-02-25 22:53:20
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  5.00it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.99it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 273.71it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:00<00:00, 274.20it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:00<00:00, 275.40it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 275.28it/s]
STAGE:2024-02-25 22:53:09 5601:5601 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:53:09 5601:5601 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:53:09 5601:5601 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:00<00:04, 207.02it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:00<00:04, 207.44it/s]Measuring inference for batch_size=1:   6%|▋         | 63/1000 [00:00<00:04, 207.58it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:00<00:04, 207.63it/s]Measuring inference for batch_size=1:  10%|█         | 105/1000 [00:00<00:04, 207.79it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:00<00:04, 207.86it/s]Measuring inference for batch_size=1:  15%|█▍        | 147/1000 [00:00<00:04, 207.93it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:00<00:04, 207.96it/s]Measuring inference for batch_size=1:  19%|█▉        | 189/1000 [00:00<00:03, 207.99it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:01<00:03, 208.01it/s]Measuring inference for batch_size=1:  23%|██▎       | 231/1000 [00:01<00:03, 207.96it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:01<00:03, 207.93it/s]Measuring inference for batch_size=1:  27%|██▋       | 273/1000 [00:01<00:03, 208.02it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:01<00:03, 208.03it/s]Measuring inference for batch_size=1:  32%|███▏      | 315/1000 [00:01<00:03, 208.06it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:01<00:03, 207.95it/s]Measuring inference for batch_size=1:  36%|███▌      | 357/1000 [00:01<00:03, 207.86it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:01<00:03, 203.58it/s]Measuring inference for batch_size=1:  40%|███▉      | 399/1000 [00:01<00:02, 200.88it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:02<00:02, 199.10it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:02<00:02, 198.00it/s]Measuring inference for batch_size=1:  46%|████▌     | 460/1000 [00:02<00:02, 197.11it/s]Measuring inference for batch_size=1:  48%|████▊     | 480/1000 [00:02<00:02, 196.55it/s]Measuring inference for batch_size=1:  50%|█████     | 500/1000 [00:02<00:02, 195.99it/s]Measuring inference for batch_size=1:  52%|█████▏    | 520/1000 [00:02<00:02, 195.54it/s]Measuring inference for batch_size=1:  54%|█████▍    | 540/1000 [00:02<00:02, 195.30it/s]Measuring inference for batch_size=1:  56%|█████▌    | 560/1000 [00:02<00:02, 195.19it/s]Measuring inference for batch_size=1:  58%|█████▊    | 580/1000 [00:02<00:02, 195.06it/s]Measuring inference for batch_size=1:  60%|██████    | 600/1000 [00:02<00:02, 194.91it/s]Measuring inference for batch_size=1:  62%|██████▏   | 620/1000 [00:03<00:01, 194.81it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [00:03<00:01, 194.86it/s]Measuring inference for batch_size=1:  66%|██████▌   | 660/1000 [00:03<00:01, 194.84it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [00:03<00:01, 194.95it/s]Measuring inference for batch_size=1:  70%|███████   | 700/1000 [00:03<00:01, 194.82it/s]Measuring inference for batch_size=1:  72%|███████▏  | 720/1000 [00:03<00:01, 194.81it/s]Measuring inference for batch_size=1:  74%|███████▍  | 740/1000 [00:03<00:01, 194.94it/s]Measuring inference for batch_size=1:  76%|███████▌  | 760/1000 [00:03<00:01, 194.83it/s]Measuring inference for batch_size=1:  78%|███████▊  | 780/1000 [00:03<00:01, 194.75it/s]Measuring inference for batch_size=1:  80%|████████  | 800/1000 [00:03<00:01, 194.80it/s]Measuring inference for batch_size=1:  82%|████████▏ | 820/1000 [00:04<00:00, 194.85it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:04<00:00, 194.88it/s]Measuring inference for batch_size=1:  86%|████████▌ | 860/1000 [00:04<00:00, 194.85it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [00:04<00:00, 194.91it/s]Measuring inference for batch_size=1:  90%|█████████ | 900/1000 [00:04<00:00, 194.88it/s]Measuring inference for batch_size=1:  92%|█████████▏| 920/1000 [00:04<00:00, 194.79it/s]Measuring inference for batch_size=1:  94%|█████████▍| 940/1000 [00:04<00:00, 194.76it/s]Measuring inference for batch_size=1:  96%|█████████▌| 960/1000 [00:04<00:00, 194.82it/s]Measuring inference for batch_size=1:  98%|█████████▊| 980/1000 [00:04<00:00, 194.78it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:05<00:00, 194.70it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:05<00:00, 199.28it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=16:  21%|██        | 21/100 [00:00<00:00, 202.60it/s]Warming up with batch_size=16:  42%|████▏     | 42/100 [00:00<00:00, 202.89it/s]Warming up with batch_size=16:  63%|██████▎   | 63/100 [00:00<00:00, 203.05it/s]Warming up with batch_size=16:  84%|████████▍ | 84/100 [00:00<00:00, 203.05it/s]Warming up with batch_size=16: 100%|██████████| 100/100 [00:00<00:00, 203.03it/s]
STAGE:2024-02-25 22:53:15 5601:5601 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:53:15 5601:5601 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:53:15 5601:5601 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=16:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=16:   2%|▏         | 20/1000 [00:00<00:04, 199.00it/s]Measuring inference for batch_size=16:   4%|▍         | 41/1000 [00:00<00:04, 199.72it/s]Measuring inference for batch_size=16:   6%|▌         | 62/1000 [00:00<00:04, 200.04it/s]Measuring inference for batch_size=16:   8%|▊         | 83/1000 [00:00<00:04, 200.29it/s]Measuring inference for batch_size=16:  10%|█         | 104/1000 [00:00<00:04, 200.39it/s]Measuring inference for batch_size=16:  12%|█▎        | 125/1000 [00:00<00:04, 200.45it/s]Measuring inference for batch_size=16:  15%|█▍        | 146/1000 [00:00<00:04, 200.48it/s]Measuring inference for batch_size=16:  17%|█▋        | 167/1000 [00:00<00:04, 200.43it/s]Measuring inference for batch_size=16:  19%|█▉        | 188/1000 [00:00<00:04, 200.40it/s]Measuring inference for batch_size=16:  21%|██        | 209/1000 [00:01<00:03, 200.33it/s]Measuring inference for batch_size=16:  23%|██▎       | 230/1000 [00:01<00:03, 200.32it/s]Measuring inference for batch_size=16:  25%|██▌       | 251/1000 [00:01<00:03, 200.38it/s]Measuring inference for batch_size=16:  27%|██▋       | 272/1000 [00:01<00:03, 200.41it/s]Measuring inference for batch_size=16:  29%|██▉       | 293/1000 [00:01<00:03, 200.42it/s]Measuring inference for batch_size=16:  31%|███▏      | 314/1000 [00:01<00:03, 200.40it/s]Measuring inference for batch_size=16:  34%|███▎      | 335/1000 [00:01<00:03, 200.33it/s]Measuring inference for batch_size=16:  36%|███▌      | 356/1000 [00:01<00:03, 200.26it/s]Measuring inference for batch_size=16:  38%|███▊      | 377/1000 [00:01<00:03, 200.22it/s]Measuring inference for batch_size=16:  40%|███▉      | 398/1000 [00:01<00:03, 200.25it/s]Measuring inference for batch_size=16:  42%|████▏     | 419/1000 [00:02<00:02, 200.32it/s]Measuring inference for batch_size=16:  44%|████▍     | 440/1000 [00:02<00:02, 200.41it/s]Measuring inference for batch_size=16:  46%|████▌     | 461/1000 [00:02<00:02, 200.48it/s]Measuring inference for batch_size=16:  48%|████▊     | 482/1000 [00:02<00:02, 200.53it/s]Measuring inference for batch_size=16:  50%|█████     | 503/1000 [00:02<00:02, 200.54it/s]Measuring inference for batch_size=16:  52%|█████▏    | 524/1000 [00:02<00:02, 200.53it/s]Measuring inference for batch_size=16:  55%|█████▍    | 545/1000 [00:02<00:02, 200.57it/s]Measuring inference for batch_size=16:  57%|█████▋    | 566/1000 [00:02<00:02, 200.61it/s]Measuring inference for batch_size=16:  59%|█████▊    | 587/1000 [00:02<00:02, 200.60it/s]Measuring inference for batch_size=16:  61%|██████    | 608/1000 [00:03<00:01, 200.61it/s]Measuring inference for batch_size=16:  63%|██████▎   | 629/1000 [00:03<00:01, 200.60it/s]Measuring inference for batch_size=16:  65%|██████▌   | 650/1000 [00:03<00:01, 200.58it/s]Measuring inference for batch_size=16:  67%|██████▋   | 671/1000 [00:03<00:01, 200.56it/s]Measuring inference for batch_size=16:  69%|██████▉   | 692/1000 [00:03<00:01, 200.51it/s]Measuring inference for batch_size=16:  71%|███████▏  | 713/1000 [00:03<00:01, 200.54it/s]Measuring inference for batch_size=16:  73%|███████▎  | 734/1000 [00:03<00:01, 200.60it/s]Measuring inference for batch_size=16:  76%|███████▌  | 755/1000 [00:03<00:01, 200.54it/s]Measuring inference for batch_size=16:  78%|███████▊  | 776/1000 [00:03<00:01, 200.55it/s]Measuring inference for batch_size=16:  80%|███████▉  | 797/1000 [00:03<00:01, 200.47it/s]Measuring inference for batch_size=16:  82%|████████▏ | 818/1000 [00:04<00:00, 198.63it/s]Measuring inference for batch_size=16:  84%|████████▍ | 838/1000 [00:04<00:00, 195.88it/s]Measuring inference for batch_size=16:  86%|████████▌ | 858/1000 [00:04<00:00, 193.97it/s]Measuring inference for batch_size=16:  88%|████████▊ | 878/1000 [00:04<00:00, 192.64it/s]Measuring inference for batch_size=16:  90%|████████▉ | 898/1000 [00:04<00:00, 191.63it/s]Measuring inference for batch_size=16:  92%|█████████▏| 918/1000 [00:04<00:00, 191.07it/s]Measuring inference for batch_size=16:  94%|█████████▍| 938/1000 [00:04<00:00, 190.65it/s]Measuring inference for batch_size=16:  96%|█████████▌| 958/1000 [00:04<00:00, 190.26it/s]Measuring inference for batch_size=16:  98%|█████████▊| 978/1000 [00:04<00:00, 189.95it/s]Measuring inference for batch_size=16: 100%|█████████▉| 997/1000 [00:05<00:00, 189.89it/s]Measuring inference for batch_size=16: 100%|██████████| 1000/1000 [00:05<00:00, 198.22it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 29.98 GB
    total: 31.28 GB
    used: 916.45 MB
  system:
    node: fp
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_16:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 94.788 us +/- 2.996 us [91.791 us, 146.627 us]
        batches_per_second: 10.56 K +/- 289.22 [6.82 K, 10.89 K]
      metrics:
        batches_per_second_max: 10894.296103896104
        batches_per_second_mean: 10558.896164227664
        batches_per_second_min: 6820.006504065041
        batches_per_second_std: 289.22424427592034
        seconds_per_batch_max: 0.00014662742614746094
        seconds_per_batch_mean: 9.47878360748291e-05
        seconds_per_batch_min: 9.179115295410156e-05
        seconds_per_batch_std: 2.996109051529508e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 24.224 us +/- 0.721 us [22.888 us, 31.471 us]
        batches_per_second: 41.31 K +/- 1.14 K [31.78 K, 43.69 K]
      metrics:
        batches_per_second_max: 43690.666666666664
        batches_per_second_mean: 41314.66150699881
        batches_per_second_min: 31775.030303030304
        batches_per_second_std: 1140.4402217415732
        seconds_per_batch_max: 3.147125244140625e-05
        seconds_per_batch_mean: 2.4224281311035156e-05
        seconds_per_batch_min: 2.288818359375e-05
        seconds_per_batch_std: 7.214746327470491e-07
    on_device_inference:
      human_readable:
        batch_latency: -4885695.383 us +/- 152.634 ms [-5178815.842 us, -4656672.001
          us]
        batches_per_second: -0.20 +/- 0.01 [-0.21, -0.19]
      metrics:
        batches_per_second_max: -0.19309433480001958
        batches_per_second_mean: -0.20488281639367892
        batches_per_second_min: -0.21474563804578636
        batches_per_second_std: 0.006519214902527506
        seconds_per_batch_max: -4.65667200088501
        seconds_per_batch_mean: -4.8856953830719
        seconds_per_batch_min: -5.178815841674805
        seconds_per_batch_std: 0.1526337502065038
    total:
      human_readable:
        batch_latency: 5.014 ms +/- 155.423 us [4.780 ms, 5.310 ms]
        batches_per_second: 199.63 +/- 6.30 [188.32, 209.20]
      metrics:
        batches_per_second_max: 209.20265349892762
        batches_per_second_mean: 199.6334553231111
        batches_per_second_min: 188.32183908045977
        batches_per_second_std: 6.3020034957135085
        seconds_per_batch_max: 0.00531005859375
        seconds_per_batch_mean: 0.0050140867233276366
        seconds_per_batch_min: 0.0047800540924072266
        seconds_per_batch_std: 0.00015542348502762832
  batch_size_16:
    cpu_to_gpu:
      human_readable:
        batch_latency: 144.952 us +/- 4.261 us [142.336 us, 244.379 us]
        batches_per_second: 6.90 K +/- 157.35 [4.09 K, 7.03 K]
      metrics:
        batches_per_second_max: 7025.634840871022
        batches_per_second_mean: 6903.345675129453
        batches_per_second_min: 4092.0039024390244
        batches_per_second_std: 157.34891472183247
        seconds_per_batch_max: 0.00024437904357910156
        seconds_per_batch_mean: 0.00014495182037353515
        seconds_per_batch_min: 0.0001423358917236328
        seconds_per_batch_std: 4.261035735584487e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.834 us +/- 0.676 us [22.888 us, 29.087 us]
        batches_per_second: 41.99 K +/- 1.11 K [34.38 K, 43.69 K]
      metrics:
        batches_per_second_max: 43690.666666666664
        batches_per_second_mean: 41987.96100795131
        batches_per_second_min: 34379.54098360656
        batches_per_second_std: 1113.1328386826867
        seconds_per_batch_max: 2.9087066650390625e-05
        seconds_per_batch_mean: 2.3834228515625e-05
        seconds_per_batch_min: 2.288818359375e-05
        seconds_per_batch_std: 6.763265685531431e-07
    on_device_inference:
      human_readable:
        batch_latency: -4866735.137 us +/- 112.631 ms [-5171967.983 us, -4789887.905
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.19]
      metrics:
        batches_per_second_max: -0.1933499981514609
        batches_per_second_mean: -0.20558289675461422
        batches_per_second_min: -0.2087731529021596
        batches_per_second_std: 0.004594785933422605
        seconds_per_batch_max: -4.78988790512085
        seconds_per_batch_mean: -4.866735136508941
        seconds_per_batch_min: -5.17196798324585
        seconds_per_batch_std: 0.11263071560975026
    total:
      human_readable:
        batch_latency: 5.041 ms +/- 114.754 us [4.963 ms, 5.452 ms]
        batches_per_second: 198.47 +/- 4.36 [183.43, 201.50]
      metrics:
        batches_per_second_max: 201.50391544559213
        batches_per_second_mean: 198.4704600417702
        batches_per_second_min: 183.42972098311904
        batches_per_second_std: 4.36486749448769
        seconds_per_batch_max: 0.005451679229736328
        seconds_per_batch_mean: 0.005041056871414184
        seconds_per_batch_min: 0.0049626827239990234
        seconds_per_batch_std: 0.00011475433243282918


