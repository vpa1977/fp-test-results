#####
fp-fp-py-id - Run 1
2024-02-25 22:55:04
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  5.14it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  5.14it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  29%|██▉       | 29/100 [00:00<00:00, 286.79it/s]Warming up with batch_size=1:  58%|█████▊    | 58/100 [00:00<00:00, 288.32it/s]Warming up with batch_size=1:  88%|████████▊ | 88/100 [00:00<00:00, 289.43it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 289.18it/s]
STAGE:2024-02-25 22:54:52 5835:5835 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:54:52 5835:5835 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:54:52 5835:5835 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 22/1000 [00:00<00:04, 217.44it/s]Measuring inference for batch_size=1:   4%|▍         | 44/1000 [00:00<00:04, 217.90it/s]Measuring inference for batch_size=1:   7%|▋         | 66/1000 [00:00<00:04, 218.20it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:00<00:04, 218.40it/s]Measuring inference for batch_size=1:  11%|█         | 110/1000 [00:00<00:04, 218.37it/s]Measuring inference for batch_size=1:  13%|█▎        | 132/1000 [00:00<00:03, 218.40it/s]Measuring inference for batch_size=1:  15%|█▌        | 154/1000 [00:00<00:03, 218.53it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:00<00:03, 218.52it/s]Measuring inference for batch_size=1:  20%|█▉        | 198/1000 [00:00<00:03, 218.57it/s]Measuring inference for batch_size=1:  22%|██▏       | 220/1000 [00:01<00:03, 218.62it/s]Measuring inference for batch_size=1:  24%|██▍       | 242/1000 [00:01<00:03, 218.67it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:01<00:03, 218.76it/s]Measuring inference for batch_size=1:  29%|██▊       | 286/1000 [00:01<00:03, 218.85it/s]Measuring inference for batch_size=1:  31%|███       | 308/1000 [00:01<00:03, 218.97it/s]Measuring inference for batch_size=1:  33%|███▎      | 330/1000 [00:01<00:03, 219.04it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:01<00:02, 218.95it/s]Measuring inference for batch_size=1:  37%|███▋      | 374/1000 [00:01<00:02, 218.85it/s]Measuring inference for batch_size=1:  40%|███▉      | 396/1000 [00:01<00:02, 218.82it/s]Measuring inference for batch_size=1:  42%|████▏     | 418/1000 [00:01<00:02, 218.92it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:02<00:02, 218.97it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:02<00:02, 219.04it/s]Measuring inference for batch_size=1:  48%|████▊     | 484/1000 [00:02<00:02, 219.06it/s]Measuring inference for batch_size=1:  51%|█████     | 506/1000 [00:02<00:02, 219.02it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:02<00:02, 218.88it/s]Measuring inference for batch_size=1:  55%|█████▌    | 550/1000 [00:02<00:02, 218.79it/s]Measuring inference for batch_size=1:  57%|█████▋    | 572/1000 [00:02<00:01, 218.70it/s]Measuring inference for batch_size=1:  59%|█████▉    | 594/1000 [00:02<00:01, 218.61it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:02<00:01, 218.49it/s]Measuring inference for batch_size=1:  64%|██████▍   | 638/1000 [00:02<00:01, 218.40it/s]Measuring inference for batch_size=1:  66%|██████▌   | 660/1000 [00:03<00:01, 218.50it/s]Measuring inference for batch_size=1:  68%|██████▊   | 682/1000 [00:03<00:01, 218.52it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [00:03<00:01, 218.67it/s]Measuring inference for batch_size=1:  73%|███████▎  | 726/1000 [00:03<00:01, 218.62it/s]Measuring inference for batch_size=1:  75%|███████▍  | 748/1000 [00:03<00:01, 218.68it/s]Measuring inference for batch_size=1:  77%|███████▋  | 770/1000 [00:03<00:01, 218.61it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [00:03<00:00, 218.58it/s]Measuring inference for batch_size=1:  81%|████████▏ | 814/1000 [00:03<00:00, 218.58it/s]Measuring inference for batch_size=1:  84%|████████▎ | 836/1000 [00:03<00:00, 218.59it/s]Measuring inference for batch_size=1:  86%|████████▌ | 858/1000 [00:03<00:00, 218.58it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [00:04<00:00, 218.60it/s]Measuring inference for batch_size=1:  90%|█████████ | 902/1000 [00:04<00:00, 218.67it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [00:04<00:00, 218.66it/s]Measuring inference for batch_size=1:  95%|█████████▍| 946/1000 [00:04<00:00, 218.70it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [00:04<00:00, 218.72it/s]Measuring inference for batch_size=1:  99%|█████████▉| 990/1000 [00:04<00:00, 218.72it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 218.67it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=64:  22%|██▏       | 22/100 [00:00<00:00, 210.97it/s]Warming up with batch_size=64:  44%|████▍     | 44/100 [00:00<00:00, 211.39it/s]Warming up with batch_size=64:  66%|██████▌   | 66/100 [00:00<00:00, 211.46it/s]Warming up with batch_size=64:  88%|████████▊ | 88/100 [00:00<00:00, 211.41it/s]Warming up with batch_size=64: 100%|██████████| 100/100 [00:00<00:00, 211.36it/s]
STAGE:2024-02-25 22:54:57 5835:5835 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:54:57 5835:5835 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:54:57 5835:5835 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=64:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=64:   2%|▏         | 21/1000 [00:00<00:04, 207.82it/s]Measuring inference for batch_size=64:   4%|▍         | 42/1000 [00:00<00:04, 208.22it/s]Measuring inference for batch_size=64:   6%|▋         | 63/1000 [00:00<00:04, 208.47it/s]Measuring inference for batch_size=64:   8%|▊         | 84/1000 [00:00<00:04, 208.54it/s]Measuring inference for batch_size=64:  10%|█         | 105/1000 [00:00<00:04, 208.63it/s]Measuring inference for batch_size=64:  13%|█▎        | 126/1000 [00:00<00:04, 208.73it/s]Measuring inference for batch_size=64:  15%|█▍        | 147/1000 [00:00<00:04, 208.79it/s]Measuring inference for batch_size=64:  17%|█▋        | 168/1000 [00:00<00:03, 208.76it/s]Measuring inference for batch_size=64:  19%|█▉        | 189/1000 [00:00<00:03, 208.74it/s]Measuring inference for batch_size=64:  21%|██        | 210/1000 [00:01<00:03, 208.76it/s]Measuring inference for batch_size=64:  23%|██▎       | 231/1000 [00:01<00:03, 208.77it/s]Measuring inference for batch_size=64:  25%|██▌       | 252/1000 [00:01<00:03, 208.80it/s]Measuring inference for batch_size=64:  27%|██▋       | 273/1000 [00:01<00:03, 208.76it/s]Measuring inference for batch_size=64:  29%|██▉       | 294/1000 [00:01<00:03, 208.83it/s]Measuring inference for batch_size=64:  32%|███▏      | 315/1000 [00:01<00:03, 209.00it/s]Measuring inference for batch_size=64:  34%|███▎      | 336/1000 [00:01<00:03, 209.00it/s]Measuring inference for batch_size=64:  36%|███▌      | 357/1000 [00:01<00:03, 208.95it/s]Measuring inference for batch_size=64:  38%|███▊      | 378/1000 [00:01<00:02, 208.99it/s]Measuring inference for batch_size=64:  40%|███▉      | 399/1000 [00:01<00:02, 208.91it/s]Measuring inference for batch_size=64:  42%|████▏     | 420/1000 [00:02<00:02, 208.97it/s]Measuring inference for batch_size=64:  44%|████▍     | 441/1000 [00:02<00:02, 208.96it/s]Measuring inference for batch_size=64:  46%|████▌     | 462/1000 [00:02<00:02, 208.95it/s]Measuring inference for batch_size=64:  48%|████▊     | 483/1000 [00:02<00:02, 209.03it/s]Measuring inference for batch_size=64:  50%|█████     | 504/1000 [00:02<00:02, 208.92it/s]Measuring inference for batch_size=64:  52%|█████▎    | 525/1000 [00:02<00:02, 208.83it/s]Measuring inference for batch_size=64:  55%|█████▍    | 546/1000 [00:02<00:02, 208.85it/s]Measuring inference for batch_size=64:  57%|█████▋    | 567/1000 [00:02<00:02, 208.85it/s]Measuring inference for batch_size=64:  59%|█████▉    | 588/1000 [00:02<00:01, 208.86it/s]Measuring inference for batch_size=64:  61%|██████    | 609/1000 [00:02<00:01, 208.89it/s]Measuring inference for batch_size=64:  63%|██████▎   | 630/1000 [00:03<00:01, 208.85it/s]Measuring inference for batch_size=64:  65%|██████▌   | 651/1000 [00:03<00:01, 208.92it/s]Measuring inference for batch_size=64:  67%|██████▋   | 672/1000 [00:03<00:01, 208.96it/s]Measuring inference for batch_size=64:  69%|██████▉   | 693/1000 [00:03<00:01, 208.92it/s]Measuring inference for batch_size=64:  71%|███████▏  | 714/1000 [00:03<00:01, 208.95it/s]Measuring inference for batch_size=64:  74%|███████▎  | 735/1000 [00:03<00:01, 208.85it/s]Measuring inference for batch_size=64:  76%|███████▌  | 756/1000 [00:03<00:01, 208.77it/s]Measuring inference for batch_size=64:  78%|███████▊  | 777/1000 [00:03<00:01, 208.83it/s]Measuring inference for batch_size=64:  80%|███████▉  | 798/1000 [00:03<00:00, 208.71it/s]Measuring inference for batch_size=64:  82%|████████▏ | 819/1000 [00:03<00:00, 208.70it/s]Measuring inference for batch_size=64:  84%|████████▍ | 840/1000 [00:04<00:00, 208.80it/s]Measuring inference for batch_size=64:  86%|████████▌ | 861/1000 [00:04<00:00, 208.83it/s]Measuring inference for batch_size=64:  88%|████████▊ | 882/1000 [00:04<00:00, 208.62it/s]Measuring inference for batch_size=64:  90%|█████████ | 903/1000 [00:04<00:00, 208.52it/s]Measuring inference for batch_size=64:  92%|█████████▏| 924/1000 [00:04<00:00, 208.42it/s]Measuring inference for batch_size=64:  94%|█████████▍| 945/1000 [00:04<00:00, 208.40it/s]Measuring inference for batch_size=64:  97%|█████████▋| 966/1000 [00:04<00:00, 208.39it/s]Measuring inference for batch_size=64:  99%|█████████▊| 987/1000 [00:04<00:00, 208.26it/s]Measuring inference for batch_size=64: 100%|██████████| 1000/1000 [00:04<00:00, 208.74it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 29.98 GB
    total: 31.28 GB
    used: 918.16 MB
  system:
    node: fp
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_64:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 91.395 us +/- 2.844 us [89.884 us, 145.674 us]
        batches_per_second: 10.95 K +/- 282.11 [6.86 K, 11.13 K]
      metrics:
        batches_per_second_max: 11125.474801061007
        batches_per_second_mean: 10950.203074784704
        batches_per_second_min: 6864.654664484452
        batches_per_second_std: 282.11137102966427
        seconds_per_batch_max: 0.0001456737518310547
        seconds_per_batch_mean: 9.139466285705566e-05
        seconds_per_batch_min: 8.988380432128906e-05
        seconds_per_batch_std: 2.84443887666208e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 22.521 us +/- 0.624 us [21.696 us, 29.325 us]
        batches_per_second: 44.43 K +/- 1.06 K [34.10 K, 46.09 K]
      metrics:
        batches_per_second_max: 46091.25274725275
        batches_per_second_mean: 44433.01517898306
        batches_per_second_min: 34100.03252032521
        batches_per_second_std: 1055.1637793867772
        seconds_per_batch_max: 2.9325485229492188e-05
        seconds_per_batch_mean: 2.252054214477539e-05
        seconds_per_batch_min: 2.1696090698242188e-05
        seconds_per_batch_std: 6.238296727216034e-07
    on_device_inference:
      human_readable:
        batch_latency: -4449652.889 us +/- 17.968 ms [-4692863.941 us, -4413856.030
          us]
        batches_per_second: -0.22 +/- 0.00 [-0.23, -0.21]
      metrics:
        batches_per_second_max: -0.21308949343753267
        batches_per_second_mean: -0.22474024750348007
        batches_per_second_min: -0.22655927001563328
        batches_per_second_std: 0.0008957609130094682
        seconds_per_batch_max: -4.413856029510498
        seconds_per_batch_mean: -4.449652889251709
        seconds_per_batch_min: -4.692863941192627
        seconds_per_batch_std: 0.017967577887692807
    total:
      human_readable:
        batch_latency: 4.570 ms +/- 19.301 us [4.532 ms, 4.879 ms]
        batches_per_second: 218.84 +/- 0.91 [204.94, 220.64]
      metrics:
        batches_per_second_max: 220.63671751709626
        batches_per_second_mean: 218.8375221038783
        batches_per_second_min: 204.94009576859182
        batches_per_second_std: 0.906730179935786
        seconds_per_batch_max: 0.004879474639892578
        seconds_per_batch_mean: 0.004569680213928223
        seconds_per_batch_min: 0.004532337188720703
        seconds_per_batch_std: 1.9301402238379873e-05
  batch_size_64:
    cpu_to_gpu:
      human_readable:
        batch_latency: 141.968 us +/- 4.297 us [140.190 us, 243.187 us]
        batches_per_second: 7.05 K +/- 161.80 [4.11 K, 7.13 K]
      metrics:
        batches_per_second_max: 7133.170068027211
        batches_per_second_mean: 7048.583387695285
        batches_per_second_min: 4112.062745098039
        batches_per_second_std: 161.8007600602939
        seconds_per_batch_max: 0.00024318695068359375
        seconds_per_batch_mean: 0.0001419684886932373
        seconds_per_batch_min: 0.00014019012451171875
        seconds_per_batch_std: 4.296871145328217e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 22.741 us +/- 0.493 us [21.935 us, 28.610 us]
        batches_per_second: 43.99 K +/- 874.70 [34.95 K, 45.59 K]
      metrics:
        batches_per_second_max: 45590.260869565216
        batches_per_second_mean: 43991.71175694167
        batches_per_second_min: 34952.53333333333
        batches_per_second_std: 874.6992759345902
        seconds_per_batch_max: 2.86102294921875e-05
        seconds_per_batch_mean: 2.2741317749023438e-05
        seconds_per_batch_min: 2.193450927734375e-05
        seconds_per_batch_std: 4.930450099825323e-07
    on_device_inference:
      human_readable:
        batch_latency: -4616998.018 us +/- 18.382 ms [-4941440.105 us, -4588287.830
          us]
        batches_per_second: -0.22 +/- 0.00 [-0.22, -0.20]
      metrics:
        batches_per_second_max: -0.20237015498770572
        batches_per_second_mean: -0.21659430847299418
        batches_per_second_min: -0.21794622241977182
        batches_per_second_std: 0.0008431984678438277
        seconds_per_batch_max: -4.588287830352783
        seconds_per_batch_mean: -4.616998018264771
        seconds_per_batch_min: -4.941440105438232
        seconds_per_batch_std: 0.018382254397482676
    total:
      human_readable:
        batch_latency: 4.787 ms +/- 20.826 us [4.757 ms, 5.219 ms]
        batches_per_second: 208.90 +/- 0.88 [191.61, 210.21]
      metrics:
        batches_per_second_max: 210.20919160026062
        batches_per_second_mean: 208.90026771039607
        batches_per_second_min: 191.60822293284605
        batches_per_second_std: 0.8755946610223782
        seconds_per_batch_max: 0.005218982696533203
        seconds_per_batch_mean: 0.004787060499191284
        seconds_per_batch_min: 0.0047571659088134766
        seconds_per_batch_std: 2.0826046755338757e-05


#####
fp-fp-py-id - Run 2
2024-02-25 22:55:22
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 277.71it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:00<00:00, 278.27it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:00<00:00, 278.96it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 279.10it/s]
STAGE:2024-02-25 22:55:10 5881:5881 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:55:10 5881:5881 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:55:10 5881:5881 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:00<00:04, 209.62it/s]Measuring inference for batch_size=1:   4%|▍         | 43/1000 [00:00<00:04, 209.95it/s]Measuring inference for batch_size=1:   6%|▋         | 65/1000 [00:00<00:04, 210.02it/s]Measuring inference for batch_size=1:   9%|▊         | 87/1000 [00:00<00:04, 210.14it/s]Measuring inference for batch_size=1:  11%|█         | 109/1000 [00:00<00:04, 210.26it/s]Measuring inference for batch_size=1:  13%|█▎        | 131/1000 [00:00<00:04, 210.29it/s]Measuring inference for batch_size=1:  15%|█▌        | 153/1000 [00:00<00:04, 210.31it/s]Measuring inference for batch_size=1:  18%|█▊        | 175/1000 [00:00<00:03, 210.35it/s]Measuring inference for batch_size=1:  20%|█▉        | 197/1000 [00:00<00:03, 210.29it/s]Measuring inference for batch_size=1:  22%|██▏       | 219/1000 [00:01<00:03, 210.32it/s]Measuring inference for batch_size=1:  24%|██▍       | 241/1000 [00:01<00:03, 210.33it/s]Measuring inference for batch_size=1:  26%|██▋       | 263/1000 [00:01<00:03, 210.27it/s]Measuring inference for batch_size=1:  28%|██▊       | 285/1000 [00:01<00:03, 210.35it/s]Measuring inference for batch_size=1:  31%|███       | 307/1000 [00:01<00:03, 210.32it/s]Measuring inference for batch_size=1:  33%|███▎      | 329/1000 [00:01<00:03, 210.36it/s]Measuring inference for batch_size=1:  35%|███▌      | 351/1000 [00:01<00:03, 210.27it/s]Measuring inference for batch_size=1:  37%|███▋      | 373/1000 [00:01<00:02, 210.27it/s]Measuring inference for batch_size=1:  40%|███▉      | 395/1000 [00:01<00:02, 210.30it/s]Measuring inference for batch_size=1:  42%|████▏     | 417/1000 [00:01<00:02, 210.36it/s]Measuring inference for batch_size=1:  44%|████▍     | 439/1000 [00:02<00:02, 210.30it/s]Measuring inference for batch_size=1:  46%|████▌     | 461/1000 [00:02<00:02, 210.32it/s]Measuring inference for batch_size=1:  48%|████▊     | 483/1000 [00:02<00:02, 210.38it/s]Measuring inference for batch_size=1:  50%|█████     | 505/1000 [00:02<00:02, 210.42it/s]Measuring inference for batch_size=1:  53%|█████▎    | 527/1000 [00:02<00:02, 210.47it/s]Measuring inference for batch_size=1:  55%|█████▍    | 549/1000 [00:02<00:02, 210.44it/s]Measuring inference for batch_size=1:  57%|█████▋    | 571/1000 [00:02<00:02, 210.38it/s]Measuring inference for batch_size=1:  59%|█████▉    | 593/1000 [00:02<00:01, 210.40it/s]Measuring inference for batch_size=1:  62%|██████▏   | 615/1000 [00:02<00:01, 210.41it/s]Measuring inference for batch_size=1:  64%|██████▎   | 637/1000 [00:03<00:01, 210.42it/s]Measuring inference for batch_size=1:  66%|██████▌   | 659/1000 [00:03<00:01, 210.44it/s]Measuring inference for batch_size=1:  68%|██████▊   | 681/1000 [00:03<00:01, 210.41it/s]Measuring inference for batch_size=1:  70%|███████   | 703/1000 [00:03<00:01, 210.33it/s]Measuring inference for batch_size=1:  72%|███████▎  | 725/1000 [00:03<00:01, 210.25it/s]Measuring inference for batch_size=1:  75%|███████▍  | 747/1000 [00:03<00:01, 210.34it/s]Measuring inference for batch_size=1:  77%|███████▋  | 769/1000 [00:03<00:01, 210.31it/s]Measuring inference for batch_size=1:  79%|███████▉  | 791/1000 [00:03<00:00, 210.33it/s]Measuring inference for batch_size=1:  81%|████████▏ | 813/1000 [00:03<00:00, 210.34it/s]Measuring inference for batch_size=1:  84%|████████▎ | 835/1000 [00:03<00:00, 210.34it/s]Measuring inference for batch_size=1:  86%|████████▌ | 857/1000 [00:04<00:00, 210.27it/s]Measuring inference for batch_size=1:  88%|████████▊ | 879/1000 [00:04<00:00, 210.31it/s]Measuring inference for batch_size=1:  90%|█████████ | 901/1000 [00:04<00:00, 210.33it/s]Measuring inference for batch_size=1:  92%|█████████▏| 923/1000 [00:04<00:00, 210.35it/s]Measuring inference for batch_size=1:  94%|█████████▍| 945/1000 [00:04<00:00, 210.37it/s]Measuring inference for batch_size=1:  97%|█████████▋| 967/1000 [00:04<00:00, 210.36it/s]Measuring inference for batch_size=1:  99%|█████████▉| 989/1000 [00:04<00:00, 210.34it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 210.32it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=64:  21%|██        | 21/100 [00:00<00:00, 204.98it/s]Warming up with batch_size=64:  42%|████▏     | 42/100 [00:00<00:00, 205.28it/s]Warming up with batch_size=64:  63%|██████▎   | 63/100 [00:00<00:00, 205.42it/s]Warming up with batch_size=64:  84%|████████▍ | 84/100 [00:00<00:00, 205.33it/s]Warming up with batch_size=64: 100%|██████████| 100/100 [00:00<00:00, 205.32it/s]
STAGE:2024-02-25 22:55:15 5881:5881 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:55:15 5881:5881 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:55:15 5881:5881 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=64:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=64:   2%|▏         | 21/1000 [00:00<00:04, 201.57it/s]Measuring inference for batch_size=64:   4%|▍         | 42/1000 [00:00<00:04, 201.97it/s]Measuring inference for batch_size=64:   6%|▋         | 63/1000 [00:00<00:04, 202.03it/s]Measuring inference for batch_size=64:   8%|▊         | 84/1000 [00:00<00:04, 202.11it/s]Measuring inference for batch_size=64:  10%|█         | 105/1000 [00:00<00:04, 202.10it/s]Measuring inference for batch_size=64:  13%|█▎        | 126/1000 [00:00<00:04, 202.00it/s]Measuring inference for batch_size=64:  15%|█▍        | 147/1000 [00:00<00:04, 201.63it/s]Measuring inference for batch_size=64:  17%|█▋        | 168/1000 [00:00<00:04, 201.62it/s]Measuring inference for batch_size=64:  19%|█▉        | 189/1000 [00:00<00:04, 201.79it/s]Measuring inference for batch_size=64:  21%|██        | 210/1000 [00:01<00:03, 201.91it/s]Measuring inference for batch_size=64:  23%|██▎       | 231/1000 [00:01<00:03, 202.04it/s]Measuring inference for batch_size=64:  25%|██▌       | 252/1000 [00:01<00:03, 202.12it/s]Measuring inference for batch_size=64:  27%|██▋       | 273/1000 [00:01<00:03, 202.16it/s]Measuring inference for batch_size=64:  29%|██▉       | 294/1000 [00:01<00:03, 202.24it/s]Measuring inference for batch_size=64:  32%|███▏      | 315/1000 [00:01<00:03, 202.33it/s]Measuring inference for batch_size=64:  34%|███▎      | 336/1000 [00:01<00:03, 202.36it/s]Measuring inference for batch_size=64:  36%|███▌      | 357/1000 [00:01<00:03, 202.36it/s]Measuring inference for batch_size=64:  38%|███▊      | 378/1000 [00:01<00:03, 202.31it/s]Measuring inference for batch_size=64:  40%|███▉      | 399/1000 [00:01<00:02, 202.35it/s]Measuring inference for batch_size=64:  42%|████▏     | 420/1000 [00:02<00:02, 202.37it/s]Measuring inference for batch_size=64:  44%|████▍     | 441/1000 [00:02<00:02, 202.34it/s]Measuring inference for batch_size=64:  46%|████▌     | 462/1000 [00:02<00:02, 202.34it/s]Measuring inference for batch_size=64:  48%|████▊     | 483/1000 [00:02<00:02, 202.30it/s]Measuring inference for batch_size=64:  50%|█████     | 504/1000 [00:02<00:02, 202.27it/s]Measuring inference for batch_size=64:  52%|█████▎    | 525/1000 [00:02<00:02, 202.16it/s]Measuring inference for batch_size=64:  55%|█████▍    | 546/1000 [00:02<00:02, 202.17it/s]Measuring inference for batch_size=64:  57%|█████▋    | 567/1000 [00:02<00:02, 202.18it/s]Measuring inference for batch_size=64:  59%|█████▉    | 588/1000 [00:02<00:02, 202.15it/s]Measuring inference for batch_size=64:  61%|██████    | 609/1000 [00:03<00:01, 202.14it/s]Measuring inference for batch_size=64:  63%|██████▎   | 630/1000 [00:03<00:01, 202.13it/s]Measuring inference for batch_size=64:  65%|██████▌   | 651/1000 [00:03<00:01, 202.17it/s]Measuring inference for batch_size=64:  67%|██████▋   | 672/1000 [00:03<00:01, 202.20it/s]Measuring inference for batch_size=64:  69%|██████▉   | 693/1000 [00:03<00:01, 202.24it/s]Measuring inference for batch_size=64:  71%|███████▏  | 714/1000 [00:03<00:01, 202.21it/s]Measuring inference for batch_size=64:  74%|███████▎  | 735/1000 [00:03<00:01, 202.24it/s]Measuring inference for batch_size=64:  76%|███████▌  | 756/1000 [00:03<00:01, 202.25it/s]Measuring inference for batch_size=64:  78%|███████▊  | 777/1000 [00:03<00:01, 200.65it/s]Measuring inference for batch_size=64:  80%|███████▉  | 798/1000 [00:03<00:01, 201.05it/s]Measuring inference for batch_size=64:  82%|████████▏ | 819/1000 [00:04<00:00, 201.36it/s]Measuring inference for batch_size=64:  84%|████████▍ | 840/1000 [00:04<00:00, 201.60it/s]Measuring inference for batch_size=64:  86%|████████▌ | 861/1000 [00:04<00:00, 201.79it/s]Measuring inference for batch_size=64:  88%|████████▊ | 882/1000 [00:04<00:00, 201.85it/s]Measuring inference for batch_size=64:  90%|█████████ | 903/1000 [00:04<00:00, 201.90it/s]Measuring inference for batch_size=64:  92%|█████████▏| 924/1000 [00:04<00:00, 201.97it/s]Measuring inference for batch_size=64:  94%|█████████▍| 945/1000 [00:04<00:00, 202.02it/s]Measuring inference for batch_size=64:  97%|█████████▋| 966/1000 [00:04<00:00, 202.08it/s]Measuring inference for batch_size=64:  99%|█████████▊| 987/1000 [00:04<00:00, 202.11it/s]Measuring inference for batch_size=64: 100%|██████████| 1000/1000 [00:04<00:00, 202.04it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 29.99 GB
    total: 31.28 GB
    used: 911.23 MB
  system:
    node: fp
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_64:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 94.563 us +/- 2.491 us [93.222 us, 138.998 us]
        batches_per_second: 10.58 K +/- 238.64 [7.19 K, 10.73 K]
      metrics:
        batches_per_second_max: 10727.120204603581
        batches_per_second_mean: 10581.24011216581
        batches_per_second_min: 7194.346483704974
        batches_per_second_std: 238.64123204801442
        seconds_per_batch_max: 0.00013899803161621094
        seconds_per_batch_mean: 9.456253051757813e-05
        seconds_per_batch_min: 9.322166442871094e-05
        seconds_per_batch_std: 2.490813838575135e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.164 us +/- 0.478 us [22.411 us, 28.610 us]
        batches_per_second: 43.19 K +/- 785.99 [34.95 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 43186.96938904753
        batches_per_second_min: 34952.53333333333
        batches_per_second_std: 785.9938580474308
        seconds_per_batch_max: 2.86102294921875e-05
        seconds_per_batch_mean: 2.3163795471191408e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 4.777698931036444e-07
    on_device_inference:
      human_readable:
        batch_latency: -4626510.978 us +/- 11.850 ms [-4832384.109 us, -4599423.885
          us]
        batches_per_second: -0.22 +/- 0.00 [-0.22, -0.21]
      metrics:
        batches_per_second_max: -0.2069371923549502
        batches_per_second_mean: -0.2161469996913113
        batches_per_second_min: -0.2174185343486537
        batches_per_second_std: 0.000545860732485091
        seconds_per_batch_max: -4.599423885345459
        seconds_per_batch_mean: -4.626510978221893
        seconds_per_batch_min: -4.83238410949707
        seconds_per_batch_std: 0.01184950559244839
    total:
      human_readable:
        batch_latency: 4.751 ms +/- 13.364 us [4.725 ms, 5.013 ms]
        batches_per_second: 210.48 +/- 0.58 [199.49, 211.63]
      metrics:
        batches_per_second_max: 211.6304556233917
        batches_per_second_mean: 210.47948593127333
        batches_per_second_min: 199.49127229488704
        batches_per_second_std: 0.5796344952015402
        seconds_per_batch_max: 0.0050127506256103516
        seconds_per_batch_mean: 0.004751093626022339
        seconds_per_batch_min: 0.004725217819213867
        seconds_per_batch_std: 1.3364366334920887e-05
  batch_size_64:
    cpu_to_gpu:
      human_readable:
        batch_latency: 147.029 us +/- 4.229 us [145.197 us, 245.571 us]
        batches_per_second: 6.81 K +/- 150.73 [4.07 K, 6.89 K]
      metrics:
        batches_per_second_max: 6887.1986863711
        batches_per_second_mean: 6805.62551941998
        batches_per_second_min: 4072.1398058252425
        batches_per_second_std: 150.7346906804445
        seconds_per_batch_max: 0.0002455711364746094
        seconds_per_batch_mean: 0.00014702868461608887
        seconds_per_batch_min: 0.00014519691467285156
        seconds_per_batch_std: 4.229229152662325e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.301 us +/- 0.662 us [22.411 us, 32.187 us]
        batches_per_second: 42.95 K +/- 1.04 K [31.07 K, 44.62 K]
      metrics:
        batches_per_second_max: 44620.255319148935
        batches_per_second_mean: 42946.10213650231
        batches_per_second_min: 31068.91851851852
        batches_per_second_std: 1038.6355668656056
        seconds_per_batch_max: 3.218650817871094e-05
        seconds_per_batch_mean: 2.3300886154174803e-05
        seconds_per_batch_min: 2.2411346435546875e-05
        seconds_per_batch_std: 6.621274569138509e-07
    on_device_inference:
      human_readable:
        batch_latency: -4770058.303 us +/- 29.262 ms [-5068640.232 us, -4734240.055
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.20]
      metrics:
        batches_per_second_max: -0.19729157213993345
        batches_per_second_mean: -0.20964862425141484
        batches_per_second_min: -0.21122714276519902
        batches_per_second_std: 0.0012359745502043776
        seconds_per_batch_max: -4.7342400550842285
        seconds_per_batch_mean: -4.770058302879334
        seconds_per_batch_min: -5.068640232086182
        seconds_per_batch_std: 0.02926157061463211
    total:
      human_readable:
        batch_latency: 4.946 ms +/- 31.034 us [4.915 ms, 5.349 ms]
        batches_per_second: 202.20 +/- 1.22 [186.96, 203.47]
      metrics:
        batches_per_second_max: 203.4687105850393
        batches_per_second_mean: 202.19718878138093
        batches_per_second_min: 186.96193278060088
        batches_per_second_std: 1.2156307688052042
        seconds_per_batch_max: 0.005348682403564453
        seconds_per_batch_mean: 0.004945853710174561
        seconds_per_batch_min: 0.004914760589599609
        seconds_per_batch_std: 3.103367501835832e-05


#####
fp-fp-py-id - Run 3
2024-02-25 22:55:38
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  5.15it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  5.15it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  29%|██▉       | 29/100 [00:00<00:00, 281.39it/s]Warming up with batch_size=1:  58%|█████▊    | 58/100 [00:00<00:00, 282.58it/s]Warming up with batch_size=1:  87%|████████▋ | 87/100 [00:00<00:00, 284.28it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 283.99it/s]
STAGE:2024-02-25 22:55:28 5931:5931 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:55:28 5931:5931 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:55:28 5931:5931 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 22/1000 [00:00<00:04, 215.40it/s]Measuring inference for batch_size=1:   4%|▍         | 44/1000 [00:00<00:04, 215.53it/s]Measuring inference for batch_size=1:   7%|▋         | 66/1000 [00:00<00:04, 215.80it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:00<00:04, 215.78it/s]Measuring inference for batch_size=1:  11%|█         | 110/1000 [00:00<00:04, 215.82it/s]Measuring inference for batch_size=1:  13%|█▎        | 132/1000 [00:00<00:04, 215.84it/s]Measuring inference for batch_size=1:  15%|█▌        | 154/1000 [00:00<00:03, 215.98it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:00<00:03, 216.00it/s]Measuring inference for batch_size=1:  20%|█▉        | 198/1000 [00:00<00:03, 215.97it/s]Measuring inference for batch_size=1:  22%|██▏       | 220/1000 [00:01<00:03, 215.98it/s]Measuring inference for batch_size=1:  24%|██▍       | 242/1000 [00:01<00:03, 215.99it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:01<00:03, 215.92it/s]Measuring inference for batch_size=1:  29%|██▊       | 286/1000 [00:01<00:03, 215.87it/s]Measuring inference for batch_size=1:  31%|███       | 308/1000 [00:01<00:03, 215.78it/s]Measuring inference for batch_size=1:  33%|███▎      | 330/1000 [00:01<00:03, 215.70it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:01<00:03, 215.67it/s]Measuring inference for batch_size=1:  37%|███▋      | 374/1000 [00:01<00:02, 215.61it/s]Measuring inference for batch_size=1:  40%|███▉      | 396/1000 [00:01<00:02, 215.66it/s]Measuring inference for batch_size=1:  42%|████▏     | 418/1000 [00:01<00:02, 215.64it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:02<00:02, 215.66it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:02<00:02, 215.63it/s]Measuring inference for batch_size=1:  48%|████▊     | 484/1000 [00:02<00:02, 215.61it/s]Measuring inference for batch_size=1:  51%|█████     | 506/1000 [00:02<00:02, 215.61it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:02<00:02, 215.60it/s]Measuring inference for batch_size=1:  55%|█████▌    | 550/1000 [00:02<00:02, 215.60it/s]Measuring inference for batch_size=1:  57%|█████▋    | 572/1000 [00:02<00:01, 215.75it/s]Measuring inference for batch_size=1:  59%|█████▉    | 594/1000 [00:02<00:01, 215.85it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:02<00:01, 215.82it/s]Measuring inference for batch_size=1:  64%|██████▍   | 638/1000 [00:02<00:01, 215.84it/s]Measuring inference for batch_size=1:  66%|██████▌   | 660/1000 [00:03<00:01, 215.85it/s]Measuring inference for batch_size=1:  68%|██████▊   | 682/1000 [00:03<00:01, 215.22it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [00:03<00:01, 211.85it/s]Measuring inference for batch_size=1:  73%|███████▎  | 726/1000 [00:03<00:01, 209.67it/s]Measuring inference for batch_size=1:  75%|███████▍  | 747/1000 [00:03<00:01, 208.18it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [00:03<00:01, 207.10it/s]Measuring inference for batch_size=1:  79%|███████▉  | 789/1000 [00:03<00:01, 206.25it/s]Measuring inference for batch_size=1:  81%|████████  | 810/1000 [00:03<00:00, 205.66it/s]Measuring inference for batch_size=1:  83%|████████▎ | 831/1000 [00:03<00:00, 205.06it/s]Measuring inference for batch_size=1:  85%|████████▌ | 852/1000 [00:03<00:00, 204.63it/s]Measuring inference for batch_size=1:  87%|████████▋ | 873/1000 [00:04<00:00, 204.23it/s]Measuring inference for batch_size=1:  89%|████████▉ | 894/1000 [00:04<00:00, 204.25it/s]Measuring inference for batch_size=1:  92%|█████████▏| 915/1000 [00:04<00:00, 204.07it/s]Measuring inference for batch_size=1:  94%|█████████▎| 936/1000 [00:04<00:00, 203.95it/s]Measuring inference for batch_size=1:  96%|█████████▌| 957/1000 [00:04<00:00, 204.01it/s]Measuring inference for batch_size=1:  98%|█████████▊| 978/1000 [00:04<00:00, 203.96it/s]Measuring inference for batch_size=1: 100%|█████████▉| 999/1000 [00:04<00:00, 203.97it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 211.86it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=64:  21%|██        | 21/100 [00:00<00:00, 208.81it/s]Warming up with batch_size=64:  42%|████▏     | 42/100 [00:00<00:00, 209.06it/s]Warming up with batch_size=64:  63%|██████▎   | 63/100 [00:00<00:00, 209.15it/s]Warming up with batch_size=64:  84%|████████▍ | 84/100 [00:00<00:00, 209.14it/s]Warming up with batch_size=64: 100%|██████████| 100/100 [00:00<00:00, 209.08it/s]
STAGE:2024-02-25 22:55:33 5931:5931 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:55:33 5931:5931 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:55:33 5931:5931 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=64:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=64:   2%|▏         | 21/1000 [00:00<00:04, 205.58it/s]Measuring inference for batch_size=64:   4%|▍         | 42/1000 [00:00<00:04, 206.05it/s]Measuring inference for batch_size=64:   6%|▋         | 63/1000 [00:00<00:04, 206.20it/s]Measuring inference for batch_size=64:   8%|▊         | 84/1000 [00:00<00:04, 206.25it/s]Measuring inference for batch_size=64:  10%|█         | 105/1000 [00:00<00:04, 206.27it/s]Measuring inference for batch_size=64:  13%|█▎        | 126/1000 [00:00<00:04, 206.37it/s]Measuring inference for batch_size=64:  15%|█▍        | 147/1000 [00:00<00:04, 206.30it/s]Measuring inference for batch_size=64:  17%|█▋        | 168/1000 [00:00<00:04, 206.27it/s]Measuring inference for batch_size=64:  19%|█▉        | 189/1000 [00:00<00:03, 206.35it/s]Measuring inference for batch_size=64:  21%|██        | 210/1000 [00:01<00:03, 206.41it/s]Measuring inference for batch_size=64:  23%|██▎       | 231/1000 [00:01<00:03, 206.33it/s]Measuring inference for batch_size=64:  25%|██▌       | 252/1000 [00:01<00:03, 206.43it/s]Measuring inference for batch_size=64:  27%|██▋       | 273/1000 [00:01<00:03, 206.46it/s]Measuring inference for batch_size=64:  29%|██▉       | 294/1000 [00:01<00:03, 206.50it/s]Measuring inference for batch_size=64:  32%|███▏      | 315/1000 [00:01<00:03, 206.49it/s]Measuring inference for batch_size=64:  34%|███▎      | 336/1000 [00:01<00:03, 206.48it/s]Measuring inference for batch_size=64:  36%|███▌      | 357/1000 [00:01<00:03, 206.42it/s]Measuring inference for batch_size=64:  38%|███▊      | 378/1000 [00:01<00:03, 206.34it/s]Measuring inference for batch_size=64:  40%|███▉      | 399/1000 [00:01<00:02, 206.37it/s]Measuring inference for batch_size=64:  42%|████▏     | 420/1000 [00:02<00:02, 206.41it/s]Measuring inference for batch_size=64:  44%|████▍     | 441/1000 [00:02<00:02, 206.45it/s]Measuring inference for batch_size=64:  46%|████▌     | 462/1000 [00:02<00:02, 206.51it/s]Measuring inference for batch_size=64:  48%|████▊     | 483/1000 [00:02<00:02, 206.60it/s]Measuring inference for batch_size=64:  50%|█████     | 504/1000 [00:02<00:02, 206.73it/s]Measuring inference for batch_size=64:  52%|█████▎    | 525/1000 [00:02<00:02, 206.74it/s]Measuring inference for batch_size=64:  55%|█████▍    | 546/1000 [00:02<00:02, 206.75it/s]Measuring inference for batch_size=64:  57%|█████▋    | 567/1000 [00:02<00:02, 206.84it/s]Measuring inference for batch_size=64:  59%|█████▉    | 588/1000 [00:02<00:01, 206.92it/s]Measuring inference for batch_size=64:  61%|██████    | 609/1000 [00:02<00:01, 206.90it/s]Measuring inference for batch_size=64:  63%|██████▎   | 630/1000 [00:03<00:01, 206.94it/s]Measuring inference for batch_size=64:  65%|██████▌   | 651/1000 [00:03<00:01, 206.88it/s]Measuring inference for batch_size=64:  67%|██████▋   | 672/1000 [00:03<00:01, 206.89it/s]Measuring inference for batch_size=64:  69%|██████▉   | 693/1000 [00:03<00:01, 206.96it/s]Measuring inference for batch_size=64:  71%|███████▏  | 714/1000 [00:03<00:01, 206.92it/s]Measuring inference for batch_size=64:  74%|███████▎  | 735/1000 [00:03<00:01, 206.90it/s]Measuring inference for batch_size=64:  76%|███████▌  | 756/1000 [00:03<00:01, 206.93it/s]Measuring inference for batch_size=64:  78%|███████▊  | 777/1000 [00:03<00:01, 207.02it/s]Measuring inference for batch_size=64:  80%|███████▉  | 798/1000 [00:03<00:00, 206.94it/s]Measuring inference for batch_size=64:  82%|████████▏ | 819/1000 [00:03<00:00, 206.97it/s]Measuring inference for batch_size=64:  84%|████████▍ | 840/1000 [00:04<00:00, 206.83it/s]Measuring inference for batch_size=64:  86%|████████▌ | 861/1000 [00:04<00:00, 206.74it/s]Measuring inference for batch_size=64:  88%|████████▊ | 882/1000 [00:04<00:00, 206.64it/s]Measuring inference for batch_size=64:  90%|█████████ | 903/1000 [00:04<00:00, 206.55it/s]Measuring inference for batch_size=64:  92%|█████████▏| 924/1000 [00:04<00:00, 206.52it/s]Measuring inference for batch_size=64:  94%|█████████▍| 945/1000 [00:04<00:00, 206.52it/s]Measuring inference for batch_size=64:  97%|█████████▋| 966/1000 [00:04<00:00, 206.54it/s]Measuring inference for batch_size=64:  99%|█████████▊| 987/1000 [00:04<00:00, 206.56it/s]Measuring inference for batch_size=64: 100%|██████████| 1000/1000 [00:04<00:00, 206.60it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 29.98 GB
    total: 31.28 GB
    used: 916.40 MB
  system:
    node: fp
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_64:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 92.820 us +/- 3.252 us [90.361 us, 141.144 us]
        batches_per_second: 10.79 K +/- 332.31 [7.08 K, 11.07 K]
      metrics:
        batches_per_second_max: 11066.765171503957
        batches_per_second_mean: 10785.056073805325
        batches_per_second_min: 7084.972972972973
        batches_per_second_std: 332.30653587502286
        seconds_per_batch_max: 0.000141143798828125
        seconds_per_batch_mean: 9.28201675415039e-05
        seconds_per_batch_min: 9.036064147949219e-05
        seconds_per_batch_std: 3.252086610934403e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 22.831 us +/- 0.628 us [21.935 us, 28.849 us]
        batches_per_second: 43.83 K +/- 1.12 K [34.66 K, 45.59 K]
      metrics:
        batches_per_second_max: 45590.260869565216
        batches_per_second_mean: 43831.349425015556
        batches_per_second_min: 34663.669421487604
        batches_per_second_std: 1121.156092669304
        seconds_per_batch_max: 2.8848648071289062e-05
        seconds_per_batch_mean: 2.2830724716186524e-05
        seconds_per_batch_min: 2.193450927734375e-05
        seconds_per_batch_std: 6.27766531956789e-07
    on_device_inference:
      human_readable:
        batch_latency: -4594211.104 us +/- 123.468 ms [-4830880.165 us, -4469632.149
          us]
        batches_per_second: -0.22 +/- 0.01 [-0.22, -0.21]
      metrics:
        batches_per_second_max: -0.2070016158182387
        batches_per_second_mean: -0.21781942790973977
        batches_per_second_min: -0.22373205819214984
        batches_per_second_std: 0.0057379618491870225
        seconds_per_batch_max: -4.469632148742676
        seconds_per_batch_mean: -4.594211103916169
        seconds_per_batch_min: -4.830880165100098
        seconds_per_batch_std: 0.12346750081448112
    total:
      human_readable:
        batch_latency: 4.717 ms +/- 125.410 us [4.591 ms, 4.955 ms]
        batches_per_second: 212.17 +/- 5.53 [201.82, 217.84]
      metrics:
        batches_per_second_max: 217.84065648696375
        batches_per_second_mean: 212.16660107633408
        batches_per_second_min: 201.8238860552401
        batches_per_second_std: 5.531189004963854
        seconds_per_batch_max: 0.004954814910888672
        seconds_per_batch_mean: 0.004716546535491944
        seconds_per_batch_min: 0.004590511322021484
        seconds_per_batch_std: 0.0001254097089700543
  batch_size_64:
    cpu_to_gpu:
      human_readable:
        batch_latency: 142.918 us +/- 4.294 us [140.905 us, 241.041 us]
        batches_per_second: 7.00 K +/- 163.99 [4.15 K, 7.10 K]
      metrics:
        batches_per_second_max: 7096.961082910321
        batches_per_second_mean: 7001.827146163372
        batches_per_second_min: 4148.6686449060335
        batches_per_second_std: 163.98935169534118
        seconds_per_batch_max: 0.0002410411834716797
        seconds_per_batch_mean: 0.00014291787147521973
        seconds_per_batch_min: 0.00014090538024902344
        seconds_per_batch_std: 4.2940802374290504e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 22.708 us +/- 0.617 us [21.696 us, 28.610 us]
        batches_per_second: 44.07 K +/- 1.08 K [34.95 K, 46.09 K]
      metrics:
        batches_per_second_max: 46091.25274725275
        batches_per_second_mean: 44065.82034939753
        batches_per_second_min: 34952.53333333333
        batches_per_second_std: 1081.562572924478
        seconds_per_batch_max: 2.86102294921875e-05
        seconds_per_batch_mean: 2.270841598510742e-05
        seconds_per_batch_min: 2.1696090698242188e-05
        seconds_per_batch_std: 6.170153063948738e-07
    on_device_inference:
      human_readable:
        batch_latency: -4665801.765 us +/- 17.420 ms [-4976416.111 us, -4627039.909
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.22, -0.20]
      metrics:
        batches_per_second_max: -0.20094782624610003
        batches_per_second_mean: -0.21432835940969672
        batches_per_second_min: -0.21612089361418838
        batches_per_second_std: 0.0007832016295778965
        seconds_per_batch_max: -4.627039909362793
        seconds_per_batch_mean: -4.665801764965058
        seconds_per_batch_min: -4.976416110992432
        seconds_per_batch_std: 0.017419985838946446
    total:
      human_readable:
        batch_latency: 4.837 ms +/- 19.834 us [4.797 ms, 5.252 ms]
        batches_per_second: 206.75 +/- 0.82 [190.40, 208.45]
      metrics:
        batches_per_second_max: 208.45405297947417
        batches_per_second_mean: 206.75161069584678
        batches_per_second_min: 190.3992010531572
        batches_per_second_std: 0.8179624542183715
        seconds_per_batch_max: 0.00525212287902832
        seconds_per_batch_mean: 0.004836800098419189
        seconds_per_batch_min: 0.004797220230102539
        seconds_per_batch_std: 1.9834365158844284e-05


