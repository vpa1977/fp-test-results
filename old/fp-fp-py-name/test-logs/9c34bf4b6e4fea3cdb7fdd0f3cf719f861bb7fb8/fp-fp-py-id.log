#####
fp-fp-py-id - Run 1
2024-02-25 23:04:24
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  60.19 M, 100.000% Params, 11.58 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.016% Params, 118.01 MMac, 1.019% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.000% Params, 1.61 MMac, 0.014% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.007% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.007% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.359% Params, 680.39 MMac, 5.875% MACs, 
    (0): Bottleneck(
      75.01 k, 0.125% Params, 236.43 MMac, 2.042% MACs, 
      (conv1): Conv2d(4.1 k, 0.007% Params, 12.85 MMac, 0.111% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.028% Params, 52.99 MMac, 0.458% MACs, 
        (0): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    2.34 M, 3.887% Params, 1.92 GMac, 16.555% MACs, 
    (0): Bottleneck(
      379.39 k, 0.630% Params, 376.02 MMac, 3.247% MACs, 
      (conv1): Conv2d(32.77 k, 0.054% Params, 102.76 MMac, 0.887% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 802.82 KMac, 0.007% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.008% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.219% Params, 103.56 MMac, 0.894% MACs, 
        (0): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (4): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (5): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (6): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (7): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    40.61 M, 67.473% Params, 8.05 GMac, 69.501% MACs, 
    (0): Bottleneck(
      1.51 M, 2.513% Params, 374.26 MMac, 3.232% MACs, 
      (conv1): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 401.41 KMac, 0.003% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.004% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 0.874% Params, 103.16 MMac, 0.891% MACs, 
        (0): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (6): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (7): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (8): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (9): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (10): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (11): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (12): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (13): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (14): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (15): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (16): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (17): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (18): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (19): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (20): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (21): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (22): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (23): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (24): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (25): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (26): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (27): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (28): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (29): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (30): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (31): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (32): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (33): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (34): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (35): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 24.861% Params, 811.02 MMac, 7.003% MACs, 
    (0): Bottleneck(
      6.04 M, 10.034% Params, 373.38 MMac, 3.224% MACs, 
      (conv1): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 200.7 KMac, 0.002% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.002% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 3.491% Params, 102.96 MMac, 0.889% MACs, 
        (0): Conv2d(2.1 M, 3.484% Params, 102.76 MMac, 0.887% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.001% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 3.404% Params, 2.05 MMac, 0.018% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  11%|█         | 11/100 [00:00<00:00, 100.39it/s]Warming up with batch_size=1:  22%|██▏       | 22/100 [00:00<00:00, 100.60it/s]Warming up with batch_size=1:  33%|███▎      | 33/100 [00:00<00:00, 100.91it/s]Warming up with batch_size=1:  44%|████▍     | 44/100 [00:00<00:00, 101.01it/s]Warming up with batch_size=1:  55%|█████▌    | 55/100 [00:00<00:00, 101.14it/s]Warming up with batch_size=1:  66%|██████▌   | 66/100 [00:00<00:00, 101.21it/s]Warming up with batch_size=1:  77%|███████▋  | 77/100 [00:00<00:00, 101.24it/s]Warming up with batch_size=1:  88%|████████▊ | 88/100 [00:00<00:00, 101.25it/s]Warming up with batch_size=1:  99%|█████████▉| 99/100 [00:00<00:00, 101.29it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 101.13it/s]
STAGE:2024-02-25 23:03:53 6682:6682 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 23:03:53 6682:6682 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 23:03:53 6682:6682 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   1%|          | 8/1000 [00:00<00:13, 75.76it/s]Measuring inference for batch_size=1:   2%|▏         | 16/1000 [00:00<00:12, 76.13it/s]Measuring inference for batch_size=1:   2%|▏         | 24/1000 [00:00<00:12, 76.30it/s]Measuring inference for batch_size=1:   3%|▎         | 32/1000 [00:00<00:12, 76.35it/s]Measuring inference for batch_size=1:   4%|▍         | 40/1000 [00:00<00:12, 76.39it/s]Measuring inference for batch_size=1:   5%|▍         | 48/1000 [00:00<00:12, 76.31it/s]Measuring inference for batch_size=1:   6%|▌         | 56/1000 [00:00<00:12, 76.32it/s]Measuring inference for batch_size=1:   6%|▋         | 64/1000 [00:00<00:12, 76.35it/s]Measuring inference for batch_size=1:   7%|▋         | 72/1000 [00:00<00:12, 76.38it/s]Measuring inference for batch_size=1:   8%|▊         | 80/1000 [00:01<00:12, 76.43it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:01<00:11, 76.43it/s]Measuring inference for batch_size=1:  10%|▉         | 96/1000 [00:01<00:11, 76.43it/s]Measuring inference for batch_size=1:  10%|█         | 104/1000 [00:01<00:11, 76.39it/s]Measuring inference for batch_size=1:  11%|█         | 112/1000 [00:01<00:11, 76.36it/s]Measuring inference for batch_size=1:  12%|█▏        | 120/1000 [00:01<00:11, 76.30it/s]Measuring inference for batch_size=1:  13%|█▎        | 128/1000 [00:01<00:11, 76.25it/s]Measuring inference for batch_size=1:  14%|█▎        | 136/1000 [00:01<00:11, 75.68it/s]Measuring inference for batch_size=1:  14%|█▍        | 144/1000 [00:01<00:11, 74.49it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:02<00:11, 73.77it/s]Measuring inference for batch_size=1:  16%|█▌        | 160/1000 [00:02<00:11, 73.29it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:02<00:11, 72.91it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:02<00:11, 72.68it/s]Measuring inference for batch_size=1:  18%|█▊        | 184/1000 [00:02<00:11, 72.49it/s]Measuring inference for batch_size=1:  19%|█▉        | 192/1000 [00:02<00:11, 72.33it/s]Measuring inference for batch_size=1:  20%|██        | 200/1000 [00:02<00:11, 72.28it/s]Measuring inference for batch_size=1:  21%|██        | 208/1000 [00:02<00:10, 72.21it/s]Measuring inference for batch_size=1:  22%|██▏       | 216/1000 [00:02<00:10, 72.20it/s]Measuring inference for batch_size=1:  22%|██▏       | 224/1000 [00:03<00:10, 72.13it/s]Measuring inference for batch_size=1:  23%|██▎       | 232/1000 [00:03<00:10, 72.09it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:03<00:10, 72.05it/s]Measuring inference for batch_size=1:  25%|██▍       | 248/1000 [00:03<00:10, 72.05it/s]Measuring inference for batch_size=1:  26%|██▌       | 256/1000 [00:03<00:10, 71.99it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:03<00:10, 71.97it/s]Measuring inference for batch_size=1:  27%|██▋       | 272/1000 [00:03<00:10, 71.94it/s]Measuring inference for batch_size=1:  28%|██▊       | 280/1000 [00:03<00:09, 72.02it/s]Measuring inference for batch_size=1:  29%|██▉       | 288/1000 [00:03<00:09, 72.06it/s]Measuring inference for batch_size=1:  30%|██▉       | 296/1000 [00:04<00:09, 72.07it/s]Measuring inference for batch_size=1:  30%|███       | 304/1000 [00:04<00:09, 72.08it/s]Measuring inference for batch_size=1:  31%|███       | 312/1000 [00:04<00:09, 72.17it/s]Measuring inference for batch_size=1:  32%|███▏      | 320/1000 [00:04<00:09, 72.17it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:04<00:09, 72.16it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:04<00:09, 72.12it/s]Measuring inference for batch_size=1:  34%|███▍      | 344/1000 [00:04<00:09, 72.14it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:04<00:08, 72.11it/s]Measuring inference for batch_size=1:  36%|███▌      | 360/1000 [00:04<00:08, 72.13it/s]Measuring inference for batch_size=1:  37%|███▋      | 368/1000 [00:05<00:08, 72.11it/s]Measuring inference for batch_size=1:  38%|███▊      | 376/1000 [00:05<00:08, 72.15it/s]Measuring inference for batch_size=1:  38%|███▊      | 384/1000 [00:05<00:08, 72.17it/s]Measuring inference for batch_size=1:  39%|███▉      | 392/1000 [00:05<00:08, 72.16it/s]Measuring inference for batch_size=1:  40%|████      | 400/1000 [00:05<00:08, 72.08it/s]Measuring inference for batch_size=1:  41%|████      | 408/1000 [00:05<00:08, 71.98it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:05<00:08, 71.91it/s]Measuring inference for batch_size=1:  42%|████▏     | 424/1000 [00:05<00:08, 71.99it/s]Measuring inference for batch_size=1:  43%|████▎     | 432/1000 [00:05<00:07, 72.07it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:06<00:07, 72.12it/s]Measuring inference for batch_size=1:  45%|████▍     | 448/1000 [00:06<00:07, 72.15it/s]Measuring inference for batch_size=1:  46%|████▌     | 456/1000 [00:06<00:07, 72.18it/s]Measuring inference for batch_size=1:  46%|████▋     | 464/1000 [00:06<00:07, 72.13it/s]Measuring inference for batch_size=1:  47%|████▋     | 472/1000 [00:06<00:07, 72.16it/s]Measuring inference for batch_size=1:  48%|████▊     | 480/1000 [00:06<00:07, 72.09it/s]Measuring inference for batch_size=1:  49%|████▉     | 488/1000 [00:06<00:07, 72.06it/s]Measuring inference for batch_size=1:  50%|████▉     | 496/1000 [00:06<00:06, 72.07it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:06<00:06, 72.13it/s]Measuring inference for batch_size=1:  51%|█████     | 512/1000 [00:07<00:06, 72.14it/s]Measuring inference for batch_size=1:  52%|█████▏    | 520/1000 [00:07<00:06, 72.12it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:07<00:06, 72.11it/s]Measuring inference for batch_size=1:  54%|█████▎    | 536/1000 [00:07<00:06, 72.10it/s]Measuring inference for batch_size=1:  54%|█████▍    | 544/1000 [00:07<00:06, 72.08it/s]Measuring inference for batch_size=1:  55%|█████▌    | 552/1000 [00:07<00:06, 72.02it/s]Measuring inference for batch_size=1:  56%|█████▌    | 560/1000 [00:07<00:06, 71.98it/s]Measuring inference for batch_size=1:  57%|█████▋    | 568/1000 [00:07<00:05, 72.01it/s]Measuring inference for batch_size=1:  58%|█████▊    | 576/1000 [00:07<00:05, 72.02it/s]Measuring inference for batch_size=1:  58%|█████▊    | 584/1000 [00:08<00:05, 72.06it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [00:08<00:05, 72.10it/s]Measuring inference for batch_size=1:  60%|██████    | 600/1000 [00:08<00:05, 72.12it/s]Measuring inference for batch_size=1:  61%|██████    | 608/1000 [00:08<00:05, 72.15it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:08<00:05, 72.14it/s]Measuring inference for batch_size=1:  62%|██████▏   | 624/1000 [00:08<00:05, 72.05it/s]Measuring inference for batch_size=1:  63%|██████▎   | 632/1000 [00:08<00:05, 72.00it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [00:08<00:05, 71.97it/s]Measuring inference for batch_size=1:  65%|██████▍   | 648/1000 [00:08<00:04, 71.98it/s]Measuring inference for batch_size=1:  66%|██████▌   | 656/1000 [00:09<00:04, 71.95it/s]Measuring inference for batch_size=1:  66%|██████▋   | 664/1000 [00:09<00:04, 72.02it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:09<00:04, 72.09it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [00:09<00:04, 72.10it/s]Measuring inference for batch_size=1:  69%|██████▉   | 688/1000 [00:09<00:04, 72.17it/s]Measuring inference for batch_size=1:  70%|██████▉   | 696/1000 [00:09<00:04, 72.15it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [00:09<00:04, 72.10it/s]Measuring inference for batch_size=1:  71%|███████   | 712/1000 [00:09<00:03, 72.03it/s]Measuring inference for batch_size=1:  72%|███████▏  | 720/1000 [00:09<00:03, 72.06it/s]Measuring inference for batch_size=1:  73%|███████▎  | 728/1000 [00:09<00:03, 72.09it/s]Measuring inference for batch_size=1:  74%|███████▎  | 736/1000 [00:10<00:03, 72.10it/s]Measuring inference for batch_size=1:  74%|███████▍  | 744/1000 [00:10<00:03, 72.18it/s]Measuring inference for batch_size=1:  75%|███████▌  | 752/1000 [00:10<00:03, 72.21it/s]Measuring inference for batch_size=1:  76%|███████▌  | 760/1000 [00:10<00:03, 72.19it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [00:10<00:03, 72.10it/s]Measuring inference for batch_size=1:  78%|███████▊  | 776/1000 [00:10<00:03, 72.11it/s]Measuring inference for batch_size=1:  78%|███████▊  | 784/1000 [00:10<00:02, 72.11it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [00:10<00:02, 72.15it/s]Measuring inference for batch_size=1:  80%|████████  | 800/1000 [00:10<00:02, 72.18it/s]Measuring inference for batch_size=1:  81%|████████  | 808/1000 [00:11<00:02, 72.19it/s]Measuring inference for batch_size=1:  82%|████████▏ | 816/1000 [00:11<00:02, 72.19it/s]Measuring inference for batch_size=1:  82%|████████▏ | 824/1000 [00:11<00:02, 72.21it/s]Measuring inference for batch_size=1:  83%|████████▎ | 832/1000 [00:11<00:02, 72.22it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:11<00:02, 72.14it/s]Measuring inference for batch_size=1:  85%|████████▍ | 848/1000 [00:11<00:02, 72.11it/s]Measuring inference for batch_size=1:  86%|████████▌ | 856/1000 [00:11<00:01, 72.07it/s]Measuring inference for batch_size=1:  86%|████████▋ | 864/1000 [00:11<00:01, 72.08it/s]Measuring inference for batch_size=1:  87%|████████▋ | 872/1000 [00:11<00:01, 72.13it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [00:12<00:01, 72.16it/s]Measuring inference for batch_size=1:  89%|████████▉ | 888/1000 [00:12<00:01, 72.17it/s]Measuring inference for batch_size=1:  90%|████████▉ | 896/1000 [00:12<00:01, 72.14it/s]Measuring inference for batch_size=1:  90%|█████████ | 904/1000 [00:12<00:01, 72.09it/s]Measuring inference for batch_size=1:  91%|█████████ | 912/1000 [00:12<00:01, 72.02it/s]Measuring inference for batch_size=1:  92%|█████████▏| 920/1000 [00:12<00:01, 72.02it/s]Measuring inference for batch_size=1:  93%|█████████▎| 928/1000 [00:12<00:00, 72.01it/s]Measuring inference for batch_size=1:  94%|█████████▎| 936/1000 [00:12<00:00, 72.02it/s]Measuring inference for batch_size=1:  94%|█████████▍| 944/1000 [00:12<00:00, 72.05it/s]Measuring inference for batch_size=1:  95%|█████████▌| 952/1000 [00:13<00:00, 72.09it/s]Measuring inference for batch_size=1:  96%|█████████▌| 960/1000 [00:13<00:00, 72.07it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [00:13<00:00, 72.11it/s]Measuring inference for batch_size=1:  98%|█████████▊| 976/1000 [00:13<00:00, 72.12it/s]Measuring inference for batch_size=1:  98%|█████████▊| 984/1000 [00:13<00:00, 72.08it/s]Measuring inference for batch_size=1:  99%|█████████▉| 992/1000 [00:13<00:00, 72.03it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 72.03it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 72.62it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=256:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=256:   8%|▊         | 8/100 [00:00<00:01, 75.56it/s]Warming up with batch_size=256:  16%|█▌        | 16/100 [00:00<00:01, 75.69it/s]Warming up with batch_size=256:  24%|██▍       | 24/100 [00:00<00:01, 75.81it/s]Warming up with batch_size=256:  32%|███▏      | 32/100 [00:00<00:00, 75.89it/s]Warming up with batch_size=256:  40%|████      | 40/100 [00:00<00:00, 75.90it/s]Warming up with batch_size=256:  48%|████▊     | 48/100 [00:00<00:00, 75.95it/s]Warming up with batch_size=256:  56%|█████▌    | 56/100 [00:00<00:00, 75.88it/s]Warming up with batch_size=256:  64%|██████▍   | 64/100 [00:00<00:00, 75.86it/s]Warming up with batch_size=256:  72%|███████▏  | 72/100 [00:00<00:00, 75.84it/s]Warming up with batch_size=256:  80%|████████  | 80/100 [00:01<00:00, 75.81it/s]Warming up with batch_size=256:  88%|████████▊ | 88/100 [00:01<00:00, 75.88it/s]Warming up with batch_size=256:  96%|█████████▌| 96/100 [00:01<00:00, 75.92it/s]Warming up with batch_size=256: 100%|██████████| 100/100 [00:01<00:00, 75.87it/s]
STAGE:2024-02-25 23:04:08 6682:6682 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 23:04:08 6682:6682 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 23:04:08 6682:6682 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=256:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=256:   1%|          | 8/1000 [00:00<00:13, 74.47it/s]Measuring inference for batch_size=256:   2%|▏         | 16/1000 [00:00<00:13, 74.66it/s]Measuring inference for batch_size=256:   2%|▏         | 24/1000 [00:00<00:13, 74.76it/s]Measuring inference for batch_size=256:   3%|▎         | 32/1000 [00:00<00:12, 74.83it/s]Measuring inference for batch_size=256:   4%|▍         | 40/1000 [00:00<00:12, 74.85it/s]Measuring inference for batch_size=256:   5%|▍         | 48/1000 [00:00<00:12, 74.94it/s]Measuring inference for batch_size=256:   6%|▌         | 56/1000 [00:00<00:12, 75.01it/s]Measuring inference for batch_size=256:   6%|▋         | 64/1000 [00:00<00:12, 75.05it/s]Measuring inference for batch_size=256:   7%|▋         | 72/1000 [00:00<00:12, 75.06it/s]Measuring inference for batch_size=256:   8%|▊         | 80/1000 [00:01<00:12, 75.05it/s]Measuring inference for batch_size=256:   9%|▉         | 88/1000 [00:01<00:12, 75.01it/s]Measuring inference for batch_size=256:  10%|▉         | 96/1000 [00:01<00:12, 75.02it/s]Measuring inference for batch_size=256:  10%|█         | 104/1000 [00:01<00:11, 75.02it/s]Measuring inference for batch_size=256:  11%|█         | 112/1000 [00:01<00:11, 75.05it/s]Measuring inference for batch_size=256:  12%|█▏        | 120/1000 [00:01<00:11, 75.05it/s]Measuring inference for batch_size=256:  13%|█▎        | 128/1000 [00:01<00:11, 75.04it/s]Measuring inference for batch_size=256:  14%|█▎        | 136/1000 [00:01<00:11, 75.05it/s]Measuring inference for batch_size=256:  14%|█▍        | 144/1000 [00:01<00:11, 75.06it/s]Measuring inference for batch_size=256:  15%|█▌        | 152/1000 [00:02<00:11, 75.08it/s]Measuring inference for batch_size=256:  16%|█▌        | 160/1000 [00:02<00:11, 75.05it/s]Measuring inference for batch_size=256:  17%|█▋        | 168/1000 [00:02<00:11, 74.99it/s]Measuring inference for batch_size=256:  18%|█▊        | 176/1000 [00:02<00:10, 74.93it/s]Measuring inference for batch_size=256:  18%|█▊        | 184/1000 [00:02<00:11, 73.91it/s]Measuring inference for batch_size=256:  19%|█▉        | 192/1000 [00:02<00:11, 72.99it/s]Measuring inference for batch_size=256:  20%|██        | 200/1000 [00:02<00:11, 72.42it/s]Measuring inference for batch_size=256:  21%|██        | 208/1000 [00:02<00:10, 72.07it/s]Measuring inference for batch_size=256:  22%|██▏       | 216/1000 [00:02<00:10, 71.84it/s]Measuring inference for batch_size=256:  22%|██▏       | 224/1000 [00:03<00:10, 71.66it/s]Measuring inference for batch_size=256:  23%|██▎       | 232/1000 [00:03<00:10, 71.42it/s]Measuring inference for batch_size=256:  24%|██▍       | 240/1000 [00:03<00:10, 71.26it/s]Measuring inference for batch_size=256:  25%|██▍       | 248/1000 [00:03<00:10, 71.17it/s]Measuring inference for batch_size=256:  26%|██▌       | 256/1000 [00:03<00:10, 71.08it/s]Measuring inference for batch_size=256:  26%|██▋       | 264/1000 [00:03<00:10, 71.02it/s]Measuring inference for batch_size=256:  27%|██▋       | 272/1000 [00:03<00:10, 71.04it/s]Measuring inference for batch_size=256:  28%|██▊       | 280/1000 [00:03<00:10, 71.73it/s]Measuring inference for batch_size=256:  29%|██▉       | 288/1000 [00:03<00:09, 72.62it/s]Measuring inference for batch_size=256:  30%|██▉       | 296/1000 [00:04<00:09, 73.30it/s]Measuring inference for batch_size=256:  30%|███       | 304/1000 [00:04<00:09, 73.78it/s]Measuring inference for batch_size=256:  31%|███       | 312/1000 [00:04<00:09, 74.16it/s]Measuring inference for batch_size=256:  32%|███▏      | 320/1000 [00:04<00:09, 74.42it/s]Measuring inference for batch_size=256:  33%|███▎      | 328/1000 [00:04<00:09, 74.61it/s]Measuring inference for batch_size=256:  34%|███▎      | 336/1000 [00:04<00:08, 74.76it/s]Measuring inference for batch_size=256:  34%|███▍      | 344/1000 [00:04<00:08, 74.87it/s]Measuring inference for batch_size=256:  35%|███▌      | 352/1000 [00:04<00:08, 74.98it/s]Measuring inference for batch_size=256:  36%|███▌      | 360/1000 [00:04<00:08, 74.99it/s]Measuring inference for batch_size=256:  37%|███▋      | 368/1000 [00:04<00:08, 74.99it/s]Measuring inference for batch_size=256:  38%|███▊      | 376/1000 [00:05<00:08, 75.00it/s]Measuring inference for batch_size=256:  38%|███▊      | 384/1000 [00:05<00:08, 74.94it/s]Measuring inference for batch_size=256:  39%|███▉      | 392/1000 [00:05<00:08, 74.96it/s]Measuring inference for batch_size=256:  40%|████      | 400/1000 [00:05<00:08, 74.93it/s]Measuring inference for batch_size=256:  41%|████      | 408/1000 [00:05<00:07, 74.91it/s]Measuring inference for batch_size=256:  42%|████▏     | 416/1000 [00:05<00:07, 74.90it/s]Measuring inference for batch_size=256:  42%|████▏     | 424/1000 [00:05<00:07, 74.91it/s]Measuring inference for batch_size=256:  43%|████▎     | 432/1000 [00:05<00:07, 74.91it/s]Measuring inference for batch_size=256:  44%|████▍     | 440/1000 [00:05<00:07, 74.86it/s]Measuring inference for batch_size=256:  45%|████▍     | 448/1000 [00:06<00:07, 74.87it/s]Measuring inference for batch_size=256:  46%|████▌     | 456/1000 [00:06<00:07, 74.88it/s]Measuring inference for batch_size=256:  46%|████▋     | 464/1000 [00:06<00:07, 74.84it/s]Measuring inference for batch_size=256:  47%|████▋     | 472/1000 [00:06<00:07, 74.86it/s]Measuring inference for batch_size=256:  48%|████▊     | 480/1000 [00:06<00:06, 74.87it/s]Measuring inference for batch_size=256:  49%|████▉     | 488/1000 [00:06<00:06, 74.94it/s]Measuring inference for batch_size=256:  50%|████▉     | 496/1000 [00:06<00:06, 74.97it/s]Measuring inference for batch_size=256:  50%|█████     | 504/1000 [00:06<00:06, 75.01it/s]Measuring inference for batch_size=256:  51%|█████     | 512/1000 [00:06<00:06, 75.01it/s]Measuring inference for batch_size=256:  52%|█████▏    | 520/1000 [00:07<00:06, 75.02it/s]Measuring inference for batch_size=256:  53%|█████▎    | 528/1000 [00:07<00:06, 75.05it/s]Measuring inference for batch_size=256:  54%|█████▎    | 536/1000 [00:07<00:06, 74.99it/s]Measuring inference for batch_size=256:  54%|█████▍    | 544/1000 [00:07<00:06, 75.03it/s]Measuring inference for batch_size=256:  55%|█████▌    | 552/1000 [00:07<00:05, 75.02it/s]Measuring inference for batch_size=256:  56%|█████▌    | 560/1000 [00:07<00:05, 75.02it/s]Measuring inference for batch_size=256:  57%|█████▋    | 568/1000 [00:07<00:05, 75.05it/s]Measuring inference for batch_size=256:  58%|█████▊    | 576/1000 [00:07<00:05, 75.05it/s]Measuring inference for batch_size=256:  58%|█████▊    | 584/1000 [00:07<00:05, 75.07it/s]Measuring inference for batch_size=256:  59%|█████▉    | 592/1000 [00:07<00:05, 75.06it/s]Measuring inference for batch_size=256:  60%|██████    | 600/1000 [00:08<00:05, 75.07it/s]Measuring inference for batch_size=256:  61%|██████    | 608/1000 [00:08<00:05, 75.05it/s]Measuring inference for batch_size=256:  62%|██████▏   | 616/1000 [00:08<00:05, 74.99it/s]Measuring inference for batch_size=256:  62%|██████▏   | 624/1000 [00:08<00:05, 75.01it/s]Measuring inference for batch_size=256:  63%|██████▎   | 632/1000 [00:08<00:04, 75.04it/s]Measuring inference for batch_size=256:  64%|██████▍   | 640/1000 [00:08<00:04, 75.05it/s]Measuring inference for batch_size=256:  65%|██████▍   | 648/1000 [00:08<00:04, 75.05it/s]Measuring inference for batch_size=256:  66%|██████▌   | 656/1000 [00:08<00:04, 75.02it/s]Measuring inference for batch_size=256:  66%|██████▋   | 664/1000 [00:08<00:04, 75.03it/s]Measuring inference for batch_size=256:  67%|██████▋   | 672/1000 [00:09<00:04, 75.05it/s]Measuring inference for batch_size=256:  68%|██████▊   | 680/1000 [00:09<00:04, 74.99it/s]Measuring inference for batch_size=256:  69%|██████▉   | 688/1000 [00:09<00:04, 74.92it/s]Measuring inference for batch_size=256:  70%|██████▉   | 696/1000 [00:09<00:04, 74.94it/s]Measuring inference for batch_size=256:  70%|███████   | 704/1000 [00:09<00:03, 74.97it/s]Measuring inference for batch_size=256:  71%|███████   | 712/1000 [00:09<00:03, 75.02it/s]Measuring inference for batch_size=256:  72%|███████▏  | 720/1000 [00:09<00:03, 75.03it/s]Measuring inference for batch_size=256:  73%|███████▎  | 728/1000 [00:09<00:03, 75.05it/s]Measuring inference for batch_size=256:  74%|███████▎  | 736/1000 [00:09<00:03, 75.07it/s]Measuring inference for batch_size=256:  74%|███████▍  | 744/1000 [00:09<00:03, 75.05it/s]Measuring inference for batch_size=256:  75%|███████▌  | 752/1000 [00:10<00:03, 75.06it/s]Measuring inference for batch_size=256:  76%|███████▌  | 760/1000 [00:10<00:03, 75.02it/s]Measuring inference for batch_size=256:  77%|███████▋  | 768/1000 [00:10<00:03, 75.01it/s]Measuring inference for batch_size=256:  78%|███████▊  | 776/1000 [00:10<00:03, 74.48it/s]Measuring inference for batch_size=256:  78%|███████▊  | 784/1000 [00:10<00:02, 73.45it/s]Measuring inference for batch_size=256:  79%|███████▉  | 792/1000 [00:10<00:02, 72.75it/s]Measuring inference for batch_size=256:  80%|████████  | 800/1000 [00:10<00:02, 73.10it/s]Measuring inference for batch_size=256:  81%|████████  | 808/1000 [00:10<00:02, 73.63it/s]Measuring inference for batch_size=256:  82%|████████▏ | 816/1000 [00:10<00:02, 74.04it/s]Measuring inference for batch_size=256:  82%|████████▏ | 824/1000 [00:11<00:02, 74.33it/s]Measuring inference for batch_size=256:  83%|████████▎ | 832/1000 [00:11<00:02, 74.43it/s]Measuring inference for batch_size=256:  84%|████████▍ | 840/1000 [00:11<00:02, 74.56it/s]Measuring inference for batch_size=256:  85%|████████▍ | 848/1000 [00:11<00:02, 74.70it/s]Measuring inference for batch_size=256:  86%|████████▌ | 856/1000 [00:11<00:01, 74.76it/s]Measuring inference for batch_size=256:  86%|████████▋ | 864/1000 [00:11<00:01, 74.84it/s]Measuring inference for batch_size=256:  87%|████████▋ | 872/1000 [00:11<00:01, 74.87it/s]Measuring inference for batch_size=256:  88%|████████▊ | 880/1000 [00:11<00:01, 74.93it/s]Measuring inference for batch_size=256:  89%|████████▉ | 888/1000 [00:11<00:01, 74.98it/s]Measuring inference for batch_size=256:  90%|████████▉ | 896/1000 [00:12<00:01, 74.99it/s]Measuring inference for batch_size=256:  90%|█████████ | 904/1000 [00:12<00:01, 74.96it/s]Measuring inference for batch_size=256:  91%|█████████ | 912/1000 [00:12<00:01, 74.91it/s]Measuring inference for batch_size=256:  92%|█████████▏| 920/1000 [00:12<00:01, 74.91it/s]Measuring inference for batch_size=256:  93%|█████████▎| 928/1000 [00:12<00:00, 74.95it/s]Measuring inference for batch_size=256:  94%|█████████▎| 936/1000 [00:12<00:00, 74.97it/s]Measuring inference for batch_size=256:  94%|█████████▍| 944/1000 [00:12<00:00, 74.97it/s]Measuring inference for batch_size=256:  95%|█████████▌| 952/1000 [00:12<00:00, 74.99it/s]Measuring inference for batch_size=256:  96%|█████████▌| 960/1000 [00:12<00:00, 75.00it/s]Measuring inference for batch_size=256:  97%|█████████▋| 968/1000 [00:12<00:00, 75.01it/s]Measuring inference for batch_size=256:  98%|█████████▊| 976/1000 [00:13<00:00, 75.00it/s]Measuring inference for batch_size=256:  98%|█████████▊| 984/1000 [00:13<00:00, 74.88it/s]Measuring inference for batch_size=256:  99%|█████████▉| 992/1000 [00:13<00:00, 74.87it/s]Measuring inference for batch_size=256: 100%|██████████| 1000/1000 [00:13<00:00, 74.89it/s]Measuring inference for batch_size=256: 100%|██████████| 1000/1000 [00:13<00:00, 74.49it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 11580687848
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 29.97 GB
    total: 31.28 GB
    used: 927.45 MB
  system:
    node: fp
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 374.76 MB
    max_inference_bytes: 392962560
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
  batch_size_256:
    max_inference: 378.48 MB
    max_inference_bytes: 396866048
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
params: 60192808
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 95.317 us +/- 5.287 us [90.599 us, 221.968 us]
        batches_per_second: 10.51 K +/- 407.83 [4.51 K, 11.04 K]
      metrics:
        batches_per_second_max: 11037.642105263158
        batches_per_second_mean: 10512.358824986672
        batches_per_second_min: 4505.160042964554
        batches_per_second_std: 407.82536745450767
        seconds_per_batch_max: 0.0002219676971435547
        seconds_per_batch_mean: 9.531688690185547e-05
        seconds_per_batch_min: 9.059906005859375e-05
        seconds_per_batch_std: 5.2872353469967325e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.559 us +/- 0.860 us [22.173 us, 34.094 us]
        batches_per_second: 42.49 K +/- 1.31 K [29.33 K, 45.10 K]
      metrics:
        batches_per_second_max: 45100.04301075269
        batches_per_second_mean: 42493.59835594901
        batches_per_second_min: 29330.797202797203
        batches_per_second_std: 1313.706879385806
        seconds_per_batch_max: 3.409385681152344e-05
        seconds_per_batch_mean: 2.3559331893920898e-05
        seconds_per_batch_min: 2.2172927856445312e-05
        seconds_per_batch_std: 8.602509403538108e-07
    on_device_inference:
      human_readable:
        batch_latency: -13635123.768 us +/- 263.524 ms [-13913279.533 us, -12891743.660
          us]
        batches_per_second: -0.07 +/- 0.00 [-0.08, -0.07]
      metrics:
        batches_per_second_max: -0.07187378055622368
        batches_per_second_mean: -0.07336856124665349
        batches_per_second_min: -0.07756902606626008
        batches_per_second_std: 0.0014777733591718922
        seconds_per_batch_max: -12.891743659973145
        seconds_per_batch_mean: -13.635123767852782
        seconds_per_batch_min: -13.91327953338623
        seconds_per_batch_std: 0.26352431210797006
    total:
      human_readable:
        batch_latency: 13.762 ms +/- 265.269 us [13.012 ms, 14.038 ms]
        batches_per_second: 72.69 +/- 1.46 [71.23, 76.85]
      metrics:
        batches_per_second_max: 76.85253591322193
        batches_per_second_mean: 72.69084622750135
        batches_per_second_min: 71.23478260869565
        batches_per_second_std: 1.4600456636746668
        seconds_per_batch_max: 0.0140380859375
        seconds_per_batch_mean: 0.013762218236923219
        seconds_per_batch_min: 0.013011932373046875
        seconds_per_batch_std: 0.0002652693721875744
  batch_size_256:
    cpu_to_gpu:
      human_readable:
        batch_latency: 143.609 us +/- 5.439 us [140.905 us, 279.665 us]
        batches_per_second: 6.97 K +/- 187.93 [3.58 K, 7.10 K]
      metrics:
        batches_per_second_max: 7096.961082910321
        batches_per_second_mean: 6970.137706591903
        batches_per_second_min: 3575.70673486786
        batches_per_second_std: 187.93006261232932
        seconds_per_batch_max: 0.0002796649932861328
        seconds_per_batch_mean: 0.00014360904693603516
        seconds_per_batch_min: 0.00014090538024902344
        seconds_per_batch_std: 5.439384943868231e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.182 us +/- 0.641 us [22.173 us, 29.087 us]
        batches_per_second: 43.17 K +/- 1.07 K [34.38 K, 45.10 K]
      metrics:
        batches_per_second_max: 45100.04301075269
        batches_per_second_mean: 43165.76084563485
        batches_per_second_min: 34379.54098360656
        batches_per_second_std: 1073.8743619171833
        seconds_per_batch_max: 2.9087066650390625e-05
        seconds_per_batch_mean: 2.3182392120361327e-05
        seconds_per_batch_min: 2.2172927856445312e-05
        seconds_per_batch_std: 6.411307445261884e-07
    on_device_inference:
      human_readable:
        batch_latency: -13244818.011 us +/- 241.060 ms [-14030495.644 us, -13081407.547
          us]
        batches_per_second: -0.08 +/- 0.00 [-0.08, -0.07]
      metrics:
        batches_per_second_max: -0.07127331958903595
        batches_per_second_mean: -0.07552524971052048
        batches_per_second_min: -0.07644437316147658
        batches_per_second_std: 0.001319997625655642
        seconds_per_batch_max: -13.08140754699707
        seconds_per_batch_mean: -13.244818011283874
        seconds_per_batch_min: -14.030495643615723
        seconds_per_batch_std: 0.2410603227259187
    total:
      human_readable:
        batch_latency: 13.417 ms +/- 242.768 us [13.255 ms, 14.205 ms]
        batches_per_second: 74.56 +/- 1.30 [70.40, 75.45]
      metrics:
        batches_per_second_max: 75.44526387739684
        batches_per_second_mean: 74.55643148131182
        batches_per_second_min: 70.39903321640175
        batches_per_second_std: 1.2957751837735554
        seconds_per_batch_max: 0.014204740524291992
        seconds_per_batch_mean: 0.01341687822341919
        seconds_per_batch_min: 0.013254642486572266
        seconds_per_batch_std: 0.00024276824191904537


#####
fp-fp-py-id - Run 2
2024-02-25 23:05:03
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  60.19 M, 100.000% Params, 11.58 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.016% Params, 118.01 MMac, 1.019% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.000% Params, 1.61 MMac, 0.014% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.007% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.007% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.359% Params, 680.39 MMac, 5.875% MACs, 
    (0): Bottleneck(
      75.01 k, 0.125% Params, 236.43 MMac, 2.042% MACs, 
      (conv1): Conv2d(4.1 k, 0.007% Params, 12.85 MMac, 0.111% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.028% Params, 52.99 MMac, 0.458% MACs, 
        (0): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    2.34 M, 3.887% Params, 1.92 GMac, 16.555% MACs, 
    (0): Bottleneck(
      379.39 k, 0.630% Params, 376.02 MMac, 3.247% MACs, 
      (conv1): Conv2d(32.77 k, 0.054% Params, 102.76 MMac, 0.887% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 802.82 KMac, 0.007% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.008% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.219% Params, 103.56 MMac, 0.894% MACs, 
        (0): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (4): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (5): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (6): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (7): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    40.61 M, 67.473% Params, 8.05 GMac, 69.501% MACs, 
    (0): Bottleneck(
      1.51 M, 2.513% Params, 374.26 MMac, 3.232% MACs, 
      (conv1): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 401.41 KMac, 0.003% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.004% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 0.874% Params, 103.16 MMac, 0.891% MACs, 
        (0): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (6): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (7): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (8): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (9): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (10): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (11): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (12): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (13): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (14): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (15): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (16): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (17): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (18): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (19): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (20): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (21): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (22): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (23): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (24): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (25): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (26): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (27): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (28): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (29): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (30): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (31): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (32): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (33): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (34): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (35): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 24.861% Params, 811.02 MMac, 7.003% MACs, 
    (0): Bottleneck(
      6.04 M, 10.034% Params, 373.38 MMac, 3.224% MACs, 
      (conv1): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 200.7 KMac, 0.002% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.002% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 3.491% Params, 102.96 MMac, 0.889% MACs, 
        (0): Conv2d(2.1 M, 3.484% Params, 102.76 MMac, 0.887% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.001% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 3.404% Params, 2.05 MMac, 0.018% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  10%|█         | 10/100 [00:00<00:00, 97.50it/s]Warming up with batch_size=1:  20%|██        | 20/100 [00:00<00:00, 97.73it/s]Warming up with batch_size=1:  30%|███       | 30/100 [00:00<00:00, 97.94it/s]Warming up with batch_size=1:  40%|████      | 40/100 [00:00<00:00, 97.98it/s]Warming up with batch_size=1:  50%|█████     | 50/100 [00:00<00:00, 98.01it/s]Warming up with batch_size=1:  60%|██████    | 60/100 [00:00<00:00, 98.04it/s]Warming up with batch_size=1:  70%|███████   | 70/100 [00:00<00:00, 98.02it/s]Warming up with batch_size=1:  80%|████████  | 80/100 [00:00<00:00, 98.04it/s]Warming up with batch_size=1:  90%|█████████ | 90/100 [00:00<00:00, 98.05it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:01<00:00, 98.07it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:01<00:00, 98.00it/s]
STAGE:2024-02-25 23:04:31 6728:6728 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 23:04:31 6728:6728 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 23:04:31 6728:6728 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   1%|          | 8/1000 [00:00<00:13, 73.00it/s]Measuring inference for batch_size=1:   2%|▏         | 16/1000 [00:00<00:13, 73.36it/s]Measuring inference for batch_size=1:   2%|▏         | 24/1000 [00:00<00:13, 73.47it/s]Measuring inference for batch_size=1:   3%|▎         | 32/1000 [00:00<00:13, 73.54it/s]Measuring inference for batch_size=1:   4%|▍         | 40/1000 [00:00<00:13, 73.55it/s]Measuring inference for batch_size=1:   5%|▍         | 48/1000 [00:00<00:12, 73.56it/s]Measuring inference for batch_size=1:   6%|▌         | 56/1000 [00:00<00:12, 73.54it/s]Measuring inference for batch_size=1:   6%|▋         | 64/1000 [00:00<00:12, 73.52it/s]Measuring inference for batch_size=1:   7%|▋         | 72/1000 [00:00<00:12, 73.55it/s]Measuring inference for batch_size=1:   8%|▊         | 80/1000 [00:01<00:12, 73.62it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:01<00:12, 73.65it/s]Measuring inference for batch_size=1:  10%|▉         | 96/1000 [00:01<00:12, 73.66it/s]Measuring inference for batch_size=1:  10%|█         | 104/1000 [00:01<00:12, 73.65it/s]Measuring inference for batch_size=1:  11%|█         | 112/1000 [00:01<00:12, 73.66it/s]Measuring inference for batch_size=1:  12%|█▏        | 120/1000 [00:01<00:11, 73.67it/s]Measuring inference for batch_size=1:  13%|█▎        | 128/1000 [00:01<00:11, 73.67it/s]Measuring inference for batch_size=1:  14%|█▎        | 136/1000 [00:01<00:11, 73.63it/s]Measuring inference for batch_size=1:  14%|█▍        | 144/1000 [00:01<00:11, 73.63it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:02<00:11, 73.62it/s]Measuring inference for batch_size=1:  16%|█▌        | 160/1000 [00:02<00:11, 73.62it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:02<00:11, 73.62it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:02<00:11, 73.61it/s]Measuring inference for batch_size=1:  18%|█▊        | 184/1000 [00:02<00:11, 73.57it/s]Measuring inference for batch_size=1:  19%|█▉        | 192/1000 [00:02<00:10, 73.56it/s]Measuring inference for batch_size=1:  20%|██        | 200/1000 [00:02<00:10, 73.55it/s]Measuring inference for batch_size=1:  21%|██        | 208/1000 [00:02<00:10, 73.54it/s]Measuring inference for batch_size=1:  22%|██▏       | 216/1000 [00:02<00:10, 73.54it/s]Measuring inference for batch_size=1:  22%|██▏       | 224/1000 [00:03<00:10, 73.53it/s]Measuring inference for batch_size=1:  23%|██▎       | 232/1000 [00:03<00:10, 73.54it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:03<00:10, 73.56it/s]Measuring inference for batch_size=1:  25%|██▍       | 248/1000 [00:03<00:10, 73.59it/s]Measuring inference for batch_size=1:  26%|██▌       | 256/1000 [00:03<00:10, 73.60it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:03<00:09, 73.61it/s]Measuring inference for batch_size=1:  27%|██▋       | 272/1000 [00:03<00:09, 73.61it/s]Measuring inference for batch_size=1:  28%|██▊       | 280/1000 [00:03<00:09, 73.60it/s]Measuring inference for batch_size=1:  29%|██▉       | 288/1000 [00:03<00:09, 73.60it/s]Measuring inference for batch_size=1:  30%|██▉       | 296/1000 [00:04<00:09, 73.59it/s]Measuring inference for batch_size=1:  30%|███       | 304/1000 [00:04<00:09, 73.60it/s]Measuring inference for batch_size=1:  31%|███       | 312/1000 [00:04<00:09, 73.61it/s]Measuring inference for batch_size=1:  32%|███▏      | 320/1000 [00:04<00:09, 73.56it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:04<00:09, 73.54it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:04<00:09, 73.51it/s]Measuring inference for batch_size=1:  34%|███▍      | 344/1000 [00:04<00:08, 73.51it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:04<00:08, 73.51it/s]Measuring inference for batch_size=1:  36%|███▌      | 360/1000 [00:04<00:08, 73.49it/s]Measuring inference for batch_size=1:  37%|███▋      | 368/1000 [00:05<00:08, 73.50it/s]Measuring inference for batch_size=1:  38%|███▊      | 376/1000 [00:05<00:08, 73.53it/s]Measuring inference for batch_size=1:  38%|███▊      | 384/1000 [00:05<00:08, 73.51it/s]Measuring inference for batch_size=1:  39%|███▉      | 392/1000 [00:05<00:08, 73.49it/s]Measuring inference for batch_size=1:  40%|████      | 400/1000 [00:05<00:08, 73.50it/s]Measuring inference for batch_size=1:  41%|████      | 408/1000 [00:05<00:08, 73.53it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:05<00:07, 73.55it/s]Measuring inference for batch_size=1:  42%|████▏     | 424/1000 [00:05<00:07, 73.55it/s]Measuring inference for batch_size=1:  43%|████▎     | 432/1000 [00:05<00:07, 73.56it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:05<00:07, 73.59it/s]Measuring inference for batch_size=1:  45%|████▍     | 448/1000 [00:06<00:07, 73.59it/s]Measuring inference for batch_size=1:  46%|████▌     | 456/1000 [00:06<00:07, 73.59it/s]Measuring inference for batch_size=1:  46%|████▋     | 464/1000 [00:06<00:07, 73.58it/s]Measuring inference for batch_size=1:  47%|████▋     | 472/1000 [00:06<00:07, 73.58it/s]Measuring inference for batch_size=1:  48%|████▊     | 480/1000 [00:06<00:07, 73.59it/s]Measuring inference for batch_size=1:  49%|████▉     | 488/1000 [00:06<00:06, 73.57it/s]Measuring inference for batch_size=1:  50%|████▉     | 496/1000 [00:06<00:06, 73.56it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:06<00:06, 73.57it/s]Measuring inference for batch_size=1:  51%|█████     | 512/1000 [00:06<00:06, 73.56it/s]Measuring inference for batch_size=1:  52%|█████▏    | 520/1000 [00:07<00:06, 73.55it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:07<00:06, 73.54it/s]Measuring inference for batch_size=1:  54%|█████▎    | 536/1000 [00:07<00:06, 73.54it/s]Measuring inference for batch_size=1:  54%|█████▍    | 544/1000 [00:07<00:06, 73.52it/s]Measuring inference for batch_size=1:  55%|█████▌    | 552/1000 [00:07<00:06, 73.53it/s]Measuring inference for batch_size=1:  56%|█████▌    | 560/1000 [00:07<00:05, 73.53it/s]Measuring inference for batch_size=1:  57%|█████▋    | 568/1000 [00:07<00:05, 73.53it/s]Measuring inference for batch_size=1:  58%|█████▊    | 576/1000 [00:07<00:05, 73.53it/s]Measuring inference for batch_size=1:  58%|█████▊    | 584/1000 [00:07<00:05, 73.55it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [00:08<00:05, 73.55it/s]Measuring inference for batch_size=1:  60%|██████    | 600/1000 [00:08<00:05, 72.56it/s]Measuring inference for batch_size=1:  61%|██████    | 608/1000 [00:08<00:05, 71.71it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:08<00:05, 71.17it/s]Measuring inference for batch_size=1:  62%|██████▏   | 624/1000 [00:08<00:05, 70.71it/s]Measuring inference for batch_size=1:  63%|██████▎   | 632/1000 [00:08<00:05, 70.51it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [00:08<00:05, 70.31it/s]Measuring inference for batch_size=1:  65%|██████▍   | 648/1000 [00:08<00:05, 70.24it/s]Measuring inference for batch_size=1:  66%|██████▌   | 656/1000 [00:08<00:04, 70.10it/s]Measuring inference for batch_size=1:  66%|██████▋   | 664/1000 [00:09<00:04, 70.03it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:09<00:04, 69.97it/s]Measuring inference for batch_size=1:  68%|██████▊   | 679/1000 [00:09<00:04, 69.98it/s]Measuring inference for batch_size=1:  69%|██████▊   | 686/1000 [00:09<00:04, 69.98it/s]Measuring inference for batch_size=1:  69%|██████▉   | 693/1000 [00:09<00:04, 69.94it/s]Measuring inference for batch_size=1:  70%|███████   | 700/1000 [00:09<00:04, 69.86it/s]Measuring inference for batch_size=1:  71%|███████   | 707/1000 [00:09<00:04, 69.84it/s]Measuring inference for batch_size=1:  71%|███████▏  | 714/1000 [00:09<00:04, 69.81it/s]Measuring inference for batch_size=1:  72%|███████▏  | 721/1000 [00:09<00:03, 69.85it/s]Measuring inference for batch_size=1:  73%|███████▎  | 728/1000 [00:09<00:03, 69.83it/s]Measuring inference for batch_size=1:  74%|███████▎  | 736/1000 [00:10<00:03, 69.90it/s]Measuring inference for batch_size=1:  74%|███████▍  | 743/1000 [00:10<00:03, 69.86it/s]Measuring inference for batch_size=1:  75%|███████▌  | 750/1000 [00:10<00:03, 69.85it/s]Measuring inference for batch_size=1:  76%|███████▌  | 757/1000 [00:10<00:03, 69.88it/s]Measuring inference for batch_size=1:  76%|███████▋  | 764/1000 [00:10<00:03, 69.89it/s]Measuring inference for batch_size=1:  77%|███████▋  | 771/1000 [00:10<00:03, 69.87it/s]Measuring inference for batch_size=1:  78%|███████▊  | 778/1000 [00:10<00:03, 69.88it/s]Measuring inference for batch_size=1:  78%|███████▊  | 785/1000 [00:10<00:03, 69.86it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [00:10<00:02, 69.83it/s]Measuring inference for batch_size=1:  80%|███████▉  | 799/1000 [00:11<00:02, 69.84it/s]Measuring inference for batch_size=1:  81%|████████  | 806/1000 [00:11<00:02, 69.85it/s]Measuring inference for batch_size=1:  81%|████████▏ | 813/1000 [00:11<00:02, 69.86it/s]Measuring inference for batch_size=1:  82%|████████▏ | 820/1000 [00:11<00:02, 69.83it/s]Measuring inference for batch_size=1:  83%|████████▎ | 827/1000 [00:11<00:02, 69.80it/s]Measuring inference for batch_size=1:  83%|████████▎ | 834/1000 [00:11<00:02, 69.81it/s]Measuring inference for batch_size=1:  84%|████████▍ | 841/1000 [00:11<00:02, 69.82it/s]Measuring inference for batch_size=1:  85%|████████▍ | 848/1000 [00:11<00:02, 69.83it/s]Measuring inference for batch_size=1:  86%|████████▌ | 855/1000 [00:11<00:02, 69.77it/s]Measuring inference for batch_size=1:  86%|████████▌ | 862/1000 [00:11<00:01, 69.82it/s]Measuring inference for batch_size=1:  87%|████████▋ | 869/1000 [00:12<00:01, 69.81it/s]Measuring inference for batch_size=1:  88%|████████▊ | 876/1000 [00:12<00:01, 69.84it/s]Measuring inference for batch_size=1:  88%|████████▊ | 883/1000 [00:12<00:01, 69.81it/s]Measuring inference for batch_size=1:  89%|████████▉ | 890/1000 [00:12<00:01, 69.83it/s]Measuring inference for batch_size=1:  90%|████████▉ | 897/1000 [00:12<00:01, 69.84it/s]Measuring inference for batch_size=1:  90%|█████████ | 904/1000 [00:12<00:01, 69.87it/s]Measuring inference for batch_size=1:  91%|█████████ | 911/1000 [00:12<00:01, 69.83it/s]Measuring inference for batch_size=1:  92%|█████████▏| 918/1000 [00:12<00:01, 69.86it/s]Measuring inference for batch_size=1:  92%|█████████▎| 925/1000 [00:12<00:01, 69.83it/s]Measuring inference for batch_size=1:  93%|█████████▎| 932/1000 [00:12<00:00, 69.82it/s]Measuring inference for batch_size=1:  94%|█████████▍| 939/1000 [00:13<00:00, 69.81it/s]Measuring inference for batch_size=1:  95%|█████████▍| 946/1000 [00:13<00:00, 69.83it/s]Measuring inference for batch_size=1:  95%|█████████▌| 953/1000 [00:13<00:00, 69.80it/s]Measuring inference for batch_size=1:  96%|█████████▌| 960/1000 [00:13<00:00, 69.78it/s]Measuring inference for batch_size=1:  97%|█████████▋| 967/1000 [00:13<00:00, 69.72it/s]Measuring inference for batch_size=1:  97%|█████████▋| 974/1000 [00:13<00:00, 69.73it/s]Measuring inference for batch_size=1:  98%|█████████▊| 981/1000 [00:13<00:00, 69.71it/s]Measuring inference for batch_size=1:  99%|█████████▉| 988/1000 [00:13<00:00, 69.77it/s]Measuring inference for batch_size=1: 100%|█████████▉| 995/1000 [00:13<00:00, 69.77it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 72.00it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=256:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=256:   8%|▊         | 8/100 [00:00<00:01, 72.86it/s]Warming up with batch_size=256:  16%|█▌        | 16/100 [00:00<00:01, 72.99it/s]Warming up with batch_size=256:  24%|██▍       | 24/100 [00:00<00:01, 73.06it/s]Warming up with batch_size=256:  32%|███▏      | 32/100 [00:00<00:00, 73.12it/s]Warming up with batch_size=256:  40%|████      | 40/100 [00:00<00:00, 73.14it/s]Warming up with batch_size=256:  48%|████▊     | 48/100 [00:00<00:00, 73.16it/s]Warming up with batch_size=256:  56%|█████▌    | 56/100 [00:00<00:00, 73.19it/s]Warming up with batch_size=256:  64%|██████▍   | 64/100 [00:00<00:00, 73.19it/s]Warming up with batch_size=256:  72%|███████▏  | 72/100 [00:00<00:00, 73.14it/s]Warming up with batch_size=256:  80%|████████  | 80/100 [00:01<00:00, 73.22it/s]Warming up with batch_size=256:  88%|████████▊ | 88/100 [00:01<00:00, 73.27it/s]Warming up with batch_size=256:  96%|█████████▌| 96/100 [00:01<00:00, 73.24it/s]Warming up with batch_size=256: 100%|██████████| 100/100 [00:01<00:00, 73.17it/s]
STAGE:2024-02-25 23:04:46 6728:6728 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 23:04:46 6728:6728 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 23:04:46 6728:6728 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=256:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=256:   1%|          | 8/1000 [00:00<00:13, 72.02it/s]Measuring inference for batch_size=256:   2%|▏         | 16/1000 [00:00<00:13, 72.29it/s]Measuring inference for batch_size=256:   2%|▏         | 24/1000 [00:00<00:13, 72.40it/s]Measuring inference for batch_size=256:   3%|▎         | 32/1000 [00:00<00:13, 72.46it/s]Measuring inference for batch_size=256:   4%|▍         | 40/1000 [00:00<00:13, 72.36it/s]Measuring inference for batch_size=256:   5%|▍         | 48/1000 [00:00<00:13, 72.38it/s]Measuring inference for batch_size=256:   6%|▌         | 56/1000 [00:00<00:13, 72.40it/s]Measuring inference for batch_size=256:   6%|▋         | 64/1000 [00:00<00:12, 72.39it/s]Measuring inference for batch_size=256:   7%|▋         | 72/1000 [00:00<00:12, 72.42it/s]Measuring inference for batch_size=256:   8%|▊         | 80/1000 [00:01<00:12, 72.42it/s]Measuring inference for batch_size=256:   9%|▉         | 88/1000 [00:01<00:12, 72.43it/s]Measuring inference for batch_size=256:  10%|▉         | 96/1000 [00:01<00:12, 72.40it/s]Measuring inference for batch_size=256:  10%|█         | 104/1000 [00:01<00:12, 72.40it/s]Measuring inference for batch_size=256:  11%|█         | 112/1000 [00:01<00:12, 72.37it/s]Measuring inference for batch_size=256:  12%|█▏        | 120/1000 [00:01<00:12, 72.37it/s]Measuring inference for batch_size=256:  13%|█▎        | 128/1000 [00:01<00:12, 72.37it/s]Measuring inference for batch_size=256:  14%|█▎        | 136/1000 [00:01<00:11, 72.40it/s]Measuring inference for batch_size=256:  14%|█▍        | 144/1000 [00:01<00:11, 72.42it/s]Measuring inference for batch_size=256:  15%|█▌        | 152/1000 [00:02<00:11, 72.46it/s]Measuring inference for batch_size=256:  16%|█▌        | 160/1000 [00:02<00:11, 72.45it/s]Measuring inference for batch_size=256:  17%|█▋        | 168/1000 [00:02<00:11, 72.46it/s]Measuring inference for batch_size=256:  18%|█▊        | 176/1000 [00:02<00:11, 72.44it/s]Measuring inference for batch_size=256:  18%|█▊        | 184/1000 [00:02<00:11, 72.47it/s]Measuring inference for batch_size=256:  19%|█▉        | 192/1000 [00:02<00:11, 72.49it/s]Measuring inference for batch_size=256:  20%|██        | 200/1000 [00:02<00:11, 72.50it/s]Measuring inference for batch_size=256:  21%|██        | 208/1000 [00:02<00:10, 72.47it/s]Measuring inference for batch_size=256:  22%|██▏       | 216/1000 [00:02<00:10, 72.44it/s]Measuring inference for batch_size=256:  22%|██▏       | 224/1000 [00:03<00:10, 72.42it/s]Measuring inference for batch_size=256:  23%|██▎       | 232/1000 [00:03<00:10, 72.44it/s]Measuring inference for batch_size=256:  24%|██▍       | 240/1000 [00:03<00:10, 72.44it/s]Measuring inference for batch_size=256:  25%|██▍       | 248/1000 [00:03<00:10, 72.49it/s]Measuring inference for batch_size=256:  26%|██▌       | 256/1000 [00:03<00:10, 71.46it/s]Measuring inference for batch_size=256:  26%|██▋       | 264/1000 [00:03<00:10, 70.79it/s]Measuring inference for batch_size=256:  27%|██▋       | 272/1000 [00:03<00:10, 70.30it/s]Measuring inference for batch_size=256:  28%|██▊       | 280/1000 [00:03<00:10, 70.79it/s]Measuring inference for batch_size=256:  29%|██▉       | 288/1000 [00:03<00:09, 71.23it/s]Measuring inference for batch_size=256:  30%|██▉       | 296/1000 [00:04<00:09, 71.60it/s]Measuring inference for batch_size=256:  30%|███       | 304/1000 [00:04<00:09, 71.84it/s]Measuring inference for batch_size=256:  31%|███       | 312/1000 [00:04<00:09, 72.03it/s]Measuring inference for batch_size=256:  32%|███▏      | 320/1000 [00:04<00:09, 72.16it/s]Measuring inference for batch_size=256:  33%|███▎      | 328/1000 [00:04<00:09, 72.22it/s]Measuring inference for batch_size=256:  34%|███▎      | 336/1000 [00:04<00:09, 72.32it/s]Measuring inference for batch_size=256:  34%|███▍      | 344/1000 [00:04<00:09, 72.37it/s]Measuring inference for batch_size=256:  35%|███▌      | 352/1000 [00:04<00:08, 72.42it/s]Measuring inference for batch_size=256:  36%|███▌      | 360/1000 [00:04<00:08, 72.42it/s]Measuring inference for batch_size=256:  37%|███▋      | 368/1000 [00:05<00:08, 72.43it/s]Measuring inference for batch_size=256:  38%|███▊      | 376/1000 [00:05<00:08, 72.45it/s]Measuring inference for batch_size=256:  38%|███▊      | 384/1000 [00:05<00:08, 72.42it/s]Measuring inference for batch_size=256:  39%|███▉      | 392/1000 [00:05<00:08, 72.42it/s]Measuring inference for batch_size=256:  40%|████      | 400/1000 [00:05<00:08, 72.45it/s]Measuring inference for batch_size=256:  41%|████      | 408/1000 [00:05<00:08, 72.51it/s]Measuring inference for batch_size=256:  42%|████▏     | 416/1000 [00:05<00:08, 71.84it/s]Measuring inference for batch_size=256:  42%|████▏     | 424/1000 [00:05<00:08, 71.29it/s]Measuring inference for batch_size=256:  43%|████▎     | 432/1000 [00:05<00:07, 71.70it/s]Measuring inference for batch_size=256:  44%|████▍     | 440/1000 [00:06<00:07, 71.98it/s]Measuring inference for batch_size=256:  45%|████▍     | 448/1000 [00:06<00:07, 72.15it/s]Measuring inference for batch_size=256:  46%|████▌     | 456/1000 [00:06<00:07, 72.31it/s]Measuring inference for batch_size=256:  46%|████▋     | 464/1000 [00:06<00:07, 72.34it/s]Measuring inference for batch_size=256:  47%|████▋     | 472/1000 [00:06<00:07, 72.38it/s]Measuring inference for batch_size=256:  48%|████▊     | 480/1000 [00:06<00:07, 72.39it/s]Measuring inference for batch_size=256:  49%|████▉     | 488/1000 [00:06<00:07, 72.43it/s]Measuring inference for batch_size=256:  50%|████▉     | 496/1000 [00:06<00:06, 72.46it/s]Measuring inference for batch_size=256:  50%|█████     | 504/1000 [00:06<00:06, 72.46it/s]Measuring inference for batch_size=256:  51%|█████     | 512/1000 [00:07<00:06, 72.48it/s]Measuring inference for batch_size=256:  52%|█████▏    | 520/1000 [00:07<00:06, 72.50it/s]Measuring inference for batch_size=256:  53%|█████▎    | 528/1000 [00:07<00:06, 72.48it/s]Measuring inference for batch_size=256:  54%|█████▎    | 536/1000 [00:07<00:06, 72.48it/s]Measuring inference for batch_size=256:  54%|█████▍    | 544/1000 [00:07<00:06, 72.47it/s]Measuring inference for batch_size=256:  55%|█████▌    | 552/1000 [00:07<00:06, 72.47it/s]Measuring inference for batch_size=256:  56%|█████▌    | 560/1000 [00:07<00:06, 72.45it/s]Measuring inference for batch_size=256:  57%|█████▋    | 568/1000 [00:07<00:05, 72.47it/s]Measuring inference for batch_size=256:  58%|█████▊    | 576/1000 [00:07<00:05, 72.49it/s]Measuring inference for batch_size=256:  58%|█████▊    | 584/1000 [00:08<00:05, 72.51it/s]Measuring inference for batch_size=256:  59%|█████▉    | 592/1000 [00:08<00:05, 72.51it/s]Measuring inference for batch_size=256:  60%|██████    | 600/1000 [00:08<00:05, 72.48it/s]Measuring inference for batch_size=256:  61%|██████    | 608/1000 [00:08<00:05, 72.46it/s]Measuring inference for batch_size=256:  62%|██████▏   | 616/1000 [00:08<00:05, 72.45it/s]Measuring inference for batch_size=256:  62%|██████▏   | 624/1000 [00:08<00:05, 72.42it/s]Measuring inference for batch_size=256:  63%|██████▎   | 632/1000 [00:08<00:05, 71.41it/s]Measuring inference for batch_size=256:  64%|██████▍   | 640/1000 [00:08<00:05, 70.72it/s]Measuring inference for batch_size=256:  65%|██████▍   | 648/1000 [00:08<00:04, 71.10it/s]Measuring inference for batch_size=256:  66%|██████▌   | 656/1000 [00:09<00:04, 71.46it/s]Measuring inference for batch_size=256:  66%|██████▋   | 664/1000 [00:09<00:04, 71.74it/s]Measuring inference for batch_size=256:  67%|██████▋   | 672/1000 [00:09<00:04, 71.96it/s]Measuring inference for batch_size=256:  68%|██████▊   | 680/1000 [00:09<00:04, 72.06it/s]Measuring inference for batch_size=256:  69%|██████▉   | 688/1000 [00:09<00:04, 72.18it/s]Measuring inference for batch_size=256:  70%|██████▉   | 696/1000 [00:09<00:04, 72.26it/s]Measuring inference for batch_size=256:  70%|███████   | 704/1000 [00:09<00:04, 72.29it/s]Measuring inference for batch_size=256:  71%|███████   | 712/1000 [00:09<00:03, 72.31it/s]Measuring inference for batch_size=256:  72%|███████▏  | 720/1000 [00:09<00:03, 72.33it/s]Measuring inference for batch_size=256:  73%|███████▎  | 728/1000 [00:10<00:03, 72.35it/s]Measuring inference for batch_size=256:  74%|███████▎  | 736/1000 [00:10<00:03, 72.34it/s]Measuring inference for batch_size=256:  74%|███████▍  | 744/1000 [00:10<00:03, 72.34it/s]Measuring inference for batch_size=256:  75%|███████▌  | 752/1000 [00:10<00:03, 72.35it/s]Measuring inference for batch_size=256:  76%|███████▌  | 760/1000 [00:10<00:03, 72.36it/s]Measuring inference for batch_size=256:  77%|███████▋  | 768/1000 [00:10<00:03, 72.37it/s]Measuring inference for batch_size=256:  78%|███████▊  | 776/1000 [00:10<00:03, 72.36it/s]Measuring inference for batch_size=256:  78%|███████▊  | 784/1000 [00:10<00:02, 72.35it/s]Measuring inference for batch_size=256:  79%|███████▉  | 792/1000 [00:10<00:02, 72.35it/s]Measuring inference for batch_size=256:  80%|████████  | 800/1000 [00:11<00:02, 72.32it/s]Measuring inference for batch_size=256:  81%|████████  | 808/1000 [00:11<00:02, 72.32it/s]Measuring inference for batch_size=256:  82%|████████▏ | 816/1000 [00:11<00:02, 72.30it/s]Measuring inference for batch_size=256:  82%|████████▏ | 824/1000 [00:11<00:02, 72.30it/s]Measuring inference for batch_size=256:  83%|████████▎ | 832/1000 [00:11<00:02, 72.34it/s]Measuring inference for batch_size=256:  84%|████████▍ | 840/1000 [00:11<00:02, 72.38it/s]Measuring inference for batch_size=256:  85%|████████▍ | 848/1000 [00:11<00:02, 72.42it/s]Measuring inference for batch_size=256:  86%|████████▌ | 856/1000 [00:11<00:01, 72.41it/s]Measuring inference for batch_size=256:  86%|████████▋ | 864/1000 [00:11<00:01, 72.42it/s]Measuring inference for batch_size=256:  87%|████████▋ | 872/1000 [00:12<00:01, 72.45it/s]Measuring inference for batch_size=256:  88%|████████▊ | 880/1000 [00:12<00:01, 72.46it/s]Measuring inference for batch_size=256:  89%|████████▉ | 888/1000 [00:12<00:01, 72.46it/s]Measuring inference for batch_size=256:  90%|████████▉ | 896/1000 [00:12<00:01, 72.45it/s]Measuring inference for batch_size=256:  90%|█████████ | 904/1000 [00:12<00:01, 72.41it/s]Measuring inference for batch_size=256:  91%|█████████ | 912/1000 [00:12<00:01, 71.44it/s]Measuring inference for batch_size=256:  92%|█████████▏| 920/1000 [00:12<00:01, 71.40it/s]Measuring inference for batch_size=256:  93%|█████████▎| 928/1000 [00:12<00:01, 71.74it/s]Measuring inference for batch_size=256:  94%|█████████▎| 936/1000 [00:12<00:00, 71.96it/s]Measuring inference for batch_size=256:  94%|█████████▍| 944/1000 [00:13<00:00, 72.12it/s]Measuring inference for batch_size=256:  95%|█████████▌| 952/1000 [00:13<00:00, 72.26it/s]Measuring inference for batch_size=256:  96%|█████████▌| 960/1000 [00:13<00:00, 72.32it/s]Measuring inference for batch_size=256:  97%|█████████▋| 968/1000 [00:13<00:00, 72.39it/s]Measuring inference for batch_size=256:  98%|█████████▊| 976/1000 [00:13<00:00, 72.43it/s]Measuring inference for batch_size=256:  98%|█████████▊| 984/1000 [00:13<00:00, 72.47it/s]Measuring inference for batch_size=256:  99%|█████████▉| 992/1000 [00:13<00:00, 72.50it/s]Measuring inference for batch_size=256: 100%|██████████| 1000/1000 [00:13<00:00, 72.38it/s]Measuring inference for batch_size=256: 100%|██████████| 1000/1000 [00:13<00:00, 72.22it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 11580687848
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 29.98 GB
    total: 31.28 GB
    used: 924.03 MB
  system:
    node: fp
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 374.76 MB
    max_inference_bytes: 392962560
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
  batch_size_256:
    max_inference: 378.48 MB
    max_inference_bytes: 396866048
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
params: 60192808
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 94.184 us +/- 5.294 us [91.314 us, 222.921 us]
        batches_per_second: 10.64 K +/- 407.09 [4.49 K, 10.95 K]
      metrics:
        batches_per_second_max: 10951.185378590078
        batches_per_second_mean: 10638.765227731486
        batches_per_second_min: 4485.886631016043
        batches_per_second_std: 407.092779871785
        seconds_per_batch_max: 0.00022292137145996094
        seconds_per_batch_mean: 9.418392181396484e-05
        seconds_per_batch_min: 9.131431579589844e-05
        seconds_per_batch_std: 5.293637214706944e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.847 us +/- 0.644 us [22.650 us, 30.994 us]
        batches_per_second: 41.96 K +/- 1.02 K [32.26 K, 44.15 K]
      metrics:
        batches_per_second_max: 44150.56842105263
        batches_per_second_mean: 41960.68089636351
        batches_per_second_min: 32263.876923076925
        batches_per_second_std: 1017.4948973584344
        seconds_per_batch_max: 3.0994415283203125e-05
        seconds_per_batch_mean: 2.3847341537475585e-05
        seconds_per_batch_min: 2.2649765014648438e-05
        seconds_per_batch_std: 6.435743337565061e-07
    on_device_inference:
      human_readable:
        batch_latency: -13755194.884 us +/- 356.074 ms [-14296832.085 us, -13394656.181
          us]
        batches_per_second: -0.07 +/- 0.00 [-0.07, -0.07]
      metrics:
        batches_per_second_max: -0.06994556514888788
        batches_per_second_mean: -0.07274808382454691
        batches_per_second_min: -0.07465663817436632
        batches_per_second_std: 0.0018649913706902787
        seconds_per_batch_max: -13.39465618133545
        seconds_per_batch_mean: -13.755194884300233
        seconds_per_batch_min: -14.296832084655762
        seconds_per_batch_std: 0.35607447336285897
    total:
      human_readable:
        batch_latency: 13.881 ms +/- 358.252 us [13.518 ms, 14.440 ms]
        batches_per_second: 72.09 +/- 1.84 [69.25, 73.98]
      metrics:
        batches_per_second_max: 73.9762249109316
        batches_per_second_mean: 72.08798279037757
        batches_per_second_min: 69.25293486336993
        batches_per_second_std: 1.8425220151284856
        seconds_per_batch_max: 0.014439821243286133
        seconds_per_batch_mean: 0.013881094217300416
        seconds_per_batch_min: 0.01351785659790039
        seconds_per_batch_std: 0.0003582517220498922
  batch_size_256:
    cpu_to_gpu:
      human_readable:
        batch_latency: 144.105 us +/- 5.421 us [141.621 us, 280.142 us]
        batches_per_second: 6.95 K +/- 185.51 [3.57 K, 7.06 K]
      metrics:
        batches_per_second_max: 7061.117845117845
        batches_per_second_mean: 6946.032619139016
        batches_per_second_min: 3569.620425531915
        batches_per_second_std: 185.5118036138641
        seconds_per_batch_max: 0.00028014183044433594
        seconds_per_batch_mean: 0.00014410519599914552
        seconds_per_batch_min: 0.00014162063598632812
        seconds_per_batch_std: 5.4205383333253895e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.690 us +/- 0.552 us [22.888 us, 31.948 us]
        batches_per_second: 42.23 K +/- 863.68 [31.30 K, 43.69 K]
      metrics:
        batches_per_second_max: 43690.666666666664
        batches_per_second_mean: 42231.91973207085
        batches_per_second_min: 31300.776119402984
        batches_per_second_std: 863.6849057472778
        seconds_per_batch_max: 3.1948089599609375e-05
        seconds_per_batch_mean: 2.3689985275268555e-05
        seconds_per_batch_min: 2.288818359375e-05
        seconds_per_batch_std: 5.520422985085008e-07
    on_device_inference:
      human_readable:
        batch_latency: -13664905.115 us +/- 163.473 ms [-14506527.901 us, -13545599.937
          us]
        batches_per_second: -0.07 +/- 0.00 [-0.07, -0.07]
      metrics:
        batches_per_second_max: -0.06893448293385458
        batches_per_second_mean: -0.07319023328102568
        batches_per_second_min: -0.07382471094809756
        batches_per_second_std: 0.0008417136305714112
        seconds_per_batch_max: -13.545599937438965
        seconds_per_batch_mean: -13.664905115127564
        seconds_per_batch_min: -14.5065279006958
        seconds_per_batch_std: 0.16347296276779685
    total:
      human_readable:
        batch_latency: 13.838 ms +/- 164.935 us [13.717 ms, 14.683 ms]
        batches_per_second: 72.27 +/- 0.83 [68.11, 72.90]
      metrics:
        batches_per_second_max: 72.90004345181194
        batches_per_second_mean: 72.27413568951467
        batches_per_second_min: 68.10814672880502
        batches_per_second_std: 0.8281933237870878
        seconds_per_batch_max: 0.014682531356811523
        seconds_per_batch_mean: 0.013838098287582397
        seconds_per_batch_min: 0.013717412948608398
        seconds_per_batch_std: 0.00016493457428527888


#####
fp-fp-py-id - Run 3
2024-02-25 23:05:39
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.73it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.73it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  60.19 M, 100.000% Params, 11.58 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.016% Params, 118.01 MMac, 1.019% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.000% Params, 1.61 MMac, 0.014% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.007% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.007% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.359% Params, 680.39 MMac, 5.875% MACs, 
    (0): Bottleneck(
      75.01 k, 0.125% Params, 236.43 MMac, 2.042% MACs, 
      (conv1): Conv2d(4.1 k, 0.007% Params, 12.85 MMac, 0.111% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.028% Params, 52.99 MMac, 0.458% MACs, 
        (0): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.117% Params, 221.98 MMac, 1.917% MACs, 
      (conv1): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.061% Params, 115.61 MMac, 0.998% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.000% Params, 401.41 KMac, 0.003% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.027% Params, 51.38 MMac, 0.444% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.001% Params, 1.61 MMac, 0.014% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.010% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    2.34 M, 3.887% Params, 1.92 GMac, 16.555% MACs, 
    (0): Bottleneck(
      379.39 k, 0.630% Params, 376.02 MMac, 3.247% MACs, 
      (conv1): Conv2d(32.77 k, 0.054% Params, 102.76 MMac, 0.887% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 802.82 KMac, 0.007% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.008% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.219% Params, 103.56 MMac, 0.894% MACs, 
        (0): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (4): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (5): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (6): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
    (7): Bottleneck(
      280.06 k, 0.465% Params, 220.17 MMac, 1.901% MACs, 
      (conv1): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.245% Params, 115.61 MMac, 0.998% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.000% Params, 200.7 KMac, 0.002% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.109% Params, 51.38 MMac, 0.444% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.002% Params, 802.82 KMac, 0.007% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.005% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    40.61 M, 67.473% Params, 8.05 GMac, 69.501% MACs, 
    (0): Bottleneck(
      1.51 M, 2.513% Params, 374.26 MMac, 3.232% MACs, 
      (conv1): Conv2d(131.07 k, 0.218% Params, 102.76 MMac, 0.887% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 401.41 KMac, 0.003% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.004% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 0.874% Params, 103.16 MMac, 0.891% MACs, 
        (0): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (6): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (7): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (8): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (9): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (10): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (11): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (12): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (13): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (14): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (15): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (16): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (17): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (18): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (19): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (20): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (21): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (22): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (23): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (24): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (25): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (26): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (27): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (28): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (29): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (30): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (31): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (32): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (33): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (34): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
    (35): Bottleneck(
      1.12 M, 1.856% Params, 219.27 MMac, 1.893% MACs, 
      (conv1): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 0.980% Params, 115.61 MMac, 0.998% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.001% Params, 100.35 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 0.436% Params, 51.38 MMac, 0.444% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.003% Params, 401.41 KMac, 0.003% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.003% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 24.861% Params, 811.02 MMac, 7.003% MACs, 
    (0): Bottleneck(
      6.04 M, 10.034% Params, 373.38 MMac, 3.224% MACs, 
      (conv1): Conv2d(524.29 k, 0.871% Params, 102.76 MMac, 0.887% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 200.7 KMac, 0.002% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.002% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 3.491% Params, 102.96 MMac, 0.889% MACs, 
        (0): Conv2d(2.1 M, 3.484% Params, 102.76 MMac, 0.887% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 7.414% Params, 218.82 MMac, 1.890% MACs, 
      (conv1): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 3.920% Params, 115.61 MMac, 0.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.002% Params, 50.18 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 1.742% Params, 51.38 MMac, 0.444% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.007% Params, 200.7 KMac, 0.002% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.001% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.001% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 3.404% Params, 2.05 MMac, 0.018% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  10%|█         | 10/100 [00:00<00:00, 95.81it/s]Warming up with batch_size=1:  20%|██        | 20/100 [00:00<00:00, 95.98it/s]Warming up with batch_size=1:  30%|███       | 30/100 [00:00<00:00, 96.22it/s]Warming up with batch_size=1:  40%|████      | 40/100 [00:00<00:00, 96.33it/s]Warming up with batch_size=1:  50%|█████     | 50/100 [00:00<00:00, 96.44it/s]Warming up with batch_size=1:  60%|██████    | 60/100 [00:00<00:00, 96.49it/s]Warming up with batch_size=1:  70%|███████   | 70/100 [00:00<00:00, 96.52it/s]Warming up with batch_size=1:  80%|████████  | 80/100 [00:00<00:00, 96.53it/s]Warming up with batch_size=1:  90%|█████████ | 90/100 [00:00<00:00, 96.54it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:01<00:00, 96.50it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:01<00:00, 96.42it/s]
STAGE:2024-02-25 23:05:09 6774:6774 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 23:05:09 6774:6774 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 23:05:09 6774:6774 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   1%|          | 8/1000 [00:00<00:13, 72.39it/s]Measuring inference for batch_size=1:   2%|▏         | 16/1000 [00:00<00:13, 72.67it/s]Measuring inference for batch_size=1:   2%|▏         | 24/1000 [00:00<00:13, 72.78it/s]Measuring inference for batch_size=1:   3%|▎         | 32/1000 [00:00<00:13, 72.82it/s]Measuring inference for batch_size=1:   4%|▍         | 40/1000 [00:00<00:13, 72.84it/s]Measuring inference for batch_size=1:   5%|▍         | 48/1000 [00:00<00:13, 72.86it/s]Measuring inference for batch_size=1:   6%|▌         | 56/1000 [00:00<00:12, 72.87it/s]Measuring inference for batch_size=1:   6%|▋         | 64/1000 [00:00<00:12, 72.86it/s]Measuring inference for batch_size=1:   7%|▋         | 72/1000 [00:00<00:12, 72.84it/s]Measuring inference for batch_size=1:   8%|▊         | 80/1000 [00:01<00:12, 72.86it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:01<00:12, 72.83it/s]Measuring inference for batch_size=1:  10%|▉         | 96/1000 [00:01<00:12, 72.84it/s]Measuring inference for batch_size=1:  10%|█         | 104/1000 [00:01<00:12, 72.83it/s]Measuring inference for batch_size=1:  11%|█         | 112/1000 [00:01<00:12, 72.82it/s]Measuring inference for batch_size=1:  12%|█▏        | 120/1000 [00:01<00:12, 72.81it/s]Measuring inference for batch_size=1:  13%|█▎        | 128/1000 [00:01<00:11, 72.79it/s]Measuring inference for batch_size=1:  14%|█▎        | 136/1000 [00:01<00:11, 72.82it/s]Measuring inference for batch_size=1:  14%|█▍        | 144/1000 [00:01<00:11, 72.82it/s]Measuring inference for batch_size=1:  15%|█▌        | 152/1000 [00:02<00:11, 72.77it/s]Measuring inference for batch_size=1:  16%|█▌        | 160/1000 [00:02<00:11, 72.73it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:02<00:11, 72.73it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:02<00:11, 72.75it/s]Measuring inference for batch_size=1:  18%|█▊        | 184/1000 [00:02<00:11, 72.76it/s]Measuring inference for batch_size=1:  19%|█▉        | 192/1000 [00:02<00:11, 72.75it/s]Measuring inference for batch_size=1:  20%|██        | 200/1000 [00:02<00:10, 72.76it/s]Measuring inference for batch_size=1:  21%|██        | 208/1000 [00:02<00:10, 72.76it/s]Measuring inference for batch_size=1:  22%|██▏       | 216/1000 [00:02<00:10, 72.81it/s]Measuring inference for batch_size=1:  22%|██▏       | 224/1000 [00:03<00:10, 72.83it/s]Measuring inference for batch_size=1:  23%|██▎       | 232/1000 [00:03<00:10, 72.87it/s]Measuring inference for batch_size=1:  24%|██▍       | 240/1000 [00:03<00:10, 72.88it/s]Measuring inference for batch_size=1:  25%|██▍       | 248/1000 [00:03<00:10, 72.87it/s]Measuring inference for batch_size=1:  26%|██▌       | 256/1000 [00:03<00:10, 72.83it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:03<00:10, 72.80it/s]Measuring inference for batch_size=1:  27%|██▋       | 272/1000 [00:03<00:10, 72.76it/s]Measuring inference for batch_size=1:  28%|██▊       | 280/1000 [00:03<00:09, 72.76it/s]Measuring inference for batch_size=1:  29%|██▉       | 288/1000 [00:03<00:09, 72.75it/s]Measuring inference for batch_size=1:  30%|██▉       | 296/1000 [00:04<00:09, 72.76it/s]Measuring inference for batch_size=1:  30%|███       | 304/1000 [00:04<00:09, 72.78it/s]Measuring inference for batch_size=1:  31%|███       | 312/1000 [00:04<00:09, 72.78it/s]Measuring inference for batch_size=1:  32%|███▏      | 320/1000 [00:04<00:09, 72.78it/s]Measuring inference for batch_size=1:  33%|███▎      | 328/1000 [00:04<00:09, 72.78it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:04<00:09, 72.75it/s]Measuring inference for batch_size=1:  34%|███▍      | 344/1000 [00:04<00:09, 72.76it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:04<00:08, 72.77it/s]Measuring inference for batch_size=1:  36%|███▌      | 360/1000 [00:04<00:08, 72.78it/s]Measuring inference for batch_size=1:  37%|███▋      | 368/1000 [00:05<00:08, 72.78it/s]Measuring inference for batch_size=1:  38%|███▊      | 376/1000 [00:05<00:08, 72.78it/s]Measuring inference for batch_size=1:  38%|███▊      | 384/1000 [00:05<00:08, 72.76it/s]Measuring inference for batch_size=1:  39%|███▉      | 392/1000 [00:05<00:08, 72.73it/s]Measuring inference for batch_size=1:  40%|████      | 400/1000 [00:05<00:08, 72.71it/s]Measuring inference for batch_size=1:  41%|████      | 408/1000 [00:05<00:08, 72.72it/s]Measuring inference for batch_size=1:  42%|████▏     | 416/1000 [00:05<00:08, 72.74it/s]Measuring inference for batch_size=1:  42%|████▏     | 424/1000 [00:05<00:07, 72.76it/s]Measuring inference for batch_size=1:  43%|████▎     | 432/1000 [00:05<00:07, 72.78it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:06<00:07, 72.77it/s]Measuring inference for batch_size=1:  45%|████▍     | 448/1000 [00:06<00:07, 72.75it/s]Measuring inference for batch_size=1:  46%|████▌     | 456/1000 [00:06<00:07, 72.79it/s]Measuring inference for batch_size=1:  46%|████▋     | 464/1000 [00:06<00:07, 72.79it/s]Measuring inference for batch_size=1:  47%|████▋     | 472/1000 [00:06<00:07, 72.77it/s]Measuring inference for batch_size=1:  48%|████▊     | 480/1000 [00:06<00:07, 72.71it/s]Measuring inference for batch_size=1:  49%|████▉     | 488/1000 [00:06<00:07, 72.71it/s]Measuring inference for batch_size=1:  50%|████▉     | 496/1000 [00:06<00:06, 72.71it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:06<00:06, 72.72it/s]Measuring inference for batch_size=1:  51%|█████     | 512/1000 [00:07<00:06, 72.72it/s]Measuring inference for batch_size=1:  52%|█████▏    | 520/1000 [00:07<00:06, 72.72it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:07<00:06, 72.76it/s]Measuring inference for batch_size=1:  54%|█████▎    | 536/1000 [00:07<00:06, 72.82it/s]Measuring inference for batch_size=1:  54%|█████▍    | 544/1000 [00:07<00:06, 72.83it/s]Measuring inference for batch_size=1:  55%|█████▌    | 552/1000 [00:07<00:06, 72.83it/s]Measuring inference for batch_size=1:  56%|█████▌    | 560/1000 [00:07<00:06, 72.84it/s]Measuring inference for batch_size=1:  57%|█████▋    | 568/1000 [00:07<00:05, 72.87it/s]Measuring inference for batch_size=1:  58%|█████▊    | 576/1000 [00:07<00:05, 72.88it/s]Measuring inference for batch_size=1:  58%|█████▊    | 584/1000 [00:08<00:05, 72.89it/s]Measuring inference for batch_size=1:  59%|█████▉    | 592/1000 [00:08<00:05, 72.90it/s]Measuring inference for batch_size=1:  60%|██████    | 600/1000 [00:08<00:05, 72.92it/s]Measuring inference for batch_size=1:  61%|██████    | 608/1000 [00:08<00:05, 72.93it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:08<00:05, 72.90it/s]Measuring inference for batch_size=1:  62%|██████▏   | 624/1000 [00:08<00:05, 72.89it/s]Measuring inference for batch_size=1:  63%|██████▎   | 632/1000 [00:08<00:05, 72.90it/s]Measuring inference for batch_size=1:  64%|██████▍   | 640/1000 [00:08<00:04, 72.89it/s]Measuring inference for batch_size=1:  65%|██████▍   | 648/1000 [00:08<00:04, 72.86it/s]Measuring inference for batch_size=1:  66%|██████▌   | 656/1000 [00:09<00:04, 72.88it/s]Measuring inference for batch_size=1:  66%|██████▋   | 664/1000 [00:09<00:04, 72.88it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:09<00:04, 72.87it/s]Measuring inference for batch_size=1:  68%|██████▊   | 680/1000 [00:09<00:04, 72.82it/s]Measuring inference for batch_size=1:  69%|██████▉   | 688/1000 [00:09<00:04, 72.78it/s]Measuring inference for batch_size=1:  70%|██████▉   | 696/1000 [00:09<00:04, 72.77it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [00:09<00:04, 72.75it/s]Measuring inference for batch_size=1:  71%|███████   | 712/1000 [00:09<00:03, 72.77it/s]Measuring inference for batch_size=1:  72%|███████▏  | 720/1000 [00:09<00:03, 72.76it/s]Measuring inference for batch_size=1:  73%|███████▎  | 728/1000 [00:10<00:03, 72.76it/s]Measuring inference for batch_size=1:  74%|███████▎  | 736/1000 [00:10<00:03, 72.76it/s]Measuring inference for batch_size=1:  74%|███████▍  | 744/1000 [00:10<00:03, 72.77it/s]Measuring inference for batch_size=1:  75%|███████▌  | 752/1000 [00:10<00:03, 72.78it/s]Measuring inference for batch_size=1:  76%|███████▌  | 760/1000 [00:10<00:03, 72.79it/s]Measuring inference for batch_size=1:  77%|███████▋  | 768/1000 [00:10<00:03, 72.76it/s]Measuring inference for batch_size=1:  78%|███████▊  | 776/1000 [00:10<00:03, 72.75it/s]Measuring inference for batch_size=1:  78%|███████▊  | 784/1000 [00:10<00:02, 72.76it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [00:10<00:02, 72.78it/s]Measuring inference for batch_size=1:  80%|████████  | 800/1000 [00:10<00:02, 72.77it/s]Measuring inference for batch_size=1:  81%|████████  | 808/1000 [00:11<00:02, 72.79it/s]Measuring inference for batch_size=1:  82%|████████▏ | 816/1000 [00:11<00:02, 72.80it/s]Measuring inference for batch_size=1:  82%|████████▏ | 824/1000 [00:11<00:02, 72.80it/s]Measuring inference for batch_size=1:  83%|████████▎ | 832/1000 [00:11<00:02, 72.79it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:11<00:02, 72.77it/s]Measuring inference for batch_size=1:  85%|████████▍ | 848/1000 [00:11<00:02, 72.76it/s]Measuring inference for batch_size=1:  86%|████████▌ | 856/1000 [00:11<00:01, 72.77it/s]Measuring inference for batch_size=1:  86%|████████▋ | 864/1000 [00:11<00:01, 72.79it/s]Measuring inference for batch_size=1:  87%|████████▋ | 872/1000 [00:11<00:01, 72.80it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [00:12<00:01, 72.75it/s]Measuring inference for batch_size=1:  89%|████████▉ | 888/1000 [00:12<00:01, 72.74it/s]Measuring inference for batch_size=1:  90%|████████▉ | 896/1000 [00:12<00:01, 72.74it/s]Measuring inference for batch_size=1:  90%|█████████ | 904/1000 [00:12<00:01, 72.73it/s]Measuring inference for batch_size=1:  91%|█████████ | 912/1000 [00:12<00:01, 72.75it/s]Measuring inference for batch_size=1:  92%|█████████▏| 920/1000 [00:12<00:01, 72.73it/s]Measuring inference for batch_size=1:  93%|█████████▎| 928/1000 [00:12<00:00, 72.74it/s]Measuring inference for batch_size=1:  94%|█████████▎| 936/1000 [00:12<00:00, 72.74it/s]Measuring inference for batch_size=1:  94%|█████████▍| 944/1000 [00:12<00:00, 72.71it/s]Measuring inference for batch_size=1:  95%|█████████▌| 952/1000 [00:13<00:00, 72.70it/s]Measuring inference for batch_size=1:  96%|█████████▌| 960/1000 [00:13<00:00, 72.69it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [00:13<00:00, 72.70it/s]Measuring inference for batch_size=1:  98%|█████████▊| 976/1000 [00:13<00:00, 72.69it/s]Measuring inference for batch_size=1:  98%|█████████▊| 984/1000 [00:13<00:00, 72.74it/s]Measuring inference for batch_size=1:  99%|█████████▉| 992/1000 [00:13<00:00, 72.74it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 72.75it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:13<00:00, 72.78it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=256:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=256:   8%|▊         | 8/100 [00:00<00:01, 72.39it/s]Warming up with batch_size=256:  16%|█▌        | 16/100 [00:00<00:01, 72.53it/s]Warming up with batch_size=256:  24%|██▍       | 24/100 [00:00<00:01, 72.59it/s]Warming up with batch_size=256:  32%|███▏      | 32/100 [00:00<00:00, 72.57it/s]Warming up with batch_size=256:  40%|████      | 40/100 [00:00<00:00, 72.56it/s]Warming up with batch_size=256:  48%|████▊     | 48/100 [00:00<00:00, 72.55it/s]Warming up with batch_size=256:  56%|█████▌    | 56/100 [00:00<00:00, 72.57it/s]Warming up with batch_size=256:  64%|██████▍   | 64/100 [00:00<00:00, 72.56it/s]Warming up with batch_size=256:  72%|███████▏  | 72/100 [00:00<00:00, 72.60it/s]Warming up with batch_size=256:  80%|████████  | 80/100 [00:01<00:00, 72.61it/s]Warming up with batch_size=256:  88%|████████▊ | 88/100 [00:01<00:00, 72.58it/s]Warming up with batch_size=256:  96%|█████████▌| 96/100 [00:01<00:00, 72.50it/s]Warming up with batch_size=256: 100%|██████████| 100/100 [00:01<00:00, 72.54it/s]
STAGE:2024-02-25 23:05:24 6774:6774 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 23:05:24 6774:6774 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 23:05:24 6774:6774 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=256:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=256:   1%|          | 8/1000 [00:00<00:13, 71.39it/s]Measuring inference for batch_size=256:   2%|▏         | 16/1000 [00:00<00:13, 71.71it/s]Measuring inference for batch_size=256:   2%|▏         | 24/1000 [00:00<00:13, 71.81it/s]Measuring inference for batch_size=256:   3%|▎         | 32/1000 [00:00<00:13, 71.88it/s]Measuring inference for batch_size=256:   4%|▍         | 40/1000 [00:00<00:13, 71.89it/s]Measuring inference for batch_size=256:   5%|▍         | 48/1000 [00:00<00:13, 71.90it/s]Measuring inference for batch_size=256:   6%|▌         | 56/1000 [00:00<00:13, 71.89it/s]Measuring inference for batch_size=256:   6%|▋         | 64/1000 [00:00<00:13, 71.90it/s]Measuring inference for batch_size=256:   7%|▋         | 72/1000 [00:01<00:12, 71.90it/s]Measuring inference for batch_size=256:   8%|▊         | 80/1000 [00:01<00:12, 71.89it/s]Measuring inference for batch_size=256:   9%|▉         | 88/1000 [00:01<00:12, 71.91it/s]Measuring inference for batch_size=256:  10%|▉         | 96/1000 [00:01<00:12, 71.91it/s]Measuring inference for batch_size=256:  10%|█         | 104/1000 [00:01<00:12, 71.90it/s]Measuring inference for batch_size=256:  11%|█         | 112/1000 [00:01<00:12, 71.91it/s]Measuring inference for batch_size=256:  12%|█▏        | 120/1000 [00:01<00:12, 71.90it/s]Measuring inference for batch_size=256:  13%|█▎        | 128/1000 [00:01<00:12, 71.88it/s]Measuring inference for batch_size=256:  14%|█▎        | 136/1000 [00:01<00:12, 71.92it/s]Measuring inference for batch_size=256:  14%|█▍        | 144/1000 [00:02<00:11, 71.90it/s]Measuring inference for batch_size=256:  15%|█▌        | 152/1000 [00:02<00:11, 71.89it/s]Measuring inference for batch_size=256:  16%|█▌        | 160/1000 [00:02<00:11, 71.90it/s]Measuring inference for batch_size=256:  17%|█▋        | 168/1000 [00:02<00:11, 71.88it/s]Measuring inference for batch_size=256:  18%|█▊        | 176/1000 [00:02<00:11, 71.88it/s]Measuring inference for batch_size=256:  18%|█▊        | 184/1000 [00:02<00:11, 71.90it/s]Measuring inference for batch_size=256:  19%|█▉        | 192/1000 [00:02<00:11, 71.90it/s]Measuring inference for batch_size=256:  20%|██        | 200/1000 [00:02<00:11, 71.91it/s]Measuring inference for batch_size=256:  21%|██        | 208/1000 [00:02<00:11, 71.90it/s]Measuring inference for batch_size=256:  22%|██▏       | 216/1000 [00:03<00:10, 71.90it/s]Measuring inference for batch_size=256:  22%|██▏       | 224/1000 [00:03<00:10, 71.87it/s]Measuring inference for batch_size=256:  23%|██▎       | 232/1000 [00:03<00:10, 71.83it/s]Measuring inference for batch_size=256:  24%|██▍       | 240/1000 [00:03<00:10, 71.82it/s]Measuring inference for batch_size=256:  25%|██▍       | 248/1000 [00:03<00:10, 71.80it/s]Measuring inference for batch_size=256:  26%|██▌       | 256/1000 [00:03<00:10, 71.78it/s]Measuring inference for batch_size=256:  26%|██▋       | 264/1000 [00:03<00:10, 71.73it/s]Measuring inference for batch_size=256:  27%|██▋       | 272/1000 [00:03<00:10, 71.72it/s]Measuring inference for batch_size=256:  28%|██▊       | 280/1000 [00:03<00:10, 71.71it/s]Measuring inference for batch_size=256:  29%|██▉       | 288/1000 [00:04<00:09, 71.71it/s]Measuring inference for batch_size=256:  30%|██▉       | 296/1000 [00:04<00:09, 71.70it/s]Measuring inference for batch_size=256:  30%|███       | 304/1000 [00:04<00:09, 71.70it/s]Measuring inference for batch_size=256:  31%|███       | 312/1000 [00:04<00:09, 71.70it/s]Measuring inference for batch_size=256:  32%|███▏      | 320/1000 [00:04<00:09, 71.71it/s]Measuring inference for batch_size=256:  33%|███▎      | 328/1000 [00:04<00:09, 71.72it/s]Measuring inference for batch_size=256:  34%|███▎      | 336/1000 [00:04<00:09, 71.74it/s]Measuring inference for batch_size=256:  34%|███▍      | 344/1000 [00:04<00:09, 71.75it/s]Measuring inference for batch_size=256:  35%|███▌      | 352/1000 [00:04<00:09, 71.76it/s]Measuring inference for batch_size=256:  36%|███▌      | 360/1000 [00:05<00:08, 71.77it/s]Measuring inference for batch_size=256:  37%|███▋      | 368/1000 [00:05<00:08, 71.80it/s]Measuring inference for batch_size=256:  38%|███▊      | 376/1000 [00:05<00:08, 71.80it/s]Measuring inference for batch_size=256:  38%|███▊      | 384/1000 [00:05<00:08, 71.81it/s]Measuring inference for batch_size=256:  39%|███▉      | 392/1000 [00:05<00:08, 71.81it/s]Measuring inference for batch_size=256:  40%|████      | 400/1000 [00:05<00:08, 71.83it/s]Measuring inference for batch_size=256:  41%|████      | 408/1000 [00:05<00:08, 71.84it/s]Measuring inference for batch_size=256:  42%|████▏     | 416/1000 [00:05<00:08, 71.85it/s]Measuring inference for batch_size=256:  42%|████▏     | 424/1000 [00:05<00:08, 71.87it/s]Measuring inference for batch_size=256:  43%|████▎     | 432/1000 [00:06<00:07, 71.87it/s]Measuring inference for batch_size=256:  44%|████▍     | 440/1000 [00:06<00:07, 71.85it/s]Measuring inference for batch_size=256:  45%|████▍     | 448/1000 [00:06<00:07, 71.84it/s]Measuring inference for batch_size=256:  46%|████▌     | 456/1000 [00:06<00:07, 71.82it/s]Measuring inference for batch_size=256:  46%|████▋     | 464/1000 [00:06<00:07, 71.84it/s]Measuring inference for batch_size=256:  47%|████▋     | 472/1000 [00:06<00:07, 71.84it/s]Measuring inference for batch_size=256:  48%|████▊     | 480/1000 [00:06<00:07, 71.84it/s]Measuring inference for batch_size=256:  49%|████▉     | 488/1000 [00:06<00:07, 71.85it/s]Measuring inference for batch_size=256:  50%|████▉     | 496/1000 [00:06<00:07, 71.85it/s]Measuring inference for batch_size=256:  50%|█████     | 504/1000 [00:07<00:06, 71.85it/s]Measuring inference for batch_size=256:  51%|█████     | 512/1000 [00:07<00:06, 71.83it/s]Measuring inference for batch_size=256:  52%|█████▏    | 520/1000 [00:07<00:06, 71.83it/s]Measuring inference for batch_size=256:  53%|█████▎    | 528/1000 [00:07<00:06, 71.85it/s]Measuring inference for batch_size=256:  54%|█████▎    | 536/1000 [00:07<00:06, 71.84it/s]Measuring inference for batch_size=256:  54%|█████▍    | 544/1000 [00:07<00:06, 71.83it/s]Measuring inference for batch_size=256:  55%|█████▌    | 552/1000 [00:07<00:06, 71.82it/s]Measuring inference for batch_size=256:  56%|█████▌    | 560/1000 [00:07<00:06, 71.82it/s]Measuring inference for batch_size=256:  57%|█████▋    | 568/1000 [00:07<00:06, 71.83it/s]Measuring inference for batch_size=256:  58%|█████▊    | 576/1000 [00:08<00:05, 71.85it/s]Measuring inference for batch_size=256:  58%|█████▊    | 584/1000 [00:08<00:05, 71.87it/s]Measuring inference for batch_size=256:  59%|█████▉    | 592/1000 [00:08<00:05, 71.87it/s]Measuring inference for batch_size=256:  60%|██████    | 600/1000 [00:08<00:05, 71.90it/s]Measuring inference for batch_size=256:  61%|██████    | 608/1000 [00:08<00:05, 71.89it/s]Measuring inference for batch_size=256:  62%|██████▏   | 616/1000 [00:08<00:05, 71.89it/s]Measuring inference for batch_size=256:  62%|██████▏   | 624/1000 [00:08<00:05, 71.87it/s]Measuring inference for batch_size=256:  63%|██████▎   | 632/1000 [00:08<00:05, 71.85it/s]Measuring inference for batch_size=256:  64%|██████▍   | 640/1000 [00:08<00:05, 71.86it/s]Measuring inference for batch_size=256:  65%|██████▍   | 648/1000 [00:09<00:04, 71.86it/s]Measuring inference for batch_size=256:  66%|██████▌   | 656/1000 [00:09<00:04, 71.86it/s]Measuring inference for batch_size=256:  66%|██████▋   | 664/1000 [00:09<00:04, 71.87it/s]Measuring inference for batch_size=256:  67%|██████▋   | 672/1000 [00:09<00:04, 71.88it/s]Measuring inference for batch_size=256:  68%|██████▊   | 680/1000 [00:09<00:04, 71.88it/s]Measuring inference for batch_size=256:  69%|██████▉   | 688/1000 [00:09<00:04, 71.87it/s]Measuring inference for batch_size=256:  70%|██████▉   | 696/1000 [00:09<00:04, 71.87it/s]Measuring inference for batch_size=256:  70%|███████   | 704/1000 [00:09<00:04, 71.88it/s]Measuring inference for batch_size=256:  71%|███████   | 712/1000 [00:09<00:04, 71.91it/s]Measuring inference for batch_size=256:  72%|███████▏  | 720/1000 [00:10<00:03, 71.92it/s]Measuring inference for batch_size=256:  73%|███████▎  | 728/1000 [00:10<00:03, 71.89it/s]Measuring inference for batch_size=256:  74%|███████▎  | 736/1000 [00:10<00:03, 71.89it/s]Measuring inference for batch_size=256:  74%|███████▍  | 744/1000 [00:10<00:03, 71.90it/s]Measuring inference for batch_size=256:  75%|███████▌  | 752/1000 [00:10<00:03, 71.93it/s]Measuring inference for batch_size=256:  76%|███████▌  | 760/1000 [00:10<00:03, 71.94it/s]Measuring inference for batch_size=256:  77%|███████▋  | 768/1000 [00:10<00:03, 71.94it/s]Measuring inference for batch_size=256:  78%|███████▊  | 776/1000 [00:10<00:03, 71.96it/s]Measuring inference for batch_size=256:  78%|███████▊  | 784/1000 [00:10<00:03, 71.99it/s]Measuring inference for batch_size=256:  79%|███████▉  | 792/1000 [00:11<00:02, 71.98it/s]Measuring inference for batch_size=256:  80%|████████  | 800/1000 [00:11<00:02, 71.97it/s]Measuring inference for batch_size=256:  81%|████████  | 808/1000 [00:11<00:02, 71.95it/s]Measuring inference for batch_size=256:  82%|████████▏ | 816/1000 [00:11<00:02, 71.95it/s]Measuring inference for batch_size=256:  82%|████████▏ | 824/1000 [00:11<00:02, 71.95it/s]Measuring inference for batch_size=256:  83%|████████▎ | 832/1000 [00:11<00:02, 71.93it/s]Measuring inference for batch_size=256:  84%|████████▍ | 840/1000 [00:11<00:02, 71.91it/s]Measuring inference for batch_size=256:  85%|████████▍ | 848/1000 [00:11<00:02, 71.91it/s]Measuring inference for batch_size=256:  86%|████████▌ | 856/1000 [00:11<00:02, 71.92it/s]Measuring inference for batch_size=256:  86%|████████▋ | 864/1000 [00:12<00:01, 71.91it/s]Measuring inference for batch_size=256:  87%|████████▋ | 872/1000 [00:12<00:01, 71.91it/s]Measuring inference for batch_size=256:  88%|████████▊ | 880/1000 [00:12<00:01, 71.91it/s]Measuring inference for batch_size=256:  89%|████████▉ | 888/1000 [00:12<00:01, 71.91it/s]Measuring inference for batch_size=256:  90%|████████▉ | 896/1000 [00:12<00:01, 71.91it/s]Measuring inference for batch_size=256:  90%|█████████ | 904/1000 [00:12<00:01, 71.90it/s]Measuring inference for batch_size=256:  91%|█████████ | 912/1000 [00:12<00:01, 71.90it/s]Measuring inference for batch_size=256:  92%|█████████▏| 920/1000 [00:12<00:01, 71.89it/s]Measuring inference for batch_size=256:  93%|█████████▎| 928/1000 [00:12<00:01, 71.88it/s]Measuring inference for batch_size=256:  94%|█████████▎| 936/1000 [00:13<00:00, 71.87it/s]Measuring inference for batch_size=256:  94%|█████████▍| 944/1000 [00:13<00:00, 71.88it/s]Measuring inference for batch_size=256:  95%|█████████▌| 952/1000 [00:13<00:00, 71.90it/s]Measuring inference for batch_size=256:  96%|█████████▌| 960/1000 [00:13<00:00, 71.92it/s]Measuring inference for batch_size=256:  97%|█████████▋| 968/1000 [00:13<00:00, 71.91it/s]Measuring inference for batch_size=256:  98%|█████████▊| 976/1000 [00:13<00:00, 71.93it/s]Measuring inference for batch_size=256:  98%|█████████▊| 984/1000 [00:13<00:00, 71.93it/s]Measuring inference for batch_size=256:  99%|█████████▉| 992/1000 [00:13<00:00, 71.92it/s]Measuring inference for batch_size=256: 100%|██████████| 1000/1000 [00:13<00:00, 71.90it/s]Measuring inference for batch_size=256: 100%|██████████| 1000/1000 [00:13<00:00, 71.86it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 11580687848
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 29.98 GB
    total: 31.28 GB
    used: 925.20 MB
  system:
    node: fp
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 374.76 MB
    max_inference_bytes: 392962560
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
  batch_size_256:
    max_inference: 378.48 MB
    max_inference_bytes: 396866048
    post_inference: 238.40 MB
    post_inference_bytes: 249983488
    pre_inference: 238.40 MB
    pre_inference_bytes: 249983488
params: 60192808
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 93.793 us +/- 5.033 us [91.553 us, 218.630 us]
        batches_per_second: 10.68 K +/- 384.94 [4.57 K, 10.92 K]
      metrics:
        batches_per_second_max: 10922.666666666666
        batches_per_second_mean: 10680.937005648293
        batches_per_second_min: 4573.941112322792
        batches_per_second_std: 384.93641531936856
        seconds_per_batch_max: 0.0002186298370361328
        seconds_per_batch_mean: 9.379339218139649e-05
        seconds_per_batch_min: 9.1552734375e-05
        seconds_per_batch_std: 5.032685818799387e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.648 us +/- 0.684 us [22.888 us, 30.994 us]
        batches_per_second: 42.32 K +/- 1.04 K [32.26 K, 43.69 K]
      metrics:
        batches_per_second_max: 43690.666666666664
        batches_per_second_mean: 42317.51001687944
        batches_per_second_min: 32263.876923076925
        batches_per_second_std: 1035.4916821236638
        seconds_per_batch_max: 3.0994415283203125e-05
        seconds_per_batch_mean: 2.3647546768188477e-05
        seconds_per_batch_min: 2.288818359375e-05
        seconds_per_batch_std: 6.835080612895047e-07
    on_device_inference:
      human_readable:
        batch_latency: -13605624.283 us +/- 33.034 ms [-14240384.102 us, -13525823.593
          us]
        batches_per_second: -0.07 +/- 0.00 [-0.07, -0.07]
      metrics:
        batches_per_second_max: -0.07022282494956344
        batches_per_second_mean: -0.07349944222163357
        batches_per_second_min: -0.07393265135493886
        batches_per_second_std: 0.00017549455167324002
        seconds_per_batch_max: -13.525823593139648
        seconds_per_batch_mean: -13.605624282836914
        seconds_per_batch_min: -14.240384101867676
        seconds_per_batch_std: 0.033033661647048566
    total:
      human_readable:
        batch_latency: 13.731 ms +/- 36.026 us [13.650 ms, 14.507 ms]
        batches_per_second: 72.83 +/- 0.19 [68.93, 73.26]
      metrics:
        batches_per_second_max: 73.26039265003843
        batches_per_second_mean: 72.82781624355965
        batches_per_second_min: 68.93084407047068
        batches_per_second_std: 0.18634691728213876
        seconds_per_batch_max: 0.014507293701171875
        seconds_per_batch_mean: 0.013731109380722046
        seconds_per_batch_min: 0.013649940490722656
        seconds_per_batch_std: 3.602565572286094e-05
  batch_size_256:
    cpu_to_gpu:
      human_readable:
        batch_latency: 144.204 us +/- 5.538 us [141.859 us, 284.672 us]
        batches_per_second: 6.94 K +/- 186.72 [3.51 K, 7.05 K]
      metrics:
        batches_per_second_max: 7049.250420168068
        batches_per_second_mean: 6941.449275323551
        batches_per_second_min: 3512.817420435511
        batches_per_second_std: 186.7207179531807
        seconds_per_batch_max: 0.0002846717834472656
        seconds_per_batch_mean: 0.00014420390129089356
        seconds_per_batch_min: 0.0001418590545654297
        seconds_per_batch_std: 5.53751350588474e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.711 us +/- 0.692 us [22.888 us, 32.425 us]
        batches_per_second: 42.20 K +/- 1.03 K [30.84 K, 43.69 K]
      metrics:
        batches_per_second_max: 43690.666666666664
        batches_per_second_mean: 42204.54941692318
        batches_per_second_min: 30840.470588235294
        batches_per_second_std: 1032.9206380026594
        seconds_per_batch_max: 3.24249267578125e-05
        seconds_per_batch_mean: 2.371096611022949e-05
        seconds_per_batch_min: 2.288818359375e-05
        seconds_per_batch_std: 6.918083496150073e-07
    on_device_inference:
      human_readable:
        batch_latency: -13733936.923 us +/- 31.801 ms [-14386624.336 us, -13667327.881
          us]
        batches_per_second: -0.07 +/- 0.00 [-0.07, -0.07]
      metrics:
        batches_per_second_max: -0.06950900896750375
        batches_per_second_mean: -0.0728127166465134
        batches_per_second_min: -0.07316719176690462
        batches_per_second_std: 0.00016536985282167475
        seconds_per_batch_max: -13.667327880859375
        seconds_per_batch_mean: -13.733936923027038
        seconds_per_batch_min: -14.386624336242676
        seconds_per_batch_std: 0.031801212373958554
    total:
      human_readable:
        batch_latency: 13.907 ms +/- 35.206 us [13.839 ms, 14.706 ms]
        batches_per_second: 71.91 +/- 0.18 [68.00, 72.26]
      metrics:
        batches_per_second_max: 72.2582779175137
        batches_per_second_mean: 71.90532820050011
        batches_per_second_min: 67.99773033088533
        batches_per_second_std: 0.17695954301620687
        seconds_per_batch_max: 0.01470637321472168
        seconds_per_batch_mean: 0.013907261848449707
        seconds_per_batch_min: 0.013839244842529297
        seconds_per_batch_std: 3.5205639600306514e-05


