#####
fp-fp-py-id - Run 1
2024-02-25 22:58:05
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.99it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.99it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 277.51it/s]Warming up with batch_size=1:  57%|█████▋    | 57/100 [00:00<00:00, 279.54it/s]Warming up with batch_size=1:  86%|████████▌ | 86/100 [00:00<00:00, 280.41it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 280.10it/s]
STAGE:2024-02-25 22:57:52 6120:6120 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:57:52 6120:6120 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:57:52 6120:6120 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 22/1000 [00:00<00:04, 210.11it/s]Measuring inference for batch_size=1:   4%|▍         | 44/1000 [00:00<00:04, 210.57it/s]Measuring inference for batch_size=1:   7%|▋         | 66/1000 [00:00<00:04, 210.65it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:00<00:04, 210.67it/s]Measuring inference for batch_size=1:  11%|█         | 110/1000 [00:00<00:04, 210.81it/s]Measuring inference for batch_size=1:  13%|█▎        | 132/1000 [00:00<00:04, 210.86it/s]Measuring inference for batch_size=1:  15%|█▌        | 154/1000 [00:00<00:04, 210.87it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:00<00:03, 210.92it/s]Measuring inference for batch_size=1:  20%|█▉        | 198/1000 [00:00<00:03, 210.94it/s]Measuring inference for batch_size=1:  22%|██▏       | 220/1000 [00:01<00:03, 210.90it/s]Measuring inference for batch_size=1:  24%|██▍       | 242/1000 [00:01<00:03, 210.85it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:01<00:03, 210.79it/s]Measuring inference for batch_size=1:  29%|██▊       | 286/1000 [00:01<00:03, 210.77it/s]Measuring inference for batch_size=1:  31%|███       | 308/1000 [00:01<00:03, 210.80it/s]Measuring inference for batch_size=1:  33%|███▎      | 330/1000 [00:01<00:03, 210.79it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:01<00:03, 210.70it/s]Measuring inference for batch_size=1:  37%|███▋      | 374/1000 [00:01<00:02, 210.59it/s]Measuring inference for batch_size=1:  40%|███▉      | 396/1000 [00:01<00:02, 210.55it/s]Measuring inference for batch_size=1:  42%|████▏     | 418/1000 [00:01<00:02, 210.51it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:02<00:02, 210.51it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:02<00:02, 210.44it/s]Measuring inference for batch_size=1:  48%|████▊     | 484/1000 [00:02<00:02, 210.38it/s]Measuring inference for batch_size=1:  51%|█████     | 506/1000 [00:02<00:02, 207.90it/s]Measuring inference for batch_size=1:  53%|█████▎    | 527/1000 [00:02<00:02, 206.04it/s]Measuring inference for batch_size=1:  55%|█████▍    | 548/1000 [00:02<00:02, 204.75it/s]Measuring inference for batch_size=1:  57%|█████▋    | 569/1000 [00:02<00:02, 203.84it/s]Measuring inference for batch_size=1:  59%|█████▉    | 590/1000 [00:02<00:02, 203.13it/s]Measuring inference for batch_size=1:  61%|██████    | 611/1000 [00:02<00:01, 202.62it/s]Measuring inference for batch_size=1:  63%|██████▎   | 632/1000 [00:03<00:01, 202.31it/s]Measuring inference for batch_size=1:  65%|██████▌   | 653/1000 [00:03<00:01, 202.07it/s]Measuring inference for batch_size=1:  67%|██████▋   | 674/1000 [00:03<00:01, 201.86it/s]Measuring inference for batch_size=1:  70%|██████▉   | 695/1000 [00:03<00:01, 201.67it/s]Measuring inference for batch_size=1:  72%|███████▏  | 716/1000 [00:03<00:01, 201.60it/s]Measuring inference for batch_size=1:  74%|███████▎  | 737/1000 [00:03<00:01, 201.49it/s]Measuring inference for batch_size=1:  76%|███████▌  | 758/1000 [00:03<00:01, 201.37it/s]Measuring inference for batch_size=1:  78%|███████▊  | 779/1000 [00:03<00:01, 201.35it/s]Measuring inference for batch_size=1:  80%|████████  | 800/1000 [00:03<00:00, 201.40it/s]Measuring inference for batch_size=1:  82%|████████▏ | 821/1000 [00:03<00:00, 201.42it/s]Measuring inference for batch_size=1:  84%|████████▍ | 842/1000 [00:04<00:00, 201.39it/s]Measuring inference for batch_size=1:  86%|████████▋ | 863/1000 [00:04<00:00, 201.39it/s]Measuring inference for batch_size=1:  88%|████████▊ | 884/1000 [00:04<00:00, 201.44it/s]Measuring inference for batch_size=1:  90%|█████████ | 905/1000 [00:04<00:00, 201.42it/s]Measuring inference for batch_size=1:  93%|█████████▎| 926/1000 [00:04<00:00, 201.51it/s]Measuring inference for batch_size=1:  95%|█████████▍| 947/1000 [00:04<00:00, 201.47it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [00:04<00:00, 201.52it/s]Measuring inference for batch_size=1:  99%|█████████▉| 989/1000 [00:04<00:00, 201.57it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 205.83it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=256:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=256:  21%|██        | 21/100 [00:00<00:00, 205.10it/s]Warming up with batch_size=256:  42%|████▏     | 42/100 [00:00<00:00, 205.43it/s]Warming up with batch_size=256:  63%|██████▎   | 63/100 [00:00<00:00, 205.45it/s]Warming up with batch_size=256:  84%|████████▍ | 84/100 [00:00<00:00, 205.58it/s]Warming up with batch_size=256: 100%|██████████| 100/100 [00:00<00:00, 205.52it/s]
STAGE:2024-02-25 22:57:58 6120:6120 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:57:58 6120:6120 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:57:58 6120:6120 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=256:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=256:   2%|▏         | 21/1000 [00:00<00:04, 202.05it/s]Measuring inference for batch_size=256:   4%|▍         | 42/1000 [00:00<00:04, 202.49it/s]Measuring inference for batch_size=256:   6%|▋         | 63/1000 [00:00<00:04, 202.63it/s]Measuring inference for batch_size=256:   8%|▊         | 84/1000 [00:00<00:04, 202.72it/s]Measuring inference for batch_size=256:  10%|█         | 105/1000 [00:00<00:04, 202.68it/s]Measuring inference for batch_size=256:  13%|█▎        | 126/1000 [00:00<00:04, 202.74it/s]Measuring inference for batch_size=256:  15%|█▍        | 147/1000 [00:00<00:04, 202.74it/s]Measuring inference for batch_size=256:  17%|█▋        | 168/1000 [00:00<00:04, 202.61it/s]Measuring inference for batch_size=256:  19%|█▉        | 189/1000 [00:00<00:04, 202.62it/s]Measuring inference for batch_size=256:  21%|██        | 210/1000 [00:01<00:03, 202.62it/s]Measuring inference for batch_size=256:  23%|██▎       | 231/1000 [00:01<00:03, 202.61it/s]Measuring inference for batch_size=256:  25%|██▌       | 252/1000 [00:01<00:03, 202.61it/s]Measuring inference for batch_size=256:  27%|██▋       | 273/1000 [00:01<00:03, 202.65it/s]Measuring inference for batch_size=256:  29%|██▉       | 294/1000 [00:01<00:03, 202.73it/s]Measuring inference for batch_size=256:  32%|███▏      | 315/1000 [00:01<00:03, 202.70it/s]Measuring inference for batch_size=256:  34%|███▎      | 336/1000 [00:01<00:03, 202.73it/s]Measuring inference for batch_size=256:  36%|███▌      | 357/1000 [00:01<00:03, 202.77it/s]Measuring inference for batch_size=256:  38%|███▊      | 378/1000 [00:01<00:03, 202.81it/s]Measuring inference for batch_size=256:  40%|███▉      | 399/1000 [00:01<00:02, 202.81it/s]Measuring inference for batch_size=256:  42%|████▏     | 420/1000 [00:02<00:02, 202.79it/s]Measuring inference for batch_size=256:  44%|████▍     | 441/1000 [00:02<00:02, 202.79it/s]Measuring inference for batch_size=256:  46%|████▌     | 462/1000 [00:02<00:02, 202.66it/s]Measuring inference for batch_size=256:  48%|████▊     | 483/1000 [00:02<00:02, 202.66it/s]Measuring inference for batch_size=256:  50%|█████     | 504/1000 [00:02<00:02, 202.62it/s]Measuring inference for batch_size=256:  52%|█████▎    | 525/1000 [00:02<00:02, 202.65it/s]Measuring inference for batch_size=256:  55%|█████▍    | 546/1000 [00:02<00:02, 202.62it/s]Measuring inference for batch_size=256:  57%|█████▋    | 567/1000 [00:02<00:02, 202.60it/s]Measuring inference for batch_size=256:  59%|█████▉    | 588/1000 [00:02<00:02, 202.57it/s]Measuring inference for batch_size=256:  61%|██████    | 609/1000 [00:03<00:01, 202.55it/s]Measuring inference for batch_size=256:  63%|██████▎   | 630/1000 [00:03<00:01, 202.55it/s]Measuring inference for batch_size=256:  65%|██████▌   | 651/1000 [00:03<00:01, 202.51it/s]Measuring inference for batch_size=256:  67%|██████▋   | 672/1000 [00:03<00:01, 202.53it/s]Measuring inference for batch_size=256:  69%|██████▉   | 693/1000 [00:03<00:01, 202.56it/s]Measuring inference for batch_size=256:  71%|███████▏  | 714/1000 [00:03<00:01, 202.67it/s]Measuring inference for batch_size=256:  74%|███████▎  | 735/1000 [00:03<00:01, 202.68it/s]Measuring inference for batch_size=256:  76%|███████▌  | 756/1000 [00:03<00:01, 202.73it/s]Measuring inference for batch_size=256:  78%|███████▊  | 777/1000 [00:03<00:01, 202.71it/s]Measuring inference for batch_size=256:  80%|███████▉  | 798/1000 [00:03<00:00, 202.73it/s]Measuring inference for batch_size=256:  82%|████████▏ | 819/1000 [00:04<00:00, 202.73it/s]Measuring inference for batch_size=256:  84%|████████▍ | 840/1000 [00:04<00:00, 202.66it/s]Measuring inference for batch_size=256:  86%|████████▌ | 861/1000 [00:04<00:00, 202.61it/s]Measuring inference for batch_size=256:  88%|████████▊ | 882/1000 [00:04<00:00, 202.60it/s]Measuring inference for batch_size=256:  90%|█████████ | 903/1000 [00:04<00:00, 202.61it/s]Measuring inference for batch_size=256:  92%|█████████▏| 924/1000 [00:04<00:00, 202.62it/s]Measuring inference for batch_size=256:  94%|█████████▍| 945/1000 [00:04<00:00, 202.62it/s]Measuring inference for batch_size=256:  97%|█████████▋| 966/1000 [00:04<00:00, 202.63it/s]Measuring inference for batch_size=256:  99%|█████████▊| 987/1000 [00:04<00:00, 200.85it/s]Measuring inference for batch_size=256: 100%|██████████| 1000/1000 [00:04<00:00, 202.50it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 29.98 GB
    total: 31.28 GB
    used: 918.37 MB
  system:
    node: fp
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_256:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 93.460 us +/- 2.886 us [91.076 us, 142.813 us]
        batches_per_second: 10.71 K +/- 286.50 [7.00 K, 10.98 K]
      metrics:
        batches_per_second_max: 10979.853403141362
        batches_per_second_mean: 10708.497550894474
        batches_per_second_min: 7002.176961602671
        batches_per_second_std: 286.4984036339594
        seconds_per_batch_max: 0.00014281272888183594
        seconds_per_batch_mean: 9.34600830078125e-05
        seconds_per_batch_min: 9.107589721679688e-05
        seconds_per_batch_std: 2.886105893100141e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.701 us +/- 0.640 us [22.650 us, 30.518 us]
        batches_per_second: 42.22 K +/- 1.02 K [32.77 K, 44.15 K]
      metrics:
        batches_per_second_max: 44150.56842105263
        batches_per_second_mean: 42219.26812678192
        batches_per_second_min: 32768.0
        batches_per_second_std: 1017.6417750584332
        seconds_per_batch_max: 3.0517578125e-05
        seconds_per_batch_mean: 2.3701190948486327e-05
        seconds_per_batch_min: 2.2649765014648438e-05
        seconds_per_batch_std: 6.396508771339331e-07
    on_device_inference:
      human_readable:
        batch_latency: -4729733.377 us +/- 107.362 ms [-4891007.900 us, -4590720.177
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.22, -0.20]
      metrics:
        batches_per_second_max: -0.2044568359726697
        batches_per_second_mean: -0.21153753122098484
        batches_per_second_min: -0.21783074583289969
        batches_per_second_std: 0.004807390038132289
        seconds_per_batch_max: -4.590720176696777
        seconds_per_batch_mean: -4.729733377456665
        seconds_per_batch_min: -4.891007900238037
        seconds_per_batch_std: 0.10736174609502198
    total:
      human_readable:
        batch_latency: 4.855 ms +/- 109.449 us [4.714 ms, 5.036 ms]
        batches_per_second: 206.09 +/- 4.65 [198.57, 212.15]
      metrics:
        batches_per_second_max: 212.1549822964087
        batches_per_second_mean: 206.09165492567328
        batches_per_second_min: 198.5657340339914
        batches_per_second_std: 4.651546433934036
        seconds_per_batch_max: 0.005036115646362305
        seconds_per_batch_mean: 0.004854680299758911
        seconds_per_batch_min: 0.004713535308837891
        seconds_per_batch_std: 0.00010944867841828042
  batch_size_256:
    cpu_to_gpu:
      human_readable:
        batch_latency: 143.216 us +/- 4.029 us [141.144 us, 244.856 us]
        batches_per_second: 6.99 K +/- 145.19 [4.08 K, 7.08 K]
      metrics:
        batches_per_second_max: 7084.972972972973
        batches_per_second_mean: 6986.432848752007
        batches_per_second_min: 4084.035053554041
        batches_per_second_std: 145.1941475468748
        seconds_per_batch_max: 0.0002448558807373047
        seconds_per_batch_mean: 0.0001432158946990967
        seconds_per_batch_min: 0.000141143798828125
        seconds_per_batch_std: 4.029030757255352e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.409 us +/- 0.546 us [22.650 us, 32.425 us]
        batches_per_second: 42.74 K +/- 868.32 [30.84 K, 44.15 K]
      metrics:
        batches_per_second_max: 44150.56842105263
        batches_per_second_mean: 42738.02102032386
        batches_per_second_min: 30840.470588235294
        batches_per_second_std: 868.3248234782894
        seconds_per_batch_max: 3.24249267578125e-05
        seconds_per_batch_mean: 2.3409366607666017e-05
        seconds_per_batch_min: 2.2649765014648438e-05
        seconds_per_batch_std: 5.45629864757485e-07
    on_device_inference:
      human_readable:
        batch_latency: -4762438.628 us +/- 28.832 ms [-5101151.943 us, -4736896.038
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.20]
      metrics:
        batches_per_second_max: -0.19603415289985662
        batches_per_second_mean: -0.20998388874585755
        batches_per_second_min: -0.2111087074671197
        batches_per_second_std: 0.0012273991428442396
        seconds_per_batch_max: -4.73689603805542
        seconds_per_batch_mean: -4.762438627719879
        seconds_per_batch_min: -5.101151943206787
        seconds_per_batch_std: 0.028831967512783926
    total:
      human_readable:
        batch_latency: 4.935 ms +/- 30.717 us [4.908 ms, 5.380 ms]
        batches_per_second: 202.66 +/- 1.21 [185.88, 203.76]
      metrics:
        batches_per_second_max: 203.75535584163225
        batches_per_second_mean: 202.66029978137138
        batches_per_second_min: 185.87653445601595
        batches_per_second_std: 1.2114100631985218
        seconds_per_batch_max: 0.005379915237426758
        seconds_per_batch_mean: 0.00493454909324646
        seconds_per_batch_min: 0.004907846450805664
        seconds_per_batch_std: 3.0716681486875465e-05


#####
fp-fp-py-id - Run 2
2024-02-25 22:58:23
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  28%|██▊       | 28/100 [00:00<00:00, 274.50it/s]Warming up with batch_size=1:  56%|█████▌    | 56/100 [00:00<00:00, 276.46it/s]Warming up with batch_size=1:  84%|████████▍ | 84/100 [00:00<00:00, 277.41it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 277.13it/s]
STAGE:2024-02-25 22:58:11 6166:6166 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:58:11 6166:6166 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:58:11 6166:6166 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 21/1000 [00:00<00:04, 209.17it/s]Measuring inference for batch_size=1:   4%|▍         | 42/1000 [00:00<00:04, 209.26it/s]Measuring inference for batch_size=1:   6%|▋         | 63/1000 [00:00<00:04, 209.46it/s]Measuring inference for batch_size=1:   8%|▊         | 84/1000 [00:00<00:04, 209.55it/s]Measuring inference for batch_size=1:  10%|█         | 105/1000 [00:00<00:04, 209.57it/s]Measuring inference for batch_size=1:  13%|█▎        | 126/1000 [00:00<00:04, 209.58it/s]Measuring inference for batch_size=1:  15%|█▍        | 147/1000 [00:00<00:04, 209.63it/s]Measuring inference for batch_size=1:  17%|█▋        | 168/1000 [00:00<00:03, 209.65it/s]Measuring inference for batch_size=1:  19%|█▉        | 189/1000 [00:00<00:03, 209.54it/s]Measuring inference for batch_size=1:  21%|██        | 210/1000 [00:01<00:03, 209.44it/s]Measuring inference for batch_size=1:  23%|██▎       | 231/1000 [00:01<00:03, 209.51it/s]Measuring inference for batch_size=1:  25%|██▌       | 252/1000 [00:01<00:03, 209.56it/s]Measuring inference for batch_size=1:  27%|██▋       | 273/1000 [00:01<00:03, 209.62it/s]Measuring inference for batch_size=1:  29%|██▉       | 294/1000 [00:01<00:03, 209.68it/s]Measuring inference for batch_size=1:  32%|███▏      | 315/1000 [00:01<00:03, 209.68it/s]Measuring inference for batch_size=1:  34%|███▎      | 336/1000 [00:01<00:03, 209.64it/s]Measuring inference for batch_size=1:  36%|███▌      | 357/1000 [00:01<00:03, 209.60it/s]Measuring inference for batch_size=1:  38%|███▊      | 378/1000 [00:01<00:02, 209.46it/s]Measuring inference for batch_size=1:  40%|███▉      | 399/1000 [00:01<00:02, 209.49it/s]Measuring inference for batch_size=1:  42%|████▏     | 420/1000 [00:02<00:02, 209.55it/s]Measuring inference for batch_size=1:  44%|████▍     | 441/1000 [00:02<00:02, 209.61it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:02<00:02, 209.63it/s]Measuring inference for batch_size=1:  48%|████▊     | 483/1000 [00:02<00:02, 209.56it/s]Measuring inference for batch_size=1:  50%|█████     | 504/1000 [00:02<00:02, 209.49it/s]Measuring inference for batch_size=1:  52%|█████▎    | 525/1000 [00:02<00:02, 209.37it/s]Measuring inference for batch_size=1:  55%|█████▍    | 546/1000 [00:02<00:02, 209.30it/s]Measuring inference for batch_size=1:  57%|█████▋    | 567/1000 [00:02<00:02, 209.24it/s]Measuring inference for batch_size=1:  59%|█████▉    | 588/1000 [00:02<00:01, 209.30it/s]Measuring inference for batch_size=1:  61%|██████    | 609/1000 [00:02<00:01, 209.25it/s]Measuring inference for batch_size=1:  63%|██████▎   | 630/1000 [00:03<00:01, 209.22it/s]Measuring inference for batch_size=1:  65%|██████▌   | 651/1000 [00:03<00:01, 209.20it/s]Measuring inference for batch_size=1:  67%|██████▋   | 672/1000 [00:03<00:01, 209.18it/s]Measuring inference for batch_size=1:  69%|██████▉   | 693/1000 [00:03<00:01, 209.11it/s]Measuring inference for batch_size=1:  71%|███████▏  | 714/1000 [00:03<00:01, 209.15it/s]Measuring inference for batch_size=1:  74%|███████▎  | 735/1000 [00:03<00:01, 209.17it/s]Measuring inference for batch_size=1:  76%|███████▌  | 756/1000 [00:03<00:01, 209.14it/s]Measuring inference for batch_size=1:  78%|███████▊  | 777/1000 [00:03<00:01, 209.15it/s]Measuring inference for batch_size=1:  80%|███████▉  | 798/1000 [00:03<00:00, 209.14it/s]Measuring inference for batch_size=1:  82%|████████▏ | 819/1000 [00:03<00:00, 209.16it/s]Measuring inference for batch_size=1:  84%|████████▍ | 840/1000 [00:04<00:00, 209.14it/s]Measuring inference for batch_size=1:  86%|████████▌ | 861/1000 [00:04<00:00, 209.15it/s]Measuring inference for batch_size=1:  88%|████████▊ | 882/1000 [00:04<00:00, 209.01it/s]Measuring inference for batch_size=1:  90%|█████████ | 903/1000 [00:04<00:00, 209.03it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [00:04<00:00, 208.94it/s]Measuring inference for batch_size=1:  94%|█████████▍| 945/1000 [00:04<00:00, 208.85it/s]Measuring inference for batch_size=1:  97%|█████████▋| 966/1000 [00:04<00:00, 208.86it/s]Measuring inference for batch_size=1:  99%|█████████▊| 987/1000 [00:04<00:00, 208.91it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 209.31it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=256:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=256:  21%|██        | 21/100 [00:00<00:00, 203.91it/s]Warming up with batch_size=256:  42%|████▏     | 42/100 [00:00<00:00, 204.06it/s]Warming up with batch_size=256:  63%|██████▎   | 63/100 [00:00<00:00, 204.20it/s]Warming up with batch_size=256:  84%|████████▍ | 84/100 [00:00<00:00, 204.27it/s]Warming up with batch_size=256: 100%|██████████| 100/100 [00:00<00:00, 204.21it/s]
STAGE:2024-02-25 22:58:16 6166:6166 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:58:16 6166:6166 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:58:16 6166:6166 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=256:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=256:   2%|▏         | 21/1000 [00:00<00:04, 200.59it/s]Measuring inference for batch_size=256:   4%|▍         | 42/1000 [00:00<00:04, 201.11it/s]Measuring inference for batch_size=256:   6%|▋         | 63/1000 [00:00<00:04, 201.36it/s]Measuring inference for batch_size=256:   8%|▊         | 84/1000 [00:00<00:04, 201.42it/s]Measuring inference for batch_size=256:  10%|█         | 105/1000 [00:00<00:04, 201.43it/s]Measuring inference for batch_size=256:  13%|█▎        | 126/1000 [00:00<00:04, 201.44it/s]Measuring inference for batch_size=256:  15%|█▍        | 147/1000 [00:00<00:04, 201.53it/s]Measuring inference for batch_size=256:  17%|█▋        | 168/1000 [00:00<00:04, 201.58it/s]Measuring inference for batch_size=256:  19%|█▉        | 189/1000 [00:00<00:04, 201.60it/s]Measuring inference for batch_size=256:  21%|██        | 210/1000 [00:01<00:03, 201.61it/s]Measuring inference for batch_size=256:  23%|██▎       | 231/1000 [00:01<00:03, 201.55it/s]Measuring inference for batch_size=256:  25%|██▌       | 252/1000 [00:01<00:03, 201.55it/s]Measuring inference for batch_size=256:  27%|██▋       | 273/1000 [00:01<00:03, 201.51it/s]Measuring inference for batch_size=256:  29%|██▉       | 294/1000 [00:01<00:03, 201.39it/s]Measuring inference for batch_size=256:  32%|███▏      | 315/1000 [00:01<00:03, 201.33it/s]Measuring inference for batch_size=256:  34%|███▎      | 336/1000 [00:01<00:03, 201.36it/s]Measuring inference for batch_size=256:  36%|███▌      | 357/1000 [00:01<00:03, 201.32it/s]Measuring inference for batch_size=256:  38%|███▊      | 378/1000 [00:01<00:03, 201.35it/s]Measuring inference for batch_size=256:  40%|███▉      | 399/1000 [00:01<00:02, 201.38it/s]Measuring inference for batch_size=256:  42%|████▏     | 420/1000 [00:02<00:02, 201.43it/s]Measuring inference for batch_size=256:  44%|████▍     | 441/1000 [00:02<00:02, 201.42it/s]Measuring inference for batch_size=256:  46%|████▌     | 462/1000 [00:02<00:02, 201.41it/s]Measuring inference for batch_size=256:  48%|████▊     | 483/1000 [00:02<00:02, 201.39it/s]Measuring inference for batch_size=256:  50%|█████     | 504/1000 [00:02<00:02, 201.43it/s]Measuring inference for batch_size=256:  52%|█████▎    | 525/1000 [00:02<00:02, 201.48it/s]Measuring inference for batch_size=256:  55%|█████▍    | 546/1000 [00:02<00:02, 201.43it/s]Measuring inference for batch_size=256:  57%|█████▋    | 567/1000 [00:02<00:02, 199.83it/s]Measuring inference for batch_size=256:  59%|█████▉    | 588/1000 [00:02<00:02, 200.40it/s]Measuring inference for batch_size=256:  61%|██████    | 609/1000 [00:03<00:01, 200.76it/s]Measuring inference for batch_size=256:  63%|██████▎   | 630/1000 [00:03<00:01, 201.04it/s]Measuring inference for batch_size=256:  65%|██████▌   | 651/1000 [00:03<00:01, 201.25it/s]Measuring inference for batch_size=256:  67%|██████▋   | 672/1000 [00:03<00:01, 201.35it/s]Measuring inference for batch_size=256:  69%|██████▉   | 693/1000 [00:03<00:01, 201.48it/s]Measuring inference for batch_size=256:  71%|███████▏  | 714/1000 [00:03<00:01, 201.43it/s]Measuring inference for batch_size=256:  74%|███████▎  | 735/1000 [00:03<00:01, 201.45it/s]Measuring inference for batch_size=256:  76%|███████▌  | 756/1000 [00:03<00:01, 201.47it/s]Measuring inference for batch_size=256:  78%|███████▊  | 777/1000 [00:03<00:01, 201.44it/s]Measuring inference for batch_size=256:  80%|███████▉  | 798/1000 [00:03<00:01, 201.34it/s]Measuring inference for batch_size=256:  82%|████████▏ | 819/1000 [00:04<00:00, 201.29it/s]Measuring inference for batch_size=256:  84%|████████▍ | 840/1000 [00:04<00:00, 201.27it/s]Measuring inference for batch_size=256:  86%|████████▌ | 861/1000 [00:04<00:00, 201.26it/s]Measuring inference for batch_size=256:  88%|████████▊ | 882/1000 [00:04<00:00, 201.27it/s]Measuring inference for batch_size=256:  90%|█████████ | 903/1000 [00:04<00:00, 201.26it/s]Measuring inference for batch_size=256:  92%|█████████▏| 924/1000 [00:04<00:00, 201.29it/s]Measuring inference for batch_size=256:  94%|█████████▍| 945/1000 [00:04<00:00, 201.34it/s]Measuring inference for batch_size=256:  97%|█████████▋| 966/1000 [00:04<00:00, 201.32it/s]Measuring inference for batch_size=256:  99%|█████████▊| 987/1000 [00:04<00:00, 201.33it/s]Measuring inference for batch_size=256: 100%|██████████| 1000/1000 [00:04<00:00, 201.31it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 29.98 GB
    total: 31.28 GB
    used: 925.67 MB
  system:
    node: fp
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_256:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 94.776 us +/- 2.653 us [93.460 us, 144.482 us]
        batches_per_second: 10.56 K +/- 249.31 [6.92 K, 10.70 K]
      metrics:
        batches_per_second_max: 10699.755102040815
        batches_per_second_mean: 10558.045130230981
        batches_per_second_min: 6921.293729372937
        batches_per_second_std: 249.30700000984854
        seconds_per_batch_max: 0.00014448165893554688
        seconds_per_batch_mean: 9.477639198303223e-05
        seconds_per_batch_min: 9.34600830078125e-05
        seconds_per_batch_std: 2.653134535462079e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.386 us +/- 0.586 us [22.650 us, 31.471 us]
        batches_per_second: 42.78 K +/- 893.63 [31.78 K, 44.15 K]
      metrics:
        batches_per_second_max: 44150.56842105263
        batches_per_second_mean: 42782.00061220533
        batches_per_second_min: 31775.030303030304
        batches_per_second_std: 893.6284744689267
        seconds_per_batch_max: 3.147125244140625e-05
        seconds_per_batch_mean: 2.3386478424072266e-05
        seconds_per_batch_min: 2.2649765014648438e-05
        seconds_per_batch_std: 5.85651351116199e-07
    on_device_inference:
      human_readable:
        batch_latency: -4648560.258 us +/- 14.186 ms [-4864768.028 us, -4614560.127
          us]
        batches_per_second: -0.22 +/- 0.00 [-0.22, -0.21]
      metrics:
        batches_per_second_max: -0.2055596472824671
        batches_per_second_mean: -0.21512235022225668
        batches_per_second_min: -0.21670537871919354
        batches_per_second_std: 0.0006492194002767031
        seconds_per_batch_max: -4.614560127258301
        seconds_per_batch_mean: -4.648560258388519
        seconds_per_batch_min: -4.864768028259277
        seconds_per_batch_std: 0.01418558504196955
    total:
      human_readable:
        batch_latency: 4.774 ms +/- 15.572 us [4.739 ms, 5.052 ms]
        batches_per_second: 209.47 +/- 0.67 [197.94, 211.03]
      metrics:
        batches_per_second_max: 211.0341635220126
        batches_per_second_mean: 209.46845771698648
        batches_per_second_min: 197.93789523360076
        batches_per_second_std: 0.670857351912931
        seconds_per_batch_max: 0.005052089691162109
        seconds_per_batch_mean: 0.004774038314819336
        seconds_per_batch_min: 0.004738569259643555
        seconds_per_batch_std: 1.5571606708233786e-05
  batch_size_256:
    cpu_to_gpu:
      human_readable:
        batch_latency: 146.730 us +/- 4.144 us [144.958 us, 245.094 us]
        batches_per_second: 6.82 K +/- 146.69 [4.08 K, 6.90 K]
      metrics:
        batches_per_second_max: 6898.526315789473
        batches_per_second_mean: 6819.274486404767
        batches_per_second_min: 4080.0622568093386
        batches_per_second_std: 146.69247440659666
        seconds_per_batch_max: 0.00024509429931640625
        seconds_per_batch_mean: 0.00014673018455505372
        seconds_per_batch_min: 0.00014495849609375
        seconds_per_batch_std: 4.144379069532337e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.554 us +/- 0.570 us [22.888 us, 29.802 us]
        batches_per_second: 42.48 K +/- 898.99 [33.55 K, 43.69 K]
      metrics:
        batches_per_second_max: 43690.666666666664
        batches_per_second_mean: 42477.55228743304
        batches_per_second_min: 33554.432
        batches_per_second_std: 898.9877914101368
        seconds_per_batch_max: 2.9802322387695312e-05
        seconds_per_batch_mean: 2.3553848266601563e-05
        seconds_per_batch_min: 2.288818359375e-05
        seconds_per_batch_std: 5.695527936689031e-07
    on_device_inference:
      human_readable:
        batch_latency: -4788019.489 us +/- 25.415 ms [-5093696.117 us, -4760255.814
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.20]
      metrics:
        batches_per_second_max: -0.1963210951245781
        batches_per_second_mean: -0.20886031165344648
        batches_per_second_min: -0.21007274380996455
        batches_per_second_std: 0.0010720472957169863
        seconds_per_batch_max: -4.760255813598633
        seconds_per_batch_mean: -4.788019488811493
        seconds_per_batch_min: -5.093696117401123
        seconds_per_batch_std: 0.02541512428849283
    total:
      human_readable:
        batch_latency: 4.964 ms +/- 27.345 us [4.936 ms, 5.373 ms]
        batches_per_second: 201.47 +/- 1.07 [186.10, 202.57]
      metrics:
        batches_per_second_max: 202.5744506157933
        batches_per_second_mean: 201.46626847814764
        batches_per_second_min: 186.09921022273494
        batches_per_second_std: 1.0672439561883034
        seconds_per_batch_max: 0.005373477935791016
        seconds_per_batch_mean: 0.004963754892349243
        seconds_per_batch_min: 0.0049364566802978516
        seconds_per_batch_std: 2.7345203746539524e-05


#####
fp-fp-py-id - Run 3
2024-02-25 22:58:39
#####
Benchmarking on 12 threads
Warming up with batch_size=1:   0%|          | 0/1 [00:00<?, ?it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  5.19it/s]Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  5.19it/s]
Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
Warning! No positional inputs found for a module, assuming batch size is 1.
ResNet(
  25.56 M, 100.000% Params, 4.12 GMac, 100.000% MACs, 
  (conv1): Conv2d(9.41 k, 0.037% Params, 118.01 MMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    215.81 k, 0.844% Params, 680.39 MMac, 16.507% MACs, 
    (0): Bottleneck(
      75.01 k, 0.293% Params, 236.43 MMac, 5.736% MACs, 
      (conv1): Conv2d(4.1 k, 0.016% Params, 12.85 MMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        16.9 k, 0.066% Params, 52.99 MMac, 1.285% MACs, 
        (0): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      70.4 k, 0.275% Params, 221.98 MMac, 5.385% MACs, 
      (conv1): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(36.86 k, 0.144% Params, 115.61 MMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(16.38 k, 0.064% Params, 51.38 MMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, 0.002% Params, 1.61 MMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 1.2 MMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.04 GMac, 25.147% MACs, 
    (0): Bottleneck(
      379.39 k, 1.484% Params, 376.02 MMac, 9.122% MACs, 
      (conv1): Conv2d(32.77 k, 0.128% Params, 102.76 MMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 802.82 KMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 903.17 KMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        132.1 k, 0.517% Params, 103.56 MMac, 2.512% MACs, 
        (0): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      280.06 k, 1.096% Params, 220.17 MMac, 5.341% MACs, 
      (conv1): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(147.46 k, 0.577% Params, 115.61 MMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, 0.001% Params, 200.7 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(65.54 k, 0.256% Params, 51.38 MMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 802.82 KMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 602.11 KMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.1 M, 27.775% Params, 1.47 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.51 M, 5.918% Params, 374.26 MMac, 9.080% MACs, 
      (conv1): Conv2d(131.07 k, 0.513% Params, 102.76 MMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 401.41 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 451.58 KMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        526.34 k, 2.059% Params, 103.16 MMac, 2.503% MACs, 
        (0): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.12 M, 4.371% Params, 219.27 MMac, 5.320% MACs, 
      (conv1): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(589.82 k, 2.308% Params, 115.61 MMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, 0.002% Params, 100.35 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(262.14 k, 1.026% Params, 51.38 MMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 401.41 KMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 301.06 KMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.96 M, 58.554% Params, 811.02 MMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 373.38 MMac, 9.059% MACs, 
      (conv1): Conv2d(524.29 k, 2.051% Params, 102.76 MMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 200.7 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 225.79 KMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.1 M, 8.222% Params, 102.96 MMac, 2.498% MACs, 
        (0): Conv2d(2.1 M, 8.206% Params, 102.76 MMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.46 M, 17.461% Params, 218.82 MMac, 5.309% MACs, 
      (conv1): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.36 M, 9.231% Params, 115.61 MMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 50.18 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.05 M, 4.103% Params, 51.38 MMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 200.7 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0, 0.000% Params, 150.53 KMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Warming up with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=1:  29%|██▉       | 29/100 [00:00<00:00, 281.02it/s]Warming up with batch_size=1:  58%|█████▊    | 58/100 [00:00<00:00, 281.16it/s]Warming up with batch_size=1:  87%|████████▋ | 87/100 [00:00<00:00, 282.27it/s]Warming up with batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 282.22it/s]
STAGE:2024-02-25 22:58:29 6212:6212 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:58:29 6212:6212 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:58:29 6212:6212 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=1:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=1:   2%|▏         | 22/1000 [00:00<00:04, 214.10it/s]Measuring inference for batch_size=1:   4%|▍         | 44/1000 [00:00<00:04, 214.41it/s]Measuring inference for batch_size=1:   7%|▋         | 66/1000 [00:00<00:04, 214.59it/s]Measuring inference for batch_size=1:   9%|▉         | 88/1000 [00:00<00:04, 214.64it/s]Measuring inference for batch_size=1:  11%|█         | 110/1000 [00:00<00:04, 214.79it/s]Measuring inference for batch_size=1:  13%|█▎        | 132/1000 [00:00<00:04, 214.78it/s]Measuring inference for batch_size=1:  15%|█▌        | 154/1000 [00:00<00:03, 214.79it/s]Measuring inference for batch_size=1:  18%|█▊        | 176/1000 [00:00<00:03, 214.84it/s]Measuring inference for batch_size=1:  20%|█▉        | 198/1000 [00:00<00:03, 214.76it/s]Measuring inference for batch_size=1:  22%|██▏       | 220/1000 [00:01<00:03, 214.90it/s]Measuring inference for batch_size=1:  24%|██▍       | 242/1000 [00:01<00:03, 214.90it/s]Measuring inference for batch_size=1:  26%|██▋       | 264/1000 [00:01<00:03, 214.91it/s]Measuring inference for batch_size=1:  29%|██▊       | 286/1000 [00:01<00:03, 214.85it/s]Measuring inference for batch_size=1:  31%|███       | 308/1000 [00:01<00:03, 214.95it/s]Measuring inference for batch_size=1:  33%|███▎      | 330/1000 [00:01<00:03, 214.89it/s]Measuring inference for batch_size=1:  35%|███▌      | 352/1000 [00:01<00:03, 214.87it/s]Measuring inference for batch_size=1:  37%|███▋      | 374/1000 [00:01<00:02, 214.89it/s]Measuring inference for batch_size=1:  40%|███▉      | 396/1000 [00:01<00:02, 214.93it/s]Measuring inference for batch_size=1:  42%|████▏     | 418/1000 [00:01<00:02, 214.86it/s]Measuring inference for batch_size=1:  44%|████▍     | 440/1000 [00:02<00:02, 214.89it/s]Measuring inference for batch_size=1:  46%|████▌     | 462/1000 [00:02<00:02, 214.75it/s]Measuring inference for batch_size=1:  48%|████▊     | 484/1000 [00:02<00:02, 214.83it/s]Measuring inference for batch_size=1:  51%|█████     | 506/1000 [00:02<00:02, 214.85it/s]Measuring inference for batch_size=1:  53%|█████▎    | 528/1000 [00:02<00:02, 214.94it/s]Measuring inference for batch_size=1:  55%|█████▌    | 550/1000 [00:02<00:02, 214.93it/s]Measuring inference for batch_size=1:  57%|█████▋    | 572/1000 [00:02<00:01, 214.79it/s]Measuring inference for batch_size=1:  59%|█████▉    | 594/1000 [00:02<00:01, 214.75it/s]Measuring inference for batch_size=1:  62%|██████▏   | 616/1000 [00:02<00:01, 214.82it/s]Measuring inference for batch_size=1:  64%|██████▍   | 638/1000 [00:02<00:01, 214.81it/s]Measuring inference for batch_size=1:  66%|██████▌   | 660/1000 [00:03<00:01, 214.85it/s]Measuring inference for batch_size=1:  68%|██████▊   | 682/1000 [00:03<00:01, 214.84it/s]Measuring inference for batch_size=1:  70%|███████   | 704/1000 [00:03<00:01, 214.88it/s]Measuring inference for batch_size=1:  73%|███████▎  | 726/1000 [00:03<00:01, 214.80it/s]Measuring inference for batch_size=1:  75%|███████▍  | 748/1000 [00:03<00:01, 214.79it/s]Measuring inference for batch_size=1:  77%|███████▋  | 770/1000 [00:03<00:01, 214.77it/s]Measuring inference for batch_size=1:  79%|███████▉  | 792/1000 [00:03<00:00, 214.80it/s]Measuring inference for batch_size=1:  81%|████████▏ | 814/1000 [00:03<00:00, 214.79it/s]Measuring inference for batch_size=1:  84%|████████▎ | 836/1000 [00:03<00:00, 214.80it/s]Measuring inference for batch_size=1:  86%|████████▌ | 858/1000 [00:03<00:00, 214.78it/s]Measuring inference for batch_size=1:  88%|████████▊ | 880/1000 [00:04<00:00, 214.69it/s]Measuring inference for batch_size=1:  90%|█████████ | 902/1000 [00:04<00:00, 214.70it/s]Measuring inference for batch_size=1:  92%|█████████▏| 924/1000 [00:04<00:00, 214.66it/s]Measuring inference for batch_size=1:  95%|█████████▍| 946/1000 [00:04<00:00, 214.70it/s]Measuring inference for batch_size=1:  97%|█████████▋| 968/1000 [00:04<00:00, 214.65it/s]Measuring inference for batch_size=1:  99%|█████████▉| 990/1000 [00:04<00:00, 213.01it/s]Measuring inference for batch_size=1: 100%|██████████| 1000/1000 [00:04<00:00, 214.54it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Warming up with batch_size=256:   0%|          | 0/100 [00:00<?, ?it/s]Warming up with batch_size=256:  21%|██        | 21/100 [00:00<00:00, 207.81it/s]Warming up with batch_size=256:  42%|████▏     | 42/100 [00:00<00:00, 208.06it/s]Warming up with batch_size=256:  63%|██████▎   | 63/100 [00:00<00:00, 208.25it/s]Warming up with batch_size=256:  84%|████████▍ | 84/100 [00:00<00:00, 208.23it/s]Warming up with batch_size=256: 100%|██████████| 100/100 [00:00<00:00, 208.19it/s]
STAGE:2024-02-25 22:58:34 6212:6212 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-02-25 22:58:34 6212:6212 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-02-25 22:58:34 6212:6212 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Measuring inference for batch_size=256:   0%|          | 0/1000 [00:00<?, ?it/s]Measuring inference for batch_size=256:   2%|▏         | 21/1000 [00:00<00:04, 204.08it/s]Measuring inference for batch_size=256:   4%|▍         | 42/1000 [00:00<00:04, 204.45it/s]Measuring inference for batch_size=256:   6%|▋         | 63/1000 [00:00<00:04, 204.82it/s]Measuring inference for batch_size=256:   8%|▊         | 84/1000 [00:00<00:04, 205.07it/s]Measuring inference for batch_size=256:  10%|█         | 105/1000 [00:00<00:04, 205.16it/s]Measuring inference for batch_size=256:  13%|█▎        | 126/1000 [00:00<00:04, 205.20it/s]Measuring inference for batch_size=256:  15%|█▍        | 147/1000 [00:00<00:04, 205.10it/s]Measuring inference for batch_size=256:  17%|█▋        | 168/1000 [00:00<00:04, 205.20it/s]Measuring inference for batch_size=256:  19%|█▉        | 189/1000 [00:00<00:03, 205.27it/s]Measuring inference for batch_size=256:  21%|██        | 210/1000 [00:01<00:03, 205.32it/s]Measuring inference for batch_size=256:  23%|██▎       | 231/1000 [00:01<00:03, 205.29it/s]Measuring inference for batch_size=256:  25%|██▌       | 252/1000 [00:01<00:03, 205.25it/s]Measuring inference for batch_size=256:  27%|██▋       | 273/1000 [00:01<00:03, 205.26it/s]Measuring inference for batch_size=256:  29%|██▉       | 294/1000 [00:01<00:03, 205.30it/s]Measuring inference for batch_size=256:  32%|███▏      | 315/1000 [00:01<00:03, 205.39it/s]Measuring inference for batch_size=256:  34%|███▎      | 336/1000 [00:01<00:03, 205.32it/s]Measuring inference for batch_size=256:  36%|███▌      | 357/1000 [00:01<00:03, 205.29it/s]Measuring inference for batch_size=256:  38%|███▊      | 378/1000 [00:01<00:03, 205.32it/s]Measuring inference for batch_size=256:  40%|███▉      | 399/1000 [00:01<00:02, 205.33it/s]Measuring inference for batch_size=256:  42%|████▏     | 420/1000 [00:02<00:02, 205.36it/s]Measuring inference for batch_size=256:  44%|████▍     | 441/1000 [00:02<00:02, 205.33it/s]Measuring inference for batch_size=256:  46%|████▌     | 462/1000 [00:02<00:02, 205.34it/s]Measuring inference for batch_size=256:  48%|████▊     | 483/1000 [00:02<00:02, 205.44it/s]Measuring inference for batch_size=256:  50%|█████     | 504/1000 [00:02<00:02, 205.49it/s]Measuring inference for batch_size=256:  52%|█████▎    | 525/1000 [00:02<00:02, 205.57it/s]Measuring inference for batch_size=256:  55%|█████▍    | 546/1000 [00:02<00:02, 205.54it/s]Measuring inference for batch_size=256:  57%|█████▋    | 567/1000 [00:02<00:02, 205.53it/s]Measuring inference for batch_size=256:  59%|█████▉    | 588/1000 [00:02<00:02, 205.58it/s]Measuring inference for batch_size=256:  61%|██████    | 609/1000 [00:02<00:01, 205.54it/s]Measuring inference for batch_size=256:  63%|██████▎   | 630/1000 [00:03<00:01, 205.60it/s]Measuring inference for batch_size=256:  65%|██████▌   | 651/1000 [00:03<00:01, 205.60it/s]Measuring inference for batch_size=256:  67%|██████▋   | 672/1000 [00:03<00:01, 205.56it/s]Measuring inference for batch_size=256:  69%|██████▉   | 693/1000 [00:03<00:01, 205.56it/s]Measuring inference for batch_size=256:  71%|███████▏  | 714/1000 [00:03<00:01, 205.58it/s]Measuring inference for batch_size=256:  74%|███████▎  | 735/1000 [00:03<00:01, 205.52it/s]Measuring inference for batch_size=256:  76%|███████▌  | 756/1000 [00:03<00:01, 205.44it/s]Measuring inference for batch_size=256:  78%|███████▊  | 777/1000 [00:03<00:01, 205.50it/s]Measuring inference for batch_size=256:  80%|███████▉  | 798/1000 [00:03<00:00, 205.49it/s]Measuring inference for batch_size=256:  82%|████████▏ | 819/1000 [00:03<00:00, 205.53it/s]Measuring inference for batch_size=256:  84%|████████▍ | 840/1000 [00:04<00:00, 205.49it/s]Measuring inference for batch_size=256:  86%|████████▌ | 861/1000 [00:04<00:00, 205.58it/s]Measuring inference for batch_size=256:  88%|████████▊ | 882/1000 [00:04<00:00, 205.52it/s]Measuring inference for batch_size=256:  90%|█████████ | 903/1000 [00:04<00:00, 205.50it/s]Measuring inference for batch_size=256:  92%|█████████▏| 924/1000 [00:04<00:00, 205.49it/s]Measuring inference for batch_size=256:  94%|█████████▍| 945/1000 [00:04<00:00, 205.50it/s]Measuring inference for batch_size=256:  97%|█████████▋| 966/1000 [00:04<00:00, 205.47it/s]Measuring inference for batch_size=256:  99%|█████████▊| 987/1000 [00:04<00:00, 205.40it/s]Measuring inference for batch_size=256: 100%|██████████| 1000/1000 [00:04<00:00, 205.39it/s]
Unable to measure energy consumption. Device must be a NVIDIA Jetson.
device: cuda
flops: 4121925096
machine_info:
  cpu:
    architecture: x86_64
    cores:
      physical: 12
      total: 24
    frequency: 3.80 GHz
    model: AMD Ryzen 9 3900X 12-Core Processor
  gpus:
  - memory: 12288.0 MB
    name: NVIDIA GeForce RTX 3060
  memory:
    available: 29.98 GB
    total: 31.28 GB
    used: 922.38 MB
  system:
    node: fp
    release: 6.5.0-9-generic
    system: Linux
memory:
  batch_size_1:
    max_inference: 242.08 MB
    max_inference_bytes: 253842944
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
  batch_size_256:
    max_inference: 244.95 MB
    max_inference_bytes: 256853504
    post_inference: 105.85 MB
    post_inference_bytes: 110994944
    pre_inference: 105.85 MB
    pre_inference_bytes: 110994944
params: 25557032
timing:
  batch_size_1:
    cpu_to_gpu:
      human_readable:
        batch_latency: 91.818 us +/- 2.653 us [90.361 us, 136.614 us]
        batches_per_second: 10.90 K +/- 270.74 [7.32 K, 11.07 K]
      metrics:
        batches_per_second_max: 11066.765171503957
        batches_per_second_mean: 10898.877681777873
        batches_per_second_min: 7319.902268760908
        batches_per_second_std: 270.7374573937699
        seconds_per_batch_max: 0.0001366138458251953
        seconds_per_batch_mean: 9.181785583496094e-05
        seconds_per_batch_min: 9.036064147949219e-05
        seconds_per_batch_std: 2.6532191307429406e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 22.797 us +/- 0.536 us [22.173 us, 28.849 us]
        batches_per_second: 43.89 K +/- 912.35 [34.66 K, 45.10 K]
      metrics:
        batches_per_second_max: 45100.04301075269
        batches_per_second_mean: 43887.52039809688
        batches_per_second_min: 34663.669421487604
        batches_per_second_std: 912.3475756936318
        seconds_per_batch_max: 2.8848648071289062e-05
        seconds_per_batch_mean: 2.2796630859375e-05
        seconds_per_batch_min: 2.2172927856445312e-05
        seconds_per_batch_std: 5.364441341773349e-07
    on_device_inference:
      human_readable:
        batch_latency: -4537082.875 us +/- 40.835 ms [-4857535.839 us, -4503935.814
          us]
        batches_per_second: -0.22 +/- 0.00 [-0.22, -0.21]
      metrics:
        batches_per_second_max: -0.20586569674990388
        batches_per_second_mean: -0.22042293567788082
        batches_per_second_min: -0.22202803088644488
        batches_per_second_std: 0.001889052442693469
        seconds_per_batch_max: -4.503935813903809
        seconds_per_batch_mean: -4.537082874774933
        seconds_per_batch_min: -4.8575358390808105
        seconds_per_batch_std: 0.04083472552912698
    total:
      human_readable:
        batch_latency: 4.658 ms +/- 42.076 us [4.624 ms, 4.981 ms]
        batches_per_second: 214.71 +/- 1.85 [200.75, 216.26]
      metrics:
        batches_per_second_max: 216.25697344676462
        batches_per_second_mean: 214.71231570022476
        batches_per_second_min: 200.75163930503038
        batches_per_second_std: 1.8469323474873678
        seconds_per_batch_max: 0.004981279373168945
        seconds_per_batch_mean: 0.0046577565670013425
        seconds_per_batch_min: 0.004624128341674805
        seconds_per_batch_std: 4.207569230224144e-05
  batch_size_256:
    cpu_to_gpu:
      human_readable:
        batch_latency: 142.652 us +/- 3.837 us [140.905 us, 239.849 us]
        batches_per_second: 7.01 K +/- 140.14 [4.17 K, 7.10 K]
      metrics:
        batches_per_second_max: 7096.961082910321
        batches_per_second_mean: 7013.721299755152
        batches_per_second_min: 4169.288270377733
        batches_per_second_std: 140.14099218464855
        seconds_per_batch_max: 0.00023984909057617188
        seconds_per_batch_mean: 0.00014265227317810059
        seconds_per_batch_min: 0.00014090538024902344
        seconds_per_batch_std: 3.8373695910887684e-06
    gpu_to_cpu:
      human_readable:
        batch_latency: 23.022 us +/- 0.591 us [22.173 us, 28.610 us]
        batches_per_second: 43.46 K +/- 991.03 [34.95 K, 45.10 K]
      metrics:
        batches_per_second_max: 45100.04301075269
        batches_per_second_mean: 43461.25709335651
        batches_per_second_min: 34952.53333333333
        batches_per_second_std: 991.0269398491376
        seconds_per_batch_max: 2.86102294921875e-05
        seconds_per_batch_mean: 2.302241325378418e-05
        seconds_per_batch_min: 2.2172927856445312e-05
        seconds_per_batch_std: 5.90779446114492e-07
    on_device_inference:
      human_readable:
        batch_latency: -4694156.126 us +/- 17.578 ms [-5036863.804 us, -4667424.202
          us]
        batches_per_second: -0.21 +/- 0.00 [-0.21, -0.20]
      metrics:
        batches_per_second_max: -0.19853623979924773
        batches_per_second_mean: -0.21303373713665288
        batches_per_second_min: -0.2142509351472544
        batches_per_second_std: 0.0007753616690628053
        seconds_per_batch_max: -4.667424201965332
        seconds_per_batch_mean: -4.694156126499176
        seconds_per_batch_min: -5.036863803863525
        seconds_per_batch_std: 0.017577985299381774
    total:
      human_readable:
        batch_latency: 4.865 ms +/- 19.987 us [4.838 ms, 5.311 ms]
        batches_per_second: 205.54 +/- 0.81 [188.29, 206.71]
      metrics:
        batches_per_second_max: 206.7076043566113
        batches_per_second_mean: 205.5404301131726
        batches_per_second_min: 188.28802298437782
        batches_per_second_std: 0.8080622653320361
        seconds_per_batch_max: 0.005311012268066406
        seconds_per_batch_mean: 0.0048653013706207275
        seconds_per_batch_min: 0.004837751388549805
        seconds_per_batch_std: 1.9986581550610282e-05


